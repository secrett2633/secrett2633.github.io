---
title: "[논문리뷰] Olmo 3"
excerpt: "arXiv에 게시된 'Olmo 3' 논문에 대한 자세한 리뷰입니다."

categories:
  - Review
tags:
  - Review
  - Large Language Models
  - Open-Source AI
  - Model Flow
  - Long-Context Reasoning
  - Instruction Following
  - Function Calling
  - Thinking Models
  - Data Curation
  - Reinforcement Learning

permalink: /ai/review/2025-12-17-Olmo-3/

toc: true
toc_sticky: true

date: 2025-12-17 00:00:00+0900+0900
last_modified_at: 2025-12-17 00:00:00+0900+0900
published: true
---
> **링크:** [논문 PDF로 바로 열기](https://arxiv.org/abs/2512.13961)

**저자:** Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, Jacob Morrison, Jake Poznanski, Kyle Lo, Luca Soldaini, Matt Jordan, Mayee Chen, Michael Noukhovitch, Nathan Lambert, Pete Walsh, Pradeep Dasigi, Robert Berry, Saumya Malik, Saurabh Shah, Scott Geng, Shane Arora, Shashank Gupta, Taira Anderson, Teng Xiao, Tyler Murray, Tyler Romero, Victoria Graf, Akari Asai, Akshita Bhagia, Alexander Wettig, Alisa Liu, Aman Rangapur, Chloe Anastasiades, Costa Huang, Dustin Schwenk, Harsh Trivedi, Ian Magnusson, Jaron Lochner, Jiacheng Liu, Lester James V. Miranda, Maarten Sap, Malia Morgan, Michael Schmitz, Michal Guerquin, Michael Wilson, Regan Huff, Ronan Le Bras, Rui Xin, Rulin Shao, Sam Skjonsberg, Shannon Zejiang Shen, Shuyue Stella Li, Tucker Wilde, Valentina Pyatkin, Will Merrill, Yapei Chang, Yuling Gu, Zhiyuan Zeng, Ashish Sabharwal, Luke Zettlemoyer, Pang Wei Koh, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi



## 핵심 연구 목표
Olmo 3는 7B 및 32B 파라미터 스케일에서 **최첨단, 완전 오픈(fully-open) 언어 및 사고 모델** 제품군을 소개하는 것을 목표로 합니다. 이 연구의 핵심은 모델의 전체 라이프사이클(모든 단계, 체크포인트, 데이터 포인트, 종속성 포함)을 **완전히 공개** 하여 무한한 커스터마이징과 연구 기회를 제공하는 것입니다.

## 핵심 방법론
모델은 **Olmo 3 Base** ( **Dolma 3 Mix** 로 사전 훈련, **Dolma 3 Dolmino Mix** 로 중간 훈련, **Dolma 3 Longmino Mix** 로 장문 컨텍스트 확장)를 기반으로 구축됩니다. 이후 **Olmo 3 Think** (단계별 추론) 및 **Olmo 3 Instruct** (간결한 지시 따르기 및 함수 호출) 모델로 세분화됩니다. 훈련은 **SFT (Supervised Finetuning), DPO (Direct Preference Optimization), RLVR (Reinforcement Learning with Verifiable Rewards)** 의 다단계 접근 방식을 사용하며, **OlmoRL** 프레임워크와 **Delta Learning** 같은 새로운 알고리즘 개선사항이 적용되었습니다.

## 주요 결과
**Olmo 3.1 Think 32B** 는 현재까지 출시된 가장 강력한 완전 오픈 사고 모델로, **6배 적은 토큰** 으로 Qwen 3 32B와 같은 최고의 오픈-웨이트 모델과의 격차를 좁혔습니다. 특히 **MATH 96.2%, HumanEvalPlus 91.5%, IFEval 93.8%** 등의 정량적 지표에서 뛰어난 성능을 보였습니다. 또한, **Olmo 3 Instruct** 모델은 동급의 다른 오픈-웨이트 모델들을 능가하며, **65K 토큰** 까지 장문 컨텍스트를 지원합니다.

## AI 실무자를 위한 시사점
**Olmo 3** 의 완전 오픈 모델 플로우는 AI 연구자와 엔지니어가 모델의 내부 작동을 깊이 이해하고, **데이터 큐레이션, 다단계 훈련(SFT, DPO, RLVR), RL 최적화** 등 모든 개발 단계에서 직접 개입하여 맞춤형 모델을 구축할 수 있는 전례 없는 기회를 제공합니다. 이는 **장문 컨텍스트 추론, 함수 호출, 코드 생성, 지시 따르기** 등 다양한 최첨단 AI 애플리케이션 개발에 중요한 기반을 제공합니다.

> ⚠️ **알림:** 이 리뷰는 AI로 작성되었습니다.