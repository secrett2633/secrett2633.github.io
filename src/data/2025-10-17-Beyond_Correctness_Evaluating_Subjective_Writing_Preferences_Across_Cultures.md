---
title: "[논문리뷰] Beyond Correctness: Evaluating Subjective Writing Preferences Across
  Cultures"
excerpt: "이 [arXiv]에 게시한 'Beyond Correctness: Evaluating Subjective Writing Preferences Across
  Cultures' 논문에 대한 자세한 리뷰입니다."

categories:
  - Review
tags:
  - Review
  - Subjective Preference Learning
  - Writing Evaluation
  - Reward Models
  - RLHF
  - Cross-Cultural AI
  - Generative Models
  - Language Model Judges
  - Genre Instability

permalink: /ai/review/2025-10-17-Beyond_Correctness_Evaluating_Subjective_Writing_Preferences_Across_Cultures/

toc: true
toc_sticky: true

date: 2025-10-17 13:09:57+0900
last_modified_at: 2025-10-17 13:09:57+0900
published: true
---
> **링크:** [논문 PDF로 바로 열기](https://arxiv.org/abs/2510.14616)

**저자:** Shuangshuang Ying, Yunwen Li, Xingwei Qu, et al.



## 핵심 연구 목표
본 논문은 기존 RLHF 보상 모델이 객관적인 품질 신호(문법 오류, 사실 정확성 등)를 제거했을 때 주관적인 쓰기 선호도 평가에서 성능이 크게 저하되는 문제를 해결하고자 합니다. 특히, 객관적 오류 탐지를 넘어 창의성, 문체적 감각, 정서적 공명과 같은 **주관적인 품질 선호도**를 포착하는 모델의 능력을 평가하는 새로운 벤치마크를 제시하고 그 한계를 분석하는 것을 목표로 합니다.

## 핵심 방법론
저자들은 **WritingPreferenceBench**라는 1,800쌍의 인간 주석 선호도 데이터셋(영어 1,200쌍, 중국어 600쌍)을 구축했습니다. 이 데이터셋은 8가지 창의적 글쓰기 장르에 걸쳐 **객관적 정확성, 사실적 정확성, 길이**와 같은 교란 요소를 중립화하여 순수하게 주관적인 품질 차이를 분리합니다. 7개의 **보상 모델** (Sequence Classifiers 및 Generative RMs)과 14개의 **제로샷 언어 모델**을 판정자로 사용하여 모델 성능을 평가하였으며, 특히 **명시적인 추론 사슬**을 생성하는 모델 아키텍처에 중점을 두었습니다.

## 주요 결과
기존 **Sequence-based Reward Models**는 주관적 선호도 작업에서 **52.7%**의 평균 정확도를 보여 무작위 수준에 불과했습니다. 반면, **Generative Reward Models**는 **81.8%** (영어) 및 **73.3%** (중국어)의 평균 정확도를 달성하여 **29% 포인트**의 성능 격차를 보였습니다. 모든 모델은 장르 간에 평균 **10.1%**의 높은 표준편차를 보이는 **심각한 장르 불안정성**을 나타냈으며, **27B 파라미터 모델**조차 **8B 모델** 대비 일관된 개선을 보이지 못했습니다.

## AI 실무자를 위한 시사점
현재 RLHF 방식은 주로 **객관적인 오류 탐지**에 초점을 맞추고 있으며, 주관적인 창의적 품질 평가에는 부적합함을 시사합니다. 따라서 AI 개발자는 주관적 선호도 모델링을 위해 **명시적인 추론 체인**을 생성하는 **Generative Reward Models**와 같은 **중간 추론 표현**이 필요하다는 점을 인지해야 합니다. 또한, **제로샷 LLM 판정자** 방식은 주관적 작업에 대해 신뢰할 수 없는 것으로 드러났으므로, **장르 불변의 선호도 학습**을 위한 새로운 훈련 목표와 하이브리드 아키텍처 연구가 요구됩니다.

> ⚠️ **알림:** 이 리뷰는 AI로 작성되었습니다.