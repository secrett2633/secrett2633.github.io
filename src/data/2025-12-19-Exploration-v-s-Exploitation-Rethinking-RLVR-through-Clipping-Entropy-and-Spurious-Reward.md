---
title: "[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward"
excerpt: "arXiv에 게시된 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward' 논문에 대한 자세한 리뷰입니다."

categories:
  - Review
tags:
  - Review
  - Reinforcement Learning
  - Large Language Models
  - Exploration-Exploitation
  - Clipping
  - Policy Entropy
  - Spurious Rewards
  - Mathematical Reasoning
  - RLVR

permalink: /ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward/

toc: true
toc_sticky: true

date: 2025-12-19 00:00:00+0900+0900
last_modified_at: 2025-12-19 00:00:00+0900+0900
published: true
---
> **링크:** [논문 PDF로 바로 열기](https://arxiv.org/abs/2512.16912)

**저자:** Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin



## 핵심 연구 목표
RLVR(Reinforcement Learning with Verifiable Rewards) 환경에서 **탐색-활용 트레이드오프** 를 재해석하고, 특히 **클리핑(clipping), 정책 엔트로피, 허위 보상(spurious reward)** 이 LLM의 추론 성능에 미치는 영향을 규명하는 것이 목표입니다. 본 연구는 허위 보상이 성능 향상으로 이어지는 근본적인 메커니즘과 정책 엔트로피가 성능에 어떻게 연결되는지에 대한 근본적인 질문에 답하고자 합니다.

## 핵심 방법론
본 연구는 **GRPO(Group Relative Policy Optimization)** 프레임워크를 사용하여 LLM을 훈련하며, 특히 **랜덤 보상(Bernoulli(1/2))** 설정을 도입하여 허위 보상 효과를 모의 실험합니다. **클리핑 바이어스(clipping bias)** 의 이론적 상한을 도출하고, **원-스텝 정책-엔트로피 시프트(one-step policy-entropy shift)** 공식화를 통해 클리핑과 정책 엔트로피 간의 결정론적 관계를 확립했습니다. 실험은 **Qwen-Math, Llama, QwQ** 등 다양한 모델 군과 크기(7B, 8B, 32B)에 걸쳐 수행되었으며, **보상 불일치 모델(reward-misalignment model)** 을 제안하여 허위 보상 효과를 설명합니다.

## 주요 결과
허위 보상 하에서 **클리핑 바이어스** 는 정책 엔트로피를 감소시켜 더 확신 있고 결정론적인 출력을 유도하지만, 단독으로는 성능 향상에 불충분합니다. 실험 결과, **R1-Distill-Llama-8B** 모델은 RLVR 훈련 후 **MATH500 Pass@1 성능이 약 0.40에서 0.74로 증가** 했으며, **Qwen2.5-Math-7B** 는 **약 0.55에서 0.70으로 증가** 하여 허위 보상이 **오염된(contaminated) 모델에만 국한되지 않고 일반적인 성능 향상** 으로 이어질 수 있음을 보여주었습니다. 또한, 정책 엔트로피와 성능 사이에 **직접적인 인과 관계는 없으며** , 엔트로피 감소가 성능 저하를 동반할 수도 있음을 확인했습니다.

## AI 실무자를 위한 시사점
**강력한 LLM 모델** 에 허위 보상을 적용하는 것은 **탐색-활용 균형을 미묘하게 조절** 하여 **실제 보상 신호 없이도 성능 향상** 을 이끌어낼 수 있는 효과적인 전략임을 시사합니다. **클리핑** 은 성능 향상의 직접적인 원동력이라기보다는 **gradient explosion을 방지하고 학습 안정성을 높이는 정규화(regularization) 역할** 을 수행하는 것으로 이해해야 합니다. LLM 훈련 시 단순히 **정책 엔트로피를 최소화하는 접근 방식** 은 주의해서 적용해야 하며, 모델의 강도, 데이터셋 난이도, 초기 정책 분포를 종합적으로 고려한 **다각적인 탐색-활용 전략** 이 필요합니다.

> ⚠️ **알림:** 이 리뷰는 AI로 작성되었습니다.