---
title: "[논문리뷰] Emu3.5: Native Multimodal Models are World Learners"
excerpt: "이 [arXiv]에 게시한 'Emu3.5: Native Multimodal Models are World Learners' 논문에 대한 자세한 리뷰입니다."

categories:
  - Review
tags:
  - Review
  - Multimodal Model
  - World Model
  - Vision-Language
  - Next-Token Prediction
  - Reinforcement Learning
  - Discrete Diffusion Adaptation
  - Image Generation
  - Any-to-Image

permalink: /ai/review/2025-10-31-Emu3.5_Native_Multimodal_Models_are_World_Learners/

toc: true
toc_sticky: true

date: 2025-10-31 18:37:31+0900
last_modified_at: 2025-10-31 18:37:31+0900
published: true
---
> **링크:** [논문 PDF로 바로 열기](https://arxiv.org/abs/2510.26583)

**저자:** Emu3.5 Team, BAAI



## 핵심 연구 목표
본 논문은 비전과 언어에 걸쳐 다음 상태를 예측하는 **대규모 멀티모달 월드 모델인 Emu3.5**를 소개합니다. **자연스러운 멀티모달 능력**을 통해 긴 시퀀스 비전-언어 생성, X2I(Any-to-Image) 생성, 복잡한 텍스트 기반 이미지 생성 및 **일반화 가능한 월드 모델링 능력**을 향상시키는 것을 목표로 합니다. 또한, **Discrete Diffusion Adaptation (DiDA)**를 통해 추론 효율성을 **약 20배 향상**시키고자 합니다.

## 핵심 방법론
Emu3.5는 **10조 개 이상의 토큰**으로 구성된 비전-언어 인터리브드 데이터셋을 활용하여 **통합된 next-token prediction objective**로 엔드투엔드 사전 훈련되었습니다. 이후, 멀티모달 추론 및 생성을 강화하기 위해 **대규모 강화 학습(RL)**으로 후속 훈련되었으며, 시각적 토큰에 대한 추론 속도를 대폭 높이기 위해 **Discrete Diffusion Adaptation (DiDA)** 기법을 도입하여 토큰별 디코딩을 양방향 병렬 예측으로 전환했습니다. 모델은 **64개 트랜스포머 레이어**를 포함한 **341억 개의 파라미터**로 구성되어 있습니다.

## 주요 결과
Emu3.5는 이미지 생성 및 편집 벤치마크(ImgEdit, GEdit-Bench)에서 **최고 성능**을 달성했으며, 특히 ImgEdit에서 **4.41점**, GEdit-Bench에서 **7.59점**을 기록했습니다. 또한, interleaved generation task에서 **Gemini 2.5 Flash Image(Nano Banana)** 대비 **World Exploration에서 65.5%, Embodied Manipulation에서 67.1%의 승률**을 보여주며 우수한 결과를 시연했습니다. **DiDA** 적용 시 이미지 추론 속도를 **최대 20배** 가속화하면서도 성능 저하가 없음을 입증했습니다.

## AI 실무자를 위한 시사점
**Emu3.5**는 **멀티모달 월드 모델**의 잠재력을 보여주며, 이는 향후 **대화형 AI 시스템 및 로봇 공학** 분야에서 실제 환경을 인지, 추론 및 행동하는 기반을 제공할 수 있습니다. **DiDA**와 같은 효율적인 추론 기법은 **대규모 멀티모달 모델**의 실용적인 배포를 가능하게 하여, 복잡한 비전-언어 태스크에서 실시간 상호작용의 문을 엽니다. 또한, 모델과 개발 과정의 **오픈 소스 공개**는 관련 연구의 활성화를 촉진할 것으로 기대됩니다.

> ⚠️ **알림:** 이 리뷰는 AI로 작성되었습니다.