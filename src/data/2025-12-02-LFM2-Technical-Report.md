---
title: "[논문리뷰] LFM2 Technical Report"
excerpt: "이 [arXiv]에 게시한 'LFM2 Technical Report' 논문에 대한 자세한 리뷰입니다."

categories:
  - Review
tags:
  - Review
  - Edge AI
  - Foundation Models
  - Hybrid Architecture
  - Knowledge Distillation
  - Multimodal AI
  - On-device Deployment
  - Efficient Inference
  - LLM Optimization

permalink: /ai/review/2025-12-02-LFM2-Technical-Report/

toc: true
toc_sticky: true

date: 2025-12-02 00:00:00+0900+0900
last_modified_at: 2025-12-02 00:00:00+0900+0900
published: true
---
> **링크:** [논문 PDF로 바로 열기](https://arxiv.org/abs/2511.23404)

**저자:** Liquid AI Team



## 핵심 연구 목표
본 논문은 **LFM2** 라는 Liquid Foundation Models 제품군을 소개하며, **효율적인 온-디바이스 배포** 와 **강력한 태스크 수행 능력** 을 동시에 달성하는 것을 목표로 합니다. 특히, CPU 및 이종 NPU 환경에서 엄격한 지연 시간, 메모리, 에너지 예산 제약 내에서 최고 품질을 제공하는 소형 모델의 필요성을 해결하고자 합니다.

## 핵심 방법론
**엣지-우선 설계 원칙** 에 따라 **하드웨어-인-더-루프 아키텍처 탐색** 을 통해 **게이티드 짧은 컨볼루션** 과 **그룹형 쿼리 어텐션(GQA)** 블록을 결합한 **하이브리드 백본** 을 구축했습니다. 학습 과정에는 **Top-K 지식 증류** 목적 함수, **난이도 정렬 데이터 기반 커리큘럼 학습** , 그리고 **세 단계의 후처리 레시피** ( **감독형 미세 조정, 길이 정규화 선호도 최적화, 모델 병합** )가 포함됩니다. 또한, 비전-언어( **LFM2-VL** ), 오디오( **LFM2-Audio** ), 정보 검색( **LFM2-ColBERT** ) 등 멀티모달 및 검색 변형 모델도 개발하여 기능을 확장했습니다.

## 주요 결과
**LFM2** 모델들은 유사 크기 모델 대비 CPU에서 **최대 2배 빠른 프리필 및 디코딩 속도** 를 달성하면서도, 벤치마크 정확도를 유지하거나 향상시켰습니다. 특히 **LFM2-2.6B** 는 **IFEval에서 79.56%** , **GSM8K에서 82.41%** 의 높은 점수를 기록했습니다. **LFM2-8B-A1B(MoE)** 모델은 **1.5B 활성 파라미터** 로 **3–4B 클래스 품질** 을 달성하며, **LFM2-VL-3B** 는 **SEEDBench 76.55** , **RealWorldQA 71.37** 를 기록하여 베이스라인을 능가했습니다. **LFM2-ColBERT-350M** 은 다국어 검색에서 평균 **NDCG@10 0.661** 이라는 강력한 성능을 보였습니다.

## AI 실무자를 위한 시사점
**LFM2** 의 **엣지-우선 설계** 와 **최적화된 성능** 는 휴대폰, 노트북, 임베디드 시스템과 같은 **저지연 온-디바이스 배포** 에 매우 적합합니다. **하이브리드 아키텍처** 와 **효율적인 학습 기법** 은 자원 제약이 있는 환경에서 강력한 모델을 개발하기 위한 청사진을 제공합니다. 또한, **멀티모달 (비전, 오디오)** 및 **검색 증강 생성(RAG)** 변형 모델의 제공은 배포 효율성을 유지하면서 실용적인 응용 분야를 확장할 수 있는 기회를 제공하며, **공개 가중치** 와 **배포 패키지** 는 모델의 즉각적인 활용을 가능하게 합니다.

> ⚠️ **알림:** 이 리뷰는 AI로 작성되었습니다.