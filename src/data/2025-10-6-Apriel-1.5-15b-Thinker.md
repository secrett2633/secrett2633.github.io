---
title: "[논문리뷰] Apriel-1.5-15b-Thinker"
excerpt: "이 [arXiv]에 게시한 'Apriel-1.5-15b-Thinker' 논문에 대한 자세한 리뷰입니다."

categories:
  - Review
tags:
  - Review
  - Multimodal Reasoning Model
  - Open-Weights Model
  - Continual Pretraining (CPT)
  - Supervised Fine-Tuning (SFT)
  - Training Design
  - Efficiency
  - Frontier Performance

permalink: /ai/review/2025-10-6-Apriel-1.5-15b-Thinker/

toc: true
toc_sticky: true

date: 2025-10-06 13:29:11+0900
last_modified_at: 2025-10-06 13:29:11+0900
published: true
---
> **링크:** [논문 PDF로 바로 열기](https://arxiv.org/abs/2510.01141)

**저자:** Shruthan Radhakrishna, Aman Tiwari, Aanjaneya Shukla, et al.



## 핵심 연구 목표
본 연구는 대규모 언어 모델(LLM)의 성능과 접근성 사이의 근본적인 한계를 극복하고, **150억 개 파라미터**의 비교적 작은 오픈-웨이트 모델인 **Apriel-1.5-15B-Thinker**가 순수한 규모 대신 **훈련 디자인**을 통해 최첨단 멀티모달 추론 성능을 달성하는 것을 목표로 합니다. 이는 제한된 인프라를 가진 조직에서도 비용 효율적으로 모델을 배포할 수 있도록 하는 것을 주요 목적으로 합니다.

## 핵심 방법론
이 모델은 **Pixtral-12B**를 기반으로 **3단계 진보적 훈련 방법론**을 사용합니다. 첫째, **깊이 업스케일링(depth upscaling)**을 통해 처음부터 사전 훈련 없이 추론 용량을 확장하고, 둘째, **단계별 연속 사전 훈련(staged continual pre-training, CPT)**을 통해 기초 텍스트 및 시각적 이해력을 개발하고 공간 구조, 구성적 이해, 미세-지각을 다루는 **타겟 합성 데이터 생성**으로 시각적 추론을 강화합니다. 셋째, 수학, 코딩, 과학 및 도구 사용에 걸친 명시적 추론 흔적을 포함하는 **고품질 텍스트 전용 SFT(Supervised Fine-Tuning)**를 통해 모델을 미세 조정합니다.

## 주요 결과
**Apriel-1.5-15B-Thinker**는 **Artificial Analysis Intelligence Index**에서 **52점**을 달성하여, 훨씬 적은 연산 자원에도 불구하고 **DeepSeek-R1-0528**과 동등한 수준을 보였습니다. 10개의 이미지 벤치마크에서 **Gemini-2.5-Flash** 및 **Claude Sonnet-3.7**과 평균 **5점** 이내의 경쟁력 있는 성능을 보였으며, 단일 GPU 배포 제약 내에서 작동합니다. CPT 2단계는 MathVerse (Vision Dominant)에서 **+9.65점**, CharXiv (Descriptive)에서 **+5.98점** 향상을 가져왔습니다.

## AI 실무자를 위한 시사점
이 연구는 **신중한 중간 훈련(mid-training) 설계**가 방대한 규모 없이도 상당한 역량 격차를 해소할 수 있음을 입증하며, 최첨단 멀티모달 추론을 제한된 인프라를 가진 조직에서도 접근 가능하게 만듭니다. 이는 AI/ML 엔지니어들에게 **데이터 품질**과 **단계별 훈련 커리큘럼**의 중요성을 강조하며, **비용-대-성능(cost-to-intelligence) 트레이드오프**를 개선하여 효율적인 AI 모델 개발 및 배포에 대한 새로운 방향을 제시합니다.

> ⚠️ **알림:** 이 리뷰는 AI로 작성되었습니다.