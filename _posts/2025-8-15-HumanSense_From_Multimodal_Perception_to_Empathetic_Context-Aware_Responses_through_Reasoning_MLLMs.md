---
title: "[논문리뷰] HumanSense: From Multimodal Perception to Empathetic Context-Aware
  Responses through Reasoning MLLMs"
excerpt: "Yi Yuan이 [arXiv]에 게시한 'HumanSense: From Multimodal Perception to Empathetic Context-Aware
  Responses through Reasoning MLLMs' 논문에 대한 자세한 리뷰입니다."

categories:
  - Review
tags:
  - Review
  - Multimodal LLMs
  - Human-Centered AI
  - Empathy
  - Context-Awareness
  - MLLM Benchmark
  - Reinforcement Learning
  - Reasoning

permalink: /ai/review/2025-8-15-HumanSense_From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs/

toc: true
toc_sticky: true

date: 2025-08-15 13:09:31+0900
last_modified_at: 2025-08-15 13:09:31+0900
published: true
---
> **링크:** [논문 PDF로 바로 열기](https://arxiv.org/abs/2508.10576)

**저자:** Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang



## 핵심 연구 목표
본 논문은 인간 중심 시나리오에서 **MLLM(Multimodal Large Language Models)**의 심층적인 이해 및 공감적, 상황 인지적 응답 능력을 평가하기 위한 **세분화된 평가 프레임워크의 부족 문제**를 해결하고자 합니다. 이를 위해 MLLM이 복잡한 인간 의도를 이해하고 상황에 맞는 피드백을 제공할 수 있도록 **HumanSense 벤치마크**를 제안하고, 이를 통해 모델의 인지 및 상호작용 능력을 향상시키는 것을 목표로 합니다.

## 핵심 방법론
연구진은 인간 중심의 인지 및 상호작용 능력을 평가하기 위해 **HumanSense 벤치마크**를 도입했습니다. 이 벤치마크는 **15개의 점진적으로 도전적인 테스트(L1-L4 계층)**로 구성되며, 총 **3,882개의 실세계 기반 질문**을 포함합니다. 또한, **다단계 옴니모달 강화 학습(multi-stage, omni-modal reinforcement learning)**과 **훈련 없는 프롬프트 강화(training-free prompt enhancement)** 기법을 사용하여 옴니 모델의 추론 능력을 향상시키고자 했습니다.

## 주요 결과
평가 결과, 현재 선도적인 **MLLM**들은 인간 수준의 성능과 비교하여 **평균 29.7%의 상당한 격차**를 보이며, 특히 고급 상호작용 지향 태스크에서 개선의 여지가 큰 것으로 나타났습니다. **시각, 청각, 텍스트 정보**를 보완하는 **옴니 모델**이 고수준 태스크에서 명확한 이점을 보였으며, **청각 입력(audio input)**이 특히 높은 수준의 태스크에서 성능을 크게 향상시켰습니다.

## AI 실무자를 위한 시사점
본 연구는 인간 중심 AI 상호작용에서 **MLLM의 현재 한계점**을 명확히 제시하고, 특히 **멀티모달 데이터(시각, 청각, 텍스트)의 통합**이 중요하다는 점을 강조합니다. AI/ML 엔지니어는 MLLM의 **추론 능력 강화**를 위해 **강화 학습(RL)** 및 **정교한 프롬프트 엔지니어링** 기법을 적극적으로 고려해야 합니다. 대규모 데이터를 활용한 **옴니모달 모델** 개발이 공감적이고 상황 인지적인 AI를 구현하는 핵심 방향임을 시사합니다.

> ⚠️ **알림:** 이 리뷰는 AI로 작성되었습니다.