[{"id":"2025-01-01-first-post","title":"블로그 개발 완료","excerpt":"새해 첫날, 기술 블로그 작성을 시작하다","date":"2025-01-01 23:10:11+0900","permalink":"/me/first-post","tags":["Me"],"text":"새해를 맞아 뭔가 의미 있는 시작을 해보고 싶다는 생각에 고민을 거듭하다가, 기술 블로그를 작성하기로 결심했습니다. 처음에는 어떤 플랫폼을 사용할지, 어떤 테마를 적용할지 선택하는 데 하루를 다 쏟아부었어요. 다양한 옵션을 비교해 보고, 구글링을 통해 마음에 드는 테마를 발견했죠. 그 테마를 기반으로 제 스타일에 맞게 커스텀하여 적용하면서 블로그의 첫 모습"},{"id":"2025-01-02-loguru","title":"[Python] logging 사용법","excerpt":"Loguru 을 이용한 logging 간단하게 사용해보자","date":"2025-01-03 01:03:11+0900","permalink":"/logging/loguru","tags":["Logging"],"text":"들어가며 로깅은 프로그램을 개발하면서 발생하는 문제를 추적하고 디버깅하는 데 매우 중요한 역할을 합니다. 파이썬에서는 기본적으로 모듈을 사용하여 로깅을 할 수 있습니다. 하지만 이 모듈은 설정이 복잡하고 사용법이 어려워 초보자에게는 다소 부담이 될 수 있습니다. 이러한 문제를 해결하기 위해 라는 라이브러리를 소개하고자 합니다. 는 사용이 간편하고 직관적이어"},{"id":"2025-01-02-start-safeline","title":"[SafeLine] SafeLine 초기 설정","excerpt":"SafeLine 초기 설정 방법을 공유합니다.","date":"2025-01-02 13:03:11+0900","permalink":"/safeline/start-safeline","tags":["SafeLine"],"text":"1. SafeLine은 무엇인가요? 들어가며 백엔드 배포를 열심히 진행했지만 보안에 대해서는 신경을 쓰지 않으셨나요? 저는 SQL Injection, XSS, CSRF 등 여러 가지 공격 방법에 대해 들어본 적은 있지만, 바쁜 일정을 이유로 보안에 대해 충분히 신경을 쓸 시간이 없었습니다. 그러던 중, SafeLine 이라는 보안 프레임워크를 알게 되었고,"},{"id":"2025-01-03-code-review-with-llm","title":"[LLM] LLM을 활용한 코드 리뷰","excerpt":"github workflow를 활용해 코드 리뷰를 자동화하기","date":"2025-01-03 01:03:11+0900","permalink":"/llm/code-review-with-llm","tags":["LLM"],"text":"들어가며 많은 기업들이 사내에서 코드 리뷰를 진행하고 있습니다. 코드 리뷰는 소프트웨어 품질을 높이는 중요한 과정이지만, 바쁜 일정으로 인해 제대로 진행되지 않거나 퀄리티가 낮아지는 경우도 많습니다. 이러한 문제를 해결하기 위해 코드 리뷰를 자동화하는 도구를 개발했습니다. 이번 포스팅에서는 그 도구를 어떻게 활용할 수 있는지 공유하고자 합니다. 1. 코드 "},{"id":"2025-01-07-create-pipeline-with-jenkins","title":"[Jenkins] Jenkins 파이프라인 생성","excerpt":"Jenkins 파이프라인 생성 방법을 공유합니다.","date":"2025-01-08 03:03:11+0900","permalink":"/jenkins/create-pipeline-with-jenkins","tags":["Jenkins"],"text":"들어가며 이전 포스트에서는 Jenkins를 설치하고 초기 설정을 완료했습니다. 이번 포스트에서는 Jenkins 파이프라인을 사용하여 서비스를 배포하는 방법에 대해 다뤄보겠습니다. 파이프라인을 통해 자동화된 배포를 설정하는 과정이므로, 실습을 통해 배포 과정을 간편하게 처리할 수 있습니다. 파이프라인 생성 1. Jenkins 대시보드에서 새로운 Item 생성"},{"id":"2025-01-07-start-jenkins","title":"[Jenkins] Jenkins 초기 설정","excerpt":"Jenkins 초기 설정 방법을 공유합니다.","date":"2025-01-07 19:03:11+0900","permalink":"/jenkins/start-jenkins","tags":["Jenkins"],"text":"들어가며 Jenkins는 지속적인 통합(CI) 및 지속적인 배포(CD)를 위한 오픈소스 자동화 도구입니다. 이 포스트에서는 Jenkins를 설치하고 기본적인 설정을 진행하는 방법을 다뤄보겠습니다. Jenkins 설치 이번 포스트에서는 Docker를 사용하여 Jenkins를 설치하는 방법을 설명하겠습니다. 만약 Docker가 설치되어 있지 않다면, Docke"},{"id":"2025-01-14-automating-update-github-profile","title":"[GitHub Actions] 깃허브 프로필 자동 업데이트하기","excerpt":"GitHub Actions를 이용해 깃허브 프로필을 자동으로 업데이트하는 방법을 공유합니다.","date":"2025-01-14 23:10:11+0900","permalink":"/github-actions/automating-update-github-profile","tags":["GitHub Actions"],"text":"들어가며 매일 자주 사용하는 GitHub, 그런데 제 프로필을 확인해보니 마지막으로 업데이트한 게 3년 전이었습니다. 이때부터 기술 스택도 최신화되지 않았고, 제 블로그 포스팅도 깃허브 프로필에 반영되지 않았습니다. 그래서 이번 기회에 프로필을 업데이트하고, 블로그 포스팅도 자동으로 업데이트되도록 만들기로 했습니다. 하지만 매번 수동으로 블로그 포스팅을 추"},{"id":"2025-01-17-get-ssl-auth-with-letsencrypt","title":"[Nginx] Let's Encrypt로 SSL 인증서 발급받기","excerpt":"Let's Encrypt로 SSL 인증서를 발급받는 방법을 공유합니다.","date":"2025-01-17 23:10:11+0900","permalink":"/nginx/get-ssl-auth-with-letsencrypt","tags":["Nginx"],"text":"들어가며 웹사이트의 보안을 강화하려면 SSL 인증서가 필수입니다. SSL 인증서를 설치하면 HTTPS를 통해 사이트와 방문자 간의 데이터가 암호화되어 전송되므로 보안성이 높아집니다. 이 포스팅에서는 무료 SSL 인증서 발급 서비스인 Let's Encrypt 를 사용하여 Nginx 서버에 SSL 인증서를 발급받고 적용하는 방법을 소개합니다. 1. 도메인 준비"},{"id":"2025-01-18-zero-downtime-deploy-with-jenkins","title":"[Jenkins] 무중단 배포를 위한 파이프라인 구성","excerpt":"Jenkins를 이용한 무중단 배포 방법을 공유합니다.","date":"2025-01-18 19:03:11+0900","permalink":"/jenkins/zero-downtime-deploy-with-jenkins","tags":["Jenkins"],"text":"무중단 배포 (BlueGreen 배포) 방법 들어가며 지난 포스트에서는 SSL 인증서 발급 과 이를 Jenkins 파이프라인 에 적용하는 방법을 다뤘습니다. 이번 포스트에서는 이 설정을 바탕으로 무중단 배포 를 구현하는 방법을 공유합니다. 무중단 배포란? 무중단 배포는 기존 서비스를 중단하지 않고 새로운 버전을 배포하는 방법입니다. 이렇게 배포를 진행하면 "},{"id":"2025-02-05-see-log-in-django","title":"[Django] DB 로그 확인하기","excerpt":"Django에서 ORM을 사용하여 데이터베이스에 접근하는 경우, 쿼리 로그를 확인하는 방법을 공유합니다.","date":"2025-02-05 19:03:11+0900","permalink":"/backend/django/see-log-in-django","tags":["Django"],"text":"들어가며 Django에서 데이터베이스에 접근할 때, 우리는 주로 ORM(ObjectRelational Mapping)을 사용합니다. ORM을 사용하면 쿼리문을 직접 작성하지 않고도 데이터베이스와 상호작용할 수 있어 매우 편리하지만, 쿼리 로그를 확인하기 어려워 최적화가 필요한 부분을 파악하기 힘들 수 있습니다. 이번 포스트에서는 Django에서 쿼리 로그를"},{"id":"2025-02-14-django-index","title":"[Django] index","excerpt":"Django에서 index의 사용법을 공유합니다.","date":"2025-02-14 07:30:00+0900","permalink":"/backend/django/index","tags":["Django"],"text":"들어가며 Django에서 인덱스 를 사용하는 예시를 먼저 살펴보겠습니다. 인덱스는 데이터베이스에서 검색 성능을 향상시키는 중요한 기능입니다. 특히 자주 조회하는 필드에 대해 인덱스를 설정하면, 조회 성능을 크게 개선할 수 있습니다. 예시 1: 기본 카테고리 모델 위와 같이 모델이 있을 때, 대부분 상태인 카테고리만 조회하는 상황이 많다면 인덱스를 사용하는 "},{"id":"2025-02-14-django-optimistic-locking","title":"[Django] 낙관적 락","excerpt":"Django에서 낙관적 락에 대해 알아보자","date":"2025-02-14 08:33:11+0900","permalink":"/backend/django/optimistic-locking","tags":["Django"],"text":"낙관적 락 (Optimistic Lock) 낙관적 락이란? 낙관적 락(Optimistic Lock)은 실제로 데이터베이스에서 락을 걸지 않고, 데이터의 컬럼을 통해 동시성을 제어하는 방법입니다. 이 방식은 비관적 락(Pessimistic Lock)과 비교했을 때 동시성 처리가 더 효율적이지만, 충돌이 발생할 가능성도 존재하고, 충돌이 발생했을 때 이를 처리"},{"id":"2025-02-14-django-pessimistic-locking","title":"[Django] 비관적 락","excerpt":"Django에서 비관적 락에 대해 알아보자","date":"2025-02-14 08:03:11+0900","permalink":"/backend/django/pessimistic-locking","tags":["Django"],"text":"비관적 락 (Pessimistic Lock) 비관적 락이란? 비관적 락은 데이터베이스에서 동시성 제어 를 위해 락을 사용하여 작업을 처리하는 방법입니다. 이 방식은 데이터 정합성 을 강하게 보장하지만, 락을 획득하고 있는 동안 다른 작업들이 대기하거나 영향을 받을 수 있는 단점이 있습니다. 이는 주로 충돌 가능성이 높거나 데이터의 정합성이 매우 중요한 상황"},{"id":"2025-02-14-django-prefetch-related","title":"[Django] prefetch_related","excerpt":"Django에서 prefetch_related의 사용법을 공유합니다.","date":"2025-02-14 06:03:11+0900","permalink":"/backend/django/prefetch-related","tags":["Django"],"text":"prefetchrelated 다음과 같은 모델이 있을때 이 모델에 접근하기 위해 조회를 하면 다음과 같은 쿼리가 나간다. 이때 조회한 category 의 name 을 조회할때는 따로 추가적인 쿼리가 나가지 않는다. 하지만 category 의 parent 를 조회할때는 다음과 같이 추가적인 쿼리가 나가는데 이를 방지하기 위해 미리 조회해두는 것이 prefet"},{"id":"2025-02-15-django-makefile","title":"[Django] Makefile 사용하기","excerpt":"Django 프로젝트에서 Makefile을 사용하는 방법을 공유합니다.","date":"2025-02-15 04:20:00+0900","permalink":"/backend/django/makefile","tags":["Django"],"text":"Makefile이란? Makefile은 자주 사용하는 명령어나 스크립트를 정의하여 간편하게 실행할 수 있도록 도와주는 파일입니다. 코드가 길거나 복잡한 경우 짧은 명령어로 선언하여 쉽게 사용 가능하며, 코드 재사용과 유지보수에 용이합니다. Django 프로젝트에서 사용하는 예시를 통해 의 사용법을 소개합니다. Makefile 예시 각 명령어의 역할 : 프로"},{"id":"2025-02-15-docker-fast-build","title":"[Docker] 빠른 빌드 방법","excerpt":"Docker 빌드 속도를 높이는 방법을 공유합니다.","date":"2025-02-14 11:30:11+0900","permalink":"/devops/docker/fast-build","tags":["Docker"],"text":"Dockerfile 최적화: 의존성 관리 개선 문제점: 매번 라이브러리 재설치 기존 Dockerfile에서는 별다른 라이브러리 설치 과정 없이 의존성을 관리하고 있었으나, 매번 Docker 이미지 빌드 시마다 라이브러리를 새로 설치 하는 문제가 발생했습니다. 이로 인해 빌드 시간이 길어지고, 이미지를 빌드하는 데 시간이 많이 소요되었죠. 해결책: 로 의존성"},{"id":"2025-02-25-aws-lambda-eventbridge","title":"[AWS] AWS Lambda와 Notion API를 활용한 15분 단위 자동 기록 시스템","excerpt":"AWS Lambda, Notion API, Slack Webhook을 활용하여 15분마다 자동으로 시간을 기록하고 Slack 알림을 받는 시스템을 구축하는 방법을 설명합니다.","date":"2025-02-25 18:57:00+0900","permalink":"/devops/aws/aws-lambda-eventbridge","tags":["AWS","Lambda","EventBridge"],"text":"들어가며 지난 글에서는 뇌과학적으로 게으름과 무기력을 극복하는 방법에 대해 다루었습니다. 그 방법 중 하나로 내가 하고 있는 행동이나 생각을 말로 설명하는 것이 있었는데, 이를 확장하여 15분 단위로 행동을 기록하면 시간을 효율적으로 사용할 뿐만 아니라 과거의 내가 어떤 행동을 했는지 기억할 수 있다는 장점이 있습니다. 그래서 이를 실천하기 위해 AWS L"},{"id":"2025-02-25-block-youtube-extention","title":"[Chrome Extention] YouTube 차단 확장 프로그램 개발하기","excerpt":"YouTube 접속을 차단하고 생산성을 높이는 크롬 확장 프로그램을 만들어보세요. 개발 과정과 설치 방법을 자세히 설명합니다.","date":"2025-02-25 18:01:00+0900","permalink":"/etc/chrome-extension/block-youtube-with-chrome-extension","tags":["Chrome Extension"],"text":"들어가며 지난 글에서는 뇌과학적으로 게으름과 무기력을 극복하는 방법에 대해 다루었습니다. 그 방법 중 하나로 불필요한 정보를 차단하는 방법 이 있었는데요. 저의 경우 YouTube 시청 시간이 길었기에, YouTube에 접속할 때마다 제가 작성한 뇌과학적으로 게으름과 무기력을 극복하는 글로 자동 리다이렉트 해주는 확장 프로그램을 만들었습니다. 이 글에서는 "},{"id":"2025-02-25-brain-hack-productivity","title":"뇌과학적으로 게으름과 무기력을 극복하는 방법","excerpt":"게으름과 무기력은 단순한 의지 부족이 아니라, 뇌의 작동 방식과 깊은 관련이 있습니다. 전두엽을 활성화하고, 불필요한 정보를 차단하며, 긍정적인 자기 암시를 활용하는 방법을 통해 효율적으로 극복하는 법을 알아보세요.","date":"2025-02-25 23:10:11+0900","permalink":"/me/brain-hack-productivity","tags":["Me"],"text":"들어가며 우리의 뇌는 기본적으로 에너지를 절약하려는 성향이 있습니다. 따라서 게으름이나 무기력함을 느끼는 것은 단순한 의지 부족이 아니라, 뇌의 본능적인 반응일 수 있습니다. 하지만 뇌과학적으로 이를 극복할 방법이 존재합니다. 1. 전두엽 강화하기 뇌의 주요 부위에는 감정을 담당하는 변연계(편도체) 와 이성을 담당하는 전두엽 이 있습니다. 게으름과 무기력을"},{"id":"2025-03-08-Pass-Certified-Solutions-Architect-Associate","title":"[AWS] AWS Solutions Architect Associate 자격증 취득 후기","excerpt":"AWS Solutions Architect Associate(SAA) 자격증 공부 방법, 시험 후기를 정리했습니다.","date":"2025-03-08 23:10:11+0900","permalink":"/me/pass-certified-solutions-architect-associate","tags":["Me"],"text":"AWS Solutions Architect Associate 자격증이란? AWS Solutions Architect Associate(SAA) 자격증은 AWS 클라우드 환경에서 아키텍처 설계를 담당하는 전문가를 인증하는 시험입니다. AWS의 다양한 서비스를 이해하고, 이를 활용하여 효율적인 아키텍처를 설계할 수 있는 능력을 평가합니다. 특히, 다음과 같은 "},{"id":"2025-09-26-pep-0001-pep-purpose-and-guidelines","title":"[Active] PEP 1 - PEP Purpose and Guidelines","excerpt":"Python Enhancement Proposal 1: 'PEP Purpose and Guidelines'에 대한 한국어 번역입니다.","date":"2025-09-26 15:31:21+0900","permalink":"/python/pep/1","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 1 PEP Purpose and Guidelines 상태: Active | 유형: Process | 작성일: 13Jun2000 PEP란 무엇인가요? PEP는 Python Enhancement Proposal 의 약자입니다. PEP는 Python 커뮤니티에 정보를 제공하거나, Python 또는 그 프로세스 및 환경을 위한 새로운 기능을 설"},{"id":"2025-09-26-pep-0002-procedure-for-adding-new-modules","title":"[Active] PEP 2 - Procedure for Adding New Modules","excerpt":"Python Enhancement Proposal 2: 'Procedure for Adding New Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 15:36:38+0900","permalink":"/python/pep/2","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 2 Procedure for Adding New Modules 상태: Active | 유형: Process | 작성일: 07Jul2001 PEP 2 – 새 모듈 추가 절차 개요 이 문서는 Python 표준 라이브러리에 새 모듈을 추가하는 절차에 대해 설명합니다. Python의 성공에 크게 기여하는 표준 라이브러리의 유용성을 유지하고, 추"},{"id":"2025-09-26-pep-0003-guidelines-for-handling-bug-reports","title":"[Withdrawn] PEP 3 - Guidelines for Handling Bug Reports","excerpt":"Python Enhancement Proposal 3: 'Guidelines for Handling Bug Reports'에 대한 한국어 번역입니다.","date":"2025-09-26 15:37:46+0900","permalink":"/python/pep/3","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3 Guidelines for Handling Bug Reports 상태: Withdrawn | 유형: Process | 작성일: 25Sep2000 PEP 3 – 버그 보고서 처리 지침 (Withdraw) 상태: 철회됨 (Withdrawn) 유형: 프로세스 (Process) 작성자: Jeremy Hylton <jeremy at alum."},{"id":"2025-09-26-pep-0004-deprecation-of-standard-modules","title":"[Active] PEP 4 - Deprecation of Standard Modules","excerpt":"Python Enhancement Proposal 4: 'Deprecation of Standard Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 15:38:53+0900","permalink":"/python/pep/4","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 4 Deprecation of Standard Modules 상태: Active | 유형: Process | 작성일: 01Oct2000 PEP 4 – 표준 모듈의 Deprecation (폐기 예정) PEP 4 – 표준 모듈의 Deprecation 작성자: Brett Cannon <brett at python.org, Martin von "},{"id":"2025-09-26-pep-0005-guidelines-for-language-evolution","title":"[Superseded] PEP 5 - Guidelines for Language Evolution","excerpt":"Python Enhancement Proposal 5: 'Guidelines for Language Evolution'에 대한 한국어 번역입니다.","date":"2025-09-26 15:40:01+0900","permalink":"/python/pep/5","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 5 Guidelines for Language Evolution 상태: Superseded | 유형: Process | 작성일: 26Oct2000 PEP 5 – 언어 진화를 위한 가이드라인 본 문서는 PEP 387에 의해 대체(Superseded)되었습니다. 작성자: Paul Prescod <paul at prescod.net 상태: S"},{"id":"2025-09-26-pep-0006-bug-fix-releases","title":"[Superseded] PEP 6 - Bug Fix Releases","excerpt":"Python Enhancement Proposal 6: 'Bug Fix Releases'에 대한 한국어 번역입니다.","date":"2025-09-26 15:42:30+0900","permalink":"/python/pep/6","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 6 Bug Fix Releases 상태: Superseded | 유형: Process | 작성일: 15Mar2001 PEP 6 – Bug Fix Releases (버그 수정 릴리스) 저자: Aahz <aahz at pythoncraft.com, Anthony Baxter <anthony at interlink.com.au 상태: Supe"},{"id":"2025-09-26-pep-0007-style-guide-for-c-code","title":"[Active] PEP 7 - Style Guide for C Code","excerpt":"Python Enhancement Proposal 7: 'Style Guide for C Code'에 대한 한국어 번역입니다.","date":"2025-09-26 15:43:52+0900","permalink":"/python/pep/7","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 7 Style Guide for C Code 상태: Active | 유형: Process | 작성일: 05Jul2001 PEP 7 – C 코드 스타일 가이드 작성자: Guido van Rossum, Barry Warsaw 상태: Active (활성) 유형: Process (프로세스) 작성일: 2001년 7월 5일 서론 이 문서는 Pyth"},{"id":"2025-09-26-pep-0008-style-guide-for-python-code","title":"[Active] PEP 8 - Style Guide for Python Code","excerpt":"Python Enhancement Proposal 8: 'Style Guide for Python Code'에 대한 한국어 번역입니다.","date":"2025-09-26 15:45:52+0900","permalink":"/python/pep/8","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8 Style Guide for Python Code 상태: Active | 유형: Process | 작성일: 05Jul2001 PEP 8 – Python 코드 스타일 가이드 번역 및 요약 서론 이 문서는 메인 Python 배포판의 표준 라이브러리를 구성하는 Python 코드에 대한 코딩 컨벤션(Coding Conventions)을 제공"},{"id":"2025-09-26-pep-0009-sample-plaintext-pep-template","title":"[Withdrawn] PEP 9 - Sample Plaintext PEP Template","excerpt":"Python Enhancement Proposal 9: 'Sample Plaintext PEP Template'에 대한 한국어 번역입니다.","date":"2025-09-26 15:47:09+0900","permalink":"/python/pep/9","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 9 Sample Plaintext PEP Template 상태: Withdrawn | 유형: Process | 작성일: 14Aug2001 PEP 9 – 샘플 일반 텍스트 PEP 템플릿 번역 및 해설 작성자: Barry Warsaw <barry at python.org 상태: 철회됨 (Withdrawn) 유형: 프로세스 (Process) "},{"id":"2025-09-26-pep-0010-voting-guidelines","title":"[Active] PEP 10 - Voting Guidelines","excerpt":"Python Enhancement Proposal 10: 'Voting Guidelines'에 대한 한국어 번역입니다.","date":"2025-09-26 15:48:17+0900","permalink":"/python/pep/10","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 10 Voting Guidelines 상태: Active | 유형: Process | 작성일: 07Mar2002 PEP 10 – 투표 지침 (Voting Guidelines) 작성자 : Barry Warsaw <barry at python.org 상태 : Active (활성) 유형 : Process (절차) 생성일 : 2002년 3월 7"},{"id":"2025-09-26-pep-0011-cpython-platform-support","title":"[Active] PEP 11 - CPython platform support","excerpt":"Python Enhancement Proposal 11: 'CPython platform support'에 대한 한국어 번역입니다.","date":"2025-09-26 15:49:46+0900","permalink":"/python/pep/11","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 11 CPython platform support 상태: Active | 유형: Process | 작성일: 07Jul2002 PEP 11 – CPython 플랫폼 지원 (CPython platform support) 개요 (Abstract) 이 PEP (Python Enhancement Proposal)는 CPython이 특정 운영 체제"},{"id":"2025-09-26-pep-0012-sample-restructuredtext-pep-template","title":"[Active] PEP 12 - Sample reStructuredText PEP Template","excerpt":"Python Enhancement Proposal 12: 'Sample reStructuredText PEP Template'에 대한 한국어 번역입니다.","date":"2025-09-26 15:52:34+0900","permalink":"/python/pep/12","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 12 Sample reStructuredText PEP Template 상태: Active | 유형: Process | 작성일: 05Aug2002 PEP 12는 형식으로 Python Enhancement Proposal (PEP)을 작성하기 위한 샘플 템플릿 문서입니다. 이 PEP의 목표는 새로운 PEP를 제출하려는 저자들이 표준화된 형"},{"id":"2025-09-26-pep-0013-python-language-governance","title":"[Active] PEP 13 - Python Language Governance","excerpt":"Python Enhancement Proposal 13: 'Python Language Governance'에 대한 한국어 번역입니다.","date":"2025-09-26 15:54:00+0900","permalink":"/python/pep/13","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 13 Python Language Governance 상태: Active | 유형: Process | 작성일: 16Dec2018 PEP 13 – Python 언어 거버넌스 요약 이 PEP는 Python의 공식 거버넌스(governance) 프로세스를 정의하며, 시간이 지남에 따라 어떻게 변경되었는지 기록합니다. 현재 Python의 거버넌"},{"id":"2025-09-26-pep-0020-the-zen-of-python","title":"[Active] PEP 20 - The Zen of Python","excerpt":"Python Enhancement Proposal 20: 'The Zen of Python'에 대한 한국어 번역입니다.","date":"2025-09-26 15:55:10+0900","permalink":"/python/pep/20","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 20 The Zen of Python 상태: Active | 유형: Informational | 작성일: 19Aug2004 PEP 20 – 파이썬의 정신 (The Zen of Python) 저자: Tim Peters <tim.peters at gmail.com 상태: Active (활성) 유형: Informational (정보성) 생성일"},{"id":"2025-09-26-pep-0042-feature-requests","title":"[Withdrawn] PEP 42 - Feature Requests","excerpt":"Python Enhancement Proposal 42: 'Feature Requests'에 대한 한국어 번역입니다.","date":"2025-09-26 15:56:44+0900","permalink":"/python/pep/42","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 42 Feature Requests 상태: Withdrawn | 유형: Process | 작성일: 12Sep2000 PEP 42 – 기능 요청 (Feature Requests) 작성자: Jeremy Hylton <jeremy at alum.mit.edu 상태: 철회됨 (Withdrawn) 유형: 프로세스 (Process) 생성일: 200"},{"id":"2025-09-26-pep-0100-python-unicode-integration","title":"[Final] PEP 100 - Python Unicode Integration","excerpt":"Python Enhancement Proposal 100: 'Python Unicode Integration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:00:20+0900","permalink":"/python/pep/100","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 100 Python Unicode Integration 상태: Final | 유형: Standards Track | 작성일: 10Mar2000 이 문서는 Python Enhancement Proposal (PEP) 100으로, Python 2.0에 유니코드(Unicode) 지원을 통합하기 위한 제안입니다. 이 제안의 주요 목표는 유니코드"},{"id":"2025-09-26-pep-0101-doing-python-releases-101","title":"[Active] PEP 101 - Doing Python Releases 101","excerpt":"Python Enhancement Proposal 101: 'Doing Python Releases 101'에 대한 한국어 번역입니다.","date":"2025-09-26 16:02:21+0900","permalink":"/python/pep/101","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 101 Doing Python Releases 101 상태: Active | 유형: Informational | 작성일: 22Aug2001 PEP 101 – Python 릴리스 수행 101 저자: Barry Warsaw, Guido van Rossum 상태: 활성 (Active) 유형: 정보 (Informational) 생성일: 2001"},{"id":"2025-09-26-pep-0102-doing-python-micro-releases","title":"[Superseded] PEP 102 - Doing Python Micro Releases","excerpt":"Python Enhancement Proposal 102: 'Doing Python Micro Releases'에 대한 한국어 번역입니다.","date":"2025-09-26 16:04:40+0900","permalink":"/python/pep/102","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 102 Doing Python Micro Releases 상태: Superseded | 유형: Informational | 작성일: 09Jan2002 PEP 102 – Python 마이크로 릴리스(Micro Releases) 수행하기 작성자: Anthony Baxter, Barry Warsaw, Guido van Rossum 상태: Su"},{"id":"2025-09-26-pep-0103-collecting-information-about-git","title":"[Withdrawn] PEP 103 - Collecting information about git","excerpt":"Python Enhancement Proposal 103: 'Collecting information about git'에 대한 한국어 번역입니다.","date":"2025-09-26 16:06:44+0900","permalink":"/python/pep/103","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 103 Collecting information about git 상태: Withdrawn | 유형: Informational | 작성일: 01Jun2015 PEP 103 – Git 정보 수집 (철회됨) 작성자: Oleg Broytman 상태: 철회됨 (Withdrawn) 유형: 정보성 (Informational) 생성일: 2015년 6"},{"id":"2025-09-26-pep-0160-python-1-6-release-schedule","title":"[Final] PEP 160 - Python 1.6 Release Schedule","excerpt":"Python Enhancement Proposal 160: 'Python 1.6 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:07:53+0900","permalink":"/python/pep/160","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 160 Python 1.6 Release Schedule 상태: Final | 유형: Informational | 작성일: 25Jul2000 PEP 160 – Python 1.6 릴리스 일정 작성자: Fred L. Drake, Jr. <fred at fdrake.net 상태: Final (최종) 유형: Informational (정보성)"},{"id":"2025-09-26-pep-0200-python-2-0-release-schedule","title":"[Final] PEP 200 - Python 2.0 Release Schedule","excerpt":"Python Enhancement Proposal 200: 'Python 2.0 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:09:15+0900","permalink":"/python/pep/200","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 200 Python 2.0 Release Schedule 상태: Final | 유형: Informational | 작성일: 12Jul2000 PEP 200은 Python 2.0 릴리스 일정을 설명하며, 주요 새 기능의 상태와 소유권을 추적하고, 메일링 리스트 포럼에서 진행된 논의를 요약하며, 추가 정보, 패치 및 기타 미해결 문제에 대한"},{"id":"2025-09-26-pep-0201-lockstep-iteration","title":"[Final] PEP 201 - Lockstep Iteration","excerpt":"Python Enhancement Proposal 201: 'Lockstep Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:10:52+0900","permalink":"/python/pep/201","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 201 Lockstep Iteration 상태: Final | 유형: Standards Track | 작성일: 13Jul2000 PEP 201 – Lockstep Iteration 작성자 : Barry Warsaw <barry at python.org 상태 : Final 유형 : Standards Track 생성일 : 2000년 7월 1"},{"id":"2025-09-26-pep-0202-list-comprehensions","title":"[Final] PEP 202 - List Comprehensions","excerpt":"Python Enhancement Proposal 202: 'List Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 16:12:08+0900","permalink":"/python/pep/202","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 202 List Comprehensions 상태: Final | 유형: Standards Track | 작성일: 13Jul2000 PEP 202 – List Comprehensions (리스트 컴프리헨션) 서론 (Introduction) 이 PEP는 Python에 제안된 문법적 확장인 List Comprehensions (리스트 컴프리헨"},{"id":"2025-09-26-pep-0203-augmented-assignments","title":"[Final] PEP 203 - Augmented Assignments","excerpt":"Python Enhancement Proposal 203: 'Augmented Assignments'에 대한 한국어 번역입니다.","date":"2025-09-26 16:13:23+0900","permalink":"/python/pep/203","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 203 Augmented Assignments 상태: Final | 유형: Standards Track | 작성일: 13Jul2000 PEP 203: 증강 할당 (Augmented Assignments) 작성자: Thomas Wouters 상태: Final (최종) 유형: Standards Track 생성일: 2000년 7월 13일 Py"},{"id":"2025-09-26-pep-0204-range-literals","title":"[Rejected] PEP 204 - Range Literals","excerpt":"Python Enhancement Proposal 204: 'Range Literals'에 대한 한국어 번역입니다.","date":"2025-09-26 16:14:46+0900","permalink":"/python/pep/204","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 204 Range Literals 상태: Rejected | 유형: Standards Track | 작성일: 14Jul2000 작성자: Thomas Wouters 상태: Rejected (거부됨) 유형: Standards Track 작성일: 2000년 7월 14일 Python 버전: 2.0 경고: 이 PEP는 거부되었습니다. 신중한 고려"},{"id":"2025-09-26-pep-0205-weak-references","title":"[Final] PEP 205 - Weak References","excerpt":"Python Enhancement Proposal 205: 'Weak References'에 대한 한국어 번역입니다.","date":"2025-09-26 16:16:10+0900","permalink":"/python/pep/205","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 205 Weak References 상태: Final | 유형: Standards Track | 작성일: 14Jul2000 PEP 205 – Weak References 작성자 : Fred L. Drake, Jr. 상태 : Final (최종) 유형 : Standards Track 생성일 : 2000년 7월 14일 Python 버전 : 2"},{"id":"2025-09-26-pep-0206-python-advanced-library","title":"[Withdrawn] PEP 206 - Python Advanced Library","excerpt":"Python Enhancement Proposal 206: 'Python Advanced Library'에 대한 한국어 번역입니다.","date":"2025-09-26 16:17:22+0900","permalink":"/python/pep/206","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 206 Python Advanced Library 상태: Withdrawn | 유형: Informational | 작성일: 14Jul2000 PEP 206 – Python Advanced Library는 Python의 \"Batteries Included\" 철학을 확장하여 고품질의 자주 사용되는 서드 파티 확장 모듈 컬렉션을 제안했던 문서"},{"id":"2025-09-26-pep-0207-rich-comparisons","title":"[Final] PEP 207 - Rich Comparisons","excerpt":"Python Enhancement Proposal 207: 'Rich Comparisons'에 대한 한국어 번역입니다.","date":"2025-09-26 16:18:40+0900","permalink":"/python/pep/207","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 207 Rich Comparisons 상태: Final | 유형: Standards Track | 작성일: 25Jul2000 PEP 207 – 풍부한 비교 (Rich Comparisons) 개요 (Abstract) 이 PEP는 비교 연산자에 대한 몇 가지 새로운 기능을 제안합니다. 클래스와 C 확장 모두에서 , , , , , 연산자를 개"},{"id":"2025-09-26-pep-0208-reworking-the-coercion-model","title":"[Final] PEP 208 - Reworking the Coercion Model","excerpt":"Python Enhancement Proposal 208: 'Reworking the Coercion Model'에 대한 한국어 번역입니다.","date":"2025-09-26 16:20:00+0900","permalink":"/python/pep/208","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 208 Reworking the Coercion Model 상태: Final | 유형: Standards Track | 작성일: 04Dec2000 PEP 208 – Coercion 모델 재작업 (Reworking the Coercion Model) 개요 (Abstract) 많은 Python 타입은 숫자 연산(numeric operatio"},{"id":"2025-09-26-pep-0209-multi-dimensional-arrays","title":"[Withdrawn] PEP 209 - Multi-dimensional Arrays","excerpt":"Python Enhancement Proposal 209: 'Multi-dimensional Arrays'에 대한 한국어 번역입니다.","date":"2025-09-26 16:21:26+0900","permalink":"/python/pep/209","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 209 Multidimensional Arrays 상태: Withdrawn | 유형: Standards Track | 작성일: 03Jan2001 PEP 209 – 다차원 배열 (Multidimensional Arrays) 저자: Paul Barrett, Travis Oliphant 상태: 철회됨 (Withdrawn) 유형: 표준 트랙 ("},{"id":"2025-09-26-pep-0210-decoupling-the-interpreter-loop","title":"[Rejected] PEP 210 - Decoupling the Interpreter Loop","excerpt":"Python Enhancement Proposal 210: 'Decoupling the Interpreter Loop'에 대한 한국어 번역입니다.","date":"2025-09-26 16:25:55+0900","permalink":"/python/pep/210","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 210 Decoupling the Interpreter Loop 상태: Rejected | 유형: Standards Track | 작성일: 15Jul2000 PEP 210 – 인터프리터 루프 분리 (Decoupling the Interpreter Loop) 개요 PEP 210은 Python 인터프리터의 메인 루프(interpreter l"},{"id":"2025-09-26-pep-0211-adding-a-new-outer-product-operator","title":"[Rejected] PEP 211 - Adding A New Outer Product Operator","excerpt":"Python Enhancement Proposal 211: 'Adding A New Outer Product Operator'에 대한 한국어 번역입니다.","date":"2025-09-26 16:27:14+0900","permalink":"/python/pep/211","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 211 Adding A New Outer Product Operator 상태: Rejected | 유형: Standards Track | 작성일: 15Jul2000 제목 및 상태 PEP 211 – 새로운 외적(Outer Product) 연산자 추가 (Adding A New Outer Product Operator) 저자: Greg Wil"},{"id":"2025-09-26-pep-0212-loop-counter-iteration","title":"[Rejected] PEP 212 - Loop Counter Iteration","excerpt":"Python Enhancement Proposal 212: 'Loop Counter Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 16:28:28+0900","permalink":"/python/pep/212","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 212 Loop Counter Iteration 상태: Rejected | 유형: Standards Track | 작성일: 22Aug2000 PEP 212 – 루프 카운터 반복 (Loop Counter Iteration) 저자: Peter SchneiderKamp <nowonder at nowonder.de 상태: Rejected (거부"},{"id":"2025-09-26-pep-0213-attribute-access-handlers","title":"[Deferred] PEP 213 - Attribute Access Handlers","excerpt":"Python Enhancement Proposal 213: 'Attribute Access Handlers'에 대한 한국어 번역입니다.","date":"2025-09-26 16:29:57+0900","permalink":"/python/pep/213","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 213 Attribute Access Handlers 상태: Deferred | 유형: Standards Track | 작성일: 21Jul2000 서론 (Introduction) Python 코드와 확장 모듈에서는 인스턴스의 클라이언트 코드가 속성을 설정하려고 시도할 때 이를 \"트랩(trap)\"하여 대신 다른 코드를 실행하는 것이 가능하"},{"id":"2025-09-26-pep-0214-extended-print-statement","title":"[Final] PEP 214 - Extended Print Statement","excerpt":"Python Enhancement Proposal 214: 'Extended Print Statement'에 대한 한국어 번역입니다.","date":"2025-09-26 16:31:25+0900","permalink":"/python/pep/214","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 214 Extended Print Statement 상태: Final | 유형: Standards Track | 작성일: 24Jul2000 PEP 214 – 문 확장 (Extended Print Statement) 개요 이 PEP (Python Enhancement Proposal)는 Python의 표준 문을 확장하여 기본 대신 모든 파"},{"id":"2025-09-26-pep-0215-string-interpolation","title":"[Superseded] PEP 215 - String Interpolation","excerpt":"Python Enhancement Proposal 215: 'String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 16:32:37+0900","permalink":"/python/pep/215","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 215 String Interpolation 상태: Superseded | 유형: Standards Track | 작성일: 24Jul2000 PEP 215 – 문자열 보간 (String Interpolation) 개요 이 문서는 Python 2.1에서 더 쉬운 문자열 포매팅을 가능하게 하는 문자열 보간(String Interpolatio"},{"id":"2025-09-26-pep-0216-docstring-format","title":"[Withdrawn] PEP 216 - Docstring Format","excerpt":"Python Enhancement Proposal 216: 'Docstring Format'에 대한 한국어 번역입니다.","date":"2025-09-26 16:33:50+0900","permalink":"/python/pep/216","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 216 Docstring Format 상태: Withdrawn | 유형: Informational | 작성일: 31Jul2000 PEP 216 – Docstring 형식 작성자: Moshe Zadka 상태: 철회됨 (Withdrawn) 유형: 정보 (Informational) 생성일: 2000년 7월 31일 대체: PEP 287에 의해 "},{"id":"2025-09-26-pep-0217-display-hook-for-interactive-use","title":"[Final] PEP 217 - Display Hook for Interactive Use","excerpt":"Python Enhancement Proposal 217: 'Display Hook for Interactive Use'에 대한 한국어 번역입니다.","date":"2025-09-26 16:34:58+0900","permalink":"/python/pep/217","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 217 Display Hook for Interactive Use 상태: Final | 유형: Standards Track | 작성일: 31Jul2000 PEP 217 – 대화형 사용을 위한 디스플레이 훅 (Display Hook for Interactive Use) 작성자: Moshe Zadka <moshez at zadka.site."},{"id":"2025-09-26-pep-0218-adding-a-built-in-set-object-type","title":"[Final] PEP 218 - Adding a Built-In Set Object Type","excerpt":"Python Enhancement Proposal 218: 'Adding a Built-In Set Object Type'에 대한 한국어 번역입니다.","date":"2025-09-26 16:36:13+0900","permalink":"/python/pep/218","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 218 Adding a BuiltIn Set Object Type 상태: Final | 유형: Standards Track | 작성일: 31Jul2000 이 문서는 Python Enhancement Proposal (PEP) 218의 내용을 한국어 Python 개발자들이 이해하기 쉽도록 번역하고 정리한 것입니다. 이 PEP는 Python"},{"id":"2025-09-26-pep-0219-stackless-python","title":"[Deferred] PEP 219 - Stackless Python","excerpt":"Python Enhancement Proposal 219: 'Stackless Python'에 대한 한국어 번역입니다.","date":"2025-09-26 16:37:30+0900","permalink":"/python/pep/219","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 219 Stackless Python 상태: Deferred | 유형: Standards Track | 작성일: 14Aug2000 PEP 219 – Stackless Python 번역 및 해설 작성자: Gordon McMillan 상태: 연기됨 (Deferred) 유형: 표준 트랙 (Standards Track) 생성일: 2000년 8월"},{"id":"2025-09-26-pep-0220-coroutines-generators-continuations","title":"[Rejected] PEP 220 - Coroutines, Generators, Continuations","excerpt":"Python Enhancement Proposal 220: 'Coroutines, Generators, Continuations'에 대한 한국어 번역입니다.","date":"2025-09-26 16:38:41+0900","permalink":"/python/pep/220","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 220 Coroutines, Generators, Continuations 상태: Rejected | 유형: Informational | 작성일: 14Aug2000 PEP 220 – 코루틴, 제너레이터, 연속 (Continuations) 번역 및 요약 문서 상태: 거부됨 (Rejected) 주의: 이 PEP는 거부되었습니다. 개요 (Ab"},{"id":"2025-09-26-pep-0221-import-as","title":"[Final] PEP 221 - Import As","excerpt":"Python Enhancement Proposal 221: 'Import As'에 대한 한국어 번역입니다.","date":"2025-09-26 16:39:52+0900","permalink":"/python/pep/221","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 221 Import As 상태: Final | 유형: Standards Track | 작성일: 15Aug2000 PEP 221 – Import As 작성자: Thomas Wouters 상태: Final (최종) 유형: Standards Track 작성일: 2000년 8월 15일 Python 버전: 2.0 서론 (Introduction) "},{"id":"2025-09-26-pep-0222-web-library-enhancements","title":"[Deferred] PEP 222 - Web Library Enhancements","excerpt":"Python Enhancement Proposal 222: 'Web Library Enhancements'에 대한 한국어 번역입니다.","date":"2025-09-26 16:41:15+0900","permalink":"/python/pep/222","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 222 Web Library Enhancements 상태: Deferred | 유형: Standards Track | 작성일: 18Aug2000 Abstract (개요) 이 PEP(Python Enhancement Proposal)는 Python 표준 라이브러리 내의 CGI (Common Gateway Interface) 개발 기능을 개"},{"id":"2025-09-26-pep-0223-change-the-meaning-ofxescapes","title":"[Final] PEP 223 - Change the Meaning of '\\x' Escapes","excerpt":"Python Enhancement Proposal 223: 'Change the Meaning of '\\x' Escapes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:42:30+0900","permalink":"/python/pep/223","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 223 Change the Meaning of\\xEscapes 상태: Final | 유형: Standards Track | 작성일: 20Aug2000 PEP 223은 Python 2.0 버전에서 이스케이프 시퀀스의 의미를 변경하는 제안입니다. 이 제안의 주요 목표는 8비트 및 유니코드(Unicode) 문자열 모두에서 이스케이프가 항상 뒤"},{"id":"2025-09-26-pep-0224-attribute-docstrings","title":"[Rejected] PEP 224 - Attribute Docstrings","excerpt":"Python Enhancement Proposal 224: 'Attribute Docstrings'에 대한 한국어 번역입니다.","date":"2025-09-26 16:43:48+0900","permalink":"/python/pep/224","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 224 Attribute Docstrings 상태: Rejected | 유형: Standards Track | 작성일: 23Aug2000 PEP 224 – 속성 Docstring (Attribute Docstrings) 개요 PEP 224는 Python 2.0 버전을 대상으로 \"속성 docstring\" 제안을 설명하는 문서입니다. 이 P"},{"id":"2025-09-26-pep-0225-elementwiseobjectwise-operators","title":"[Rejected] PEP 225 - Elementwise/Objectwise Operators","excerpt":"Python Enhancement Proposal 225: 'Elementwise/Objectwise Operators'에 대한 한국어 번역입니다.","date":"2025-09-26 16:45:47+0900","permalink":"/python/pep/225","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 225 Elementwise/Objectwise Operators 상태: Rejected | 유형: Standards Track | 작성일: 19Sep2000 경고: 이 PEP는 거부 되었습니다. 이 PEP 대신 PEP 465에서 제시된 접근 방식이 최종적으로 채택되었습니다. PEP 465의 '거부된 아이디어(Rejected Ideas)"},{"id":"2025-09-26-pep-0226-python-2-1-release-schedule","title":"[Final] PEP 226 - Python 2.1 Release Schedule","excerpt":"Python Enhancement Proposal 226: 'Python 2.1 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 16:47:09+0900","permalink":"/python/pep/226","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 226 Python 2.1 Release Schedule 상태: Final | 유형: Informational | 작성일: 16Oct2000 PEP 226 – Python 2.1 릴리스 스케줄 작성자: Jeremy Hylton <jeremy at alum.mit.edu 상태: Final (최종) 유형: Informational (정보성)"},{"id":"2025-09-26-pep-0227-statically-nested-scopes","title":"[Final] PEP 227 - Statically Nested Scopes","excerpt":"Python Enhancement Proposal 227: 'Statically Nested Scopes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:48:35+0900","permalink":"/python/pep/227","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 227 Statically Nested Scopes 상태: Final | 유형: Standards Track | 작성일: 01Nov2000 PEP 227 – 정적으로 중첩된 스코프 (Statically Nested Scopes) 번역 및 요약 초록 (Abstract) 이 PEP는 Python 2.2에 정적으로 중첩된 스코프(Lexical"},{"id":"2025-09-26-pep-0228-reworking-pythons-numeric-model","title":"[Withdrawn] PEP 228 - Reworking Python’s Numeric Model","excerpt":"Python Enhancement Proposal 228: 'Reworking Python’s Numeric Model'에 대한 한국어 번역입니다.","date":"2025-09-26 16:49:50+0900","permalink":"/python/pep/228","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 228 Reworking Python’s Numeric Model 상태: Withdrawn | 유형: Standards Track | 작성일: 04Nov2000 PEP 228 – Python 숫자 모델 재작업 제안 (Reworking Python's Numeric Model) 작성자 : Moshe Zadka, Guido van Rossu"},{"id":"2025-09-26-pep-0229-using-distutils-to-build-python","title":"[Final] PEP 229 - Using Distutils to Build Python","excerpt":"Python Enhancement Proposal 229: 'Using Distutils to Build Python'에 대한 한국어 번역입니다.","date":"2025-09-26 16:51:00+0900","permalink":"/python/pep/229","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 229 Using Distutils to Build Python 상태: Final | 유형: Standards Track | 작성일: 16Nov2000 PEP 229: Distutils를 사용하여 Python 빌드 서론 메커니즘은 다음과 같은 몇 가지 문제점을 가지고 있었습니다: 모든 가능한 모듈을 얻으려면 파일의 특정 부분을 수동으로 "},{"id":"2025-09-26-pep-0230-warning-framework","title":"[Final] PEP 230 - Warning Framework","excerpt":"Python Enhancement Proposal 230: 'Warning Framework'에 대한 한국어 번역입니다.","date":"2025-09-26 16:53:00+0900","permalink":"/python/pep/230","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 230 Warning Framework 상태: Final | 유형: Standards Track | 작성일: 28Nov2000 PEP 230 – 경고 프레임워크 이 문서는 Python Enhancement Proposal (PEP) 230의 내용을 한국어 사용자가 이해하기 쉽게 번역하고 정리한 것입니다. Python 개발자들이 이 PEP"},{"id":"2025-09-26-pep-0231-findattr","title":"[Rejected] PEP 231 - __findattr__()","excerpt":"Python Enhancement Proposal 231: '__findattr__()'에 대한 한국어 번역입니다.","date":"2025-09-26 16:58:10+0900","permalink":"/python/pep/231","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 231 findattr() 상태: Rejected | 유형: Standards Track | 작성일: 30Nov2000 PEP 231 – 에 대한 번역 및 요약 상태: Rejected (거부됨) 작성자: Barry Warsaw 생성일: 2000년 11월 30일 Python 버전: 2.1 소개 PEP 231은 Python 인스턴스의 속성 "},{"id":"2025-09-26-pep-0232-function-attributes","title":"[Final] PEP 232 - Function Attributes","excerpt":"Python Enhancement Proposal 232: 'Function Attributes'에 대한 한국어 번역입니다.","date":"2025-09-26 16:59:42+0900","permalink":"/python/pep/232","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 232 Function Attributes 상태: Final | 유형: Standards Track | 작성일: 02Dec2000 PEP 232 – 함수 속성 (Function Attributes) 서론 (Introduction) 이 PEP(Python Enhancement Proposal)는 함수 및 메서드에 속성 딕셔너리(attrib"},{"id":"2025-09-26-pep-0233-python-online-help","title":"[Deferred] PEP 233 - Python Online Help","excerpt":"Python Enhancement Proposal 233: 'Python Online Help'에 대한 한국어 번역입니다.","date":"2025-09-26 17:00:50+0900","permalink":"/python/pep/233","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 233 Python Online Help 상태: Deferred | 유형: Standards Track | 작성일: 11Dec2000 PEP 233 – Python 온라인 도움말 작성자: Paul Prescod 상태: 연기됨 (Deferred) 유형: 표준 트랙 (Standards Track) 생성일: 2000년 12월 11일 Pytho"},{"id":"2025-09-26-pep-0234-iterators","title":"[Final] PEP 234 - Iterators","excerpt":"Python Enhancement Proposal 234: 'Iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 17:02:22+0900","permalink":"/python/pep/234","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 234 Iterators 상태: Final | 유형: Standards Track | 작성일: 30Jan2001 PEP 234 – Iterators 작성자: KaPing Yee, Guido van Rossum 상태: Final (최종) 유형: Standards Track 생성일: 2001년 1월 30일 Python 버전: 2.1 (Pyt"},{"id":"2025-09-26-pep-0235-import-on-case-insensitive-platforms","title":"[Final] PEP 235 - Import on Case-Insensitive Platforms","excerpt":"Python Enhancement Proposal 235: 'Import on Case-Insensitive Platforms'에 대한 한국어 번역입니다.","date":"2025-09-26 17:03:34+0900","permalink":"/python/pep/235","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 235 Import on CaseInsensitive Platforms 상태: Final | 유형: Standards Track | 작성일: 21Feb2001 PEP 235 – 대소문자 구분 없는 플랫폼에서의 임포트 (Import on CaseInsensitive Platforms) 개요 PEP 235는 파일 시스템이 대소문자를 구분하지"},{"id":"2025-09-26-pep-0236-back-to-the-future","title":"[Final] PEP 236 - Back to the __future__","excerpt":"Python Enhancement Proposal 236: 'Back to the __future__'에 대한 한국어 번역입니다.","date":"2025-09-26 17:05:02+0900","permalink":"/python/pep/236","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 236 Back to the future 상태: Final | 유형: Standards Track | 작성일: 26Feb2001 동기 (Motivation) 때때로 Python은 핵심 언어 구문의 공표된 의미(semantics)에 호환되지 않는 변경을 가하거나, 의도치 않은(구현에 의존하는) 동작을 변경합니다. 이러한 변경은 신중하게 이"},{"id":"2025-09-26-pep-0237-unifying-long-integers-and-integers","title":"[Final] PEP 237 - Unifying Long Integers and Integers","excerpt":"Python Enhancement Proposal 237: 'Unifying Long Integers and Integers'에 대한 한국어 번역입니다.","date":"2025-09-26 17:06:31+0900","permalink":"/python/pep/237","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 237 Unifying Long Integers and Integers 상태: Final | 유형: Standards Track | 작성일: 11Mar2001 PEP 237 – Long Integers와 Integers의 통합 작성자: Moshe Zadka, Guido van Rossum 상태: Final (최종) 유형: Standard"},{"id":"2025-09-26-pep-0238-changing-the-division-operator","title":"[Final] PEP 238 - Changing the Division Operator","excerpt":"Python Enhancement Proposal 238: 'Changing the Division Operator'에 대한 한국어 번역입니다.","date":"2025-09-26 17:08:05+0900","permalink":"/python/pep/238","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 238 Changing the Division Operator 상태: Final | 유형: Standards Track | 작성일: 11Mar2001 PEP 238 – 나눗셈 연산자 변경 (Changing the Division Operator) 개요 (Abstract) 현재 Python의 나눗셈 연산자 ()는 숫자 인수에 대해 모호한 "},{"id":"2025-09-26-pep-0239-adding-a-rational-type-to-python","title":"[Rejected] PEP 239 - Adding a Rational Type to Python","excerpt":"Python Enhancement Proposal 239: 'Adding a Rational Type to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:09:22+0900","permalink":"/python/pep/239","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 239 Adding a Rational Type to Python 상태: Rejected | 유형: Standards Track | 작성일: 11Mar2001 PEP 239 – Python에 유리수 타입 추가 작성자: Christopher A. Craig, Moshe Zadka 상태: 거부됨 (Rejected) 유형: Standards "},{"id":"2025-09-26-pep-0240-adding-a-rational-literal-to-python","title":"[Rejected] PEP 240 - Adding a Rational Literal to Python","excerpt":"Python Enhancement Proposal 240: 'Adding a Rational Literal to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:10:32+0900","permalink":"/python/pep/240","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 240 Adding a Rational Literal to Python 상태: Rejected | 유형: Standards Track | 작성일: 11Mar2001 PEP 240 – Python에 유리수 리터럴 추가 작성자: Christopher A. Craig, Moshe Zadka 상태: Rejected (거부됨) 유형: Standa"},{"id":"2025-09-26-pep-0241-metadata-for-python-software-packages","title":"[Superseded] PEP 241 - Metadata for Python Software Packages","excerpt":"Python Enhancement Proposal 241: 'Metadata for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 17:11:45+0900","permalink":"/python/pep/241","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 241 Metadata for Python Software Packages 상태: Superseded | 유형: Standards Track | 작성일: 12Mar2001 PEP 241은 Python 소프트웨어 패키지에 메타데이터를 추가하는 메커니즘을 설명합니다. 이 문서는 필드 이름, 의미 및 사용법에 대한 세부 사항을 포함합니다. 이"},{"id":"2025-09-26-pep-0242-numeric-kinds","title":"[Withdrawn] PEP 242 - Numeric Kinds","excerpt":"Python Enhancement Proposal 242: 'Numeric Kinds'에 대한 한국어 번역입니다.","date":"2025-09-26 17:13:00+0900","permalink":"/python/pep/242","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 242 Numeric Kinds 상태: Withdrawn | 유형: Standards Track | 작성일: 17Mar2001 PEP 242 – 숫자 종류 (Numeric Kinds) 요약 (Abstract) 이 제안은 사용자가 숫자 연산의 정밀도(precision)와 범위(range)를 선택적으로 제어할 수 있도록 하여, 한 번 작성된"},{"id":"2025-09-26-pep-0243-module-repository-upload-mechanism","title":"[Withdrawn] PEP 243 - Module Repository Upload Mechanism","excerpt":"Python Enhancement Proposal 243: 'Module Repository Upload Mechanism'에 대한 한국어 번역입니다.","date":"2025-09-26 17:14:15+0900","permalink":"/python/pep/243","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 243 Module Repository Upload Mechanism 상태: Withdrawn | 유형: Standards Track | 작성일: 18Mar2001 PEP 243 – 모듈 저장소 업로드 메커니즘 번역 및 요약 이 문서는 Python Enhancement Proposal (PEP) 243, \"모듈 저장소 업로드 메커니즘\"에"},{"id":"2025-09-26-pep-0244-thedirectivestatement","title":"[Rejected] PEP 244 - Thedirectivestatement","excerpt":"Python Enhancement Proposal 244: 'Thedirectivestatement'에 대한 한국어 번역입니다.","date":"2025-09-26 17:15:30+0900","permalink":"/python/pep/244","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 244 Thedirectivestatement 상태: Rejected | 유형: Standards Track | 작성일: 20Mar2001 PEP 244 – 문 (The directive statement) 작성자: Martin von Löwis 상태: 거부됨 (Rejected) 유형: Standards Track 생성일: 2001년 3"},{"id":"2025-09-26-pep-0245-python-interface-syntax","title":"[Rejected] PEP 245 - Python Interface Syntax","excerpt":"Python Enhancement Proposal 245: 'Python Interface Syntax'에 대한 한국어 번역입니다.","date":"2025-09-26 17:16:57+0900","permalink":"/python/pep/245","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 245 Python Interface Syntax 상태: Rejected | 유형: Standards Track | 작성일: 11Jan2001 PEP 245 – Python 인터페이스 문법 (Python Interface Syntax) 작성자: Michel Pelletier 상태: Rejected (거부됨) 유형: Standards Tr"},{"id":"2025-09-26-pep-0246-object-adaptation","title":"[Rejected] PEP 246 - Object Adaptation","excerpt":"Python Enhancement Proposal 246: 'Object Adaptation'에 대한 한국어 번역입니다.","date":"2025-09-26 17:20:56+0900","permalink":"/python/pep/246","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 246 Object Adaptation 상태: Rejected | 유형: Standards Track | 작성일: 21Mar2001 PEP 246 – 객체 어댑테이션 (Object Adaptation) 작성자: Alex Martelli, Clark C. Evans 상태: Rejected (거부됨) 유형: Standards Track 생성"},{"id":"2025-09-26-pep-0247-api-for-cryptographic-hash-functions","title":"[Final] PEP 247 - API for Cryptographic Hash Functions","excerpt":"Python Enhancement Proposal 247: 'API for Cryptographic Hash Functions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:22:10+0900","permalink":"/python/pep/247","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 247 API for Cryptographic Hash Functions 상태: Final | 유형: Informational | 작성일: 23Mar2001 PEP 247 – 암호화 해시 함수용 API 개요 이 문서는 MD5 또는 SHA와 같은 암호화 해싱 알고리즘을 구현하는 다양한 모듈들을 위해 표준화된 API를 정의합니다. 이를 통해"},{"id":"2025-09-26-pep-0248-python-database-api-specification-v1-0","title":"[Final] PEP 248 - Python Database API Specification v1.0","excerpt":"Python Enhancement Proposal 248: 'Python Database API Specification v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:23:30+0900","permalink":"/python/pep/248","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 248 Python Database API Specification v1.0 상태: Final | 유형: Informational | 작성일: 08May1996 서론 (Introduction) 이 API는 데이터베이스에 접근하는 데 사용되는 Python 모듈들 간의 유사성을 장려하기 위해 정의되었습니다. 이를 통해, 더 쉽게 이해할 수 "},{"id":"2025-09-26-pep-0249-python-database-api-specification-v2-0","title":"[Final] PEP 249 - Python Database API Specification v2.0","excerpt":"Python Enhancement Proposal 249: 'Python Database API Specification v2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:25:38+0900","permalink":"/python/pep/249","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 249 Python Database API Specification v2.0 상태: Final | 유형: Informational | 작성일: 12Apr1999 서론 이 API는 데이터베이스 접근에 사용되는 Python 모듈 간의 유사성을 장려하기 위해 정의되었습니다. 이를 통해 일관성을 확보하여 모듈을 더 쉽게 이해하고, 데이터베이스 "},{"id":"2025-09-26-pep-0250-using-site-packages-on-windows","title":"[Final] PEP 250 - Using site-packages on Windows","excerpt":"Python Enhancement Proposal 250: 'Using site-packages on Windows'에 대한 한국어 번역입니다.","date":"2025-09-26 17:26:48+0900","permalink":"/python/pep/250","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 250 Using sitepackages on Windows 상태: Final | 유형: Standards Track | 작성일: 30Mar2001 PEP 250 – Windows에서 사용하기 초록 (Abstract) 표준 Python 배포판에는 디렉토리가 포함되어 있으며, 이는 Unix 플랫폼에서 로컬로 설치된 모듈 및 패키지를 저장하"},{"id":"2025-09-26-pep-0251-python-2-2-release-schedule","title":"[Final] PEP 251 - Python 2.2 Release Schedule","excerpt":"Python Enhancement Proposal 251: 'Python 2.2 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 17:27:59+0900","permalink":"/python/pep/251","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 251 Python 2.2 Release Schedule 상태: Final | 유형: Informational | 작성일: 17Apr2001 PEP 251 – Python 2.2 릴리스 일정 이 문서는 Python 2.2의 개발 및 릴리스 일정을 설명하며, 주로 PEP 규모의 항목들에 초점을 맞춥니다. 작은 버그 수정 및 변경 사항은 첫"},{"id":"2025-09-26-pep-0252-making-types-look-more-like-classes","title":"[Final] PEP 252 - Making Types Look More Like Classes","excerpt":"Python Enhancement Proposal 252: 'Making Types Look More Like Classes'에 대한 한국어 번역입니다.","date":"2025-09-26 17:30:03+0900","permalink":"/python/pep/252","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 252 Making Types Look More Like Classes 상태: Final | 유형: Standards Track | 작성일: 19Apr2001 PEP 252 – 타입을 클래스처럼 보이게 만들기 작성자: Guido van Rossum <guido at python.org 상태: 최종 (Final) 유형: 표준 트랙 (Sta"},{"id":"2025-09-26-pep-0253-subtyping-built-in-types","title":"[Final] PEP 253 - Subtyping Built-in Types","excerpt":"Python Enhancement Proposal 253: 'Subtyping Built-in Types'에 대한 한국어 번역입니다.","date":"2025-09-26 17:32:18+0900","permalink":"/python/pep/253","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 253 Subtyping Builtin Types 상태: Final | 유형: Standards Track | 작성일: 14May2001 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 C 및 Python에서 내장 타입(builtin types)의 서브타입(subtype) 생성을 가능하게 하는 타"},{"id":"2025-09-26-pep-0254-making-classes-look-more-like-types","title":"[Rejected] PEP 254 - Making Classes Look More Like Types","excerpt":"Python Enhancement Proposal 254: 'Making Classes Look More Like Types'에 대한 한국어 번역입니다.","date":"2025-09-26 17:33:28+0900","permalink":"/python/pep/254","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 254 Making Classes Look More Like Types 상태: Rejected | 유형: Standards Track | 작성일: 18Jun2001 PEP 254 – 클래스를 타입처럼 보이게 만들기 (Making Classes Look More Like Types) 요약 이 PEP는 작성되지 않은 상태로 제출된 초안(st"},{"id":"2025-09-26-pep-0255-simple-generators","title":"[Final] PEP 255 - Simple Generators","excerpt":"Python Enhancement Proposal 255: 'Simple Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 17:35:06+0900","permalink":"/python/pep/255","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 255 Simple Generators 상태: Final | 유형: Standards Track | 작성일: 18May2001 PEP 255 – 간단한 제너레이터 (Simple Generators) 초록 (Abstract) 이 PEP는 Python에 제너레이터(generators)의 개념과 함께 사용되는 새로운 문(statement)인 "},{"id":"2025-09-26-pep-0256-docstring-processing-system-framework","title":"[Rejected] PEP 256 - Docstring Processing System Framework","excerpt":"Python Enhancement Proposal 256: 'Docstring Processing System Framework'에 대한 한국어 번역입니다.","date":"2025-09-26 17:36:23+0900","permalink":"/python/pep/256","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 256 Docstring Processing System Framework 상태: Rejected | 유형: Standards Track | 작성일: 01Jun2001 PEP 256 – Docstring 처리 시스템 프레임워크 (Docstring Processing System Framework) 작성자 : David Goodger 상태"},{"id":"2025-09-26-pep-0257-docstring-conventions","title":"[Active] PEP 257 - Docstring Conventions","excerpt":"Python Enhancement Proposal 257: 'Docstring Conventions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:37:43+0900","permalink":"/python/pep/257","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 257 Docstring Conventions 상태: Active | 유형: Informational | 작성일: 29May2001 PEP 257 – Docstring Convention 개요 (Abstract) 이 PEP는 Python docstring(독스트링)과 관련된 의미론 및 컨벤션(관례)을 문서화합니다. 도입 배경 (Ratio"},{"id":"2025-09-26-pep-0258-docutils-design-specification","title":"[Rejected] PEP 258 - Docutils Design Specification","excerpt":"Python Enhancement Proposal 258: 'Docutils Design Specification'에 대한 한국어 번역입니다.","date":"2025-09-26 17:39:51+0900","permalink":"/python/pep/258","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 258 Docutils Design Specification 상태: Rejected | 유형: Standards Track | 작성일: 31May2001 PEP 258 – Docutils 디자인 사양 PEP 258 – Docutils 디자인 사양 번역 및 정리 작성자 (Author) : David Goodger <goodger at py"},{"id":"2025-09-26-pep-0259-omit-printing-newline-after-newline","title":"[Rejected] PEP 259 - Omit printing newline after newline","excerpt":"Python Enhancement Proposal 259: 'Omit printing newline after newline'에 대한 한국어 번역입니다.","date":"2025-09-26 17:41:02+0900","permalink":"/python/pep/259","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 259 Omit printing newline after newline 상태: Rejected | 유형: Standards Track | 작성일: 11Jun2001 PEP 259 – 개행 문자 뒤에 개행 문자 출력 생략 (Omit printing newline after newline) 작성자: Guido van Rossum 상태: Re"},{"id":"2025-09-26-pep-0260-simplify-xrange","title":"[Final] PEP 260 - Simplify xrange()","excerpt":"Python Enhancement Proposal 260: 'Simplify xrange()'에 대한 한국어 번역입니다.","date":"2025-09-26 17:42:10+0900","permalink":"/python/pep/260","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 260 Simplify xrange() 상태: Final | 유형: Standards Track | 작성일: 26Jun2001 PEP 260 – 단순화 요약 (Abstract) 이 PEP는 객체에서 나 와 같이 거의 사용되지 않는 일부 동작을 제거할 것을 제안합니다. 문제점 (Problem) 함수는 주로 다음과 같은 한 가지 용도로 사용"},{"id":"2025-09-26-pep-0261-support-for-wide-unicode-characters","title":"[Final] PEP 261 - Support for “wide” Unicode characters","excerpt":"Python Enhancement Proposal 261: 'Support for “wide” Unicode characters'에 대한 한국어 번역입니다.","date":"2025-09-26 17:43:32+0900","permalink":"/python/pep/261","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 261 Support for “wide” Unicode characters 상태: Final | 유형: Standards Track | 작성일: 27Jun2001 PEP 261 – \"와이드(Wide)\" 유니코드 문자 지원 개요 (Abstract) 이 PEP(Python Enhancement Proposal) 261은 Python 2.2에"},{"id":"2025-09-26-pep-0262-a-database-of-installed-python-packages","title":"[Rejected] PEP 262 - A Database of Installed Python Packages","excerpt":"Python Enhancement Proposal 262: 'A Database of Installed Python Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 17:44:46+0900","permalink":"/python/pep/262","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 262 A Database of Installed Python Packages 상태: Rejected | 유형: Standards Track | 작성일: 08Jul2001 PEP 262 – 설치된 Python 패키지 데이터베이스 개요 이 PEP는 시스템에 설치된 Python 소프트웨어의 데이터베이스 형식을 설명합니다. 참고: 이 PEP는"},{"id":"2025-09-26-pep-0263-defining-python-source-code-encodings","title":"[Final] PEP 263 - Defining Python Source Code Encodings","excerpt":"Python Enhancement Proposal 263: 'Defining Python Source Code Encodings'에 대한 한국어 번역입니다.","date":"2025-09-26 17:46:12+0900","permalink":"/python/pep/263","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 263 Defining Python Source Code Encodings 상태: Final | 유형: Standards Track | 작성일: 06Jun2001 PEP 263: Python 소스 코드 인코딩 정의 개요 이 PEP는 Python 소스 파일의 인코딩을 선언하는 문법을 도입할 것을 제안합니다. 이 인코딩 정보는 Python "},{"id":"2025-09-26-pep-0264-future-statements-in-simulated-shells","title":"[Final] PEP 264 - Future statements in simulated shells","excerpt":"Python Enhancement Proposal 264: 'Future statements in simulated shells'에 대한 한국어 번역입니다.","date":"2025-09-26 17:47:23+0900","permalink":"/python/pep/264","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 264 Future statements in simulated shells 상태: Final | 유형: Standards Track | 작성일: 30Jul2001 PEP 264: 시뮬레이션 셸에서의 문 (Future statements in simulated shells) 개요 PEP 264는 \"시뮬레이션 대화형 셸\" (simulated"},{"id":"2025-09-26-pep-0265-sorting-dictionaries-by-value","title":"[Rejected] PEP 265 - Sorting Dictionaries by Value","excerpt":"Python Enhancement Proposal 265: 'Sorting Dictionaries by Value'에 대한 한국어 번역입니다.","date":"2025-09-26 17:48:48+0900","permalink":"/python/pep/265","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 265 Sorting Dictionaries by Value 상태: Rejected | 유형: Standards Track | 작성일: 08Aug2001 PEP 265 – 값(Value)으로 딕셔너리 정렬 1. 개요 (Abstract) 이 PEP는 딕셔너리에 \"값(value)으로 정렬\"하는 기능을 추가할 것을 제안합니다. 이 제안의 주된"},{"id":"2025-09-26-pep-0266-optimizing-global-variableattribute-access","title":"[Withdrawn] PEP 266 - Optimizing Global Variable/Attribute Access","excerpt":"Python Enhancement Proposal 266: 'Optimizing Global Variable/Attribute Access'에 대한 한국어 번역입니다.","date":"2025-09-26 17:49:31+0900","permalink":"/python/pep/266","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 266 Optimizing Global Variable/Attribute Access 상태: Withdrawn | 유형: Standards Track | 작성일: 13Aug2001 PEP 266 – Optimizing Global Variable/Attribute Access 작성자: Skip Montanaro <skip at pobox"},{"id":"2025-09-26-pep-0267-optimized-access-to-module-namespaces","title":"[Deferred] PEP 267 - Optimized Access to Module Namespaces","excerpt":"Python Enhancement Proposal 267: 'Optimized Access to Module Namespaces'에 대한 한국어 번역입니다.","date":"2025-09-26 17:50:38+0900","permalink":"/python/pep/267","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 267 Optimized Access to Module Namespaces 상태: Deferred | 유형: Standards Track | 작성일: 23May2001 PEP 267 – 모듈 네임스페이스에 대한 접근 최적화 작성자: Jeremy Hylton 상태: 연기됨 (Deferred) 유형: 표준 트랙 (Standards Track"},{"id":"2025-09-26-pep-0268-extended-http-functionality-and-webdav","title":"[Rejected] PEP 268 - Extended HTTP functionality and WebDAV","excerpt":"Python Enhancement Proposal 268: 'Extended HTTP functionality and WebDAV'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:10+0900","permalink":"/python/pep/268","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 268 Extended HTTP functionality and WebDAV 상태: Rejected | 유형: Standards Track | 작성일: 20Aug2001 PEP 268 – 확장된 HTTP 기능 및 WebDAV 저자: Greg Stein 상태: Rejected (거부됨) 유형: Standards Track 생성일: 2001"},{"id":"2025-09-26-pep-0269-pgen-module-for-python","title":"[Deferred] PEP 269 - Pgen Module for Python","excerpt":"Python Enhancement Proposal 269: 'Pgen Module for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:28+0900","permalink":"/python/pep/269","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 269 Pgen Module for Python 상태: Deferred | 유형: Standards Track | 작성일: 24Aug2001 PEP 269 – Python용 Pgen 모듈 개요 PEP 269는 Python 파서를 생성하는 데 사용되는 파서 제너레이터(parser generator)를 Python 모듈로 노출할 것을 제안합"},{"id":"2025-09-26-pep-0270-uniq-method-for-list-objects","title":"[Rejected] PEP 270 - uniq method for list objects","excerpt":"Python Enhancement Proposal 270: 'uniq method for list objects'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:43+0900","permalink":"/python/pep/270","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 270 uniq method for list objects 상태: Rejected | 유형: Standards Track | 작성일: 21Aug2001 PEP 270: 객체를 위한 메서드 작성자: Jason Petrone 상태: Rejected (거절됨) 유형: Standards Track 생성일: 2001년 8월 21일 Python 버"},{"id":"2025-09-26-pep-0271-prefixing-sys-path-by-command-line-option","title":"[Rejected] PEP 271 - Prefixing sys.path by command line option","excerpt":"Python Enhancement Proposal 271: 'Prefixing sys.path by command line option'에 대한 한국어 번역입니다.","date":"2025-09-26 17:51:56+0900","permalink":"/python/pep/271","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 271 Prefixing sys.path by command line option 상태: Rejected | 유형: Standards Track | 작성일: 15Aug2001 PEP 271 – 에 명령줄 옵션으로 접두사 추가 작성자: Frédéric B. Giacometti 상태: 거부됨 (Rejected) 유형: 표준 트랙 (Stand"},{"id":"2025-09-26-pep-0272-api-for-block-encryption-algorithms-v1-0","title":"[Final] PEP 272 - API for Block Encryption Algorithms v1.0","excerpt":"Python Enhancement Proposal 272: 'API for Block Encryption Algorithms v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 17:52:18+0900","permalink":"/python/pep/272","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 272 API for Block Encryption Algorithms v1.0 상태: Final | 유형: Informational | 작성일: 18Sep2001 파이썬 PEP 272: 블록 암호화 알고리즘을 위한 API v1.0 개요 이 문서는 DES 또는 Rijndael과 같은 비밀 키(secretkey) 블록 암호화 알고리즘을 위"},{"id":"2025-09-26-pep-0273-import-modules-from-zip-archives","title":"[Final] PEP 273 - Import Modules from Zip Archives","excerpt":"Python Enhancement Proposal 273: 'Import Modules from Zip Archives'에 대한 한국어 번역입니다.","date":"2025-09-26 17:52:44+0900","permalink":"/python/pep/273","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 273 Import Modules from Zip Archives 상태: Final | 유형: Standards Track | 작성일: 11Oct2001 개요 이 문서는 Python Enhancement Proposal (PEP) 273, \"Import Modules from Zip Archives\"의 내용을 한국어 사용자가 이해하기 쉽"},{"id":"2025-09-26-pep-0274-dict-comprehensions","title":"[Final] PEP 274 - Dict Comprehensions","excerpt":"Python Enhancement Proposal 274: 'Dict Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:53:00+0900","permalink":"/python/pep/274","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 274 Dict Comprehensions 상태: Final | 유형: Standards Track | 작성일: 25Oct2001 PEP 274 – Dict Comprehensions (딕셔너리 컴프리헨션) 개요 PEP 274는 Python에 과 유사한 문법적 확장으로 \"dictionary comprehension\" 또는 줄여서 \"dic"},{"id":"2025-09-26-pep-0275-switching-on-multiple-values","title":"[Rejected] PEP 275 - Switching on Multiple Values","excerpt":"Python Enhancement Proposal 275: 'Switching on Multiple Values'에 대한 한국어 번역입니다.","date":"2025-09-26 17:53:28+0900","permalink":"/python/pep/275","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 275 Switching on Multiple Values 상태: Rejected | 유형: Standards Track | 작성일: 10Nov2001 PEP 275 – 다중 값 스위칭 (Switching on Multiple Values) 저자: MarcAndré Lemburg 상태: Rejected (거절됨) 유형: Standards"},{"id":"2025-09-26-pep-0276-simple-iterator-for-ints","title":"[Rejected] PEP 276 - Simple Iterator for ints","excerpt":"Python Enhancement Proposal 276: 'Simple Iterator for ints'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:04+0900","permalink":"/python/pep/276","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 276 Simple Iterator for ints 상태: Rejected | 유형: Standards Track | 작성일: 12Nov2001 PEP 276 – 를 위한 단순 이터레이터 작성자: Jim Althoff 상태: 거부됨 (Rejected) 유형: 표준 트랙 (Standards Track) 생성일: 2001년 11월 12일 P"},{"id":"2025-09-26-pep-0277-unicode-file-name-support-for-windows-nt","title":"[Final] PEP 277 - Unicode file name support for Windows NT","excerpt":"Python Enhancement Proposal 277: 'Unicode file name support for Windows NT'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:18+0900","permalink":"/python/pep/277","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 277 Unicode file name support for Windows NT 상태: Final | 유형: Standards Track | 작성일: 11Jan2002 PEP 277: Windows NT용 유니코드 파일명 지원 요약 (Abstract) 이 PEP는 Windows NT 운영체제에서 유니코드 파일명을 시스템의 와이드 문자(w"},{"id":"2025-09-26-pep-0278-universal-newline-support","title":"[Final] PEP 278 - Universal Newline Support","excerpt":"Python Enhancement Proposal 278: 'Universal Newline Support'에 대한 한국어 번역입니다.","date":"2025-09-26 17:54:43+0900","permalink":"/python/pep/278","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 278 Universal Newline Support 상태: Final | 유형: Standards Track | 작성일: 14Jan2002 PEP 278 – 유니버설 뉴라인 지원 개요 이 PEP는 Python이 현재 플랫폼의 기본 줄바꿈 (newline) 형식이 아닌 파일에서도 I/O를 지원하는 방법을 제안합니다. 이를 통해 각 플랫폼"},{"id":"2025-09-26-pep-0279-the-enumerate-built-in-function","title":"[Final] PEP 279 - The enumerate() built-in function","excerpt":"Python Enhancement Proposal 279: 'The enumerate() built-in function'에 대한 한국어 번역입니다.","date":"2025-09-26 17:55:04+0900","permalink":"/python/pep/279","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 279 The enumerate() builtin function 상태: Final | 유형: Standards Track | 작성일: 30Jan2002 PEP 279 – 내장 함수 작성자: Raymond Hettinger 상태: 최종 (Final) 유형: 표준 트랙 (Standards Track) 생성일: 2002년 1월 30일 Pyt"},{"id":"2025-09-26-pep-0280-optimizing-access-to-globals","title":"[Deferred] PEP 280 - Optimizing access to globals","excerpt":"Python Enhancement Proposal 280: 'Optimizing access to globals'에 대한 한국어 번역입니다.","date":"2025-09-26 17:55:42+0900","permalink":"/python/pep/280","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 280 Optimizing access to globals 상태: Deferred | 유형: Standards Track | 작성일: 10Feb2002 python class cell(object): def init(self): self.objptr = NULL self.cellptr = NULL python class celldict("},{"id":"2025-09-26-pep-0281-loop-counter-iteration-with-range-and-xrange","title":"[Rejected] PEP 281 - Loop Counter Iteration with range and xrange","excerpt":"Python Enhancement Proposal 281: 'Loop Counter Iteration with range and xrange'에 대한 한국어 번역입니다.","date":"2025-09-26 17:56:00+0900","permalink":"/python/pep/281","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 281 Loop Counter Iteration with range and xrange 상태: Rejected | 유형: Standards Track | 작성일: 11Feb2002 PEP 281 – 및 를 이용한 루프 카운터 반복 (Loop Counter Iteration with range and xrange) 작성자: Magnus L"},{"id":"2025-09-26-pep-0282-a-logging-system","title":"[Final] PEP 282 - A Logging System","excerpt":"Python Enhancement Proposal 282: 'A Logging System'에 대한 한국어 번역입니다.","date":"2025-09-26 17:56:46+0900","permalink":"/python/pep/282","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 282 A Logging System 상태: Final | 유형: Standards Track | 작성일: 04Feb2002 파이썬 PEP 282 – 로깅 시스템 (A Logging System) 초록 (Abstract) 이 PEP(Python Enhancement Proposal)는 파이썬 표준 라이브러리에 제안된 로깅 패키지를 설명합"},{"id":"2025-09-26-pep-0283-python-2-3-release-schedule","title":"[Final] PEP 283 - Python 2.3 Release Schedule","excerpt":"Python Enhancement Proposal 283: 'Python 2.3 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 17:57:14+0900","permalink":"/python/pep/283","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 283 Python 2.3 Release Schedule 상태: Final | 유형: Informational | 작성일: 27Feb2002 PEP 283 – Python 2.3 릴리스 일정 작성자: Guido van Rossum 상태: Final (최종) 유형: Informational (정보성) 주제: Release (릴리스) 생성일"},{"id":"2025-09-26-pep-0284-integer-for-loops","title":"[Rejected] PEP 284 - Integer for-loops","excerpt":"Python Enhancement Proposal 284: 'Integer for-loops'에 대한 한국어 번역입니다.","date":"2025-09-26 17:57:40+0900","permalink":"/python/pep/284","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 284 Integer forloops 상태: Rejected | 유형: Standards Track | 작성일: 01Mar2002 PEP 284 – 정수 루프 작성자: David Eppstein, Gregory Ewing 상태: Rejected (거부됨) 유형: Standards Track 생성일: 2002년 3월 1일 Python 버전"},{"id":"2025-09-26-pep-0285-adding-a-bool-type","title":"[Final] PEP 285 - Adding a bool type","excerpt":"Python Enhancement Proposal 285: 'Adding a bool type'에 대한 한국어 번역입니다.","date":"2025-09-26 17:58:00+0900","permalink":"/python/pep/285","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 285 Adding a bool type 상태: Final | 유형: Standards Track | 작성일: 08Mar2002 PEP 285 – 타입 추가 제안 개요 PEP 285는 Python에 새로운 내장(builtin) 타입인 을 도입하고, 두 개의 상수 와 를 추가할 것을 제안합니다. 이 타입은 내부적으로 타입의 서브타입(sub"},{"id":"2025-09-26-pep-0286-enhanced-argument-tuples","title":"[Deferred] PEP 286 - Enhanced Argument Tuples","excerpt":"Python Enhancement Proposal 286: 'Enhanced Argument Tuples'에 대한 한국어 번역입니다.","date":"2025-09-26 17:58:16+0900","permalink":"/python/pep/286","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 286 Enhanced Argument Tuples 상태: Deferred | 유형: Standards Track | 작성일: 03Mar2002 PEP 286: Enhanced Argument Tuples 개요 PEP 286은 함수가 인자 변환기(argument converter)를 통해 새로운 메모리를 생성할 때 발생하는 어려운 메모리"},{"id":"2025-09-26-pep-0287-restructuredtext-docstring-format","title":"[Active] PEP 287 - reStructuredText Docstring Format","excerpt":"Python Enhancement Proposal 287: 'reStructuredText Docstring Format'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:15+0900","permalink":"/python/pep/287","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 287 reStructuredText Docstring Format 상태: Active | 유형: Informational | 작성일: 25Mar2002 PEP 287 – reStructuredText Docstring Format 개요 (Abstract) 이 PEP는 Python docstring, PEPs 및 관련 문서에서 구조화된 "},{"id":"2025-09-26-pep-0288-generators-attributes-and-exceptions","title":"[Withdrawn] PEP 288 - Generators Attributes and Exceptions","excerpt":"Python Enhancement Proposal 288: 'Generators Attributes and Exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:36+0900","permalink":"/python/pep/288","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 288 Generators Attributes and Exceptions 상태: Withdrawn | 유형: Standards Track | 작성일: 21Mar2002 PEP 288 – 제너레이터 속성 및 예외 (Generators Attributes and Exceptions) 작성자: Raymond Hettinger <python a"},{"id":"2025-09-26-pep-0289-generator-expressions","title":"[Final] PEP 289 - Generator Expressions","excerpt":"Python Enhancement Proposal 289: 'Generator Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 17:59:59+0900","permalink":"/python/pep/289","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 289 Generator Expressions 상태: Final | 유형: Standards Track | 작성일: 30Jan2002 PEP 289 – 제너레이터 표현식 (Generator Expressions) 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 (PEP 202)과 (PEP 255)"},{"id":"2025-09-26-pep-0290-code-migration-and-modernization","title":"[Active] PEP 290 - Code Migration and Modernization","excerpt":"Python Enhancement Proposal 290: 'Code Migration and Modernization'에 대한 한국어 번역입니다.","date":"2025-09-26 18:00:45+0900","permalink":"/python/pep/290","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 290 Code Migration and Modernization 상태: Active | 유형: Informational | 작성일: 06Jun2002 PEP 290 – 코드 마이그레이션 및 현대화 작성자: Raymond Hettinger <python at rcn.com 상태: Active (활성) 유형: Informational (정"},{"id":"2025-09-26-pep-0291-backward-compatibility-for-the-python-2-standard-library","title":"[Superseded] PEP 291 - Backward Compatibility for the Python 2 Standard Library","excerpt":"Python Enhancement Proposal 291: 'Backward Compatibility for the Python 2 Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:05+0900","permalink":"/python/pep/291","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 291 Backward Compatibility for the Python 2 Standard Library 상태: Superseded | 유형: Informational | 작성일: 06Jun2002 PEP 291 – Python 2 표준 라이브러리의 하위 호환성 (Backward Compatibility for the Python 2"},{"id":"2025-09-26-pep-0292-simpler-string-substitutions","title":"[Final] PEP 292 - Simpler String Substitutions","excerpt":"Python Enhancement Proposal 292: 'Simpler String Substitutions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:32+0900","permalink":"/python/pep/292","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 292 Simpler String Substitutions 상태: Final | 유형: Standards Track | 작성일: 18Jun2002 PEP 292 – 더 간단한 문자열 치환 (Simpler String Substitutions) 작성자: Barry Warsaw <barry at python.org 상태: Final 유형: "},{"id":"2025-09-26-pep-0293-codec-error-handling-callbacks","title":"[Final] PEP 293 - Codec Error Handling Callbacks","excerpt":"Python Enhancement Proposal 293: 'Codec Error Handling Callbacks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:04:55+0900","permalink":"/python/pep/293","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 293 Codec Error Handling Callbacks 상태: Final | 유형: Standards Track | 작성일: 18Jun2002 PEP 293 – 코덱 에러 핸들링 콜백 작성자 : Walter Dörwald 상태 : Final (최종) 타입 : Standards Track (표준 트랙) 생성일 : 2002년 6월 1"},{"id":"2025-09-26-pep-0294-type-names-in-the-types-module","title":"[Rejected] PEP 294 - Type Names in the types Module","excerpt":"Python Enhancement Proposal 294: 'Type Names in the types Module'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:08+0900","permalink":"/python/pep/294","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 294 Type Names in the types Module 상태: Rejected | 유형: Standards Track | 작성일: 19Jun2002 PEP 294 – 모듈의 타입 이름 작성자: Oren Tirosh 상태: Rejected (거부됨) 유형: Standards Track 생성일: 2002년 6월 19일 Python 버"},{"id":"2025-09-26-pep-0295-interpretation-of-multiline-string-constants","title":"[Rejected] PEP 295 - Interpretation of multiline string constants","excerpt":"Python Enhancement Proposal 295: 'Interpretation of multiline string constants'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:22+0900","permalink":"/python/pep/295","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 295 Interpretation of multiline string constants 상태: Rejected | 유형: Standards Track | 작성일: 22Jul2002 PEP 295 – 여러 줄 문자열 상수의 해석 상태 : Rejected (거부됨) 유형 : Standards Track 생성일 : 2002년 7월 22일 Py"},{"id":"2025-09-26-pep-0296-adding-a-bytes-object-type","title":"[Withdrawn] PEP 296 - Adding a bytes Object Type","excerpt":"Python Enhancement Proposal 296: 'Adding a bytes Object Type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:05:57+0900","permalink":"/python/pep/296","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 296 Adding a bytes Object Type 상태: Withdrawn | 유형: Standards Track | 작성일: 12Jul2002 PEP 296은 Python 2.3 버전을 위해 객체 타입 추가를 제안했던 문서입니다. 이 제안은 나중에 PEP 358로 대체되어 철회(Withdrawn)되었습니다. PEP 296 – 객체"},{"id":"2025-09-26-pep-0297-support-for-system-upgrades","title":"[Rejected] PEP 297 - Support for System Upgrades","excerpt":"Python Enhancement Proposal 297: 'Support for System Upgrades'에 대한 한국어 번역입니다.","date":"2025-09-26 18:06:11+0900","permalink":"/python/pep/297","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 297 Support for System Upgrades 상태: Rejected | 유형: Standards Track | 작성일: 19Jul2001 PEP 297 – 시스템 업그레이드 지원 작성자: MarcAndré Lemburg <mal at lemburg.com 상태: Rejected (거부됨) 유형: Standards Track "},{"id":"2025-09-26-pep-0298-the-locked-buffer-interface","title":"[Withdrawn] PEP 298 - The Locked Buffer Interface","excerpt":"Python Enhancement Proposal 298: 'The Locked Buffer Interface'에 대한 한국어 번역입니다.","date":"2025-09-26 18:06:42+0900","permalink":"/python/pep/298","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 298 The Locked Buffer Interface 상태: Withdrawn | 유형: Standards Track | 작성일: 26Jul2002 PEP 298 – 잠금된 버퍼 인터페이스 개요 이 문서는 Python 2.3 버전에서 제안된 '잠금된 버퍼 인터페이스(Locked Buffer Interface)'에 대한 PEP(Pyth"},{"id":"2025-09-26-pep-0299-special-main-function-in-modules","title":"[Rejected] PEP 299 - Special __main__() function in modules","excerpt":"Python Enhancement Proposal 299: 'Special __main__() function in modules'에 대한 한국어 번역입니다.","date":"2025-09-26 18:07:00+0900","permalink":"/python/pep/299","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 299 Special main() function in modules 상태: Rejected | 유형: Standards Track | 작성일: 12Aug2002 PEP 299 – 모듈 내 특별한 함수 작성자 (Author): Jeff Epler <jepler at unpythonic.net 상태 (Status): 반려됨 (Rejecte"},{"id":"2025-09-26-pep-0301-package-index-and-metadata-for-distutils","title":"[Final] PEP 301 - Package Index and Metadata for Distutils","excerpt":"Python Enhancement Proposal 301: 'Package Index and Metadata for Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 18:07:29+0900","permalink":"/python/pep/301","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 301 Package Index and Metadata for Distutils 상태: Final | 유형: Standards Track | 작성일: 24Oct2002 PEP 301 – Distutils를 위한 패키지 인덱스 및 메타데이터 초록 (Abstract) 이 PEP는 Distutils 패키징 시스템에 몇 가지 확장 기능을 제안합"},{"id":"2025-09-26-pep-0302-new-import-hooks","title":"[Final] PEP 302 - New Import Hooks","excerpt":"Python Enhancement Proposal 302: 'New Import Hooks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:08:12+0900","permalink":"/python/pep/302","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 302 New Import Hooks 상태: Final | 유형: Standards Track | 작성일: 19Dec2002 PEP 302 – 새로운 임포트 훅 (New Import Hooks) 작성자 : Just van Rossum, Paul Moore 상태 : Final (최종) 유형 : Standards Track (표준 트랙) 작"},{"id":"2025-09-26-pep-0303-extend-divmod-for-multiple-divisors","title":"[Rejected] PEP 303 - Extend divmod() for Multiple Divisors","excerpt":"Python Enhancement Proposal 303: 'Extend divmod() for Multiple Divisors'에 대한 한국어 번역입니다.","date":"2025-09-26 18:08:34+0900","permalink":"/python/pep/303","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 303 Extend divmod() for Multiple Divisors 상태: Rejected | 유형: Standards Track | 작성일: 31Dec2002 PEP 303 – Extend divmod() for Multiple Divisors (다중 제수를 위한 divmod() 확장) 개요 이 PEP는 내장 함수 의 확장을 제"},{"id":"2025-09-26-pep-0304-controlling-generation-of-bytecode-files","title":"[Withdrawn] PEP 304 - Controlling Generation of Bytecode Files","excerpt":"Python Enhancement Proposal 304: 'Controlling Generation of Bytecode Files'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:05+0900","permalink":"/python/pep/304","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 304 Controlling Generation of Bytecode Files 상태: Withdrawn | 유형: Standards Track | 작성일: 22Jan2003 PEP 304 – 바이트코드 파일 생성 제어 (Controlling Generation of Bytecode Files) 개요 이 PEP는 컴파일된 Python 바"},{"id":"2025-09-26-pep-0305-csv-file-api","title":"[Final] PEP 305 - CSV File API","excerpt":"Python Enhancement Proposal 305: 'CSV File API'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:42+0900","permalink":"/python/pep/305","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 305 CSV File API 상태: Final | 유형: Standards Track | 작성일: 26Jan2003 PEP 305 – CSV 파일 API 작성자: Kevin Altis, Dave Cole, Andrew McNamara, Skip Montanaro, Cliff Wells 논의처: Csv list 상태: Final (최종)"},{"id":"2025-09-26-pep-0306-how-to-change-pythons-grammar","title":"[Withdrawn] PEP 306 - How to Change Python’s Grammar","excerpt":"Python Enhancement Proposal 306: 'How to Change Python’s Grammar'에 대한 한국어 번역입니다.","date":"2025-09-26 18:09:57+0900","permalink":"/python/pep/306","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 306 How to Change Python’s Grammar 상태: Withdrawn | 유형: Informational | 작성일: 29Jan2003 PEP 306 – Python 문법 변경 방법 작성자: Michael Hudson, Jack Diederich, Alyssa Coghlan, Benjamin Peterson 상태: 철회"},{"id":"2025-09-26-pep-0307-extensions-to-the-pickle-protocol","title":"[Final] PEP 307 - Extensions to the pickle protocol","excerpt":"Python Enhancement Proposal 307: 'Extensions to the pickle protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 18:10:28+0900","permalink":"/python/pep/307","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 307 Extensions to the pickle protocol 상태: Final | 유형: Standards Track | 작성일: 31Jan2003 PEP 307 – pickle 프로토콜 확장 개요 이 PEP는 Python 2.3에서 도입된 새로운 pickle 프로토콜을 설명하며, 기존 객체들의 처리 문제를 해결하고 크기를 최적화"},{"id":"2025-09-26-pep-0308-conditional-expressions","title":"[Final] PEP 308 - Conditional Expressions","excerpt":"Python Enhancement Proposal 308: 'Conditional Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:10:46+0900","permalink":"/python/pep/308","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 308 Conditional Expressions 상태: Final | 유형: Standards Track | 작성일: 07Feb2003 PEP 308 – 조건부 표현식 (Conditional Expressions) 번역 및 요약 개요 PEP 308은 Python에 조건부 표현식을 추가하는 제안입니다. 이 PEP는 \"X if C else"},{"id":"2025-09-26-pep-0309-partial-function-application","title":"[Final] PEP 309 - Partial Function Application","excerpt":"Python Enhancement Proposal 309: 'Partial Function Application'에 대한 한국어 번역입니다.","date":"2025-09-26 18:11:12+0900","permalink":"/python/pep/309","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 309 Partial Function Application 상태: Final | 유형: Standards Track | 작성일: 08Feb2003 PEP 309 – Partial Function Application (부분 함수 적용) 요약 이 PEP는 호출 가능한(callable) 객체와 부분적인 인자 리스트(위치 인자 및 키워드 인자"},{"id":"2025-09-26-pep-0310-reliable-acquisitionrelease-pairs","title":"[Rejected] PEP 310 - Reliable Acquisition/Release Pairs","excerpt":"Python Enhancement Proposal 310: 'Reliable Acquisition/Release Pairs'에 대한 한국어 번역입니다.","date":"2025-09-26 18:11:37+0900","permalink":"/python/pep/310","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 310 Reliable Acquisition/Release Pairs 상태: Rejected | 유형: Standards Track | 작성일: 18Dec2002 PEP 310 – 신뢰할 수 있는 자원 획득/해제 쌍 작성자: Michael Hudson, Paul Moore 상태: Rejected (거부됨) 유형: Standards Tra"},{"id":"2025-09-26-pep-0311-simplified-global-interpreter-lock-acquisition-for-extensions","title":"[Final] PEP 311 - Simplified Global Interpreter Lock Acquisition for Extensions","excerpt":"Python Enhancement Proposal 311: 'Simplified Global Interpreter Lock Acquisition for Extensions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:03+0900","permalink":"/python/pep/311","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 311 Simplified Global Interpreter Lock Acquisition for Extensions 상태: Final | 유형: Standards Track | 작성일: 05Feb2003 다음은 PEP 311 문서의 내용을 한국어 사용자가 이해하기 쉽게 번역하고 정리한 것입니다. PEP 311 – 확장 모듈을 위한 GI"},{"id":"2025-09-26-pep-0312-simple-implicit-lambda","title":"[Deferred] PEP 312 - Simple Implicit Lambda","excerpt":"Python Enhancement Proposal 312: 'Simple Implicit Lambda'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:24+0900","permalink":"/python/pep/312","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 312 Simple Implicit Lambda 상태: Deferred | 유형: Standards Track | 작성일: 11Feb2003 PEP 312 – 간단한 암묵적 람다 (Simple Implicit Lambda) 번역 및 요약 개요 이 문서는 Python Enhancement Proposal (PEP) 312의 내용을 한국어 "},{"id":"2025-09-26-pep-0313-adding-roman-numeral-literals-to-python","title":"[Rejected] PEP 313 - Adding Roman Numeral Literals to Python","excerpt":"Python Enhancement Proposal 313: 'Adding Roman Numeral Literals to Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:12:44+0900","permalink":"/python/pep/313","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 313 Adding Roman Numeral Literals to Python 상태: Rejected | 유형: Standards Track | 작성일: 01Apr2003 PEP 313 – Python에 로마 숫자 리터럴 추가 제안 작성자: Mike Meyer <mwm at mired.org 상태: Rejected (거부됨) 유형: St"},{"id":"2025-09-26-pep-0314-metadata-for-python-software-packages-1-1","title":"[Superseded] PEP 314 - Metadata for Python Software Packages 1.1","excerpt":"Python Enhancement Proposal 314: 'Metadata for Python Software Packages 1.1'에 대한 한국어 번역입니다.","date":"2025-09-26 18:13:21+0900","permalink":"/python/pep/314","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 314 Metadata for Python Software Packages 1.1 상태: Superseded | 유형: Standards Track | 작성일: 12Apr2003 PEP 314 – Python 소프트웨어 패키지용 메타데이터 1.1 목표 이 문서는 Python 패키지에 메타데이터를 추가하는 메커니즘을 설명합니다. 필드 이름"},{"id":"2025-09-26-pep-0315-enhanced-while-loop","title":"[Rejected] PEP 315 - Enhanced While Loop","excerpt":"Python Enhancement Proposal 315: 'Enhanced While Loop'에 대한 한국어 번역입니다.","date":"2025-09-26 18:13:38+0900","permalink":"/python/pep/315","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 315 Enhanced While Loop 상태: Rejected | 유형: Standards Track | 작성일: 25Apr2003 개요 (Abstract) 이 PEP는 루프의 시작 부분에 선택적인 \"do\" 절을 추가하여 루프 코드를 더 명확하게 만들고, 코드 중복으로 인한 오류를 줄이는 것을 제안합니다. 공지 (Notice) 이 P"},{"id":"2025-09-26-pep-0316-programming-by-contract-for-python","title":"[Deferred] PEP 316 - Programming by Contract for Python","excerpt":"Python Enhancement Proposal 316: 'Programming by Contract for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:27:40+0900","permalink":"/python/pep/316","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 316 Programming by Contract for Python 상태: Deferred | 유형: Standards Track | 작성일: 02May2003 PEP 316 – Python을 위한 계약 기반 프로그래밍 (Programming by Contract for Python) 개요 이 문서는 Python에서 \"계약 기반 프로그"},{"id":"2025-09-26-pep-0317-eliminate-implicit-exception-instantiation","title":"[Rejected] PEP 317 - Eliminate Implicit Exception Instantiation","excerpt":"Python Enhancement Proposal 317: 'Eliminate Implicit Exception Instantiation'에 대한 한국어 번역입니다.","date":"2025-09-26 18:28:16+0900","permalink":"/python/pep/317","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 317 Eliminate Implicit Exception Instantiation 상태: Rejected | 유형: Standards Track | 작성일: 06May2003 PEP 317 – 암시적 예외 인스턴스화 제거 (Eliminate Implicit Exception Instantiation) 저자: Steven Taschuk "},{"id":"2025-09-26-pep-0318-decorators-for-functions-and-methods","title":"[Final] PEP 318 - Decorators for Functions and Methods","excerpt":"Python Enhancement Proposal 318: 'Decorators for Functions and Methods'에 대한 한국어 번역입니다.","date":"2025-09-26 18:28:53+0900","permalink":"/python/pep/318","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 318 Decorators for Functions and Methods 상태: Final | 유형: Standards Track | 작성일: 05Jun2003 PEP 318 – 함수 및 메서드를 위한 데코레이터 (Decorators for Functions and Methods) 개요 (Abstract) 기존에는 함수나 메서드를 변형("},{"id":"2025-09-26-pep-0319-python-synchronizeasynchronize-block","title":"[Rejected] PEP 319 - Python Synchronize/Asynchronize Block","excerpt":"Python Enhancement Proposal 319: 'Python Synchronize/Asynchronize Block'에 대한 한국어 번역입니다.","date":"2025-09-26 18:29:20+0900","permalink":"/python/pep/319","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 319 Python Synchronize/Asynchronize Block 상태: Rejected | 유형: Standards Track | 작성일: 24Feb2003 PEP 319 – Python Synchronize/Asynchronize 블록 상태: 거부됨 (Rejected) 작성자: Michel Pelletier 생성일: 2003"},{"id":"2025-09-26-pep-0320-python-2-4-release-schedule","title":"[Final] PEP 320 - Python 2.4 Release Schedule","excerpt":"Python Enhancement Proposal 320: 'Python 2.4 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 18:29:42+0900","permalink":"/python/pep/320","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 320 Python 2.4 Release Schedule 상태: Final | 유형: Informational | 작성일: 29Jul2003 PEP 320: Python 2.4 릴리스 일정 본 문서는 Python 2.4의 개발 및 릴리스 일정을 설명하는 PEP (Python Enhancement Proposal)입니다. 이 PEP는 주로"},{"id":"2025-09-26-pep-0321-datetime-parsing-and-formatting","title":"[Withdrawn] PEP 321 - Date/Time Parsing and Formatting","excerpt":"Python Enhancement Proposal 321: 'Date/Time Parsing and Formatting'에 대한 한국어 번역입니다.","date":"2025-09-26 18:30:01+0900","permalink":"/python/pep/321","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 321 Date/Time Parsing and Formatting 상태: Withdrawn | 유형: Standards Track | 작성일: 16Sep2003 PEP 321: 날짜/시간 구문 분석 및 형식 지정 (Date/Time Parsing and Formatting) 개요 PEP 321은 Python의 모듈에 문자열 형태의 날짜 "},{"id":"2025-09-26-pep-0322-reverse-iteration","title":"[Final] PEP 322 - Reverse Iteration","excerpt":"Python Enhancement Proposal 322: 'Reverse Iteration'에 대한 한국어 번역입니다.","date":"2025-09-26 18:30:23+0900","permalink":"/python/pep/322","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 322 Reverse Iteration 상태: Final | 유형: Standards Track | 작성일: 24Sep2003 PEP 322 – 역방향 이터레이션 (Reverse Iteration) 개요 이 제안은 시퀀스(sequence) 객체에 대한 역방향 이터레이션(reverse iteration)을 지원하는 내장 함수(builtin"},{"id":"2025-09-26-pep-0323-copyable-iterators","title":"[Deferred] PEP 323 - Copyable Iterators","excerpt":"Python Enhancement Proposal 323: 'Copyable Iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:01+0900","permalink":"/python/pep/323","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 323 Copyable Iterators 상태: Deferred | 유형: Standards Track | 작성일: 25Oct2003 PEP 323 – 복사 가능한 이터레이터 (Copyable Iterators) 개요 이 PEP (Python Enhancement Proposal)는 일부 이터레이터 (iterator) 유형이 메서드를 통"},{"id":"2025-09-26-pep-0324-subprocess-new-process-module","title":"[Final] PEP 324 - subprocess - New process module","excerpt":"Python Enhancement Proposal 324: 'subprocess - New process module'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:27+0900","permalink":"/python/pep/324","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 324 subprocess New process module 상태: Final | 유형: Standards Track | 작성일: 19Nov2003 PEP 324 – 모듈: 새로운 프로세스 모듈 개요 (Abstract) 이 PEP는 새로운 프로세스를 시작하고 통신하는 기능을 제공하는 모듈에 대해 설명합니다. 동기 (Motivation) "},{"id":"2025-09-26-pep-0325-resource-release-support-for-generators","title":"[Rejected] PEP 325 - Resource-Release Support for Generators","excerpt":"Python Enhancement Proposal 325: 'Resource-Release Support for Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:31:57+0900","permalink":"/python/pep/325","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 325 ResourceRelease Support for Generators 상태: Rejected | 유형: Standards Track | 작성일: 25Aug2003 PEP 325 – 제너레이터를 위한 리소스 해제 지원 작성자 : Samuele Pedroni 상태 : 반려됨 (Rejected) 유형 : 표준 트랙 (Standards "},{"id":"2025-09-26-pep-0326-a-case-for-top-and-bottom-values","title":"[Rejected] PEP 326 - A Case for Top and Bottom Values","excerpt":"Python Enhancement Proposal 326: 'A Case for Top and Bottom Values'에 대한 한국어 번역입니다.","date":"2025-09-26 20:37:04+0900","permalink":"/python/pep/326","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 326 A Case for Top and Bottom Values 상태: Rejected | 유형: Standards Track | 작성일: 20Dec2003 PEP 326 – Top 및 Bottom 값에 대한 제안 (Max, Min 상수) 요약 PEP 326은 모든 다른 객체보다 높거나 낮게 비교되는 두 개의 싱글톤(singleton)"},{"id":"2025-09-26-pep-0327-decimal-data-type","title":"[Final] PEP 327 - Decimal Data Type","excerpt":"Python Enhancement Proposal 327: 'Decimal Data Type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:35:48+0900","permalink":"/python/pep/327","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 327 Decimal Data Type 상태: Final | 유형: Standards Track | 작성일: 17Oct2003 PEP 327 – Decimal 데이터 타입 초록 (Abstract) 이 PEP의 주요 아이디어는 이진 부동 소수점(binary floating point)이 너무 부정확하여 소수점 계산이 필요한 모든 경우에 사"},{"id":"2025-09-26-pep-0328-imports-multi-line-and-absoluterelative","title":"[Final] PEP 328 - Imports: Multi-Line and Absolute/Relative","excerpt":"Python Enhancement Proposal 328: 'Imports: Multi-Line and Absolute/Relative'에 대한 한국어 번역입니다.","date":"2025-09-26 18:36:33+0900","permalink":"/python/pep/328","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 328 Imports: MultiLine and Absolute/Relative 상태: Final | 유형: Standards Track | 작성일: 21Dec2003 PEP 328 – 임포트: 여러 줄 및 절대/상대 경로 (Imports: MultiLine and Absolute/Relative) 작성자: Aahz 상태: Final ("},{"id":"2025-09-26-pep-0329-treating-builtins-as-constants-in-the-standard-library","title":"[Rejected] PEP 329 - Treating Builtins as Constants in the Standard Library","excerpt":"Python Enhancement Proposal 329: 'Treating Builtins as Constants in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 20:37:36+0900","permalink":"/python/pep/329","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 329 Treating Builtins as Constants in the Standard Library 상태: Rejected | 유형: Standards Track | 작성일: 18Apr2004 PEP 329 – 표준 라이브러리에서 Builtin을 상수로 취급하기 작성자: Raymond Hettinger <python at rcn.c"},{"id":"2025-09-26-pep-0330-python-bytecode-verification","title":"[Rejected] PEP 330 - Python Bytecode Verification","excerpt":"Python Enhancement Proposal 330: 'Python Bytecode Verification'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:18+0900","permalink":"/python/pep/330","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 330 Python Bytecode Verification 상태: Rejected | 유형: Standards Track | 작성일: 17Jun2004 PEP 330 – Python 바이트코드 검증 (Python Bytecode Verification) 요약 (Abstract) Python Virtual Machine (PVM) 바이트코"},{"id":"2025-09-26-pep-0331-locale-independent-floatstring-conversions","title":"[Final] PEP 331 - Locale-Independent Float/String Conversions","excerpt":"Python Enhancement Proposal 331: 'Locale-Independent Float/String Conversions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:41+0900","permalink":"/python/pep/331","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 331 LocaleIndependent Float/String Conversions 상태: Final | 유형: Standards Track | 작성일: 19Jul2003 PEP 331 – 로케일 독립적인 float/string 변환 ===================================================== 저자: "},{"id":"2025-09-26-pep-0332-byte-vectors-and-stringunicode-unification","title":"[Rejected] PEP 332 - Byte vectors and String/Unicode Unification","excerpt":"Python Enhancement Proposal 332: 'Byte vectors and String/Unicode Unification'에 대한 한국어 번역입니다.","date":"2025-09-26 18:38:55+0900","permalink":"/python/pep/332","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 332 Byte vectors and String/Unicode Unification 상태: Rejected | 유형: Standards Track | 작성일: 11Aug2004 PEP 332 – 바이트 벡터 및 문자열/유니코드 통합 제목: PEP 332 – Byte vectors and String/Unicode Unification "},{"id":"2025-09-26-pep-0333-python-web-server-gateway-interface-v1-0","title":"[Final] PEP 333 - Python Web Server Gateway Interface v1.0","excerpt":"Python Enhancement Proposal 333: 'Python Web Server Gateway Interface v1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 18:42:49+0900","permalink":"/python/pep/333","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 333 Python Web Server Gateway Interface v1.0 상태: Final | 유형: Informational | 작성일: 07Dec2003 PEP 333 – Python Web Server Gateway Interface v1.0 서문 참고: Python 3.x를 지원하며 커뮤니티의 정오표, 추가 사항 및 명확화"},{"id":"2025-09-26-pep-0334-simple-coroutines-via-suspenditeration","title":"[Withdrawn] PEP 334 - Simple Coroutines via SuspendIteration","excerpt":"Python Enhancement Proposal 334: 'Simple Coroutines via SuspendIteration'에 대한 한국어 번역입니다.","date":"2025-09-26 18:43:20+0900","permalink":"/python/pep/334","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 334 Simple Coroutines via SuspendIteration 상태: Withdrawn | 유형: Standards Track | 작성일: 26Aug2004 PEP 334 – SuspendIteration을 통한 간단한 코루틴 (Simple Coroutines via SuspendIteration) 작성자: Clark C."},{"id":"2025-09-26-pep-0336-make-none-callable","title":"[Rejected] PEP 336 - Make None Callable","excerpt":"Python Enhancement Proposal 336: 'Make None Callable'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:11+0900","permalink":"/python/pep/336","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 336 Make None Callable 상태: Rejected | 유형: Standards Track | 작성일: 28Oct2004 PEP 336 – None을 호출 가능하게 만들기 (Make None Callable) 저자 : Andrew McClelland <eternalsquire at comcast.net 상태 : Rejecte"},{"id":"2025-09-26-pep-0337-logging-usage-in-the-standard-library","title":"[Deferred] PEP 337 - Logging Usage in the Standard Library","excerpt":"Python Enhancement Proposal 337: 'Logging Usage in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:28+0900","permalink":"/python/pep/337","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 337 Logging Usage in the Standard Library 상태: Deferred | 유형: Standards Track | 작성일: 02Oct2004 PEP 337 – 표준 라이브러리에서의 로깅 사용법 이 문서는 Python 표준 라이브러리 내에서 로깅 시스템 (PEP 282)을 사용하는 표준을 정의합니다. 초록 (Ab"},{"id":"2025-09-26-pep-0338-executing-modules-as-scripts","title":"[Final] PEP 338 - Executing modules as scripts","excerpt":"Python Enhancement Proposal 338: 'Executing modules as scripts'에 대한 한국어 번역입니다.","date":"2025-09-26 18:45:56+0900","permalink":"/python/pep/338","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 338 Executing modules as scripts 상태: Final | 유형: Standards Track | 작성일: 16Oct2004 PEP 338 – 모듈을 스크립트로 실행하기 작성자: Alyssa Coghlan 상태: Final (최종) 유형: Standards Track 생성일: 2004년 10월 16일 Python 버"},{"id":"2025-09-26-pep-0339-design-of-the-cpython-compiler","title":"[Withdrawn] PEP 339 - Design of the CPython Compiler","excerpt":"Python Enhancement Proposal 339: 'Design of the CPython Compiler'에 대한 한국어 번역입니다.","date":"2025-09-26 18:46:48+0900","permalink":"/python/pep/339","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 339 Design of the CPython Compiler 상태: Withdrawn | 유형: Informational | 작성일: 02Feb2005 참고: 이 PEP는 철회되었으며 Python 개발자 가이드로 이동되었습니다. 개요 (Abstract) 과거(Python 2.4까지)에는 소스 코드(source code)를 바이트코드(b"},{"id":"2025-09-26-pep-0340-anonymous-block-statements","title":"[Rejected] PEP 340 - Anonymous Block Statements","excerpt":"Python Enhancement Proposal 340: 'Anonymous Block Statements'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:04+0900","permalink":"/python/pep/340","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 340 Anonymous Block Statements 상태: Rejected | 유형: Standards Track | 작성일: 27Apr2005 PEP 340 – 익명 블록 문 (Anonymous Block Statements) 작성자: Guido van Rossum 상태: 거부됨 (Rejected) 유형: 표준 트랙 (Standar"},{"id":"2025-09-26-pep-0341-unifying-try-except-and-try-finally","title":"[Final] PEP 341 - Unifying try-except and try-finally","excerpt":"Python Enhancement Proposal 341: 'Unifying try-except and try-finally'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:18+0900","permalink":"/python/pep/341","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 341 Unifying tryexcept and tryfinally 상태: Final | 유형: Standards Track | 작성일: 04May2005 PEP 341 – 와 의 통합 작성자: Georg Brandl 상태: Final (최종) 유형: Standards Track 생성일: 2005년 5월 4일 Python 버전: 2.5 "},{"id":"2025-09-26-pep-0342-coroutines-via-enhanced-generators","title":"[Final] PEP 342 - Coroutines via Enhanced Generators","excerpt":"Python Enhancement Proposal 342: 'Coroutines via Enhanced Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 18:48:58+0900","permalink":"/python/pep/342","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 342 Coroutines via Enhanced Generators 상태: Final | 유형: Standards Track | 작성일: 10May2005 PEP 342: Enhanced Generators를 통한 코루틴 (Coroutines via Enhanced Generators) 서론 (Introduction) 이 PEP는 제너"},{"id":"2025-09-26-pep-0344-exception-chaining-and-embedded-tracebacks","title":"[Superseded] PEP 344 - Exception Chaining and Embedded Tracebacks","excerpt":"Python Enhancement Proposal 344: 'Exception Chaining and Embedded Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-26 18:54:00+0900","permalink":"/python/pep/344","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 344 Exception Chaining and Embedded Tracebacks 상태: Superseded | 유형: Standards Track | 작성일: 12May2005 PEP 344 – 예외 연쇄 (Exception Chaining) 및 내장 트레이스백 (Embedded Tracebacks) 개요 (Abstract) 이 PE"},{"id":"2025-09-26-pep-0345-metadata-for-python-software-packages-1-2","title":"[Superseded] PEP 345 - Metadata for Python Software Packages 1.2","excerpt":"Python Enhancement Proposal 345: 'Metadata for Python Software Packages 1.2'에 대한 한국어 번역입니다.","date":"2025-09-26 18:54:36+0900","permalink":"/python/pep/345","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 345 Metadata for Python Software Packages 1.2 상태: Superseded | 유형: Standards Track | 작성일: 28Apr2005 PEP 345 – Python 소프트웨어 패키지용 메타데이터 1.2 이 문서는 Python 배포판에 메타데이터를 추가하는 메커니즘을 설명합니다. 필드 이름, 의"},{"id":"2025-09-26-pep-0346-user-defined-with-statements","title":"[Withdrawn] PEP 346 - User Defined (”with”) Statements","excerpt":"Python Enhancement Proposal 346: 'User Defined (”with”) Statements'에 대한 한국어 번역입니다.","date":"2025-09-26 18:55:56+0900","permalink":"/python/pep/346","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 346 User Defined (”with”) Statements 상태: Withdrawn | 유형: Standards Track | 작성일: 06May2005 PEP 346 – 사용자 정의 (\"with\") 문 (User Defined (\"with\") Statements) 작성자 : Alyssa Coghlan <ncoghlan at gm"},{"id":"2025-09-26-pep-0347-migrating-the-python-cvs-to-subversion","title":"[Final] PEP 347 - Migrating the Python CVS to Subversion","excerpt":"Python Enhancement Proposal 347: 'Migrating the Python CVS to Subversion'에 대한 한국어 번역입니다.","date":"2025-09-26 18:56:24+0900","permalink":"/python/pep/347","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 347 Migrating the Python CVS to Subversion 상태: Final | 유형: Process | 작성일: 14Jul2004 PEP 347 – Python CVS를 Subversion으로 마이그레이션 개요 이 문서는 Python 소스 코드를 SourceForge.net의 CVS 저장소에서 svn.python.or"},{"id":"2025-09-26-pep-0348-exception-reorganization-for-python-3-0","title":"[Rejected] PEP 348 - Exception Reorganization for Python 3.0","excerpt":"Python Enhancement Proposal 348: 'Exception Reorganization for Python 3.0'에 대한 한국어 번역입니다.","date":"2025-09-26 18:56:58+0900","permalink":"/python/pep/348","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 348 Exception Reorganization for Python 3.0 상태: Rejected | 유형: Standards Track | 작성일: 28Jul2005 PEP 348 – Python 3.0을 위한 예외 재구성 (반려됨) 작성자: Brett Cannon <brett at python.org 상태: 반려됨 (Rejecte"},{"id":"2025-09-26-pep-0349-allow-str-to-return-unicode-strings","title":"[Rejected] PEP 349 - Allow str() to return unicode strings","excerpt":"Python Enhancement Proposal 349: 'Allow str() to return unicode strings'에 대한 한국어 번역입니다.","date":"2025-09-26 18:57:18+0900","permalink":"/python/pep/349","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 349 Allow str() to return unicode strings 상태: Rejected | 유형: Standards Track | 작성일: 02Aug2005 PEP 349 – 함수가 유니코드(Unicode) 문자열을 반환하도록 허용 작성자: Neil Schemenauer 상태: Rejected (거부됨) 유형: Standard"},{"id":"2025-09-26-pep-0350-codetags","title":"[Rejected] PEP 350 - Codetags","excerpt":"Python Enhancement Proposal 350: 'Codetags'에 대한 한국어 번역입니다.","date":"2025-09-26 18:58:11+0900","permalink":"/python/pep/350","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 350 Codetags 상태: Rejected | 유형: Informational | 작성일: 27Jun2005 PEP 350 – 코드태그 (Codetags) 거부 공지 이 PEP는 거부되었습니다. 커뮤니티가 이 제안에 관심을 가질 수는 있지만, 표준 라이브러리가 이 표준을 따르도록 만들려는 의도는 없습니다. 개요 이 정보성(Inform"},{"id":"2025-09-26-pep-0351-the-freeze-protocol","title":"[Rejected] PEP 351 - The freeze protocol","excerpt":"Python Enhancement Proposal 351: 'The freeze protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 18:58:26+0900","permalink":"/python/pep/351","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 351 The freeze protocol 상태: Rejected | 유형: Standards Track | 작성일: 14Apr2005 PEP 351 – The freeze protocol (동결 프로토콜) 작성자: Barry Warsaw <barry at python.org 상태: Rejected (거부됨) 유형: Standards T"},{"id":"2025-09-26-pep-0352-required-superclass-for-exceptions","title":"[Final] PEP 352 - Required Superclass for Exceptions","excerpt":"Python Enhancement Proposal 352: 'Required Superclass for Exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:06+0900","permalink":"/python/pep/352","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 352 Required Superclass for Exceptions 상태: Final | 유형: Standards Track | 작성일: 27Oct2005 PEP 352 – 예외를 위한 필수 슈퍼클래스 (Required Superclass for Exceptions) 작성자: Brett Cannon, Guido van Rossum 상태"},{"id":"2025-09-26-pep-0353-using-ssize-t-as-the-index-type","title":"[Final] PEP 353 - Using ssize_t as the index type","excerpt":"Python Enhancement Proposal 353: 'Using ssize_t as the index type'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:36+0900","permalink":"/python/pep/353","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 353 Using ssizet as the index type 상태: Final | 유형: Standards Track | 작성일: 18Dec2005 PEP 353 – 를 인덱스 타입으로 사용하기 개요 (Abstract) Python 2.4에서는 시퀀스의 인덱스가 C 타입 로 제한되었습니다. 이로 인해 64비트 머신에서는 시퀀스가 전체 "},{"id":"2025-09-26-pep-0354-enumerations-in-python","title":"[Superseded] PEP 354 - Enumerations in Python","excerpt":"Python Enhancement Proposal 354: 'Enumerations in Python'에 대한 한국어 번역입니다.","date":"2025-09-26 18:59:58+0900","permalink":"/python/pep/354","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 354 Enumerations in Python 상태: Superseded | 유형: Standards Track | 작성일: 20Dec2005 PEP 354 – Python의 Enumeration (열거형) 작성자: Ben Finney 상태: 폐기됨 (Superseded) 유형: 표준 트랙 (Standards Track) 생성일: 20"},{"id":"2025-09-26-pep-0356-python-2-5-release-schedule","title":"[Final] PEP 356 - Python 2.5 Release Schedule","excerpt":"Python Enhancement Proposal 356: 'Python 2.5 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 19:03:07+0900","permalink":"/python/pep/356","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 356 Python 2.5 Release Schedule 상태: Final | 유형: Informational | 작성일: 07Feb2006 PEP 356 – Python 2.5 릴리스 스케줄 이 문서는 Python 2.5의 개발 및 릴리스 스케줄을 설명하는 PEP(Python Enhancement Proposal)입니다. 주로 PEP "},{"id":"2025-09-26-pep-0357-allowing-any-object-to-be-used-for-slicing","title":"[Final] PEP 357 - Allowing Any Object to be Used for Slicing","excerpt":"Python Enhancement Proposal 357: 'Allowing Any Object to be Used for Slicing'에 대한 한국어 번역입니다.","date":"2025-09-26 19:03:31+0900","permalink":"/python/pep/357","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 357 Allowing Any Object to be Used for Slicing 상태: Final | 유형: Standards Track | 작성일: 09Feb2006 PEP 357 – 슬라이싱에 모든 객체 사용 허용 작성자: Travis Oliphant 상태: Final (최종) 유형: Standards Track 작성일: 2006"},{"id":"2025-09-26-pep-0358-the-bytes-object","title":"[Final] PEP 358 - The “bytes” Object","excerpt":"Python Enhancement Proposal 358: 'The “bytes” Object'에 대한 한국어 번역입니다.","date":"2025-09-26 19:04:13+0900","permalink":"/python/pep/358","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 358 The “bytes” Object 상태: Final | 유형: Standards Track | 작성일: 15Feb2006 PEP 358 – \"bytes\" 객체 작성자: Neil Schemenauer, Guido van Rossum 상태: Final (최종) 유형: Standards Track 작성일: 2006년 2월 15일 Pyt"},{"id":"2025-09-26-pep-0359-the-make-statement","title":"[Withdrawn] PEP 359 - The “make” Statement","excerpt":"Python Enhancement Proposal 359: 'The “make” Statement'에 대한 한국어 번역입니다.","date":"2025-09-26 19:06:53+0900","permalink":"/python/pep/359","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 359 The “make” Statement 상태: Withdrawn | 유형: Standards Track | 작성일: 05Apr2006 PEP 359 – \"make\" Statement 작성자: Steven Bethard <steven.bethard at gmail.com 상태: 철회됨 (Withdrawn) 유형: 표준 트랙 (Stan"},{"id":"2025-09-26-pep-0360-externally-maintained-packages","title":"[Final] PEP 360 - Externally Maintained Packages","excerpt":"Python Enhancement Proposal 360: 'Externally Maintained Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 19:07:10+0900","permalink":"/python/pep/360","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 360 Externally Maintained Packages 상태: Final | 유형: Process | 작성일: 30May2006 PEP 360 – 외부 관리 패키지 (Externally Maintained Packages) 요약 (Abstract) Python 표준 라이브러리 (stdlib) 외부에 개발된 훌륭한 Python 소프"},{"id":"2025-09-26-pep-0361-python-2-6-and-3-0-release-schedule","title":"[Final] PEP 361 - Python 2.6 and 3.0 Release Schedule","excerpt":"Python Enhancement Proposal 361: 'Python 2.6 and 3.0 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 19:07:30+0900","permalink":"/python/pep/361","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 361 Python 2.6 and 3.0 Release Schedule 상태: Final | 유형: Informational | 작성일: 29Jun2006 PEP 361 – Python 2.6 및 3.0 릴리스 스케줄 저자: Neal Norwitz, Barry Warsaw 상태: 최종 (Final) 유형: 정보성 (Informationa"},{"id":"2025-09-26-pep-0362-function-signature-object","title":"[Final] PEP 362 - Function Signature Object","excerpt":"Python Enhancement Proposal 362: 'Function Signature Object'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:03+0900","permalink":"/python/pep/362","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 362 Function Signature Object 상태: Final | 유형: Standards Track | 작성일: 21Aug2006 PEP 362 – 함수 시그니처 객체 작성자 : Brett Cannon, Jiwon Seo, Yury Selivanov, Larry Hastings 상태 : Final 유형 : Standards T"},{"id":"2025-09-26-pep-0363-syntax-for-dynamic-attribute-access","title":"[Rejected] PEP 363 - Syntax For Dynamic Attribute Access","excerpt":"Python Enhancement Proposal 363: 'Syntax For Dynamic Attribute Access'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:27+0900","permalink":"/python/pep/363","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 363 Syntax For Dynamic Attribute Access 상태: Rejected | 유형: Standards Track | 작성일: 29Jan2007 PEP 363 – 동적 속성 접근을 위한 새로운 문법 제안 (Syntax For Dynamic Attribute Access) 상태: 거부됨 (Rejected) 이 문서는 P"},{"id":"2025-09-26-pep-0364-transitioning-to-the-py3k-standard-library","title":"[Withdrawn] PEP 364 - Transitioning to the Py3K Standard Library","excerpt":"Python Enhancement Proposal 364: 'Transitioning to the Py3K Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 19:08:59+0900","permalink":"/python/pep/364","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 364 Transitioning to the Py3K Standard Library 상태: Withdrawn | 유형: Standards Track | 작성일: 01Mar2007 PEP 364 – Py3K 표준 라이브러리로의 전환 (Transitioning to the Py3K Standard Library) 작성자: Barry Wars"},{"id":"2025-09-26-pep-0365-adding-the-pkg-resources-module","title":"[Rejected] PEP 365 - Adding the pkg_resources module","excerpt":"Python Enhancement Proposal 365: 'Adding the pkg_resources module'에 대한 한국어 번역입니다.","date":"2025-09-26 19:09:15+0900","permalink":"/python/pep/365","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 365 Adding the pkgresources module 상태: Rejected | 유형: Standards Track | 작성일: 30Apr2007 PEP 365 – 모듈 추가 작성자: Phillip J. Eby 상태: Rejected (거절됨) 유형: Standards Track 주제: Packaging (패키징) 생성일: 20"},{"id":"2025-09-26-pep-0366-main-module-explicit-relative-imports","title":"[Final] PEP 366 - Main module explicit relative imports","excerpt":"Python Enhancement Proposal 366: 'Main module explicit relative imports'에 대한 한국어 번역입니다.","date":"2025-09-26 20:49:35+0900","permalink":"/python/pep/366","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 366 Main module explicit relative imports 상태: Final | 유형: Standards Track | 작성일: 01May2007 PEP 366 – 메인 모듈의 명시적 상대 경로 임포트 작성자 : Alyssa Coghlan 상태 : Final (최종) 유형 : Standards Track (표준 트랙) 생"},{"id":"2025-09-26-pep-0367-new-super","title":"[Superseded] PEP 367 - New Super","excerpt":"Python Enhancement Proposal 367: 'New Super'에 대한 한국어 번역입니다.","date":"2025-09-26 20:50:02+0900","permalink":"/python/pep/367","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 367 New Super 상태: Superseded | 유형: Standards Track | 작성일: 28Apr2007 PEP 367 – New Super 개요 이 문서는 Python Enhancement Proposal (PEP) 367에 대한 번역 및 정리입니다. 이 PEP는 타입의 새로운 구문 설탕(syntactic sugar) "},{"id":"2025-09-26-pep-0368-standard-image-protocol-and-class","title":"[Deferred] PEP 368 - Standard image protocol and class","excerpt":"Python Enhancement Proposal 368: 'Standard image protocol and class'에 대한 한국어 번역입니다.","date":"2025-09-26 20:53:03+0900","permalink":"/python/pep/368","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 368 Standard image protocol and class 상태: Deferred | 유형: Standards Track | 작성일: 28Jun2007 PEP 368 – 표준 이미지 프로토콜 및 클래스 작성자: Lino Mastrodomenico 상태: 연기됨 (Deferred) 유형: Standards Track 생성일: 20"},{"id":"2025-09-26-pep-0369-post-import-hooks","title":"[Withdrawn] PEP 369 - Post import hooks","excerpt":"Python Enhancement Proposal 369: 'Post import hooks'에 대한 한국어 번역입니다.","date":"2025-09-26 20:53:39+0900","permalink":"/python/pep/369","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 369 Post import hooks 상태: Withdrawn | 유형: Standards Track | 작성일: 02Jan2008 PEP 369 – Post import hooks 작성자 : Christian Heimes <christian at python.org 상태 : 철회됨 (Withdrawn) 유형 : Standards Tr"},{"id":"2025-09-26-pep-0370-per-user-site-packages-directory","title":"[Final] PEP 370 - Per user site-packages directory","excerpt":"Python Enhancement Proposal 370: 'Per user site-packages directory'에 대한 한국어 번역입니다.","date":"2025-09-26 20:54:08+0900","permalink":"/python/pep/370","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 370 Per user sitepackages directory 상태: Final | 유형: Standards Track | 작성일: 11Jan2008 PEP 370 – 사용자별 sitepackages 디렉터리 작성자: Christian Heimes 상태: Final 유형: Standards Track 생성일: 2008년 1월 11일 P"},{"id":"2025-09-26-pep-0371-addition-of-the-multiprocessing-package-to-the-standard-library","title":"[Final] PEP 371 - Addition of the multiprocessing package to the standard library","excerpt":"Python Enhancement Proposal 371: 'Addition of the multiprocessing package to the standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 20:55:33+0900","permalink":"/python/pep/371","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 371 Addition of the multiprocessing package to the standard library 상태: Final | 유형: Standards Track | 작성일: 06May2008 PEP 371 – 표준 라이브러리에 패키지 추가 개요 (Abstract) 이 PEP는 패키지를 \"multiprocessing\"으로"},{"id":"2025-09-26-pep-0372-adding-an-ordered-dictionary-to-collections","title":"[Final] PEP 372 - Adding an ordered dictionary to collections","excerpt":"Python Enhancement Proposal 372: 'Adding an ordered dictionary to collections'에 대한 한국어 번역입니다.","date":"2025-09-26 20:56:08+0900","permalink":"/python/pep/372","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 372 Adding an ordered dictionary to collections 상태: Final | 유형: Standards Track | 작성일: 15Jun2008 PEP 372 – 모듈에 순서가 있는 딕셔너리 추가 개요 이 PEP는 모듈에 \"OrderedDict\"라는 이름의 순서가 있는 딕셔너리(ordered dictionar"},{"id":"2025-09-26-pep-0373-python-2-7-release-schedule","title":"[Final] PEP 373 - Python 2.7 Release Schedule","excerpt":"Python Enhancement Proposal 373: 'Python 2.7 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 20:56:29+0900","permalink":"/python/pep/373","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 373 Python 2.7 Release Schedule 상태: Final | 유형: Informational | 작성일: 03Nov2008 제목 PEP 373 – Python 2.7 릴리스 스케줄 작성자 및 상태 작성자: Benjamin Peterson <benjamin at python.org 상태: Final (최종) 유형: Inf"},{"id":"2025-09-26-pep-0374-choosing-a-distributed-vcs-for-the-python-project","title":"[Final] PEP 374 - Choosing a distributed VCS for the Python project","excerpt":"Python Enhancement Proposal 374: 'Choosing a distributed VCS for the Python project'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:02+0900","permalink":"/python/pep/374","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 374 Choosing a distributed VCS for the Python project 상태: Final | 유형: Process | 작성일: 07Nov2008 PEP 374 – Python 프로젝트를 위한 분산 VCS 선택 작성자: Brett Cannon, Stephen J. Turnbull, Alexandre Vassalot"},{"id":"2025-09-26-pep-0375-python-3-1-release-schedule","title":"[Final] PEP 375 - Python 3.1 Release Schedule","excerpt":"Python Enhancement Proposal 375: 'Python 3.1 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:16+0900","permalink":"/python/pep/375","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 375 Python 3.1 Release Schedule 상태: Final | 유형: Informational | 작성일: 08Feb2009 개요 이 문서는 Python 3.1 버전의 개발 및 릴리스 스케줄을 설명합니다. 주요 내용은 PEP(Python Enhancement Proposal) 규모의 제안 사항에 중점을 둡니다. 작은 기능"},{"id":"2025-09-26-pep-0376-database-of-installed-python-distributions","title":"[Final] PEP 376 - Database of Installed Python Distributions","excerpt":"Python Enhancement Proposal 376: 'Database of Installed Python Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 20:59:42+0900","permalink":"/python/pep/376","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 376 Database of Installed Python Distributions 상태: Final | 유형: Standards Track | 작성일: 22Feb2009 PEP 376 – 설치된 Python 배포판 데이터베이스 초록 (Abstract) 이 PEP의 목표는 시스템에 설치된 프로젝트 배포판을 관리하는 표준 인프라를 제공하여"},{"id":"2025-09-26-pep-0377-allow-enter-methods-to-skip-the-statement-body","title":"[Rejected] PEP 377 - Allow __enter__() methods to skip the statement body","excerpt":"Python Enhancement Proposal 377: 'Allow __enter__() methods to skip the statement body'에 대한 한국어 번역입니다.","date":"2025-09-26 21:00:13+0900","permalink":"/python/pep/377","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 377 Allow enter() methods to skip the statement body 상태: Rejected | 유형: Standards Track | 작성일: 08Mar2009 PEP 377 – 메서드가 문 본문 건너뛰기 허용 저자: Alyssa Coghlan 상태: Rejected (반려됨) 유형: Standards Trac"},{"id":"2025-09-26-pep-0378-format-specifier-for-thousands-separator","title":"[Final] PEP 378 - Format Specifier for Thousands Separator","excerpt":"Python Enhancement Proposal 378: 'Format Specifier for Thousands Separator'에 대한 한국어 번역입니다.","date":"2025-09-26 21:01:13+0900","permalink":"/python/pep/378","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 378 Format Specifier for Thousands Separator 상태: Final | 유형: Standards Track | 작성일: 12Mar2009 PEP 378 – Format Specifier for Thousands Separator 저자: Raymond Hettinger <python at rcn.com 상태:"},{"id":"2025-09-26-pep-0379-adding-an-assignment-expression","title":"[Withdrawn] PEP 379 - Adding an Assignment Expression","excerpt":"Python Enhancement Proposal 379: 'Adding an Assignment Expression'에 대한 한국어 번역입니다.","date":"2025-09-26 21:01:34+0900","permalink":"/python/pep/379","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 379 Adding an Assignment Expression 상태: Withdrawn | 유형: Standards Track | 작성일: 14Mar2009 작성자: Jervis Whitley <jervisau at gmail.com 상태: 철회됨 (Withdrawn) 유형: 표준 트랙 (Standards Track) 생성일: 2009"},{"id":"2025-09-26-pep-0380-syntax-for-delegating-to-a-subgenerator","title":"[Final] PEP 380 - Syntax for Delegating to a Subgenerator","excerpt":"Python Enhancement Proposal 380: 'Syntax for Delegating to a Subgenerator'에 대한 한국어 번역입니다.","date":"2025-09-26 21:02:19+0900","permalink":"/python/pep/380","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 380 Syntax for Delegating to a Subgenerator 상태: Final | 유형: Standards Track | 작성일: 13Feb2009 PEP 380 – 서브제너레이터 위임 문법 (Syntax for Delegating to a Subgenerator) 작성자: Gregory Ewing 상태: 최종 (Fin"},{"id":"2025-09-26-pep-0381-mirroring-infrastructure-for-pypi","title":"[Withdrawn] PEP 381 - Mirroring infrastructure for PyPI","excerpt":"Python Enhancement Proposal 381: 'Mirroring infrastructure for PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:02:58+0900","permalink":"/python/pep/381","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 381 Mirroring infrastructure for PyPI 상태: Withdrawn | 유형: Standards Track | 작성일: 21Mar2009 PEP 381 – PyPI 미러링 인프라 (PEP 381 – Mirroring infrastructure for PyPI) 작성자: Tarek Ziadé, Martin von "},{"id":"2025-09-26-pep-0382-namespace-packages","title":"[Rejected] PEP 382 - Namespace Packages","excerpt":"Python Enhancement Proposal 382: 'Namespace Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 21:03:27+0900","permalink":"/python/pep/382","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 382 Namespace Packages 상태: Rejected | 유형: Standards Track | 작성일: 02Apr2009 PEP 382 – 네임스페이스 패키지 (Namespace Packages) 작성자: Martin von Löwis 상태: Rejected (거부됨) 유형: Standards Track (표준 트랙) 생성일"},{"id":"2025-09-26-pep-0383-non-decodable-bytes-in-system-character-interfaces","title":"[Final] PEP 383 - Non-decodable Bytes in System Character Interfaces","excerpt":"Python Enhancement Proposal 383: 'Non-decodable Bytes in System Character Interfaces'에 대한 한국어 번역입니다.","date":"2025-09-26 21:03:51+0900","permalink":"/python/pep/383","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 383 Nondecodable Bytes in System Character Interfaces 상태: Final | 유형: Standards Track | 작성일: 22Apr2009 PEP 383 – 시스템 문자 인터페이스의 디코딩 불가능한 바이트 처리 개요 (Abstract) POSIX 시스템에서 파일 이름, 환경 변수, 명령줄 인수"},{"id":"2025-09-26-pep-0384-defining-a-stable-abi","title":"[Final] PEP 384 - Defining a Stable ABI","excerpt":"Python Enhancement Proposal 384: 'Defining a Stable ABI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:04:32+0900","permalink":"/python/pep/384","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 384 Defining a Stable ABI 상태: Final | 유형: Standards Track | 작성일: 17May2009 PEP 384 – 안정적인 ABI 정의 (Defining a Stable ABI) 이 문서는 Python 3의 수명 주기 동안 안정적으로 유지되며, 여러 Python 버전 간에 바이너리 호환성을 보장하는 "},{"id":"2025-09-26-pep-0385-migrating-from-subversion-to-mercurial","title":"[Final] PEP 385 - Migrating from Subversion to Mercurial","excerpt":"Python Enhancement Proposal 385: 'Migrating from Subversion to Mercurial'에 대한 한국어 번역입니다.","date":"2025-09-26 21:05:29+0900","permalink":"/python/pep/385","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 385 Migrating from Subversion to Mercurial 상태: Final | 유형: Process | 작성일: 25May2009 PEP 385 – Subversion에서 Mercurial로 마이그레이션 저자: Dirkjan Ochtman, Antoine Pitrou, Georg Brandl 상태: Final (최종)"},{"id":"2025-09-26-pep-0386-changing-the-version-comparison-module-in-distutils","title":"[Superseded] PEP 386 - Changing the version comparison module in Distutils","excerpt":"Python Enhancement Proposal 386: 'Changing the version comparison module in Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 21:07:16+0900","permalink":"/python/pep/386","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 386 Changing the version comparison module in Distutils 상태: Superseded | 유형: Standards Track | 작성일: 04Jun2009 Title: PEP 386 – Changing the version comparison module in Distutils (Distutils"},{"id":"2025-09-26-pep-0387-backwards-compatibility-policy","title":"[Active] PEP 387 - Backwards Compatibility Policy","excerpt":"Python Enhancement Proposal 387: 'Backwards Compatibility Policy'에 대한 한국어 번역입니다.","date":"2025-09-26 21:07:43+0900","permalink":"/python/pep/387","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 387 Backwards Compatibility Policy 상태: Active | 유형: Process | 작성일: 18Jun2009 PEP 387 – 하위 호환성 정책 (Backwards Compatibility Policy) 작성자: Benjamin Peterson PEP 담당자: Brett Cannon 상태: Active (활성"},{"id":"2025-09-26-pep-0389-argparse-new-command-line-parsing-module","title":"[Final] PEP 389 - argparse - New Command Line Parsing Module","excerpt":"Python Enhancement Proposal 389: 'argparse - New Command Line Parsing Module'에 대한 한국어 번역입니다.","date":"2025-09-26 21:08:50+0900","permalink":"/python/pep/389","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 389 argparse New Command Line Parsing Module 상태: Final | 유형: Standards Track | 작성일: 25Sep2009 PEP 389 – argparse 새로운 커맨드 라인 파싱 모듈 개요 이 PEP는 Python 2.7 및 3.2 표준 라이브러리에 모듈을 포함할 것을 제안합니다. 동기 모"},{"id":"2025-09-26-pep-0390-static-metadata-for-distutils","title":"[Rejected] PEP 390 - Static metadata for Distutils","excerpt":"Python Enhancement Proposal 390: 'Static metadata for Distutils'에 대한 한국어 번역입니다.","date":"2025-09-26 21:09:17+0900","permalink":"/python/pep/390","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 390 Static metadata for Distutils 상태: Rejected | 유형: Standards Track | 작성일: 10Oct2009 PEP 390 – Distutils를 위한 정적 메타데이터 작성자: Tarek Ziadé BDFLDelegate: Alyssa Coghlan 상태: Rejected (거부됨) 유형: S"},{"id":"2025-09-26-pep-0391-dictionary-based-configuration-for-logging","title":"[Final] PEP 391 - Dictionary-Based Configuration For Logging","excerpt":"Python Enhancement Proposal 391: 'Dictionary-Based Configuration For Logging'에 대한 한국어 번역입니다.","date":"2025-09-26 21:19:23+0900","permalink":"/python/pep/391","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 391 DictionaryBased Configuration For Logging 상태: Final | 유형: Standards Track | 작성일: 15Oct2009 PEP 391 – 로깅을 위한 딕셔너리 기반 설정 개요 이 PEP(Python Enhancement Proposal)는 딕셔너리를 사용하여 로깅(logging) 설정을 "},{"id":"2025-09-26-pep-0392-python-3-2-release-schedule","title":"[Final] PEP 392 - Python 3.2 Release Schedule","excerpt":"Python Enhancement Proposal 392: 'Python 3.2 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:19:41+0900","permalink":"/python/pep/392","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 392 Python 3.2 Release Schedule 상태: Final | 유형: Informational | 작성일: 30Dec2009 PEP 392 – Python 3.2 릴리즈 일정 번역 및 정리 본 문서는 Python 3.2 시리즈의 개발 및 릴리즈 일정을 설명합니다. 이 일정은 주로 PEP 규모의 항목들을 다룹니다. 개요 ("},{"id":"2025-09-26-pep-0393-flexible-string-representation","title":"[Final] PEP 393 - Flexible String Representation","excerpt":"Python Enhancement Proposal 393: 'Flexible String Representation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:20:21+0900","permalink":"/python/pep/393","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 393 Flexible String Representation 상태: Final | 유형: Standards Track | 작성일: 24Jan2010 PEP 393 – 유연한 문자열 표현 (Flexible String Representation) 개요 (Abstract) 이 PEP는 유니코드 문자열 타입이 여러 내부 표현 방식을 지원하도"},{"id":"2025-09-26-pep-0394-the-python-command-on-unix-like-systems","title":"[Active] PEP 394 - The “python” Command on Unix-Like Systems","excerpt":"Python Enhancement Proposal 394: 'The “python” Command on Unix-Like Systems'에 대한 한국어 번역입니다.","date":"2025-09-26 21:21:20+0900","permalink":"/python/pep/394","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 394 The “python” Command on UnixLike Systems 상태: Active | 유형: Informational | 작성일: 02Mar2011 PEP 394 – Unix 계열 시스템의 \"python\" 명령어 작성자: Kerrick Staley, Alyssa Coghlan, Barry Warsaw, Petr Vikt"},{"id":"2025-09-26-pep-0395-qualified-names-for-modules","title":"[Withdrawn] PEP 395 - Qualified Names for Modules","excerpt":"Python Enhancement Proposal 395: 'Qualified Names for Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 21:22:00+0900","permalink":"/python/pep/395","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 395 Qualified Names for Modules 상태: Withdrawn | 유형: Standards Track | 작성일: 04Mar2011 PEP 395 – 모듈의 정규화된 이름 (Qualified Names for Modules) 개요 PEP 395는 Python의 임포트(import) 시스템, 객체 직렬화(serializ"},{"id":"2025-09-26-pep-0396-module-version-numbers","title":"[Withdrawn] PEP 396 - Module Version Numbers","excerpt":"Python Enhancement Proposal 396: 'Module Version Numbers'에 대한 한국어 번역입니다.","date":"2025-09-26 21:22:37+0900","permalink":"/python/pep/396","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 396 Module Version Numbers 상태: Withdrawn | 유형: Informational | 작성일: 16Mar2011 PEP 396 – Module Version Numbers 작성자 : Barry Warsaw <barry at python.org 상태 : 철회됨 (Withdrawn) 유형 : 정보성 (Informa"},{"id":"2025-09-26-pep-0397-python-launcher-for-windows","title":"[Final] PEP 397 - Python launcher for Windows","excerpt":"Python Enhancement Proposal 397: 'Python launcher for Windows'에 대한 한국어 번역입니다.","date":"2025-09-26 21:23:27+0900","permalink":"/python/pep/397","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 397 Python launcher for Windows 상태: Final | 유형: Standards Track | 작성일: 15Mar2011 PEP 397 – Windows용 Python 런처 작성자: Mark Hammond, Martin von Löwis 상태: Final (최종) 유형: Standards Track 생성일: 201"},{"id":"2025-09-26-pep-0398-python-3-3-release-schedule","title":"[Final] PEP 398 - Python 3.3 Release Schedule","excerpt":"Python Enhancement Proposal 398: 'Python 3.3 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:23:58+0900","permalink":"/python/pep/398","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 398 Python 3.3 Release Schedule 상태: Final | 유형: Informational | 작성일: 23Mar2011 PEP 398 – Python 3.3 릴리스 스케줄 ================================== 작성자: Georg Brandl <georg at python.org 상태: Fin"},{"id":"2025-09-26-pep-0399-pure-pythonc-accelerator-module-compatibility-requirements","title":"[Final] PEP 399 - Pure Python/C Accelerator Module Compatibility Requirements","excerpt":"Python Enhancement Proposal 399: 'Pure Python/C Accelerator Module Compatibility Requirements'에 대한 한국어 번역입니다.","date":"2025-09-26 21:24:25+0900","permalink":"/python/pep/399","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 399 Pure Python/C Accelerator Module Compatibility Requirements 상태: Final | 유형: Informational | 작성일: 04Apr2011 PEP 399 – 순수 Python/C 가속 모듈 호환성 요구사항 번역 및 정리 개요 이 문서는 Python Enhancement Propo"},{"id":"2025-09-26-pep-0400-deprecate-codecs-streamreader-and-codecs-streamwriter","title":"[Deferred] PEP 400 - Deprecate codecs.StreamReader and codecs.StreamWriter","excerpt":"Python Enhancement Proposal 400: 'Deprecate codecs.StreamReader and codecs.StreamWriter'에 대한 한국어 번역입니다.","date":"2025-09-26 21:24:56+0900","permalink":"/python/pep/400","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 400 Deprecate codecs.StreamReader and codecs.StreamWriter 상태: Deferred | 유형: Standards Track | 작성일: 28May2011 PEP 400 – 및 Deprecation 제안 개요 이 PEP는 와 가 유사한 API를 제공하지만, 가 더 많은 기능과 더 빠른 성능을 제공"},{"id":"2025-09-26-pep-0401-bdfl-retirement","title":"[April Fool!] PEP 401 - BDFL Retirement","excerpt":"Python Enhancement Proposal 401: 'BDFL Retirement'에 대한 한국어 번역입니다.","date":"2025-09-26 21:25:14+0900","permalink":"/python/pep/401","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 401 BDFL Retirement 상태: April Fool! | 유형: Process | 작성일: 01Apr2009 PEP 401 – BDFL 은퇴 작성자: Barry Warsaw, Brett Cannon 상태: 만우절! (April Fool!) 유형: Process (프로세스) 생성일: 2009년 4월 1일 후속 기록: 2009년 "},{"id":"2025-09-26-pep-0402-simplified-package-layout-and-partitioning","title":"[Rejected] PEP 402 - Simplified Package Layout and Partitioning","excerpt":"Python Enhancement Proposal 402: 'Simplified Package Layout and Partitioning'에 대한 한국어 번역입니다.","date":"2025-09-26 21:26:40+0900","permalink":"/python/pep/402","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 402 Simplified Package Layout and Partitioning 상태: Rejected | 유형: Standards Track | 작성일: 12Jul2011 PEP 402 – 패키지 레이아웃 및 분할 간소화 작성자: Phillip J. Eby 상태: Rejected (거부됨) 유형: Standards Track 주제:"},{"id":"2025-09-26-pep-0403-general-purpose-decorator-clause-aka-in-clause","title":"[Deferred] PEP 403 - General purpose decorator clause (aka “@in” clause)","excerpt":"Python Enhancement Proposal 403: 'General purpose decorator clause (aka “@in” clause)'에 대한 한국어 번역입니다.","date":"2025-09-26 21:27:31+0900","permalink":"/python/pep/403","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 403 General purpose decorator clause (aka “@in” clause) 상태: Deferred | 유형: Standards Track | 작성일: 13Oct2011 PEP 403 – 범용 데코레이터 절 (일명 \"@in\" 절) 개요 (Abstract) 이 PEP는 함수 또는 클래스 정의의 이름 바인딩(name "},{"id":"2025-09-26-pep-0404-python-2-8-un-release-schedule","title":"[Final] PEP 404 - Python 2.8 Un-release Schedule","excerpt":"Python Enhancement Proposal 404: 'Python 2.8 Un-release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:27:55+0900","permalink":"/python/pep/404","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 404 Python 2.8 Unrelease Schedule 상태: Final | 유형: Informational | 작성일: 09Nov2011 PEP 404 – Python 2.8 미출시 일정 작성자: Barry Warsaw <barry at python.org 상태: Final 유형: Informational 주제: Release 작"},{"id":"2025-09-26-pep-0405-python-virtual-environments","title":"[Final] PEP 405 - Python Virtual Environments","excerpt":"Python Enhancement Proposal 405: 'Python Virtual Environments'에 대한 한국어 번역입니다.","date":"2025-09-26 21:28:37+0900","permalink":"/python/pep/405","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 405 Python Virtual Environments 상태: Final | 유형: Standards Track | 작성일: 13Jun2011 PEP 405 – Python 가상 환경 이 문서는 Python 3.3에 도입된 내장 가상 환경() 메커니즘을 설명하는 PEP 405에 대한 한국어 번역 및 요약입니다. 이 PEP는 기존 서드파"},{"id":"2025-09-26-pep-0406-improved-encapsulation-of-import-state","title":"[Withdrawn] PEP 406 - Improved Encapsulation of Import State","excerpt":"Python Enhancement Proposal 406: 'Improved Encapsulation of Import State'에 대한 한국어 번역입니다.","date":"2025-09-26 21:29:05+0900","permalink":"/python/pep/406","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 406 Improved Encapsulation of Import State 상태: Withdrawn | 유형: Standards Track | 작성일: 04Jul2011 PEP 406 – Import 상태의 캡슐화 개선 상태: Withdrawn (철회됨) 유형: Standards Track 작성자: Alyssa Coghlan, Greg"},{"id":"2025-09-26-pep-0407-new-release-cycle-and-introducing-long-term-support-versions","title":"[Deferred] PEP 407 - New release cycle and introducing long-term support versions","excerpt":"Python Enhancement Proposal 407: 'New release cycle and introducing long-term support versions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:29:25+0900","permalink":"/python/pep/407","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 407 New release cycle and introducing longterm support versions 상태: Deferred | 유형: Process | 작성일: 12Jan2012 PEP 407 – 새로운 릴리스 주기 및 장기 지원 버전 도입 =============================================="},{"id":"2025-09-26-pep-0408-standard-library-preview-package","title":"[Rejected] PEP 408 - Standard library __preview__ package","excerpt":"Python Enhancement Proposal 408: 'Standard library __preview__ package'에 대한 한국어 번역입니다.","date":"2025-09-26 21:30:04+0900","permalink":"/python/pep/408","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 408 Standard library preview package 상태: Rejected | 유형: Standards Track | 작성일: 07Jan2012 PEP 408 – 표준 라이브러리 패키지 작성자: Alyssa Coghlan, Eli Bendersky 상태: Rejected (거부됨) 유형: Standards Track (표준"},{"id":"2025-09-26-pep-0409-suppressing-exception-context","title":"[Final] PEP 409 - Suppressing exception context","excerpt":"Python Enhancement Proposal 409: 'Suppressing exception context'에 대한 한국어 번역입니다.","date":"2025-09-26 21:30:26+0900","permalink":"/python/pep/409","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 409 Suppressing exception context 상태: Final | 유형: Standards Track | 작성일: 26Jan2012 PEP 409 – 예외 컨텍스트 억제 (Suppressing exception context) 개요 (Abstract) PEP 3134의 미해결 문제 중 하나는 예외 컨텍스트를 억제하는 방법"},{"id":"2025-09-26-pep-0410-use-decimal-decimal-type-for-timestamps","title":"[Rejected] PEP 410 - Use decimal.Decimal type for timestamps","excerpt":"Python Enhancement Proposal 410: 'Use decimal.Decimal type for timestamps'에 대한 한국어 번역입니다.","date":"2025-09-26 21:31:14+0900","permalink":"/python/pep/410","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 410 Use decimal.Decimal type for timestamps 상태: Rejected | 유형: Standards Track | 작성일: 01Feb2012 PEP 410 – 타임스탬프에 타입 사용 작성자: Victor Stinner 상태: Rejected (거부됨) 유형: Standards Track 생성일: 2012년 "},{"id":"2025-09-26-pep-0411-provisional-packages-in-the-python-standard-library","title":"[Superseded] PEP 411 - Provisional packages in the Python standard library","excerpt":"Python Enhancement Proposal 411: 'Provisional packages in the Python standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:31:48+0900","permalink":"/python/pep/411","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 411 Provisional packages in the Python standard library 상태: Superseded | 유형: Informational | 작성일: 10Feb2012 PEP 411 – Python 표준 라이브러리의 잠정적(Provisional) 패키지 작성자: Alyssa Coghlan, Eli Bendersk"},{"id":"2025-09-26-pep-0412-key-sharing-dictionary","title":"[Final] PEP 412 - Key-Sharing Dictionary","excerpt":"Python Enhancement Proposal 412: 'Key-Sharing Dictionary'에 대한 한국어 번역입니다.","date":"2025-09-26 21:32:26+0900","permalink":"/python/pep/412","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 412 KeySharing Dictionary 상태: Final | 유형: Standards Track | 작성일: 08Feb2012 PEP 412 – 키 공유 딕셔너리 (KeySharing Dictionary) 개요 (Abstract) 이 PEP는 파이썬 내장 딕셔너리 타입인 의 구현 변경을 제안합니다. 새로운 구현은 객체의 속성 딕셔"},{"id":"2025-09-26-pep-0413-faster-evolution-of-the-python-standard-library","title":"[Withdrawn] PEP 413 - Faster evolution of the Python Standard Library","excerpt":"Python Enhancement Proposal 413: 'Faster evolution of the Python Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:33:07+0900","permalink":"/python/pep/413","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 413 Faster evolution of the Python Standard Library 상태: Withdrawn | 유형: Process | 작성일: 24Feb2012 PEP 413 – Python 표준 라이브러리의 더 빠른 발전 (철회됨) 작성자: Alyssa Coghlan 상태: 철회됨 (Withdrawn) 유형: Process"},{"id":"2025-09-26-pep-0414-explicit-unicode-literal-for-python-3-3","title":"[Final] PEP 414 - Explicit Unicode Literal for Python 3.3","excerpt":"Python Enhancement Proposal 414: 'Explicit Unicode Literal for Python 3.3'에 대한 한국어 번역입니다.","date":"2025-09-26 21:33:55+0900","permalink":"/python/pep/414","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 414 Explicit Unicode Literal for Python 3.3 상태: Final | 유형: Standards Track | 작성일: 15Feb2012 PEP 414 – Python 3.3을 위한 명시적 유니코드 리터럴 작성자: Armin Ronacher, Alyssa Coghlan 상태: Final (최종) 유형: Sta"},{"id":"2025-09-26-pep-0415-implement-context-suppression-with-exception-attributes","title":"[Final] PEP 415 - Implement context suppression with exception attributes","excerpt":"Python Enhancement Proposal 415: 'Implement context suppression with exception attributes'에 대한 한국어 번역입니다.","date":"2025-09-26 21:34:10+0900","permalink":"/python/pep/415","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 415 Implement context suppression with exception attributes 상태: Final | 유형: Standards Track | 작성일: 26Feb2012 PEP 0415: 예외 속성을 이용한 컨텍스트 억제 구현 개요 (Abstract) PEP 0415는 구문을 통해 예외 컨텍스트 표시를 명시적으로"},{"id":"2025-09-26-pep-0416-add-a-frozendict-builtin-type","title":"[Rejected] PEP 416 - Add a frozendict builtin type","excerpt":"Python Enhancement Proposal 416: 'Add a frozendict builtin type'에 대한 한국어 번역입니다.","date":"2025-09-26 21:34:46+0900","permalink":"/python/pep/416","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 416 Add a frozendict builtin type 상태: Rejected | 유형: Standards Track | 작성일: 29Feb2012 파이썬 PEP 416: frozendict 내장 타입 추가 제안 (거부됨) 개요 이 문서는 PEP (Python Enhancement Proposal) 416의 내용을 한국어 사용자가 "},{"id":"2025-09-26-pep-0417-including-mock-in-the-standard-library","title":"[Final] PEP 417 - Including mock in the Standard Library","excerpt":"Python Enhancement Proposal 417: 'Including mock in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:35:01+0900","permalink":"/python/pep/417","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 417 Including mock in the Standard Library 상태: Final | 유형: Standards Track | 작성일: 12Mar2012 PEP 417: 라이브러리의 표준 라이브러리 포함 작성자: Michael Foord 상태: Final (최종) 타입: Standards Track 생성일: 2012년 3월 1"},{"id":"2025-09-26-pep-0418-add-monotonic-time-performance-counter-and-process-time-functions","title":"[Final] PEP 418 - Add monotonic time, performance counter, and process time functions","excerpt":"Python Enhancement Proposal 418: 'Add monotonic time, performance counter, and process time functions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:35:49+0900","permalink":"/python/pep/418","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 418 Add monotonic time, performance counter, and process time functions 상태: Final | 유형: Standards Track | 작성일: 26Mar2012 PEP 418 – 단조 증가 시간, 성능 카운터 및 프로세스 시간 함수 추가 요약 (Abstract) 이 PEP는 Pyth"},{"id":"2025-09-26-pep-0419-protecting-cleanup-statements-from-interruptions","title":"[Deferred] PEP 419 - Protecting cleanup statements from interruptions","excerpt":"Python Enhancement Proposal 419: 'Protecting cleanup statements from interruptions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:36:35+0900","permalink":"/python/pep/419","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 419 Protecting cleanup statements from interruptions 상태: Deferred | 유형: Standards Track | 작성일: 06Apr2012 파이썬 PEP 419: 정리 코드 블록 보호 요약 (Abstract) 이 PEP는 절 또는 Context Manager (컨텍스트 관리자)의 정리 과정"},{"id":"2025-09-26-pep-0420-implicit-namespace-packages","title":"[Final] PEP 420 - Implicit Namespace Packages","excerpt":"Python Enhancement Proposal 420: 'Implicit Namespace Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 21:37:09+0900","permalink":"/python/pep/420","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 420 Implicit Namespace Packages 상태: Final | 유형: Standards Track | 작성일: 19Apr2012 PEP 420 – 암시적 네임스페이스 패키지 (Implicit Namespace Packages) 번역 및 요약 초록 (Abstract) PEP 420은 단일 Python 패키지를 디스크상의 여"},{"id":"2025-09-26-pep-0421-adding-sys-implementation","title":"[Final] PEP 421 - Adding sys.implementation","excerpt":"Python Enhancement Proposal 421: 'Adding sys.implementation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:37:48+0900","permalink":"/python/pep/421","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 421 Adding sys.implementation 상태: Final | 유형: Standards Track | 작성일: 26Apr2012 PEP 421 – 추가 개요 이 PEP (Python Enhancement Proposal)는 모듈에 새로운 속성인 을 도입합니다. 이 속성은 현재 실행 중인 인터프리터 구현에 대한 통합된 정보를 "},{"id":"2025-09-26-pep-0422-simpler-customisation-of-class-creation","title":"[Withdrawn] PEP 422 - Simpler customisation of class creation","excerpt":"Python Enhancement Proposal 422: 'Simpler customisation of class creation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:38:32+0900","permalink":"/python/pep/422","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 422 Simpler customisation of class creation 상태: Withdrawn | 유형: Standards Track | 작성일: 05Jun2012 The provided document is PEP 422 – \"Simpler customisation of class creation\". 참고: 이 PEP는 PEP"},{"id":"2025-09-26-pep-0423-naming-conventions-and-recipes-related-to-packaging","title":"[Deferred] PEP 423 - Naming conventions and recipes related to packaging","excerpt":"Python Enhancement Proposal 423: 'Naming conventions and recipes related to packaging'에 대한 한국어 번역입니다.","date":"2025-09-26 21:39:09+0900","permalink":"/python/pep/423","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 423 Naming conventions and recipes related to packaging 상태: Deferred | 유형: Informational | 작성일: 24May2012 PEP 423은 파이썬 프로젝트, 패키지, 모듈, 그리고 네임스페이스 패키지의 이름 지정 규칙과 관련하여 배포 작성자들을 위한 가이드라인과 노하우를 "},{"id":"2025-09-26-pep-0424-a-method-for-exposing-a-length-hint","title":"[Final] PEP 424 - A method for exposing a length hint","excerpt":"Python Enhancement Proposal 424: 'A method for exposing a length hint'에 대한 한국어 번역입니다.","date":"2025-09-26 21:39:24+0900","permalink":"/python/pep/424","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 424 A method for exposing a length hint 상태: Final | 유형: Standards Track | 작성일: 14Jul2012 PEP 424 – 길이 힌트 노출을 위한 메서드 저자: Alex Gaynor 상태: Final (최종) 유형: Standards Track (표준 추적) 생성일: 2012년 7월 "},{"id":"2025-09-26-pep-0425-compatibility-tags-for-built-distributions","title":"[Final] PEP 425 - Compatibility Tags for Built Distributions","excerpt":"Python Enhancement Proposal 425: 'Compatibility Tags for Built Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:40:01+0900","permalink":"/python/pep/425","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 425 Compatibility Tags for Built Distributions 상태: Final | 유형: Standards Track | 작성일: 27Jul2012 PEP 425 – 빌드된 배포판을 위한 호환성 태그 (Compatibility Tags for Built Distributions) 개요 이 문서는 Python 패키징"},{"id":"2025-09-26-pep-0426-metadata-for-python-software-packages-2-0","title":"[Withdrawn] PEP 426 - Metadata for Python Software Packages 2.0","excerpt":"Python Enhancement Proposal 426: 'Metadata for Python Software Packages 2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 21:40:33+0900","permalink":"/python/pep/426","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 426 Metadata for Python Software Packages 2.0 상태: Withdrawn | 유형: Informational | 작성일: 30Aug2012 PEP 426 – Python 소프트웨어 패키지 메타데이터 2.0 (Metadata for Python Software Packages 2.0) 참고 : 이 PEP는"},{"id":"2025-09-26-pep-0427-the-wheel-binary-package-format-1-0","title":"[Final] PEP 427 - The Wheel Binary Package Format 1.0","excerpt":"Python Enhancement Proposal 427: 'The Wheel Binary Package Format 1.0'에 대한 한국어 번역입니다.","date":"2025-09-26 21:41:28+0900","permalink":"/python/pep/427","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 427 The Wheel Binary Package Format 1.0 상태: Final | 유형: Standards Track | 작성일: 20Sep2012 PEP 427: Wheel 이진 패키지 형식 1.0 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 \"wheel\"이라는 이름의 Python"},{"id":"2025-09-26-pep-0428-the-pathlib-module-object-oriented-filesystem-paths","title":"[Final] PEP 428 - The pathlib module – object-oriented filesystem paths","excerpt":"Python Enhancement Proposal 428: 'The pathlib module – object-oriented filesystem paths'에 대한 한국어 번역입니다.","date":"2025-09-26 21:42:10+0900","permalink":"/python/pep/428","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 428 The pathlib module – objectoriented filesystem paths 상태: Final | 유형: Standards Track | 작성일: 30Jul2012 PEP 428은 모듈을 표준 라이브러리에 포함할 것을 제안합니다. 이 모듈의 목표는 파일 시스템 경로와 관련된 일반적인 작업을 처리하기 위한 간단한 "},{"id":"2025-09-26-pep-0429-python-3-4-release-schedule","title":"[Final] PEP 429 - Python 3.4 Release Schedule","excerpt":"Python Enhancement Proposal 429: 'Python 3.4 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 21:42:34+0900","permalink":"/python/pep/429","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 429 Python 3.4 Release Schedule 상태: Final | 유형: Informational | 작성일: 17Oct2012 PEP 429 – Python 3.4 릴리스 스케줄 이 문서는 Python 3.4의 개발 및 릴리스 스케줄을 설명하며, 주로 PEP (Python Enhancement Proposal) 규모의 항목"},{"id":"2025-09-26-pep-0430-migrating-to-python-3-as-the-default-online-documentation","title":"[Final] PEP 430 - Migrating to Python 3 as the default online documentation","excerpt":"Python Enhancement Proposal 430: 'Migrating to Python 3 as the default online documentation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:43:01+0900","permalink":"/python/pep/430","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 430 Migrating to Python 3 as the default online documentation 상태: Final | 유형: Informational | 작성일: 27Oct2012 PEP 430은 에서 제공되는 Python 온라인 문서의 기본 버전을 Python 2.7에서 Python 3.3으로 마이그레이션하기 위한 전략을"},{"id":"2025-09-26-pep-0431-time-zone-support-improvements","title":"[Superseded] PEP 431 - Time zone support improvements","excerpt":"Python Enhancement Proposal 431: 'Time zone support improvements'에 대한 한국어 번역입니다.","date":"2025-09-26 21:43:32+0900","permalink":"/python/pep/431","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 431 Time zone support improvements 상태: Superseded | 유형: Standards Track | 작성일: 11Dec2012 PEP 431 – 시간대 지원 개선 개요 (Abstract) 이 PEP는 Python 표준 라이브러리에 구체적인 시간대 지원을 구현하고, 일광 절약 시간(DST) 변경 시 모호한 "},{"id":"2025-09-26-pep-0432-restructuring-the-cpython-startup-sequence","title":"[Withdrawn] PEP 432 - Restructuring the CPython startup sequence","excerpt":"Python Enhancement Proposal 432: 'Restructuring the CPython startup sequence'에 대한 한국어 번역입니다.","date":"2025-09-26 21:44:16+0900","permalink":"/python/pep/432","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 432 Restructuring the CPython startup sequence 상태: Withdrawn | 유형: Standards Track | 작성일: 28Dec2012 PEP 432 – CPython 시작 시퀀스 재구성 개요 이 문서는 CPython 인터프리터의 시작 시퀀스를 재구성하여, 참조 인터프리터 실행 파일의 초기화 동"},{"id":"2025-09-26-pep-0433-easier-suppression-of-file-descriptor-inheritance","title":"[Superseded] PEP 433 - Easier suppression of file descriptor inheritance","excerpt":"Python Enhancement Proposal 433: 'Easier suppression of file descriptor inheritance'에 대한 한국어 번역입니다.","date":"2025-09-26 21:45:26+0900","permalink":"/python/pep/433","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 433 Easier suppression of file descriptor inheritance 상태: Superseded | 유형: Standards Track | 작성일: 10Jan2013 PEP 433 – 파일 디스크립터 상속을 더 쉽게 억제하기 (Easier suppression of file descriptor inheritan"},{"id":"2025-09-26-pep-0434-idle-enhancement-exception-for-all-branches","title":"[Active] PEP 434 - IDLE Enhancement Exception for All Branches","excerpt":"Python Enhancement Proposal 434: 'IDLE Enhancement Exception for All Branches'에 대한 한국어 번역입니다.","date":"2025-09-26 21:45:59+0900","permalink":"/python/pep/434","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 434 IDLE Enhancement Exception for All Branches 상태: Active | 유형: Informational | 작성일: 16Feb2013 PEP 434 – 모든 브랜치에 대한 IDLE 향상 기능 예외 작성자: Todd Rovito, Terry Reedy BDFL 위임자 (BDFLDelegate): Aly"},{"id":"2025-09-26-pep-0435-adding-an-enum-type-to-the-python-standard-library","title":"[Final] PEP 435 - Adding an Enum type to the Python standard library","excerpt":"Python Enhancement Proposal 435: 'Adding an Enum type to the Python standard library'에 대한 한국어 번역입니다.","date":"2025-09-26 21:46:35+0900","permalink":"/python/pep/435","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 435 Adding an Enum type to the Python standard library 상태: Final | 유형: Standards Track | 작성일: 23Feb2013 PEP 435 – Python 표준 라이브러리에 Enum 타입 추가 요약 (Abstract) 이 PEP는 Python 표준 라이브러리에 열거형(enume"},{"id":"2025-09-26-pep-0436-the-argument-clinic-dsl","title":"[Final] PEP 436 - The Argument Clinic DSL","excerpt":"Python Enhancement Proposal 436: 'The Argument Clinic DSL'에 대한 한국어 번역입니다.","date":"2025-09-26 21:51:01+0900","permalink":"/python/pep/436","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 436 The Argument Clinic DSL 상태: Final | 유형: Standards Track | 작성일: 22Feb2013 다음은 PEP 436 – The Argument Clinic DSL 문서의 번역 및 요약입니다. 이 문서는 CPython 구현에서 내장 함수(builtin function)의 인자 처리를 용이하게 하기"},{"id":"2025-09-26-pep-0437-a-dsl-for-specifying-signatures-annotations-and-argument-converters","title":"[Rejected] PEP 437 - A DSL for specifying signatures, annotations and argument converters","excerpt":"Python Enhancement Proposal 437: 'A DSL for specifying signatures, annotations and argument converters'에 대한 한국어 번역입니다.","date":"2025-09-26 21:51:38+0900","permalink":"/python/pep/437","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 437 A DSL for specifying signatures, annotations and argument converters 상태: Rejected | 유형: Standards Track | 작성일: 11Mar2013 PEP 437 – 시그니처, 어노테이션 및 인자 컨버터 지정을 위한 DSL 작성자: Stefan Krah 상태: R"},{"id":"2025-09-26-pep-0438-transitioning-to-release-file-hosting-on-pypi","title":"[Superseded] PEP 438 - Transitioning to release-file hosting on PyPI","excerpt":"Python Enhancement Proposal 438: 'Transitioning to release-file hosting on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 21:52:24+0900","permalink":"/python/pep/438","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 438 Transitioning to releasefile hosting on PyPI 상태: Superseded | 유형: Process | 작성일: 15Mar2013 PEP 438 – PyPI에서 릴리스 파일 호스팅으로의 전환 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 (PyPI) 패키지"},{"id":"2025-09-26-pep-0439-inclusion-of-implicit-pip-bootstrap-in-python-installation","title":"[Rejected] PEP 439 - Inclusion of implicit pip bootstrap in Python installation","excerpt":"Python Enhancement Proposal 439: 'Inclusion of implicit pip bootstrap in Python installation'에 대한 한국어 번역입니다.","date":"2025-09-26 21:52:55+0900","permalink":"/python/pep/439","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 439 Inclusion of implicit pip bootstrap in Python installation 상태: Rejected | 유형: Standards Track | 작성일: 18Mar2013 PEP 439 – Python 설치 시 암묵적인 pip 부트스트랩 포함 요약 (Abstract) 이 PEP는 Python 사용자들이 "},{"id":"2025-09-26-pep-0440-version-identification-and-dependency-specification","title":"[Final] PEP 440 - Version Identification and Dependency Specification","excerpt":"Python Enhancement Proposal 440: 'Version Identification and Dependency Specification'에 대한 한국어 번역입니다.","date":"2025-09-26 21:53:38+0900","permalink":"/python/pep/440","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 440 Version Identification and Dependency Specification 상태: Final | 유형: Standards Track | 작성일: 18Mar2013 PEP 440 – 버전 식별 및 의존성 명세 (Version Identification and Dependency Specification) 개요 (A"},{"id":"2025-09-26-pep-0441-improving-python-zip-application-support","title":"[Final] PEP 441 - Improving Python ZIP Application Support","excerpt":"Python Enhancement Proposal 441: 'Improving Python ZIP Application Support'에 대한 한국어 번역입니다.","date":"2025-09-26 21:54:04+0900","permalink":"/python/pep/441","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 441 Improving Python ZIP Application Support 상태: Final | 유형: Standards Track | 작성일: 30Mar2013 PEP 441 – Python ZIP 애플리케이션 지원 개선 이 문서는 Python 3.5 버전에서 도입된 PEP 441의 주요 내용을 한국어 사용자들이 이해하기 쉽게 번"},{"id":"2025-09-26-pep-0442-safe-object-finalization","title":"[Final] PEP 442 - Safe object finalization","excerpt":"Python Enhancement Proposal 442: 'Safe object finalization'에 대한 한국어 번역입니다.","date":"2025-09-26 21:54:42+0900","permalink":"/python/pep/442","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 442 Safe object finalization 상태: Final | 유형: Standards Track | 작성일: 18May2013 파이썬 개발자를 위한 PEP 442 – 안전한 객체 마무리 (Safe Object Finalization) 요약 (Abstract) 이 PEP(Python Enhancement Proposal)는 현"},{"id":"2025-09-26-pep-0443-single-dispatch-generic-functions","title":"[Final] PEP 443 - Single-dispatch generic functions","excerpt":"Python Enhancement Proposal 443: 'Single-dispatch generic functions'에 대한 한국어 번역입니다.","date":"2025-09-26 21:55:16+0900","permalink":"/python/pep/443","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 443 Singledispatch generic functions 상태: Final | 유형: Standards Track | 작성일: 22May2013 PEP 443: Singledispatch Generic Functions 개요 (Abstract) 이 PEP는 표준 라이브러리 모듈에 \"단일 디스패치 제네릭 함수(singledispa"},{"id":"2025-09-26-pep-0444-python-web3-interface","title":"[Deferred] PEP 444 - Python Web3 Interface","excerpt":"Python Enhancement Proposal 444: 'Python Web3 Interface'에 대한 한국어 번역입니다.","date":"2025-09-26 21:57:54+0900","permalink":"/python/pep/444","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 444 Python Web3 Interface 상태: Deferred | 유형: Informational | 작성일: 19Jul2010 PEP 444 – Python Web3 인터페이스 저자: Chris McDonough, Armin Ronacher 논의: WebSIG list 상태: 연기됨 (Deferred) 유형: 정보 제공 (Inf"},{"id":"2025-09-26-pep-0445-add-new-apis-to-customize-python-memory-allocators","title":"[Final] PEP 445 - Add new APIs to customize Python memory allocators","excerpt":"Python Enhancement Proposal 445: 'Add new APIs to customize Python memory allocators'에 대한 한국어 번역입니다.","date":"2025-09-26 21:58:25+0900","permalink":"/python/pep/445","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 445 Add new APIs to customize Python memory allocators 상태: Final | 유형: Standards Track | 작성일: 15Jun2013 PEP 445 – Python 메모리 할당자 사용자 정의를 위한 새로운 API 추가 개요 이 PEP는 Python 메모리 할당자를 사용자 정의하기 위한 "},{"id":"2025-09-26-pep-0446-make-newly-created-file-descriptors-non-inheritable","title":"[Final] PEP 446 - Make newly created file descriptors non-inheritable","excerpt":"Python Enhancement Proposal 446: 'Make newly created file descriptors non-inheritable'에 대한 한국어 번역입니다.","date":"2025-09-26 21:58:51+0900","permalink":"/python/pep/446","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 446 Make newly created file descriptors noninheritable 상태: Final | 유형: Standards Track | 작성일: 05Aug2013 PEP 446: 새로 생성되는 파일 디스크립터를 기본적으로 비상속으로 만들기 요약 (Abstract) 자식 프로세스로 파일 디스크립터가 누출(leakin"},{"id":"2025-09-26-pep-0447-add-getdescriptor-method-to-metaclass","title":"[Deferred] PEP 447 - Add __getdescriptor__ method to metaclass","excerpt":"Python Enhancement Proposal 447: 'Add __getdescriptor__ method to metaclass'에 대한 한국어 번역입니다.","date":"2025-09-26 21:59:30+0900","permalink":"/python/pep/447","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 447 Add getdescriptor method to metaclass 상태: Deferred | 유형: Standards Track | 작성일: 12Jun2013 PEP 447 – 메타클래스(Metaclass)에 메서드 추가 제안 작성자: Ronald Oussoren 상태: 연기됨 (Deferred) 유형: 표준 트랙 (Standa"},{"id":"2025-09-26-pep-0448-additional-unpacking-generalizations","title":"[Final] PEP 448 - Additional Unpacking Generalizations","excerpt":"Python Enhancement Proposal 448: 'Additional Unpacking Generalizations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:00+0900","permalink":"/python/pep/448","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 448 Additional Unpacking Generalizations 상태: Final | 유형: Standards Track | 작성일: 29Jun2013 PEP 448 – 추가적인 언패킹 일반화 (Additional Unpacking Generalizations) 요약 (Abstract) 이 PEP(Python Enhancemen"},{"id":"2025-09-26-pep-0449-removal-of-the-pypi-mirror-auto-discovery-and-naming-scheme","title":"[Final] PEP 449 - Removal of the PyPI Mirror Auto Discovery and Naming Scheme","excerpt":"Python Enhancement Proposal 449: 'Removal of the PyPI Mirror Auto Discovery and Naming Scheme'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:20+0900","permalink":"/python/pep/449","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 449 Removal of the PyPI Mirror Auto Discovery and Naming Scheme 상태: Final | 유형: Process | 작성일: 04Aug2013 PEP 449는 PyPI (Python Package Index) 미러의 자동 검색 및 명명 체계를 제거하기 위한 제안입니다. 이 PEP는 기존 미러링"},{"id":"2025-09-26-pep-0450-adding-a-statistics-module-to-the-standard-library","title":"[Final] PEP 450 - Adding A Statistics Module To The Standard Library","excerpt":"Python Enhancement Proposal 450: 'Adding A Statistics Module To The Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 22:00:57+0900","permalink":"/python/pep/450","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 450 Adding A Statistics Module To The Standard Library 상태: Final | 유형: Standards Track | 작성일: 01Aug2013 PEP 450 – 표준 라이브러리에 통계 모듈 추가 작성자: Steven D'Aprano 상태: Final 유형: Standards Track 생성일: "},{"id":"2025-09-26-pep-0451-a-modulespec-type-for-the-import-system","title":"[Final] PEP 451 - A ModuleSpec Type for the Import System","excerpt":"Python Enhancement Proposal 451: 'A ModuleSpec Type for the Import System'에 대한 한국어 번역입니다.","date":"2025-09-26 22:02:47+0900","permalink":"/python/pep/451","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 451 A ModuleSpec Type for the Import System 상태: Final | 유형: Standards Track | 작성일: 08Aug2013 PEP 451 – Import 시스템을 위한 ModuleSpec 타입 작성자: Eric Snow BDFLDelegate: Brett Cannon, Alyssa Coghlan"},{"id":"2025-09-26-pep-0452-api-for-cryptographic-hash-functions-v2-0","title":"[Final] PEP 452 - API for Cryptographic Hash Functions v2.0","excerpt":"Python Enhancement Proposal 452: 'API for Cryptographic Hash Functions v2.0'에 대한 한국어 번역입니다.","date":"2025-09-26 22:03:16+0900","permalink":"/python/pep/452","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 452 API for Cryptographic Hash Functions v2.0 상태: Final | 유형: Informational | 작성일: 15Aug2013 PEP 452 – 암호화 해시 함수용 API v2.0 번역 및 정리 개요 (Abstract) MD5 또는 SHA와 같은 암호화 해싱 알고리즘을 구현하는 여러 모듈이 존재합니"},{"id":"2025-09-26-pep-0453-explicit-bootstrapping-of-pip-in-python-installations","title":"[Final] PEP 453 - Explicit bootstrapping of pip in Python installations","excerpt":"Python Enhancement Proposal 453: 'Explicit bootstrapping of pip in Python installations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:03:55+0900","permalink":"/python/pep/453","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 453 Explicit bootstrapping of pip in Python installations 상태: Final | 유형: Standards Track | 작성일: 10Aug2013 파이썬 PEP 0453은 \"Explicit bootstrapping of pip in Python installations\"이라는 제목의 제안서입니"},{"id":"2025-09-26-pep-0454-add-a-new-tracemalloc-module-to-trace-python-memory-allocations","title":"[Final] PEP 454 - Add a new tracemalloc module to trace Python memory allocations","excerpt":"Python Enhancement Proposal 454: 'Add a new tracemalloc module to trace Python memory allocations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:04:39+0900","permalink":"/python/pep/454","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 454 Add a new tracemalloc module to trace Python memory allocations 상태: Final | 유형: Standards Track | 작성일: 03Sep2013 PEP 454 – Python 메모리 할당 추적을 위한 새로운 모듈 추가 초록 (Abstract) 이 PEP는 Python에 의해"},{"id":"2025-09-26-pep-0455-adding-a-key-transforming-dictionary-to-collections","title":"[Rejected] PEP 455 - Adding a key-transforming dictionary to collections","excerpt":"Python Enhancement Proposal 455: 'Adding a key-transforming dictionary to collections'에 대한 한국어 번역입니다.","date":"2025-09-26 22:05:06+0900","permalink":"/python/pep/455","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 455 Adding a keytransforming dictionary to collections 상태: Rejected | 유형: Standards Track | 작성일: 13Sep2013 PEP 455 – 모듈에 키 변환 딕셔너리 추가 개요 (Abstract) 이 PEP는 모듈을 위한 새로운 데이터 구조인 \"TransformDict\""},{"id":"2025-09-26-pep-0456-secure-and-interchangeable-hash-algorithm","title":"[Final] PEP 456 - Secure and interchangeable hash algorithm","excerpt":"Python Enhancement Proposal 456: 'Secure and interchangeable hash algorithm'에 대한 한국어 번역입니다.","date":"2025-09-26 22:05:49+0900","permalink":"/python/pep/456","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 456 Secure and interchangeable hash algorithm 상태: Final | 유형: Standards Track | 작성일: 27Sep2013 PEP 456 – 안전하고 상호 교환 가능한 해시 알고리즘 (Secure and interchangeable hash algorithm) 초록 (Abstract) 이 P"},{"id":"2025-09-26-pep-0457-notation-for-positional-only-parameters","title":"[Final] PEP 457 - Notation For Positional-Only Parameters","excerpt":"Python Enhancement Proposal 457: 'Notation For Positional-Only Parameters'에 대한 한국어 번역입니다.","date":"2025-09-26 22:06:22+0900","permalink":"/python/pep/457","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 457 Notation For PositionalOnly Parameters 상태: Final | 유형: Informational | 작성일: 08Oct2013 PEP 457 – 위치 전용 매개변수 표기법 작성자 : Larry Hastings <larry at hastings.org 논의 : PythonDev list 상태 : Final"},{"id":"2025-09-26-pep-0458-secure-pypi-downloads-with-signed-repository-metadata","title":"[Accepted] PEP 458 - Secure PyPI downloads with signed repository metadata","excerpt":"Python Enhancement Proposal 458: 'Secure PyPI downloads with signed repository metadata'에 대한 한국어 번역입니다.","date":"2025-09-26 22:07:13+0900","permalink":"/python/pep/458","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 458 Secure PyPI downloads with signed repository metadata 상태: Accepted | 유형: Standards Track | 작성일: 27Sep2013 PEP 458은 PyPI(Python Package Index) 다운로드의 보안 강화를 위한 제안서입니다. 이 PEP는 사용자가 PyPI에서 "},{"id":"2025-09-26-pep-0459-standard-metadata-extensions-for-python-software-packages","title":"[Withdrawn] PEP 459 - Standard Metadata Extensions for Python Software Packages","excerpt":"Python Enhancement Proposal 459: 'Standard Metadata Extensions for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:08:18+0900","permalink":"/python/pep/459","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 459 Standard Metadata Extensions for Python Software Packages 상태: Withdrawn | 유형: Standards Track | 작성일: 11Nov2013 PEP 459 – Python 소프트웨어 패키지를 위한 표준 메타데이터 확장 작성자: Alyssa Coghlan BDFLDelegat"},{"id":"2025-09-26-pep-0460-add-binary-interpolation-and-formatting","title":"[Withdrawn] PEP 460 - Add binary interpolation and formatting","excerpt":"Python Enhancement Proposal 460: 'Add binary interpolation and formatting'에 대한 한국어 번역입니다.","date":"2025-09-26 22:08:41+0900","permalink":"/python/pep/460","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 460 Add binary interpolation and formatting 상태: Withdrawn | 유형: Standards Track | 작성일: 06Jan2014 PEP 460 – 바이너리 보간 및 포매팅 추가 (Add binary interpolation and formatting) 작성자 : Antoine Pitrou 상태"},{"id":"2025-09-26-pep-0461-adding-formatting-to-bytes-and-bytearray","title":"[Final] PEP 461 - Adding % formatting to bytes and bytearray","excerpt":"Python Enhancement Proposal 461: 'Adding % formatting to bytes and bytearray'에 대한 한국어 번역입니다.","date":"2025-09-26 22:09:29+0900","permalink":"/python/pep/461","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 461 Adding % formatting to bytes and bytearray 상태: Final | 유형: Standards Track | 작성일: 13Jan2014 PEP 461: 및 에 포매팅 추가 작성자: Ethan Furman 상태: Final (최종) 유형: Standards Track 생성일: 2014년 1월 13일 Py"},{"id":"2025-09-26-pep-0462-core-development-workflow-automation-for-cpython","title":"[Withdrawn] PEP 462 - Core development workflow automation for CPython","excerpt":"Python Enhancement Proposal 462: 'Core development workflow automation for CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 22:10:21+0900","permalink":"/python/pep/462","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 462 Core development workflow automation for CPython 상태: Withdrawn | 유형: Process | 작성일: 23Jan2014 PEP 462 – CPython 핵심 개발 워크플로우 자동화 작성자: Alyssa Coghlan 상태: 철회됨 (Withdrawn) 유형: 프로세스 (Process"},{"id":"2025-09-26-pep-0463-exception-catching-expressions","title":"[Rejected] PEP 463 - Exception-catching expressions","excerpt":"Python Enhancement Proposal 463: 'Exception-catching expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 22:11:32+0900","permalink":"/python/pep/463","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 463 Exceptioncatching expressions 상태: Rejected | 유형: Standards Track | 작성일: 15Feb2014 PEP 463 – 예외 처리 표현식 (Exceptioncatching expressions) 번역 및 정리 개요 PEP 463은 표현식(expression) 내에서 예외(exceptio"},{"id":"2025-09-26-pep-0464-removal-of-the-pypi-mirror-authenticity-api","title":"[Final] PEP 464 - Removal of the PyPI Mirror Authenticity API","excerpt":"Python Enhancement Proposal 464: 'Removal of the PyPI Mirror Authenticity API'에 대한 한국어 번역입니다.","date":"2025-09-26 22:11:47+0900","permalink":"/python/pep/464","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 464 Removal of the PyPI Mirror Authenticity API 상태: Final | 유형: Process | 작성일: 02Mar2014 PEP 464 – PyPI 미러 인증 API 제거 요약 (Abstract) 이 PEP(Python Enhancement Proposal)는 PyPI(Python Package In"},{"id":"2025-09-26-pep-0465-a-dedicated-infix-operator-for-matrix-multiplication","title":"[Final] PEP 465 - A dedicated infix operator for matrix multiplication","excerpt":"Python Enhancement Proposal 465: 'A dedicated infix operator for matrix multiplication'에 대한 한국어 번역입니다.","date":"2025-09-26 22:12:46+0900","permalink":"/python/pep/465","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 465 A dedicated infix operator for matrix multiplication 상태: Final | 유형: Standards Track | 작성일: 20Feb2014 PEP 465: 행렬 곱셈을 위한 전용 중위(Infix) 연산자 추가 제안 요약 이 PEP는 Python에 행렬 곱셈을 위한 새로운 이항(binary"},{"id":"2025-09-26-pep-0466-network-security-enhancements-for-python-2-7-x","title":"[Final] PEP 466 - Network Security Enhancements for Python 2.7.x","excerpt":"Python Enhancement Proposal 466: 'Network Security Enhancements for Python 2.7.x'에 대한 한국어 번역입니다.","date":"2025-09-26 22:13:59+0900","permalink":"/python/pep/466","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 466 Network Security Enhancements for Python 2.7.x 상태: Final | 유형: Standards Track | 작성일: 23Mar2014 PEP 466 – Python 2.7.x의 네트워크 보안 강화 작성자: Alyssa Coghlan <ncoghlan at gmail.com 상태: Final 유"},{"id":"2025-09-26-pep-0467-minor-api-improvements-for-binary-sequences","title":"[Draft] PEP 467 - Minor API improvements for binary sequences","excerpt":"Python Enhancement Proposal 467: 'Minor API improvements for binary sequences'에 대한 한국어 번역입니다.","date":"2025-09-26 22:14:25+0900","permalink":"/python/pep/467","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 467 Minor API improvements for binary sequences 상태: Draft | 유형: Standards Track | 작성일: 30Mar2014 PEP 467 – 바이너리 시퀀스를 위한 사소한 API 개선 (Minor API improvements for binary sequences) 개요 (Abstract"},{"id":"2025-09-26-pep-0468-preserving-the-order-of-kwargs-in-a-function-","title":"[Final] PEP 468 - Preserving the order of **kwargs in a function.","excerpt":"Python Enhancement Proposal 468: 'Preserving the order of ** kwargs in a function.'에 대한 한국어 번역입니다.","date":"2025-09-26 22:15:02+0900","permalink":"/python/pep/468","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 468 Preserving the order of kwargs in a function. 상태: Final | 유형: Standards Track | 작성일: 05Apr2014 PEP 468 – 함수 내 순서 유지 개요 (Abstract) 함수 정의에서 구문은 다른 이름 있는 매개변수에 해당하지 않는 모든 키워드 인자(keyword ar"},{"id":"2025-09-26-pep-0469-migration-of-dict-iteration-code-to-python-3","title":"[Withdrawn] PEP 469 - Migration of dict iteration code to Python 3","excerpt":"Python Enhancement Proposal 469: 'Migration of dict iteration code to Python 3'에 대한 한국어 번역입니다.","date":"2025-09-26 22:15:48+0900","permalink":"/python/pep/469","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 469 Migration of dict iteration code to Python 3 상태: Withdrawn | 유형: Standards Track | 작성일: 18Apr2014 PEP 469 – Python 3로의 순회(Iteration) 코드 마이그레이션 저자: Alyssa Coghlan 상태: 철회됨 (Withdrawn) 유형:"},{"id":"2025-09-26-pep-0470-removing-external-hosting-support-on-pypi","title":"[Final] PEP 470 - Removing External Hosting Support on PyPI","excerpt":"Python Enhancement Proposal 470: 'Removing External Hosting Support on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 22:17:18+0900","permalink":"/python/pep/470","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 470 Removing External Hosting Support on PyPI 상태: Final | 유형: Process | 작성일: 12May2014 PEP 470 – PyPI 외부 호스팅 지원 제거 초록 (Abstract) PEP 470은 PyPI(Python Package Index) 외부에서 파일을 호스팅하는 기능과 PEP 4"},{"id":"2025-09-26-pep-0471-os-scandir-function-a-better-and-faster-directory-iterator","title":"[Final] PEP 471 - os.scandir() function – a better and faster directory iterator","excerpt":"Python Enhancement Proposal 471: 'os.scandir() function – a better and faster directory iterator'에 대한 한국어 번역입니다.","date":"2025-09-26 22:18:05+0900","permalink":"/python/pep/471","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 471 os.scandir() function – a better and faster directory iterator 상태: Final | 유형: Standards Track | 작성일: 30May2014 다음은 PEP 471 – 함수 – 더 좋고 빠른 디렉토리 이터레이터에 대한 번역 및 요약입니다. 이 PEP는 Python 3.5에 "},{"id":"2025-09-26-pep-0472-support-for-indexing-with-keyword-arguments","title":"[Rejected] PEP 472 - Support for indexing with keyword arguments","excerpt":"Python Enhancement Proposal 472: 'Support for indexing with keyword arguments'에 대한 한국어 번역입니다.","date":"2025-09-26 22:19:17+0900","permalink":"/python/pep/472","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 472 Support for indexing with keyword arguments 상태: Rejected | 유형: Standards Track | 작성일: 24Jun2014 PEP 472 – 키워드 인수를 사용한 인덱싱 지원 (Rejected) 작성자: Stefano Borini, Joseph MartinotLagarde 논의 채널"},{"id":"2025-09-26-pep-0473-adding-structured-data-to-built-in-exceptions","title":"[Rejected] PEP 473 - Adding structured data to built-in exceptions","excerpt":"Python Enhancement Proposal 473: 'Adding structured data to built-in exceptions'에 대한 한국어 번역입니다.","date":"2025-09-26 22:19:47+0900","permalink":"/python/pep/473","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 473 Adding structured data to builtin exceptions 상태: Rejected | 유형: Standards Track | 작성일: 29Mar2014 PEP 473 – 내장 예외에 구조화된 데이터 추가 작성자: Sebastian Kreft <skreft at deezer.com 상태: Rejected (거부"},{"id":"2025-09-26-pep-0474-creating-forge-python-org","title":"[Withdrawn] PEP 474 - Creating forge.python.org","excerpt":"Python Enhancement Proposal 474: 'Creating forge.python.org'에 대한 한국어 번역입니다.","date":"2025-09-26 22:21:06+0900","permalink":"/python/pep/474","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 474 Creating forge.python.org 상태: Withdrawn | 유형: Process | 작성일: 19Jul2014 PEP 474 – forge.python.org 생성 개요 이 PEP (Python Enhancement Proposal)는 새로운 PSF(Python Software Foundation) 제공 리소스인 "},{"id":"2025-09-26-pep-0475-retry-system-calls-failing-with-eintr","title":"[Final] PEP 475 - Retry system calls failing with EINTR","excerpt":"Python Enhancement Proposal 475: 'Retry system calls failing with EINTR'에 대한 한국어 번역입니다.","date":"2025-09-26 22:21:40+0900","permalink":"/python/pep/475","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 475 Retry system calls failing with EINTR 상태: Final | 유형: Standards Track | 작성일: 29Jul2014 PEP 475 – 오류로 실패하는 시스템 호출 재시도 초록 (Abstract) 표준 라이브러리에서 제공하는 시스템 호출 래퍼(wrapper)는 오류로 실패할 경우 자동으로 재시"},{"id":"2025-09-26-pep-0476-enabling-certificate-verification-by-default-for-stdlib-http-clients","title":"[Final] PEP 476 - Enabling certificate verification by default for stdlib http clients","excerpt":"Python Enhancement Proposal 476: 'Enabling certificate verification by default for stdlib http clients'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:04+0900","permalink":"/python/pep/476","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 476 Enabling certificate verification by default for stdlib http clients 상태: Final | 유형: Standards Track | 작성일: 28Aug2014 PEP 476 – HTTP 클라이언트에 기본적으로 인증서 검증 활성화 초록 (Abstract) 현재 Python 표준 라"},{"id":"2025-09-26-pep-0477-backport-ensurepip-pep-453-to-python-2-7","title":"[Final] PEP 477 - Backport ensurepip (PEP 453) to Python 2.7","excerpt":"Python Enhancement Proposal 477: 'Backport ensurepip (PEP 453) to Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:24+0900","permalink":"/python/pep/477","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 477 Backport ensurepip (PEP 453) to Python 2.7 상태: Final | 유형: Standards Track | 작성일: 26Aug2014 PEP 477 – Python 2.7에 ensurepip (PEP 453) 백포트 개요 이 문서는 PEP 453에 의해 Python 3.4에 추가된 모듈을 Python"},{"id":"2025-09-26-pep-0478-python-3-5-release-schedule","title":"[Final] PEP 478 - Python 3.5 Release Schedule","excerpt":"Python Enhancement Proposal 478: 'Python 3.5 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 22:22:47+0900","permalink":"/python/pep/478","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 478 Python 3.5 Release Schedule 상태: Final | 유형: Informational | 작성일: 22Sep2014 PEP 478 – Python 3.5 릴리스 일정 이 문서는 Python 3.5의 개발 및 릴리스 일정에 대해 설명하며, 주로 PEP(Python Enhancement Proposal)에 해당하는 "},{"id":"2025-09-26-pep-0479-change-stopiteration-handling-inside-generators","title":"[Final] PEP 479 - Change StopIteration handling inside generators","excerpt":"Python Enhancement Proposal 479: 'Change StopIteration handling inside generators'에 대한 한국어 번역입니다.","date":"2025-09-26 22:23:28+0900","permalink":"/python/pep/479","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 479 Change StopIteration handling inside generators 상태: Final | 유형: Standards Track | 작성일: 15Nov2014 PEP 479 – 제너레이터 내 처리 방식 변경 제안 작성자: Chris Angelico, Guido van Rossum 상태: Final (최종) 유형: S"},{"id":"2025-09-26-pep-0480-surviving-a-compromise-of-pypi-end-to-end-signing-of-packages","title":"[Draft] PEP 480 - Surviving a Compromise of PyPI: End-to-end signing of packages","excerpt":"Python Enhancement Proposal 480: 'Surviving a Compromise of PyPI: End-to-end signing of packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:26:39+0900","permalink":"/python/pep/480","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 480 Surviving a Compromise of PyPI: Endtoend signing of packages 상태: Draft | 유형: Standards Track | 작성일: 08Oct2014 PEP 480 – PyPI 침해로부터 생존: 패키지의 종단 간 서명 (Endtoend signing of packages) 초록 (Ab"},{"id":"2025-09-26-pep-0481-migrate-cpython-to-git-github-and-phabricator","title":"[Withdrawn] PEP 481 - Migrate CPython to Git, Github, and Phabricator","excerpt":"Python Enhancement Proposal 481: 'Migrate CPython to Git, Github, and Phabricator'에 대한 한국어 번역입니다.","date":"2025-09-26 22:27:20+0900","permalink":"/python/pep/481","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 481 Migrate CPython to Git, Github, and Phabricator 상태: Withdrawn | 유형: Process | 작성일: 29Nov2014 PEP 481: CPython을 Git, GitHub, Phabricator로 이전하기 작성자: Donald Stufft 상태: 철회됨 (Withdrawn) 유형: "},{"id":"2025-09-26-pep-0482-literature-overview-for-type-hints","title":"[Final] PEP 482 - Literature Overview for Type Hints","excerpt":"Python Enhancement Proposal 482: 'Literature Overview for Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:27:40+0900","permalink":"/python/pep/482","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 482 Literature Overview for Type Hints 상태: Final | 유형: Informational | 작성일: 08Jan2015 PEP 482 – 타입 힌트에 대한 문헌 개요 초록 (Abstract) 이 PEP는 타입 힌트 (Type Hinting)와 관련된 세 가지 PEP 중 하나입니다. 이 문서 (PEP 48"},{"id":"2025-09-26-pep-0483-the-theory-of-type-hints","title":"[Final] PEP 483 - The Theory of Type Hints","excerpt":"Python Enhancement Proposal 483: 'The Theory of Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:28:39+0900","permalink":"/python/pep/483","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 483 The Theory of Type Hints 상태: Final | 유형: Informational | 작성일: 19Dec2014 PEP 483 – 타입 힌트의 이론 (The Theory of Type Hints) 개요 (Abstract) 이 PEP는 PEP 484에서 참조되는 타입 이론을 설명합니다. 서론 (Introduction"},{"id":"2025-09-26-pep-0484-type-hints","title":"[Final] PEP 484 - Type Hints","excerpt":"Python Enhancement Proposal 484: 'Type Hints'에 대한 한국어 번역입니다.","date":"2025-09-26 22:32:07+0900","permalink":"/python/pep/484","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 484 Type Hints 상태: Final | 유형: Standards Track | 작성일: 29Sep2014 PEP 484 – 타입 힌트 (Type Hints) 한국어 번역 및 정리 개요 PEP 3107은 함수 어노테이션(function annotations)을 위한 문법을 도입했지만, 그 의미는 의도적으로 정의되지 않은 채로 남겨"},{"id":"2025-09-26-pep-0485-a-function-for-testing-approximate-equality","title":"[Final] PEP 485 - A Function for testing approximate equality","excerpt":"Python Enhancement Proposal 485: 'A Function for testing approximate equality'에 대한 한국어 번역입니다.","date":"2025-09-26 22:32:41+0900","permalink":"/python/pep/485","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 485 A Function for testing approximate equality 상태: Final | 유형: Standards Track | 작성일: 20Jan2015 PEP 485 – 근사치 동등성 테스트 함수 초록 (Abstract) 이 PEP는 두 값이 서로 근사적으로 같거나 \"가까운\"지 여부를 판단하는 함수를 표준 라이브러리"},{"id":"2025-09-26-pep-0486-make-the-python-launcher-aware-of-virtual-environments","title":"[Final] PEP 486 - Make the Python Launcher aware of virtual environments","excerpt":"Python Enhancement Proposal 486: 'Make the Python Launcher aware of virtual environments'에 대한 한국어 번역입니다.","date":"2025-09-26 22:33:02+0900","permalink":"/python/pep/486","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 486 Make the Python Launcher aware of virtual environments 상태: Final | 유형: Standards Track | 작성일: 12Feb2015 작성자: Paul Moore 상태: 최종 (Final) 타입: 표준 트랙 (Standards Track) 생성일: 2015년 2월 12일 Pyth"},{"id":"2025-09-26-pep-0487-simpler-customisation-of-class-creation","title":"[Final] PEP 487 - Simpler customisation of class creation","excerpt":"Python Enhancement Proposal 487: 'Simpler customisation of class creation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:33:21+0900","permalink":"/python/pep/487","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 487 Simpler customisation of class creation 상태: Final | 유형: Standards Track | 작성일: 27Feb2015 PEP 487 – 클래스 생성 사용자 정의 간소화 개요 이 PEP (Python Enhancement Proposal)는 클래스 생성 시 사용자 정의 방식을 간소화하기 위해"},{"id":"2025-09-26-pep-0488-elimination-of-pyo-files","title":"[Final] PEP 488 - Elimination of PYO files","excerpt":"Python Enhancement Proposal 488: 'Elimination of PYO files'에 대한 한국어 번역입니다.","date":"2025-09-26 22:34:01+0900","permalink":"/python/pep/488","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 488 Elimination of PYO files 상태: Final | 유형: Standards Track | 작성일: 20Feb2015 PEP 488 – PYO 파일 제거 초록 (Abstract) 이 PEP는 Python에서 PYO 파일 개념을 제거할 것을 제안합니다. 바이트코드 파일이 최적화 수준에 따라 분리되어 저장되는 방식을 계"},{"id":"2025-09-26-pep-0489-multi-phase-extension-module-initialization","title":"[Final] PEP 489 - Multi-phase extension module initialization","excerpt":"Python Enhancement Proposal 489: 'Multi-phase extension module initialization'에 대한 한국어 번역입니다.","date":"2025-09-26 22:35:12+0900","permalink":"/python/pep/489","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 489 Multiphase extension module initialization 상태: Final | 유형: Standards Track | 작성일: 11Aug2013 PEP 489 – 다단계 확장 모듈 초기화 개요 (Abstract) 이 PEP는 내장(builtin) 및 확장 모듈(extension module)이 임포트(impor"},{"id":"2025-09-26-pep-0490-chain-exceptions-at-c-level","title":"[Rejected] PEP 490 - Chain exceptions at C level","excerpt":"Python Enhancement Proposal 490: 'Chain exceptions at C level'에 대한 한국어 번역입니다.","date":"2025-09-26 22:35:41+0900","permalink":"/python/pep/490","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 490 Chain exceptions at C level 상태: Rejected | 유형: Standards Track | 작성일: 25Mar2015 PEP 490 – C 수준에서 예외 연결 (Chain exceptions at C level) 작성자: Victor Stinner <vstinner at python.org 상태: Reje"},{"id":"2025-09-26-pep-0491-the-wheel-binary-package-format-1-9","title":"[Deferred] PEP 491 - The Wheel Binary Package Format 1.9","excerpt":"Python Enhancement Proposal 491: 'The Wheel Binary Package Format 1.9'에 대한 한국어 번역입니다.","date":"2025-09-26 22:36:44+0900","permalink":"/python/pep/491","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 491 The Wheel Binary Package Format 1.9 상태: Deferred | 유형: Standards Track | 작성일: 16Apr2015 PEP 491 – Wheel 바이너리 패키지 형식 1.9 초록 (Abstract) 이 PEP는 Python을 위한 빌드된 패키지 형식의 두 번째 버전인 \"wheel\"에 대해 "},{"id":"2025-09-26-pep-0492-coroutines-with-async-and-await-syntax","title":"[Final] PEP 492 - Coroutines with async and await syntax","excerpt":"Python Enhancement Proposal 492: 'Coroutines with async and await syntax'에 대한 한국어 번역입니다.","date":"2025-09-26 22:37:54+0900","permalink":"/python/pep/492","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 492 Coroutines with async and await syntax 상태: Final | 유형: Standards Track | 작성일: 09Apr2015 PEP 492 – 및 구문을 사용한 코루틴 저자: Yury Selivanov 상태: Final 유형: Standards Track 생성일: 2015년 4월 9일 Python "},{"id":"2025-09-26-pep-0493-https-verification-migration-tools-for-python-2-7","title":"[Final] PEP 493 - HTTPS verification migration tools for Python 2.7","excerpt":"Python Enhancement Proposal 493: 'HTTPS verification migration tools for Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 22:38:56+0900","permalink":"/python/pep/493","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 493 HTTPS verification migration tools for Python 2.7 상태: Final | 유형: Standards Track | 작성일: 10May2015 PEP 493 – Python 2.7용 HTTPS 검증 마이그레이션 도구 작성자: Alyssa Coghlan, Robert Kuska, MarcAndré "},{"id":"2025-09-26-pep-0494-python-3-6-release-schedule","title":"[Final] PEP 494 - Python 3.6 Release Schedule","excerpt":"Python Enhancement Proposal 494: 'Python 3.6 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 22:39:23+0900","permalink":"/python/pep/494","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 494 Python 3.6 Release Schedule 상태: Final | 유형: Informational | 작성일: 30May2015 PEP 494: Python 3.6 릴리스 일정 이 문서는 Python 3.6의 개발 및 릴리스 일정을 설명하는 Python Enhancement Proposal (PEP)입니다. 이 PEP는 주로"},{"id":"2025-09-26-pep-0495-local-time-disambiguation","title":"[Final] PEP 495 - Local Time Disambiguation","excerpt":"Python Enhancement Proposal 495: 'Local Time Disambiguation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:40:14+0900","permalink":"/python/pep/495","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 495 Local Time Disambiguation 상태: Final | 유형: Standards Track | 작성일: 02Aug2015 PEP 495: 로컬 시간 중의성 해소 초록 (Abstract) 이 PEP는 및 클래스의 인스턴스에 라는 새로운 속성을 추가하는 것을 제안합니다. 이 속성은 로컬 시간이 동일하지만 실제 시점은 다른"},{"id":"2025-09-26-pep-0496-environment-markers","title":"[Rejected] PEP 496 - Environment Markers","excerpt":"Python Enhancement Proposal 496: 'Environment Markers'에 대한 한국어 번역입니다.","date":"2025-09-26 22:40:36+0900","permalink":"/python/pep/496","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 496 Environment Markers 상태: Rejected | 유형: Informational | 작성일: 03Jul2015 PEP 496 – 환경 마커 (Environment Markers) 번역 및 해설 PEP 상태 이 PEP가 처음 초안으로 작성된 후, 환경 마커를 포함한 의존성 선언 구문을 완전히 명세화하기 위해 PEP 5"},{"id":"2025-09-26-pep-0497-a-standard-mechanism-for-backward-compatibility","title":"[Rejected] PEP 497 - A standard mechanism for backward compatibility","excerpt":"Python Enhancement Proposal 497: 'A standard mechanism for backward compatibility'에 대한 한국어 번역입니다.","date":"2025-09-26 22:41:11+0900","permalink":"/python/pep/497","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 497 A standard mechanism for backward compatibility 상태: Rejected | 유형: Process | 작성일: 04Aug2015 PEP 497 – 하위 호환성을 위한 표준 메커니즘 작성자: Ed Schofield <ed at pythoncharmers.com PEP 위임자: Brett Canno"},{"id":"2025-09-26-pep-0498-literal-string-interpolation","title":"[Final] PEP 498 - Literal String Interpolation","excerpt":"Python Enhancement Proposal 498: 'Literal String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 22:42:20+0900","permalink":"/python/pep/498","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 498 Literal String Interpolation 상태: Final | 유형: Standards Track | 작성일: 01Aug2015 PEP 498 – 리터럴 문자열 보간 (Literal String Interpolation) 번역 및 해설 작성자: Eric V. Smith 상태: Final 타입: Standards Trac"},{"id":"2025-09-26-pep-0499-python-mfooshould-also-bindfooinsys-modules","title":"[Deferred] PEP 499 - python-mfooshould also bind'foo'insys.modules","excerpt":"Python Enhancement Proposal 499: 'python-mfooshould also bind'foo'insys.modules'에 대한 한국어 번역입니다.","date":"2025-09-26 22:42:49+0900","permalink":"/python/pep/499","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 499 pythonmfooshould also bind'foo'insys.modules 상태: Deferred | 유형: Standards Track | 작성일: 07Aug2015 PEP 499 – 실행 시 모듈도 에 바인딩되어야 합니다. 개요 이 문서는 Python 개발자가 형태로 모듈을 메인 프로그램으로 실행할 때 발생하는 문제를 해"},{"id":"2025-09-26-pep-0500-a-protocol-for-delegating-datetime-methods-to-their-tzinfo-implementations","title":"[Rejected] PEP 500 - A protocol for delegating datetime methods to their tzinfo implementations","excerpt":"Python Enhancement Proposal 500: 'A protocol for delegating datetime methods to their tzinfo implementations'에 대한 한국어 번역입니다.","date":"2025-09-26 22:43:18+0900","permalink":"/python/pep/500","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 500 A protocol for delegating datetime methods to their tzinfo implementations 상태: Rejected | 유형: Standards Track | 작성일: 08Aug2015 PEP 500 – 메서드를 구현체로 위임하기 위한 프로토콜 작성자 : Alexander Belopolsk"},{"id":"2025-09-26-pep-0501-general-purpose-template-literal-strings","title":"[Withdrawn] PEP 501 - General purpose template literal strings","excerpt":"Python Enhancement Proposal 501: 'General purpose template literal strings'에 대한 한국어 번역입니다.","date":"2025-09-26 22:44:34+0900","permalink":"/python/pep/501","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 501 General purpose template literal strings 상태: Withdrawn | 유형: Standards Track | 작성일: 08Aug2015 PEP 501은 \"범용 템플릿 리터럴 문자열(General purpose template literal strings)\"에 대한 제안서였으나, 현재는 PEP 750"},{"id":"2025-09-26-pep-0502-string-interpolation-extended-discussion","title":"[Rejected] PEP 502 - String Interpolation - Extended Discussion","excerpt":"Python Enhancement Proposal 502: 'String Interpolation - Extended Discussion'에 대한 한국어 번역입니다.","date":"2025-09-26 22:45:25+0900","permalink":"/python/pep/502","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 502 String Interpolation Extended Discussion 상태: Rejected | 유형: Informational | 작성일: 10Aug2015 PEP 502 – String Interpolation 확장된 논의 (Rejected) 이 문서는 Python 3.6에 도입된 \"fstring\" (Literal Stri"},{"id":"2025-09-26-pep-0503-simple-repository-api","title":"[Final] PEP 503 - Simple Repository API","excerpt":"Python Enhancement Proposal 503: 'Simple Repository API'에 대한 한국어 번역입니다.","date":"2025-09-26 22:45:48+0900","permalink":"/python/pep/503","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 503 Simple Repository API 상태: Final | 유형: Standards Track | 작성일: 04Sep2015 PEP 503 – Simple Repository API 작성자: Donald Stufft BDFLDelegate: Donald Stufft 토론: DistutilsSIG list 상태: Final 유형:"},{"id":"2025-09-26-pep-0504-using-the-system-rng-by-default","title":"[Withdrawn] PEP 504 - Using the System RNG by default","excerpt":"Python Enhancement Proposal 504: 'Using the System RNG by default'에 대한 한국어 번역입니다.","date":"2025-09-26 22:47:22+0900","permalink":"/python/pep/504","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 504 Using the System RNG by default 상태: Withdrawn | 유형: Standards Track | 작성일: 15Sep2015 PEP 504 – 기본적으로 시스템 RNG 사용 (Using the System RNG by default) 저자: Alyssa Coghlan <ncoghlan at gmail.c"},{"id":"2025-09-26-pep-0505-none-aware-operators","title":"[Deferred] PEP 505 - None-aware operators","excerpt":"Python Enhancement Proposal 505: 'None-aware operators'에 대한 한국어 번역입니다.","date":"2025-09-26 22:52:11+0900","permalink":"/python/pep/505","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 505 Noneaware operators 상태: Deferred | 유형: Standards Track | 작성일: 18Sep2015 작성자: Mark E. Haase, Steve Dower 상태: Deferred (보류됨) 유형: Standards Track 생성일: 2015년 9월 18일 Python 버전: 3.8 개요 (Abstr"},{"id":"2025-09-26-pep-0506-adding-a-secrets-module-to-the-standard-library","title":"[Final] PEP 506 - Adding A Secrets Module To The Standard Library","excerpt":"Python Enhancement Proposal 506: 'Adding A Secrets Module To The Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-26 22:53:00+0900","permalink":"/python/pep/506","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 506 Adding A Secrets Module To The Standard Library 상태: Final | 유형: Standards Track | 작성일: 19Sep2015 PEP 506 – 표준 라이브러리에 모듈 추가 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 토큰 생성과 같은 일반"},{"id":"2025-09-26-pep-0507-migrate-cpython-to-git-and-gitlab","title":"[Rejected] PEP 507 - Migrate CPython to Git and GitLab","excerpt":"Python Enhancement Proposal 507: 'Migrate CPython to Git and GitLab'에 대한 한국어 번역입니다.","date":"2025-09-26 22:53:39+0900","permalink":"/python/pep/507","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 507 Migrate CPython to Git and GitLab 상태: Rejected | 유형: Process | 작성일: 30Sep2015 PEP 507 – CPython을 Git 및 GitLab으로 이전 제안 작성자: Barry Warsaw <barry at python.org 상태: Rejected (거부됨) 유형: Proce"},{"id":"2025-09-26-pep-0508-dependency-specification-for-python-software-packages","title":"[Final] PEP 508 - Dependency specification for Python Software Packages","excerpt":"Python Enhancement Proposal 508: 'Dependency specification for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-26 22:54:08+0900","permalink":"/python/pep/508","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 508 Dependency specification for Python Software Packages 상태: Final | 유형: Standards Track | 작성일: 11Nov2015 PEP 508: Python 소프트웨어 패키지 의존성 명세 개요 이 PEP(Python Enhancement Proposal)는 Python 패키지"},{"id":"2025-09-26-pep-0509-add-a-private-version-to-dict","title":"[Superseded] PEP 509 - Add a private version to dict","excerpt":"Python Enhancement Proposal 509: 'Add a private version to dict'에 대한 한국어 번역입니다.","date":"2025-09-26 22:55:06+0900","permalink":"/python/pep/509","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 509 Add a private version to dict 상태: Superseded | 유형: Standards Track | 작성일: 04Jan2016 PEP 509 – dict에 Private 버전 추가 초록 (Abstract) 이 PEP는 내장 타입에 새로운 private 버전을 추가할 것을 제안합니다. 이 버전은 각 딕셔너리 "},{"id":"2025-09-26-pep-0510-specialize-functions-with-guards","title":"[Rejected] PEP 510 - Specialize functions with guards","excerpt":"Python Enhancement Proposal 510: 'Specialize functions with guards'에 대한 한국어 번역입니다.","date":"2025-09-26 22:55:51+0900","permalink":"/python/pep/510","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 510 Specialize functions with guards 상태: Rejected | 유형: Standards Track | 작성일: 04Jan2016 PEP 510 – 가드(Guards)를 통한 함수 특수화 (Specialize functions with guards) 작성자 : Victor Stinner 상태 : 거부됨 (Re"},{"id":"2025-09-26-pep-0511-api-for-code-transformers","title":"[Rejected] PEP 511 - API for code transformers","excerpt":"Python Enhancement Proposal 511: 'API for code transformers'에 대한 한국어 번역입니다.","date":"2025-09-26 22:56:47+0900","permalink":"/python/pep/511","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 511 API for code transformers 상태: Rejected | 유형: Standards Track | 작성일: 04Jan2016 PEP 511 – 코드 트랜스포머를 위한 API (API for code transformers) 상태: 이 PEP는 저자에 의해 되었습니다. 거부 사유 이 PEP는 일반 Python 언어와 "},{"id":"2025-09-26-pep-0512-migrating-from-hg-python-org-to-github","title":"[Final] PEP 512 - Migrating from hg.python.org to GitHub","excerpt":"Python Enhancement Proposal 512: 'Migrating from hg.python.org to GitHub'에 대한 한국어 번역입니다.","date":"2025-09-26 22:57:36+0900","permalink":"/python/pep/512","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 512 Migrating from hg.python.org to GitHub 상태: Final | 유형: Process | 작성일: 17Jan2015 PEP 512 – hg.python.org에서 GitHub로 마이그레이션 요약 (Abstract) 이 PEP는 Python의 개발 프로세스를 에 호스팅되던 Mercurial 에서 GitHu"},{"id":"2025-09-26-pep-0513-a-platform-tag-for-portable-linux-built-distributions","title":"[Superseded] PEP 513 - A Platform Tag for Portable Linux Built Distributions","excerpt":"Python Enhancement Proposal 513: 'A Platform Tag for Portable Linux Built Distributions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:03:09+0900","permalink":"/python/pep/513","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 513 A Platform Tag for Portable Linux Built Distributions 상태: Superseded | 유형: Informational | 작성일: 19Jan2016 PEP 513 – 휴대용 Linux 빌드 배포판을 위한 플랫폼 태그 개요 (Abstract) 이 PEP(Python Enhancement Pr"},{"id":"2025-09-26-pep-0514-python-registration-in-the-windows-registry","title":"[Active] PEP 514 - Python registration in the Windows registry","excerpt":"Python Enhancement Proposal 514: 'Python registration in the Windows registry'에 대한 한국어 번역입니다.","date":"2025-09-26 23:09:52+0900","permalink":"/python/pep/514","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 514 Python registration in the Windows registry 상태: Active | 유형: Informational | 작성일: 02Feb2016 PEP 514: Windows 레지스트리에 Python 등록 (Python registration in the Windows registry) 개요 (Abstract)"},{"id":"2025-09-26-pep-0515-underscores-in-numeric-literals","title":"[Final] PEP 515 - Underscores in Numeric Literals","excerpt":"Python Enhancement Proposal 515: 'Underscores in Numeric Literals'에 대한 한국어 번역입니다.","date":"2025-09-26 23:10:13+0900","permalink":"/python/pep/515","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 515 Underscores in Numeric Literals 상태: Final | 유형: Standards Track | 작성일: 10Feb2016 PEP 515 – 숫자 리터럴의 밑줄 (Underscores in Numeric Literals) 작성자: Georg Brandl, Serhiy Storchaka 상태: Final (최종"},{"id":"2025-09-26-pep-0516-build-system-abstraction-for-pipconda-etc","title":"[Rejected] PEP 516 - Build system abstraction for pip/conda etc","excerpt":"Python Enhancement Proposal 516: 'Build system abstraction for pip/conda etc'에 대한 한국어 번역입니다.","date":"2025-09-26 23:11:03+0900","permalink":"/python/pep/516","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 516 Build system abstraction for pip/conda etc 상태: Rejected | 유형: Standards Track | 작성일: 26Oct2015 PEP 516 – pip/conda 등을 위한 빌드 시스템 추상화 요약 (Abstract) 이 PEP는 및 기타 배포 또는 설치 도구들이 Python 소스 트리("},{"id":"2025-09-26-pep-0517-a-build-system-independent-format-for-source-trees","title":"[Final] PEP 517 - A build-system independent format for source trees","excerpt":"Python Enhancement Proposal 517: 'A build-system independent format for source trees'에 대한 한국어 번역입니다.","date":"2025-09-26 23:11:54+0900","permalink":"/python/pep/517","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 517 A buildsystem independent format for source trees 상태: Final | 유형: Standards Track | 작성일: 30Sep2015 PEP 517 – 소스 트리를 위한 빌드 시스템 독립적인 형식 본 문서는 PEP 517의 내용을 한국어 Python 개발자들을 위해 번역하고 정리한 것입니"},{"id":"2025-09-26-pep-0518-specifying-minimum-build-system-requirements-for-python-projects","title":"[Final] PEP 518 - Specifying Minimum Build System Requirements for Python Projects","excerpt":"Python Enhancement Proposal 518: 'Specifying Minimum Build System Requirements for Python Projects'에 대한 한국어 번역입니다.","date":"2025-09-26 23:12:43+0900","permalink":"/python/pep/518","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 518 Specifying Minimum Build System Requirements for Python Projects 상태: Final | 유형: Standards Track | 작성일: 10May2016 PEP 518 – Python 프로젝트를 위한 최소 빌드 시스템 요구사항 지정 초록 (Abstract) 이 PEP는 Python"},{"id":"2025-09-26-pep-0519-adding-a-file-system-path-protocol","title":"[Final] PEP 519 - Adding a file system path protocol","excerpt":"Python Enhancement Proposal 519: 'Adding a file system path protocol'에 대한 한국어 번역입니다.","date":"2025-09-26 23:13:13+0900","permalink":"/python/pep/519","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 519 Adding a file system path protocol 상태: Final | 유형: Standards Track | 작성일: 11May2016 다음은 Python Enhancement Proposal (PEP) 519, \"Adding a file system path protocol\"의 내용 요약 및 번역입니다. 이 PEP"},{"id":"2025-09-26-pep-0520-preserving-class-attribute-definition-order","title":"[Final] PEP 520 - Preserving Class Attribute Definition Order","excerpt":"Python Enhancement Proposal 520: 'Preserving Class Attribute Definition Order'에 대한 한국어 번역입니다.","date":"2025-09-26 23:13:52+0900","permalink":"/python/pep/520","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 520 Preserving Class Attribute Definition Order 상태: Final | 유형: Standards Track | 작성일: 07Jun2016 PEP 520 – 클래스 속성 정의 순서 유지 (Preserving Class Attribute Definition Order) 요약 (Abstract) 클래스 정의"},{"id":"2025-09-26-pep-0521-managing-global-context-via-with-blocks-in-generators-and-coroutines","title":"[Withdrawn] PEP 521 - Managing global context via ‘with’ blocks in generators and coroutines","excerpt":"Python Enhancement Proposal 521: 'Managing global context via ‘with’ blocks in generators and coroutines'에 대한 한국어 번역입니다.","date":"2025-09-26 23:14:33+0900","permalink":"/python/pep/521","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 521 Managing global context via ‘with’ blocks in generators and coroutines 상태: Withdrawn | 유형: Standards Track | 작성일: 27Apr2015 PEP 521은 및 내 블록을 통해 전역 컨텍스트를 관리하는 방법을 제안했지만, PEP 567에 찬성하여 (철"},{"id":"2025-09-26-pep-0522-allow-blockingioerror-in-security-sensitive-apis","title":"[Rejected] PEP 522 - Allow BlockingIOError in security sensitive APIs","excerpt":"Python Enhancement Proposal 522: 'Allow BlockingIOError in security sensitive APIs'에 대한 한국어 번역입니다.","date":"2025-09-26 23:15:31+0900","permalink":"/python/pep/522","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 522 Allow BlockingIOError in security sensitive APIs 상태: Rejected | 유형: Standards Track | 작성일: 16Jun2016 PEP 522: 보안에 민감한 API에서 허용 (거부됨) 요약 (Abstract) 표준 라이브러리 내의 여러 API는 보안에 민감한 작업에 적합한 무작"},{"id":"2025-09-26-pep-0523-adding-a-frame-evaluation-api-to-cpython","title":"[Final] PEP 523 - Adding a frame evaluation API to CPython","excerpt":"Python Enhancement Proposal 523: 'Adding a frame evaluation API to CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 23:16:26+0900","permalink":"/python/pep/523","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 523 Adding a frame evaluation API to CPython 상태: Final | 유형: Standards Track | 작성일: 16May2016 PEP 523 – CPython에 프레임 평가 API 추가 개요 (Abstract) 이 PEP는 CPython의 C API를 확장하여 인터프리터별 프레임 평가 함수를 지정"},{"id":"2025-09-26-pep-0524-make-os-urandom-blocking-on-linux","title":"[Final] PEP 524 - Make os.urandom() blocking on Linux","excerpt":"Python Enhancement Proposal 524: 'Make os.urandom() blocking on Linux'에 대한 한국어 번역입니다.","date":"2025-09-26 23:16:54+0900","permalink":"/python/pep/524","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 524 Make os.urandom() blocking on Linux 상태: Final | 유형: Standards Track | 작성일: 20Jun2016 PEP 524: Linux에서 을 블로킹 방식으로 변경 초록 (Abstract) 이 PEP는 Python의 보안 강화를 위해 Linux 3.17 이상 버전에서 함수가 운영체제의 U"},{"id":"2025-09-26-pep-0525-asynchronous-generators","title":"[Final] PEP 525 - Asynchronous Generators","excerpt":"Python Enhancement Proposal 525: 'Asynchronous Generators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:17:46+0900","permalink":"/python/pep/525","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 525 Asynchronous Generators 상태: Final | 유형: Standards Track | 작성일: 28Jul2016 PEP 525 – 비동기 제너레이터 (Asynchronous Generators) 개요 (Abstract) PEP 492는 Python 3.5에 네이티브 코루틴(native coroutines)과 / "},{"id":"2025-09-26-pep-0526-syntax-for-variable-annotations","title":"[Final] PEP 526 - Syntax for Variable Annotations","excerpt":"Python Enhancement Proposal 526: 'Syntax for Variable Annotations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:02+0900","permalink":"/python/pep/526","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 526 Syntax for Variable Annotations 상태: Final | 유형: Standards Track | 작성일: 09Aug2016 PEP 526 – 변수 어노테이션 구문 (Syntax for Variable Annotations) 개요 (Abstract) PEP 484는 타입 힌트(type hints), 즉 타입 어"},{"id":"2025-09-26-pep-0527-removing-underused-file-typesextensions-on-pypi","title":"[Final] PEP 527 - Removing Un(der)used file types/extensions on PyPI","excerpt":"Python Enhancement Proposal 527: 'Removing Un(der)used file types/extensions on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:32+0900","permalink":"/python/pep/527","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 527 Removing Un(der)used file types/extensions on PyPI 상태: Final | 유형: Standards Track | 작성일: 23Aug2016 PEP 527 – PyPI에서 미사용/활용도가 낮은 파일 형식/확장자 제거 작성자: Donald Stufft 상태: Final 유형: Standards "},{"id":"2025-09-26-pep-0528-change-windows-console-encoding-to-utf-8","title":"[Final] PEP 528 - Change Windows console encoding to UTF-8","excerpt":"Python Enhancement Proposal 528: 'Change Windows console encoding to UTF-8'에 대한 한국어 번역입니다.","date":"2025-09-26 23:19:55+0900","permalink":"/python/pep/528","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 528 Change Windows console encoding to UTF8 상태: Final | 유형: Standards Track | 작성일: 27Aug2016 PEP 528 – Windows 콘솔 인코딩을 UTF8로 변경 개요 (Abstract) 과거에 Python은 Windows 운영체제와의 상호작용을 위해 ANSI API를 주"},{"id":"2025-09-26-pep-0529-change-windows-filesystem-encoding-to-utf-8","title":"[Final] PEP 529 - Change Windows filesystem encoding to UTF-8","excerpt":"Python Enhancement Proposal 529: 'Change Windows filesystem encoding to UTF-8'에 대한 한국어 번역입니다.","date":"2025-09-26 23:20:53+0900","permalink":"/python/pep/529","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 529 Change Windows filesystem encoding to UTF8 상태: Final | 유형: Standards Track | 작성일: 27Aug2016 PEP 529 – Windows 파일 시스템 인코딩을 UTF8로 변경 작성자: Steve Dower 상태: 최종 (Final) 유형: 표준 트랙 (Standards T"},{"id":"2025-09-26-pep-0530-asynchronous-comprehensions","title":"[Final] PEP 530 - Asynchronous Comprehensions","excerpt":"Python Enhancement Proposal 530: 'Asynchronous Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:21:12+0900","permalink":"/python/pep/530","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 530 Asynchronous Comprehensions 상태: Final | 유형: Standards Track | 작성일: 03Sep2016 PEP 530 – 비동기 컴프리헨션 (Asynchronous Comprehensions) 개요 PEP 530은 Python 3.6에 도입된 기능으로, / 문법을 사용하는 비동기 버전의 , , 컴"},{"id":"2025-09-26-pep-0531-existence-checking-operators","title":"[Withdrawn] PEP 531 - Existence checking operators","excerpt":"Python Enhancement Proposal 531: 'Existence checking operators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:22:20+0900","permalink":"/python/pep/531","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 531 Existence checking operators 상태: Withdrawn | 유형: Standards Track | 작성일: 25Oct2016 PEP 531 – 존재 확인 연산자 (Existence Checking Operators) 상태: 철회됨 (Withdrawn) 초록 (Abstract) PEP 505와 관련 논의에서 영"},{"id":"2025-09-26-pep-0532-a-circuit-breaking-protocol-and-binary-operators","title":"[Deferred] PEP 532 - A circuit breaking protocol and binary operators","excerpt":"Python Enhancement Proposal 532: 'A circuit breaking protocol and binary operators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:23:40+0900","permalink":"/python/pep/532","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 532 A circuit breaking protocol and binary operators 상태: Deferred | 유형: Standards Track | 작성일: 30Oct2016 PEP 532 – 회로 차단(Circuit Breaking) 프로토콜 및 이항(Binary) 연산자 PEP 연기 (PEP Deferral) 이 PEP에"},{"id":"2025-09-26-pep-0533-deterministic-cleanup-for-iterators","title":"[Deferred] PEP 533 - Deterministic cleanup for iterators","excerpt":"Python Enhancement Proposal 533: 'Deterministic cleanup for iterators'에 대한 한국어 번역입니다.","date":"2025-09-26 23:25:05+0900","permalink":"/python/pep/533","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 533 Deterministic cleanup for iterators 상태: Deferred | 유형: Standards Track | 작성일: 18Oct2016 PEP 533 – 이터레이터의 확정적 정리 (Deterministic cleanup for iterators) 요약 (Abstract) 본 PEP는 이터레이터 프로토콜에 새로"},{"id":"2025-09-26-pep-0534-improved-errors-for-missing-standard-library-modules","title":"[Deferred] PEP 534 - Improved Errors for Missing Standard Library Modules","excerpt":"Python Enhancement Proposal 534: 'Improved Errors for Missing Standard Library Modules'에 대한 한국어 번역입니다.","date":"2025-09-26 23:25:45+0900","permalink":"/python/pep/534","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 534 Improved Errors for Missing Standard Library Modules 상태: Deferred | 유형: Standards Track | 작성일: 05Sep2016 PEP 534 – 누락된 표준 라이브러리 모듈에 대한 오류 메시지 개선 개요 Python은 종종 전체 표준 라이브러리 없이 빌드되거나 배포됩니다"},{"id":"2025-09-26-pep-0535-rich-comparison-chaining","title":"[Deferred] PEP 535 - Rich comparison chaining","excerpt":"Python Enhancement Proposal 535: 'Rich comparison chaining'에 대한 한국어 번역입니다.","date":"2025-09-26 23:26:10+0900","permalink":"/python/pep/535","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 535 Rich comparison chaining 상태: Deferred | 유형: Standards Track | 작성일: 12Nov2016 PEP 535 – 풍부한 비교 체인 (Rich comparison chaining) 개요 (Abstract) PEP 535는 PEP 335에서 영감을 받고 PEP 532에 설명된 회로 차단(ci"},{"id":"2025-09-26-pep-0536-final-grammar-for-literal-string-interpolation","title":"[Withdrawn] PEP 536 - Final Grammar for Literal String Interpolation","excerpt":"Python Enhancement Proposal 536: 'Final Grammar for Literal String Interpolation'에 대한 한국어 번역입니다.","date":"2025-09-26 23:26:52+0900","permalink":"/python/pep/536","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 536 Final Grammar for Literal String Interpolation 상태: Withdrawn | 유형: Standards Track | 작성일: 11Dec2016 PEP 536 – 리터럴 문자열 보간법(Literal String Interpolation)을 위한 최종 문법 작성자: Philipp Angerer 상태"},{"id":"2025-09-26-pep-0537-python-3-7-release-schedule","title":"[Final] PEP 537 - Python 3.7 Release Schedule","excerpt":"Python Enhancement Proposal 537: 'Python 3.7 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 23:27:21+0900","permalink":"/python/pep/537","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 537 Python 3.7 Release Schedule 상태: Final | 유형: Informational | 작성일: 23Dec2016 PEP 537 – Python 3.7 릴리스 스케줄 메타 정보 작성자(Author): Ned Deily <nad at python.org 상태(Status): Final 타입(Type): Infor"},{"id":"2025-09-26-pep-0538-coercing-the-legacy-c-locale-to-a-utf-8-based-locale","title":"[Final] PEP 538 - Coercing the legacy C locale to a UTF-8 based locale","excerpt":"Python Enhancement Proposal 538: 'Coercing the legacy C locale to a UTF-8 based locale'에 대한 한국어 번역입니다.","date":"2025-09-26 23:28:03+0900","permalink":"/python/pep/538","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 538 Coercing the legacy C locale to a UTF8 based locale 상태: Final | 유형: Standards Track | 작성일: 28Dec2016 PEP 538 – 레거시 C locale을 UTF8 기반 locale로 강제 변환 (Coercing the legacy C locale to a UTF"},{"id":"2025-09-26-pep-0539-a-new-c-api-for-thread-local-storage-in-cpython","title":"[Final] PEP 539 - A New C-API for Thread-Local Storage in CPython","excerpt":"Python Enhancement Proposal 539: 'A New C-API for Thread-Local Storage in CPython'에 대한 한국어 번역입니다.","date":"2025-09-26 23:28:37+0900","permalink":"/python/pep/539","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 539 A New CAPI for ThreadLocal Storage in CPython 상태: Final | 유형: Standards Track | 작성일: 20Dec2016 PEP 539 – CPython에서 스레드 로컬 스토리지를 위한 새로운 CAPI 이 문서는 CPython 인터프리터 내에서 스레드 로컬 스토리지(Thread Lo"},{"id":"2025-09-26-pep-0540-add-a-new-utf-8-mode","title":"[Final] PEP 540 - Add a new UTF-8 Mode","excerpt":"Python Enhancement Proposal 540: 'Add a new UTF-8 Mode'에 대한 한국어 번역입니다.","date":"2025-09-26 23:29:04+0900","permalink":"/python/pep/540","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 540 Add a new UTF8 Mode 상태: Final | 유형: Standards Track | 작성일: 05Jan2016 PEP 540: 새로운 UTF8 모드 추가 요약 (Abstract) PEP 540은 Python의 UTF8 활용을 강화하기 위한 새로운 \"UTF8 모드\" 추가를 제안합니다. 이 모드가 활성화되면 Python은"},{"id":"2025-09-26-pep-0541-package-index-name-retention","title":"[Final] PEP 541 - Package Index Name Retention","excerpt":"Python Enhancement Proposal 541: 'Package Index Name Retention'에 대한 한국어 번역입니다.","date":"2025-09-26 23:29:44+0900","permalink":"/python/pep/541","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 541 Package Index Name Retention 상태: Final | 유형: Process | 작성일: 12Jan2017 PEP 541 – Package Index 이름 유지 (Package Index Name Retention) 작성자: Łukasz Langa BDFLDelegate: Mark Mangoba 상태: 최종 (F"},{"id":"2025-09-26-pep-0542-dot-notation-assignment-in-function-header","title":"[Rejected] PEP 542 - Dot Notation Assignment In Function Header","excerpt":"Python Enhancement Proposal 542: 'Dot Notation Assignment In Function Header'에 대한 한국어 번역입니다.","date":"2025-09-26 23:30:04+0900","permalink":"/python/pep/542","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 542 Dot Notation Assignment In Function Header 상태: Rejected | 유형: Standards Track | 작성일: 10Feb2017 PEP 542 – 함수 헤더 내 점 표기법 할당 (Dot Notation Assignment In Function Header) 작성자: Markus Meskan"},{"id":"2025-09-26-pep-0543-a-unified-tls-api-for-python","title":"[Withdrawn] PEP 543 - A Unified TLS API for Python","excerpt":"Python Enhancement Proposal 543: 'A Unified TLS API for Python'에 대한 한국어 번역입니다.","date":"2025-09-26 23:30:50+0900","permalink":"/python/pep/543","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 543 A Unified TLS API for Python 상태: Withdrawn | 유형: Standards Track | 작성일: 17Oct2016 PEP 543 – Python을 위한 통합 TLS API 작성자 : Cory Benfield, Christian Heimes 상태 : 철회됨 (Withdrawn) 유형 : Standar"},{"id":"2025-09-26-pep-0544-protocols-structural-subtyping-static-duck-typing","title":"[Final] PEP 544 - Protocols: Structural subtyping (static duck typing)","excerpt":"Python Enhancement Proposal 544: 'Protocols: Structural subtyping (static duck typing)'에 대한 한국어 번역입니다.","date":"2025-09-26 23:32:41+0900","permalink":"/python/pep/544","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 544 Protocols: Structural subtyping (static duck typing) 상태: Final | 유형: Standards Track | 작성일: 05Mar2017 PEP 544 – Protocols: 구조적 서브타이핑 (정적 덕 타이핑) 이 문서는 Python Enhancement Proposal (PEP) 5"},{"id":"2025-09-26-pep-0545-python-documentation-translations","title":"[Active] PEP 545 - Python Documentation Translations","excerpt":"Python Enhancement Proposal 545: 'Python Documentation Translations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:33:31+0900","permalink":"/python/pep/545","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 545 Python Documentation Translations 상태: Active | 유형: Process | 작성일: 04Mar2017 PEP 545 – Python 문서 번역 (Python Documentation Translations) 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 "},{"id":"2025-09-26-pep-0546-backport-ssl-memorybio-and-ssl-sslobject-to-python-2-7","title":"[Rejected] PEP 546 - Backport ssl.MemoryBIO and ssl.SSLObject to Python 2.7","excerpt":"Python Enhancement Proposal 546: 'Backport ssl.MemoryBIO and ssl.SSLObject to Python 2.7'에 대한 한국어 번역입니다.","date":"2025-09-26 23:33:55+0900","permalink":"/python/pep/546","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 546 Backport ssl.MemoryBIO and ssl.SSLObject to Python 2.7 상태: Rejected | 유형: Standards Track | 작성일: 30May2017 PEP 546 – Python 2.7에 및 백포팅 제안 (거부됨) 개요 이 문서는 Python 3의 및 클래스를 Python 2.7 버전으로"},{"id":"2025-09-26-pep-0547-running-extension-modules-using-the-m-option","title":"[Deferred] PEP 547 - Running extension modules using the -m option","excerpt":"Python Enhancement Proposal 547: 'Running extension modules using the -m option'에 대한 한국어 번역입니다.","date":"2025-09-26 23:34:23+0900","permalink":"/python/pep/547","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 547 Running extension modules using the m option 상태: Deferred | 유형: Standards Track | 작성일: 25May2017 PEP 547 – 옵션을 사용하여 확장 모듈 실행 저자: Marcel Plch, Petr Viktorin 상태: 연기됨 (Deferred) 유형: 표준 트랙 "},{"id":"2025-09-26-pep-0548-more-flexible-loop-control","title":"[Rejected] PEP 548 - More Flexible Loop Control","excerpt":"Python Enhancement Proposal 548: 'More Flexible Loop Control'에 대한 한국어 번역입니다.","date":"2025-09-26 23:35:01+0900","permalink":"/python/pep/548","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 548 More Flexible Loop Control 상태: Rejected | 유형: Standards Track | 작성일: 05Sep2017 Title: PEP 548 – 더욱 유연한 반복문 제어 (More Flexible Loop Control) 서론 (Introduction) PEP 548이 무엇을 제안했는지 간략하게 설명. "},{"id":"2025-09-26-pep-0549-instance-descriptors","title":"[Rejected] PEP 549 - Instance Descriptors","excerpt":"Python Enhancement Proposal 549: 'Instance Descriptors'에 대한 한국어 번역입니다.","date":"2025-09-26 23:35:20+0900","permalink":"/python/pep/549","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 549 Instance Descriptors 상태: Rejected | 유형: Standards Track | 작성일: 04Sep2017 PEP 549 – 인스턴스 디스크립터 (Instance Descriptors) 작성자: Larry Hastings 상태: Rejected (거부됨) 생성일: 2017년 9월 4일 Python 버전: 3"},{"id":"2025-09-26-pep-0550-execution-context","title":"[Withdrawn] PEP 550 - Execution Context","excerpt":"Python Enhancement Proposal 550: 'Execution Context'에 대한 한국어 번역입니다.","date":"2025-09-26 23:36:24+0900","permalink":"/python/pep/550","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 550 Execution Context 상태: Withdrawn | 유형: Standards Track | 작성일: 11Aug2017 PEP 550 – Execution Context 문서를 한국어로 번역하고 요약합니다. PEP 550: 실행 컨텍스트 (Execution Context) 폐기됨 (Withdrawn) 요약: PEP 550은"},{"id":"2025-09-26-pep-0551-security-transparency-in-the-python-runtime","title":"[Withdrawn] PEP 551 - Security transparency in the Python runtime","excerpt":"Python Enhancement Proposal 551: 'Security transparency in the Python runtime'에 대한 한국어 번역입니다.","date":"2025-09-26 23:37:57+0900","permalink":"/python/pep/551","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 551 Security transparency in the Python runtime 상태: Withdrawn | 유형: Informational | 작성일: 23Aug2017 PEP 551 – Python 런타임의 보안 투명성 (Security transparency in the Python runtime) 저자: Steve Dower"},{"id":"2025-09-26-pep-0552-deterministic-pycs","title":"[Final] PEP 552 - Deterministic pycs","excerpt":"Python Enhancement Proposal 552: 'Deterministic pycs'에 대한 한국어 번역입니다.","date":"2025-09-26 23:38:24+0900","permalink":"/python/pep/552","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 552 Deterministic pycs 상태: Final | 유형: Standards Track | 작성일: 04Sep2017 PEP 552 – 결정론적 파일 개요 이 PEP는 파일 형식을 확장하여, 파일의 결정론적(deterministic) 특성을 강화할 것을 제안합니다. 도입 배경 (Rationale) 재현 가능한 빌드(Reprod"},{"id":"2025-09-26-pep-0553-built-in-breakpoint","title":"[Final] PEP 553 - Built-in breakpoint()","excerpt":"Python Enhancement Proposal 553: 'Built-in breakpoint()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:38:53+0900","permalink":"/python/pep/553","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 553 Builtin breakpoint() 상태: Final | 유형: Standards Track | 작성일: 05Sep2017 PEP 553 – 내장 함수 개요 이 PEP는 호출 지점에서 Python 디버거를 실행하는 새로운 내장 함수 를 추가할 것을 제안합니다. 또한, 어떤 디버거를 사용할지 설정할 수 있도록 모듈에 두 개의 새로"},{"id":"2025-09-26-pep-0554-multiple-interpreters-in-the-stdlib","title":"[Superseded] PEP 554 - Multiple Interpreters in the Stdlib","excerpt":"Python Enhancement Proposal 554: 'Multiple Interpreters in the Stdlib'에 대한 한국어 번역입니다.","date":"2025-09-26 23:40:00+0900","permalink":"/python/pep/554","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 554 Multiple Interpreters in the Stdlib 상태: Superseded | 유형: Standards Track | 작성일: 05Sep2017 주어진 URL의 PEP 554 문서를 바탕으로 한국어 번역 및 요약을 제공합니다. 이 문서는 Python 3.12에서 [PEP 684]를 통해 perinterpreter "},{"id":"2025-09-26-pep-0555-context-local-variables-contextvars","title":"[Withdrawn] PEP 555 - Context-local variables (contextvars)","excerpt":"Python Enhancement Proposal 555: 'Context-local variables (contextvars)'에 대한 한국어 번역입니다.","date":"2025-09-26 23:40:56+0900","permalink":"/python/pep/555","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 555 Contextlocal variables (contextvars) 상태: Withdrawn | 유형: Standards Track | 작성일: 06Sep2017 PEP 555: Contextlocal variables (contextvars) (컨텍스트로컬 변수) 작성자 : Koos Zevenhoven 상태 : 철회됨 (Withd"},{"id":"2025-09-26-pep-0556-threaded-garbage-collection","title":"[Deferred] PEP 556 - Threaded garbage collection","excerpt":"Python Enhancement Proposal 556: 'Threaded garbage collection'에 대한 한국어 번역입니다.","date":"2025-09-26 23:41:31+0900","permalink":"/python/pep/556","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 556 Threaded garbage collection 상태: Deferred | 유형: Standards Track | 작성일: 08Sep2017 PEP 556 – Threaded garbage collection 작성자: Antoine Pitrou 상태: 연기됨 (Deferred) 유형: 표준 트랙 (Standards Track) "},{"id":"2025-09-26-pep-0557-data-classes","title":"[Final] PEP 557 - Data Classes","excerpt":"Python Enhancement Proposal 557: 'Data Classes'에 대한 한국어 번역입니다.","date":"2025-09-26 23:42:21+0900","permalink":"/python/pep/557","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 557 Data Classes 상태: Final | 유형: Standards Track | 작성일: 02Jun2017 PEP 557 – Data Classes (데이터 클래스) 번역 및 요약 개요 PEP 557은 Python 3.7부터 표준 라이브러리에 추가된 모듈을 설명하는 문서입니다. 이 PEP는 주로 데이터를 저장하는 데 사용되는 "},{"id":"2025-09-26-pep-0558-defined-semantics-for-locals","title":"[Withdrawn] PEP 558 - Defined semantics for locals()","excerpt":"Python Enhancement Proposal 558: 'Defined semantics for locals()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:10+0900","permalink":"/python/pep/558","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 558 Defined semantics for locals() 상태: Withdrawn | 유형: Standards Track | 작성일: 08Sep2017 PEP 558은 내장 함수의 동작을 명확히 정의하려는 제안이었으나, 2021년 12월에 PEP 667과의 내용 통합으로 인해 철회되었습니다. 이 문서는 의 의미론적 정의와 관련된 배"},{"id":"2025-09-26-pep-0559-built-in-noop","title":"[Rejected] PEP 559 - Built-in noop()","excerpt":"Python Enhancement Proposal 559: 'Built-in noop()'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:26+0900","permalink":"/python/pep/559","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 559 Builtin noop() 상태: Rejected | 유형: Standards Track | 작성일: 08Sep2017 PEP 559 – Builtin | peps.python.org 문서를 번역하고 정리합니다. PEP 559 – Builtin 작성자: Barry Warsaw <barry at python.org 상태: Rejec"},{"id":"2025-09-26-pep-0560-core-support-for-typing-module-and-generic-types","title":"[Final] PEP 560 - Core support for typing module and generic types","excerpt":"Python Enhancement Proposal 560: 'Core support for typing module and generic types'에 대한 한국어 번역입니다.","date":"2025-09-26 23:45:50+0900","permalink":"/python/pep/560","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 560 Core support for typing module and generic types 상태: Final | 유형: Standards Track | 작성일: 03Sep2017 PEP 560 – 모듈 및 제네릭 타입에 대한 핵심 지원 개요 PEP 560은 모듈과 제네릭 타입( types)을 더 효율적으로 지원하기 위해 CPython"},{"id":"2025-09-26-pep-0561-distributing-and-packaging-type-information","title":"[Final] PEP 561 - Distributing and Packaging Type Information","excerpt":"Python Enhancement Proposal 561: 'Distributing and Packaging Type Information'에 대한 한국어 번역입니다.","date":"2025-09-26 23:46:31+0900","permalink":"/python/pep/561","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 561 Distributing and Packaging Type Information 상태: Final | 유형: Standards Track | 작성일: 09Sep2017 PEP 561 – 타입 정보 배포 및 패키징 개요 (Abstract) PEP 484는 점진적이고 쉽게 채택할 수 있는 목표를 가지고 Python에 타입 힌팅(Type"},{"id":"2025-09-26-pep-0562-module-getattr-and-dir","title":"[Final] PEP 562 - Module __getattr__ and __dir__","excerpt":"Python Enhancement Proposal 562: 'Module __getattr__ and __dir__'에 대한 한국어 번역입니다.","date":"2025-09-26 23:47:20+0900","permalink":"/python/pep/562","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 562 Module getattr and dir 상태: Final | 유형: Standards Track | 작성일: 09Sep2017 I have successfully browsed the PEP 0562 document. Now I will translate and summarize it according to the given i"},{"id":"2025-09-26-pep-0563-postponed-evaluation-of-annotations","title":"[Superseded] PEP 563 - Postponed Evaluation of Annotations","excerpt":"Python Enhancement Proposal 563: 'Postponed Evaluation of Annotations'에 대한 한국어 번역입니다.","date":"2025-09-26 23:48:31+0900","permalink":"/python/pep/563","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 563 Postponed Evaluation of Annotations 상태: Superseded | 유형: Standards Track | 작성일: 08Sep2017 PEP 563 – 어노테이션 지연 평가 (Postponed Evaluation of Annotations) 상태: 이 PEP에서 제안된 기능은 기본 동작이 되지 못했으며,"},{"id":"2025-09-26-pep-0564-add-new-time-functions-with-nanosecond-resolution","title":"[Final] PEP 564 - Add new time functions with nanosecond resolution","excerpt":"Python Enhancement Proposal 564: 'Add new time functions with nanosecond resolution'에 대한 한국어 번역입니다.","date":"2025-09-26 23:48:58+0900","permalink":"/python/pep/564","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 564 Add new time functions with nanosecond resolution 상태: Final | 유형: Standards Track | 작성일: 16Oct2017 Here's the translated and summarized content of PEP 564, adhering to the specified gui"},{"id":"2025-09-26-pep-0565-show-deprecationwarning-in-main","title":"[Final] PEP 565 - Show DeprecationWarning in __main__","excerpt":"Python Enhancement Proposal 565: 'Show DeprecationWarning in __main__'에 대한 한국어 번역입니다.","date":"2025-09-26 23:49:30+0900","permalink":"/python/pep/565","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 565 Show DeprecationWarning in main 상태: Final | 유형: Standards Track | 작성일: 12Nov2017 PEP 565: 모듈에서 표시 (Show DeprecationWarning in ) 요약 (Abstract) Python 2.7 및 3.2부터 은 기본적으로 숨겨져 있었습니다. 이는 개발"},{"id":"2025-09-26-pep-0566-metadata-for-python-software-packages-2-1","title":"[Final] PEP 566 - Metadata for Python Software Packages 2.1","excerpt":"Python Enhancement Proposal 566: 'Metadata for Python Software Packages 2.1'에 대한 한국어 번역입니다.","date":"2025-09-26 23:49:52+0900","permalink":"/python/pep/566","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 566 Metadata for Python Software Packages 2.1 상태: Final | 유형: Standards Track | 작성일: 01Dec2017 PEP 566 – Python 소프트웨어 패키지용 메타데이터 2.1 초록 (Abstract) 이 PEP (Python Enhancement Proposal)는 Pytho"},{"id":"2025-09-26-pep-0567-context-variables","title":"[Final] PEP 567 - Context Variables","excerpt":"Python Enhancement Proposal 567: 'Context Variables'에 대한 한국어 번역입니다.","date":"2025-09-26 23:50:36+0900","permalink":"/python/pep/567","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 567 Context Variables 상태: Final | 유형: Standards Track | 작성일: 12Dec2017 PEP 567 – Context Variables 한국어 번역 및 요약 초록 (Abstract) 이 PEP(Python Enhancement Proposal)는 컨텍스트 변수(context variables) 를"},{"id":"2025-09-26-pep-0568-generator-sensitivity-for-context-variables","title":"[Deferred] PEP 568 - Generator-sensitivity for Context Variables","excerpt":"Python Enhancement Proposal 568: 'Generator-sensitivity for Context Variables'에 대한 한국어 번역입니다.","date":"2025-09-26 23:51:20+0900","permalink":"/python/pep/568","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 568 Generatorsensitivity for Context Variables 상태: Deferred | 유형: Standards Track | 작성일: 04Jan2018 다음은 Python Enhancement Proposal (PEP) 568 문서의 한국어 번역 및 정리입니다. 이 문서는 에 대한 감도 (Generatorsens"},{"id":"2025-09-26-pep-0569-python-3-8-release-schedule","title":"[Final] PEP 569 - Python 3.8 Release Schedule","excerpt":"Python Enhancement Proposal 569: 'Python 3.8 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-26 23:51:49+0900","permalink":"/python/pep/569","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 569 Python 3.8 Release Schedule 상태: Final | 유형: Informational | 작성일: 27Jan2018 PEP 569 – Python 3.8 릴리스 스케줄 작성자: Łukasz Langa <lukasz at python.org 상태: Final 유형: Informational 주제: Release 생"},{"id":"2025-09-26-pep-0570-python-positional-only-parameters","title":"[Final] PEP 570 - Python Positional-Only Parameters","excerpt":"Python Enhancement Proposal 570: 'Python Positional-Only Parameters'에 대한 한국어 번역입니다.","date":"2025-09-26 23:56:05+0900","permalink":"/python/pep/570","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 570 Python PositionalOnly Parameters 상태: Final | 유형: Standards Track | 작성일: 20Jan2018 Now I have the content of PEP 0570. I will proceed with the translation and summarization according to "},{"id":"2025-09-26-pep-0571-the-manylinux2010-platform-tag","title":"[Superseded] PEP 571 - The manylinux2010 Platform Tag","excerpt":"Python Enhancement Proposal 571: 'The manylinux2010 Platform Tag'에 대한 한국어 번역입니다.","date":"2025-09-26 23:56:56+0900","permalink":"/python/pep/571","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 571 The manylinux2010 Platform Tag 상태: Superseded | 유형: Informational | 작성일: 05Feb2018 PEP 571 – manylinux2010 플랫폼 태그 작성자: Mark Williams, Geoffrey Thomas, Thomas Kluyver BDFLDelegate: Alyss"},{"id":"2025-09-26-pep-0572-assignment-expressions","title":"[Final] PEP 572 - Assignment Expressions","excerpt":"Python Enhancement Proposal 572: 'Assignment Expressions'에 대한 한국어 번역입니다.","date":"2025-09-26 23:58:55+0900","permalink":"/python/pep/572","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 572 Assignment Expressions 상태: Final | 유형: Standards Track | 작성일: 28Feb2018 다음은 PEP 572 – 할당 표현식(Assignment Expressions) 문서의 번역 및 정리 내용입니다. 이 PEP는 Python 3.8에 도입된 연산자를 설명하며, 이를 통해 표현식 내부에서 "},{"id":"2025-09-26-pep-0573-module-state-access-from-c-extension-methods","title":"[Final] PEP 573 - Module State Access from C Extension Methods","excerpt":"Python Enhancement Proposal 573: 'Module State Access from C Extension Methods'에 대한 한국어 번역입니다.","date":"2025-09-26 23:59:37+0900","permalink":"/python/pep/573","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 573 Module State Access from C Extension Methods 상태: Final | 유형: Standards Track | 작성일: 02Jun2016 PEP 573 – C 확장 메서드에서 모듈 상태 접근 개요 (Abstract) 이 PEP(Python Enhancement Proposal) 573은 CPython"},{"id":"2025-09-27-pep-0000-index-of-python-enhancement-proposals-peps","title":"[Active] PEP 0 - Index of Python Enhancement Proposals (PEPs)","excerpt":"Python Enhancement Proposal 0: 'Index of Python Enhancement Proposals (PEPs)'에 대한 한국어 번역입니다.","date":"2025-09-27 21:19:02+0900","permalink":"/python/pep/0","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 0 Index of Python Enhancement Proposals (PEPs) 상태: Active | 유형: Informational | 작성일: 13Jul2000 PEP 0 – Python Enhancement Proposals (PEPs) 색인 소개 PEP 번호는 PEP 편집자에 의해 할당되며, 한 번 할당되면 변경되지 않습니다"},{"id":"2025-09-27-pep-0335-overloadable-boolean-operators","title":"[Rejected] PEP 335 - Overloadable Boolean Operators","excerpt":"Python Enhancement Proposal 335: 'Overloadable Boolean Operators'에 대한 한국어 번역입니다.","date":"2025-09-27 00:55:17+0900","permalink":"/python/pep/335","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 335 Overloadable Boolean Operators 상태: Rejected | 유형: Standards Track | 작성일: 29Aug2004 PEP 335: 오버로딩 가능한 불리언 연산자 (Overloadable Boolean Operators) 본 문서는 거부(Rejected)된 제안입니다. 개요 (Abstract) 이 "},{"id":"2025-09-27-pep-0343-the-with-statement","title":"[Final] PEP 343 - The “with” Statement","excerpt":"Python Enhancement Proposal 343: 'The “with” Statement'에 대한 한국어 번역입니다.","date":"2025-09-27 01:01:20+0900","permalink":"/python/pep/343","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 343 The “with” Statement 상태: Final | 유형: Standards Track | 작성일: 13May2005 PEP 343 – 문 초록 (Abstract) 이 PEP(Python Enhancement Proposal)는 Python 언어에 새로운 문을 추가하여, 문의 표준적인 사용 패턴을 간결하게 만들 수 있도록 "},{"id":"2025-09-27-pep-0355-path-object-oriented-filesystem-paths","title":"[Rejected] PEP 355 - Path - Object oriented filesystem paths","excerpt":"Python Enhancement Proposal 355: 'Path - Object oriented filesystem paths'에 대한 한국어 번역입니다.","date":"2025-09-27 01:12:42+0900","permalink":"/python/pep/355","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 355 Path Object oriented filesystem paths 상태: Rejected | 유형: Standards Track | 작성일: 24Jan2006 PEP 355 – Path: 객체 지향 파일 시스템 경로 (거부됨) 개요 이 문서는 Python Enhancement Proposal (PEP) 355의 내용을 한국어로 "},{"id":"2025-09-27-pep-0599-the-manylinux2014-platform-tag","title":"[Superseded] PEP 599 - The manylinux2014 Platform Tag","excerpt":"Python Enhancement Proposal 599: 'The manylinux2014 Platform Tag'에 대한 한국어 번역입니다.","date":"2025-09-27 00:14:20+0900","permalink":"/python/pep/599","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 599 The manylinux2014 Platform Tag 상태: Superseded | 유형: Informational | 작성일: 29Apr2019 PEP 599 – manylinux2014 플랫폼 태그 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 기존 PEP 513에서 도입된 태그를 "},{"id":"2025-09-27-pep-0608-coordinated-python-release","title":"[Rejected] PEP 608 - Coordinated Python release","excerpt":"Python Enhancement Proposal 608: 'Coordinated Python release'에 대한 한국어 번역입니다.","date":"2025-09-27 00:19:32+0900","permalink":"/python/pep/608","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 608 Coordinated Python release 상태: Rejected | 유형: Standards Track | 작성일: 25Oct2019 PEP 608 – Coordinated Python release는 Python 릴리스를 특정 필수 프로젝트들의 호환 가능한 버전이 준비될 때까지 보류하는 것을 제안하는 문서입니다. 이 제안"},{"id":"2025-09-27-pep-0626-precise-line-numbers-for-debugging-and-other-tools-","title":"[Final] PEP 626 - Precise line numbers for debugging and other tools.","excerpt":"Python Enhancement Proposal 626: 'Precise line numbers for debugging and other tools.'에 대한 한국어 번역입니다.","date":"2025-09-27 00:29:42+0900","permalink":"/python/pep/626","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 626 Precise line numbers for debugging and other tools. 상태: Final | 유형: Standards Track | 작성일: 15Jul2020 PEP 626 – 디버깅 및 기타 도구를 위한 정밀한 줄 번호 (Precise line numbers for debugging and other too"},{"id":"2025-09-27-pep-0627-recording-installed-projects","title":"[Final] PEP 627 - Recording installed projects","excerpt":"Python Enhancement Proposal 627: 'Recording installed projects'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:10+0900","permalink":"/python/pep/627","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 627 Recording installed projects 상태: Final | 유형: Standards Track | 작성일: 15Jul2020 PEP 627 – 설치된 프로젝트 기록 (Recording installed projects) 요약 (Abstract) 이 PEP는 기존의 PEP 376(설치된 Python 배포판 데이터베이스"},{"id":"2025-09-27-pep-0628-addmath-tau","title":"[Final] PEP 628 - Addmath.tau","excerpt":"Python Enhancement Proposal 628: 'Addmath.tau'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:25+0900","permalink":"/python/pep/628","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 628 Addmath.tau 상태: Final | 유형: Standards Track | 작성일: 28Jun2011 PEP 628 – 추가 제안 개요 이 문서는 Python 표준 라이브러리에 원 상수 를 추가할 것을 제안합니다. 의 개념은 원의 둘레(circumference)와 반지름(radius)의 비율이 둘레와 지름(diameter)"},{"id":"2025-09-27-pep-0629-versioning-pypis-simple-api","title":"[Final] PEP 629 - Versioning PyPI’s Simple API","excerpt":"Python Enhancement Proposal 629: 'Versioning PyPI’s Simple API'에 대한 한국어 번역입니다.","date":"2025-09-27 00:30:52+0900","permalink":"/python/pep/629","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 629 Versioning PyPI’s Simple API 상태: Final | 유형: Standards Track | 작성일: 16Jul2020 PEP 629 – PyPI의 Simple API 버전 관리 초록 (Abstract) 이 PEP는 PyPI(Python Package Index)의 Simple API에 버전 관리 방식을 추가할"},{"id":"2025-09-27-pep-0630-isolating-extension-modules","title":"[Final] PEP 630 - Isolating Extension Modules","excerpt":"Python Enhancement Proposal 630: 'Isolating Extension Modules'에 대한 한국어 번역입니다.","date":"2025-09-27 01:13:23+0900","permalink":"/python/pep/630","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 630 Isolating Extension Modules 상태: Final | 유형: Informational | 작성일: 25Aug2020 PEP 630 – 확장 모듈 격리 (Isolating Extension Modules) 개요 (Abstract) 기존에는 Python 확장 모듈의 상태(state)가 프로세스 전역 범위(proces"},{"id":"2025-09-27-pep-0631-dependency-specification-in-pyproject-toml-based-on-pep-508","title":"[Superseded] PEP 631 - Dependency specification in pyproject.toml based on PEP 508","excerpt":"Python Enhancement Proposal 631: 'Dependency specification in pyproject.toml based on PEP 508'에 대한 한국어 번역입니다.","date":"2025-09-27 01:15:39+0900","permalink":"/python/pep/631","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 631 Dependency specification in pyproject.toml based on PEP 508 상태: Superseded | 유형: Standards Track | 작성일: 20Aug2020 PEP 631: 을 통한 의존성 명세 (PEP 508 기반) 작성자: Ofek Lev 스폰서: Paul Ganssle 상태: S"},{"id":"2025-09-27-pep-0632-deprecate-distutils-module","title":"[Final] PEP 632 - Deprecate distutils module","excerpt":"Python Enhancement Proposal 632: 'Deprecate distutils module'에 대한 한국어 번역입니다.","date":"2025-09-27 01:16:21+0900","permalink":"/python/pep/632","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 632 Deprecate distutils module 상태: Final | 유형: Standards Track | 작성일: 03Sep2020 PEP 632 – 모듈 Deprecate (사용 중단) 개요 (Abstract) 모듈은 오랫동안 패키지를 대신 사용할 것을 권장해 왔습니다. 는 최근 의 전체 복사본을 통합하여 더 이상 표준 라이"},{"id":"2025-09-27-pep-0633-dependency-specification-in-pyproject-toml-using-an-exploded-toml-table","title":"[Rejected] PEP 633 - Dependency specification in pyproject.toml using an exploded TOML table","excerpt":"Python Enhancement Proposal 633: 'Dependency specification in pyproject.toml using an exploded TOML table'에 대한 한국어 번역입니다.","date":"2025-09-27 01:25:08+0900","permalink":"/python/pep/633","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 633 Dependency specification in pyproject.toml using an exploded TOML table 상태: Rejected | 유형: Standards Track | 작성일: 02Sep2020 PEP 633 – pyproject.toml에서 확장된 TOML 테이블을 사용한 의존성 명세 (Dependen"},{"id":"2025-09-27-pep-0634-structural-pattern-matching-specification","title":"[Final] PEP 634 - Structural Pattern Matching: Specification","excerpt":"Python Enhancement Proposal 634: 'Structural Pattern Matching: Specification'에 대한 한국어 번역입니다.","date":"2025-09-27 01:25:49+0900","permalink":"/python/pep/634","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 634 Structural Pattern Matching: Specification 상태: Final | 유형: Standards Track | 작성일: 12Sep2020 개요 이 문서는 Python 3.10에 도입된 문의 핵심 기능인 구조적 패턴 매칭(Structural Pattern Matching)에 대한 기술 명세를 제공합니다. "},{"id":"2025-09-27-pep-0635-structural-pattern-matching-motivation-and-rationale","title":"[Final] PEP 635 - Structural Pattern Matching: Motivation and Rationale","excerpt":"Python Enhancement Proposal 635: 'Structural Pattern Matching: Motivation and Rationale'에 대한 한국어 번역입니다.","date":"2025-09-27 01:26:51+0900","permalink":"/python/pep/635","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 635 Structural Pattern Matching: Motivation and Rationale 상태: Final | 유형: Informational | 작성일: 12Sep2020 PEP 635 – 구조적 패턴 매칭: 동기 및 근거 (Structural Pattern Matching: Motivation and Rationale)"},{"id":"2025-09-27-pep-0636-structural-pattern-matching-tutorial","title":"[Final] PEP 636 - Structural Pattern Matching: Tutorial","excerpt":"Python Enhancement Proposal 636: 'Structural Pattern Matching: Tutorial'에 대한 한국어 번역입니다.","date":"2025-09-27 01:29:01+0900","permalink":"/python/pep/636","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 636 Structural Pattern Matching: Tutorial 상태: Final | 유형: Informational | 작성일: 12Sep2020 PEP 636 – 구조적 패턴 매칭: 튜토리얼 Python Enhancement Proposals Python » PEP Index » PEP 636 PEP 636 – Struct"},{"id":"2025-09-27-pep-0637-support-for-indexing-with-keyword-arguments","title":"[Rejected] PEP 637 - Support for indexing with keyword arguments","excerpt":"Python Enhancement Proposal 637: 'Support for indexing with keyword arguments'에 대한 한국어 번역입니다.","date":"2025-09-27 01:29:49+0900","permalink":"/python/pep/637","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 637 Support for indexing with keyword arguments 상태: Rejected | 유형: Standards Track | 작성일: 24Aug2020 PEP 637: 키워드 인자를 사용한 인덱싱 지원 (결론: 거부됨) 요약 PEP 637은 Python의 아이템 접근(item access), 즉 (대괄호)를 사"},{"id":"2025-09-27-pep-0638-syntactic-macros","title":"[Draft] PEP 638 - Syntactic Macros","excerpt":"Python Enhancement Proposal 638: 'Syntactic Macros'에 대한 한국어 번역입니다.","date":"2025-09-27 01:30:41+0900","permalink":"/python/pep/638","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 638 Syntactic Macros 상태: Draft | 유형: Standards Track | 작성일: 24Sep2020 PEP 638 – 구문 매크로 (Syntactic Macros) 작성자: Mark Shannon 상태: 초안 (Draft) 생성일: 2020년 9월 24일 요약 (Abstract) 이 PEP는 Python에 구문 "},{"id":"2025-09-27-pep-0639-improving-license-clarity-with-better-package-metadata","title":"[Final] PEP 639 - Improving License Clarity with Better Package Metadata","excerpt":"Python Enhancement Proposal 639: 'Improving License Clarity with Better Package Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 01:31:09+0900","permalink":"/python/pep/639","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 639 Improving License Clarity with Better Package Metadata 상태: Final | 유형: Standards Track | 작성일: 15Aug2019 PEP 639 – 더 나은 패키지 메타데이터로 라이선스 명확성 향상 개요 이 PEP (Python Enhancement Proposal)는 Pyt"},{"id":"2025-09-27-pep-0640-unused-variable-syntax","title":"[Rejected] PEP 640 - Unused variable syntax","excerpt":"Python Enhancement Proposal 640: 'Unused variable syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 01:31:38+0900","permalink":"/python/pep/640","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 640 Unused variable syntax 상태: Rejected | 유형: Standards Track | 작성일: 04Oct2020 PEP 640: 사용되지 않는 변수 구문 제안 번역 및 해설 거부(Rejection) 사유 PEP 640은 스티어링 위원회에 의해 거부되었습니다. 관련 내용은 다음 링크에서 확인할 수 있습니다: h"},{"id":"2025-09-27-pep-0641-using-an-underscore-in-the-version-portion-of-python-3-10-compatibility-tags","title":"[Rejected] PEP 641 - Using an underscore in the version portion of Python 3.10 compatibility tags","excerpt":"Python Enhancement Proposal 641: 'Using an underscore in the version portion of Python 3.10 compatibility tags'에 대한 한국어 번역입니다.","date":"2025-09-27 01:32:00+0900","permalink":"/python/pep/641","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 641 Using an underscore in the version portion of Python 3.10 compatibility tags 상태: Rejected | 유형: Standards Track | 작성일: 20Oct2020 PEP 641: Python 3.10 호환성 태그 버전 부분에 언더스코어 사용 작성자: Brett C"},{"id":"2025-09-27-pep-0642-explicit-pattern-syntax-for-structural-pattern-matching","title":"[Rejected] PEP 642 - Explicit Pattern Syntax for Structural Pattern Matching","excerpt":"Python Enhancement Proposal 642: 'Explicit Pattern Syntax for Structural Pattern Matching'에 대한 한국어 번역입니다.","date":"2025-09-27 01:33:04+0900","permalink":"/python/pep/642","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 642 Explicit Pattern Syntax for Structural Pattern Matching 상태: Rejected | 유형: Standards Track | 작성일: 26Sep2020 PEP 642: 구조적 패턴 매칭을 위한 명시적 패턴 구문 초록 (Abstract) 이 PEP는 PEP 634의 구조적 패턴 매칭(stru"},{"id":"2025-09-27-pep-0643-metadata-for-package-source-distributions","title":"[Final] PEP 643 - Metadata for Package Source Distributions","excerpt":"Python Enhancement Proposal 643: 'Metadata for Package Source Distributions'에 대한 한국어 번역입니다.","date":"2025-09-27 01:33:51+0900","permalink":"/python/pep/643","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 643 Metadata for Package Source Distributions 상태: Final | 유형: Standards Track | 작성일: 24Oct2020 PEP 643 – 패키지 소스 배포판을 위한 메타데이터 개요 (Abstract) Python 패키지 메타데이터는 에 정의된 표준 형식으로 배포 파일에 저장됩니다. 그러나"},{"id":"2025-09-27-pep-0644-require-openssl-1-1-1-or-newer","title":"[Final] PEP 644 - Require OpenSSL 1.1.1 or newer","excerpt":"Python Enhancement Proposal 644: 'Require OpenSSL 1.1.1 or newer'에 대한 한국어 번역입니다.","date":"2025-09-27 01:34:31+0900","permalink":"/python/pep/644","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 644 Require OpenSSL 1.1.1 or newer 상태: Final | 유형: Standards Track | 작성일: 27Oct2020 PEP 644 – OpenSSL 1.1.1 이상 버전 요구 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 CPython의 표준 라이브러리가 Ope"},{"id":"2025-09-27-pep-0645-allow-writing-optional-types-asx","title":"[Withdrawn] PEP 645 - Allow writing optional types asx?","excerpt":"Python Enhancement Proposal 645: 'Allow writing optional types asx?'에 대한 한국어 번역입니다.","date":"2025-09-27 01:34:51+0900","permalink":"/python/pep/645","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 645 Allow writing optional types asx? 상태: Withdrawn | 유형: Standards Track | 작성일: 25Aug2020 PEP 645 – 타입 표기를 위한 문법 허용 제안 (철회됨) 개요 이 문서는 대신 와 같이 연산자를 사용하여 타입을 표기하는 새로운 문법을 추가할 것을 제안했습니다. PEP "},{"id":"2025-09-27-pep-0646-variadic-generics","title":"[Final] PEP 646 - Variadic Generics","excerpt":"Python Enhancement Proposal 646: 'Variadic Generics'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:00+0900","permalink":"/python/pep/646","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 646 Variadic Generics 상태: Final | 유형: Standards Track | 작성일: 16Sep2020 PEP 646 – 가변 제네릭 (Variadic Generics) 개요 PEP 484는 단일 타입으로 매개변수화되는 제네릭을 생성하기 위한 를 도입했습니다. 이 PEP에서는 임의의 수의 타입으로 매개변수화를 가능"},{"id":"2025-09-27-pep-0647-user-defined-type-guards","title":"[Final] PEP 647 - User-Defined Type Guards","excerpt":"Python Enhancement Proposal 647: 'User-Defined Type Guards'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:28+0900","permalink":"/python/pep/647","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 647 UserDefined Type Guards 상태: Final | 유형: Standards Track | 작성일: 07Oct2020 PEP 647은 Python의 타입 힌팅 시스템을 강화하여, 런타임 검사에 기반한 조건부 타입 좁히기(conditional type narrowing) 기능을 사용자 정의 함수로 확장할 수 있도록 라는"},{"id":"2025-09-27-pep-0648-extensible-customizations-of-the-interpreter-at-startup","title":"[Rejected] PEP 648 - Extensible customizations of the interpreter at startup","excerpt":"Python Enhancement Proposal 648: 'Extensible customizations of the interpreter at startup'에 대한 한국어 번역입니다.","date":"2025-09-27 01:37:58+0900","permalink":"/python/pep/648","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 648 Extensible customizations of the interpreter at startup 상태: Rejected | 유형: Standards Track | 작성일: 30Dec2020 PEP 648: 인터프리터 시작 시 확장 가능한 사용자 정의 요약 (Abstract) 이 PEP는 사용자가 시작 시 실행될 파일을 설치할 "},{"id":"2025-09-27-pep-0649-deferred-evaluation-of-annotations-using-descriptors","title":"[Accepted] PEP 649 - Deferred Evaluation Of Annotations Using Descriptors","excerpt":"Python Enhancement Proposal 649: 'Deferred Evaluation Of Annotations Using Descriptors'에 대한 한국어 번역입니다.","date":"2025-09-27 01:39:54+0900","permalink":"/python/pep/649","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 649 Deferred Evaluation Of Annotations Using Descriptors 상태: Accepted | 유형: Standards Track | 작성일: 11Jan2021 PEP 649 – Descriptors를 사용한 Annotation 지연 평가 (Deferred Evaluation Of Annotations "},{"id":"2025-09-27-pep-0650-specifying-installer-requirements-for-python-projects","title":"[Withdrawn] PEP 650 - Specifying Installer Requirements for Python Projects","excerpt":"Python Enhancement Proposal 650: 'Specifying Installer Requirements for Python Projects'에 대한 한국어 번역입니다.","date":"2025-09-27 01:40:48+0900","permalink":"/python/pep/650","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 650 Specifying Installer Requirements for Python Projects 상태: Withdrawn | 유형: Standards Track | 작성일: 16Jul2020 PEP 650 – Python 프로젝트를 위한 설치 프로그램 요구사항 명세 (Withdrawn) 작성자: Vikram Jayanthi, Du"},{"id":"2025-09-27-pep-0651-robust-stack-overflow-handling","title":"[Rejected] PEP 651 - Robust Stack Overflow Handling","excerpt":"Python Enhancement Proposal 651: 'Robust Stack Overflow Handling'에 대한 한국어 번역입니다.","date":"2025-09-27 01:41:16+0900","permalink":"/python/pep/651","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 651 Robust Stack Overflow Handling 상태: Rejected | 유형: Standards Track | 작성일: 18Jan2021 PEP 651 – 견고한 스택 오버플로우 처리 작성자: Mark Shannon 상태: 거절됨 (Rejected) 유형: 표준 트랙 (Standards Track) 생성일: 2021년 "},{"id":"2025-09-27-pep-0652-maintaining-the-stable-abi","title":"[Final] PEP 652 - Maintaining the Stable ABI","excerpt":"Python Enhancement Proposal 652: 'Maintaining the Stable ABI'에 대한 한국어 번역입니다.","date":"2025-09-27 01:41:49+0900","permalink":"/python/pep/652","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 652 Maintaining the Stable ABI 상태: Final | 유형: Standards Track | 작성일: 09Feb2021 PEP 652 – Stable ABI 유지 관리 개요 PEP 652는 CPython의 Limited CAPI와 Stable ABI(PEP 384에서 도입)를 단일하고 명확한 파일로 공식화하고, 이"},{"id":"2025-09-27-pep-0653-precise-semantics-for-pattern-matching","title":"[Draft] PEP 653 - Precise Semantics for Pattern Matching","excerpt":"Python Enhancement Proposal 653: 'Precise Semantics for Pattern Matching'에 대한 한국어 번역입니다.","date":"2025-09-27 09:52:27+0900","permalink":"/python/pep/653","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 653 Precise Semantics for Pattern Matching 상태: Draft | 유형: Standards Track | 작성일: 09Feb2021 PEP 653 – 패턴 매칭을 위한 정밀한 의미론 저자 : Mark Shannon 상태 : Draft (초안) 유형 : Standards Track (표준 트랙) 생성일 : "},{"id":"2025-09-27-pep-0654-exception-groups-and-except","title":"[Final] PEP 654 - Exception Groups and except*","excerpt":"Python Enhancement Proposal 654: 'Exception Groups and except*'에 대한 한국어 번역입니다.","date":"2025-09-27 09:52:52+0900","permalink":"/python/pep/654","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 654 Exception Groups and except 상태: Final | 유형: Standards Track | 작성일: 22Feb2021 PEP 654 – 예외 그룹 및 (Exception Groups and except) 한국어 번역 및 요약 초록 (Abstract) 이 문서는 프로그램이 여러 개의 관련 없는 예외를 동시에 발생"},{"id":"2025-09-27-pep-0655-marking-individual-typeddict-items-as-required-or-potentially-missing","title":"[Final] PEP 655 - Marking individual TypedDict items as required or potentially-missing","excerpt":"Python Enhancement Proposal 655: 'Marking individual TypedDict items as required or potentially-missing'에 대한 한국어 번역입니다.","date":"2025-09-27 09:53:14+0900","permalink":"/python/pep/655","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 655 Marking individual TypedDict items as required or potentiallymissing 상태: Final | 유형: Standards Track | 작성일: 30Jan2021 PEP 655 – TypedDict 개별 항목을 필수 또는 선택으로 표시하기 개요 PEP 589는 모든 키가 필수인 와 "},{"id":"2025-09-27-pep-0656-platform-tag-for-linux-distributions-using-musl","title":"[Final] PEP 656 - Platform Tag for Linux Distributions Using Musl","excerpt":"Python Enhancement Proposal 656: 'Platform Tag for Linux Distributions Using Musl'에 대한 한국어 번역입니다.","date":"2025-09-27 09:53:36+0900","permalink":"/python/pep/656","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 656 Platform Tag for Linux Distributions Using Musl 상태: Final | 유형: Standards Track | 작성일: 17Mar2021 PEP 656 – Musl을 사용하는 Linux 배포판을 위한 플랫폼 태그 개요 이 PEP 656은 기반의 Linux 배포판에서 Python 바이너리 패키지를"},{"id":"2025-09-27-pep-0657-include-fine-grained-error-locations-in-tracebacks","title":"[Final] PEP 657 - Include Fine Grained Error Locations in Tracebacks","excerpt":"Python Enhancement Proposal 657: 'Include Fine Grained Error Locations in Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:06+0900","permalink":"/python/pep/657","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 657 Include Fine Grained Error Locations in Tracebacks 상태: Final | 유형: Standards Track | 작성일: 08May2021 PEP 657 – 트레이스백에 상세한 오류 위치 정보 포함 (Include Fine Grained Error Locations in Tracebacks)"},{"id":"2025-09-27-pep-0658-serve-distribution-metadata-in-the-simple-repository-api","title":"[Accepted] PEP 658 - Serve Distribution Metadata in the Simple Repository API","excerpt":"Python Enhancement Proposal 658: 'Serve Distribution Metadata in the Simple Repository API'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:18+0900","permalink":"/python/pep/658","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 658 Serve Distribution Metadata in the Simple Repository API 상태: Accepted | 유형: Standards Track | 작성일: 10May2021 PEP 658 – Simple Repository API에서 배포 메타데이터 제공 개요 이 문서는 PEP 503 \"simple\" 저장소 "},{"id":"2025-09-27-pep-0659-specializing-adaptive-interpreter","title":"[Final] PEP 659 - Specializing Adaptive Interpreter","excerpt":"Python Enhancement Proposal 659: 'Specializing Adaptive Interpreter'에 대한 한국어 번역입니다.","date":"2025-09-27 09:54:44+0900","permalink":"/python/pep/659","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 659 Specializing Adaptive Interpreter 상태: Final | 유형: Informational | 작성일: 13Apr2021 PEP 659: 특수화 적응형 인터프리터 (Specializing Adaptive Interpreter) 요약 (Abstract) 동적 언어용 가상 머신(Virtual Machine)이 "},{"id":"2025-09-27-pep-0660-editable-installs-for-pyproject-toml-based-builds-wheel-based","title":"[Final] PEP 660 - Editable installs for pyproject.toml based builds (wheel based)","excerpt":"Python Enhancement Proposal 660: 'Editable installs for pyproject.toml based builds (wheel based)'에 대한 한국어 번역입니다.","date":"2025-09-27 09:55:28+0900","permalink":"/python/pep/660","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 660 Editable installs for pyproject.toml based builds (wheel based) 상태: Final | 유형: Standards Track | 작성일: 30Mar2021 PEP 660 – 기반 빌드를 위한 편집 가능(Editable) 설치 (휠 기반) 초록 (Abstract) 이 문서는 패키지를 편"},{"id":"2025-09-27-pep-0661-sentinel-values","title":"[Deferred] PEP 661 - Sentinel Values","excerpt":"Python Enhancement Proposal 661: 'Sentinel Values'에 대한 한국어 번역입니다.","date":"2025-09-27 09:55:59+0900","permalink":"/python/pep/661","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 661 Sentinel Values 상태: Deferred | 유형: Standards Track | 작성일: 06Jun2021 PEP 661 – Sentinel 값 (Sentinel Values) 작성자 : Tal Einat 논의처 : Discourse 스레드 상태 : 연기됨 (Deferred) 유형 : 표준 트랙 (Standards "},{"id":"2025-09-27-pep-0662-editable-installs-via-virtual-wheels","title":"[Rejected] PEP 662 - Editable installs via virtual wheels","excerpt":"Python Enhancement Proposal 662: 'Editable installs via virtual wheels'에 대한 한국어 번역입니다.","date":"2025-09-27 09:56:52+0900","permalink":"/python/pep/662","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 662 Editable installs via virtual wheels 상태: Rejected | 유형: Standards Track | 작성일: 28May2021 PEP 662 – 가상 휠(Virtual Wheels)을 통한 편집 가능 설치 (Editable Installs) 개요 (Abstract) 이 문서는 PEP 517에서 도입"},{"id":"2025-09-27-pep-0663-standardizing-enum-str-repr-and-format-behaviors","title":"[Rejected] PEP 663 - Standardizing Enum str(), repr(), and format() behaviors","excerpt":"Python Enhancement Proposal 663: 'Standardizing Enum str(), repr(), and format() behaviors'에 대한 한국어 번역입니다.","date":"2025-09-27 09:57:11+0900","permalink":"/python/pep/663","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 663 Standardizing Enum str(), repr(), and format() behaviors 상태: Rejected | 유형: Informational | 작성일: 30Jun2021 PEP 663 – Enum의 , , 동작 표준화 작성자: Ethan Furman 상태: Rejected (거부됨) 타입: Informatio"},{"id":"2025-09-27-pep-0664-python-3-11-release-schedule","title":"[Active] PEP 664 - Python 3.11 Release Schedule","excerpt":"Python Enhancement Proposal 664: 'Python 3.11 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 09:57:22+0900","permalink":"/python/pep/664","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 664 Python 3.11 Release Schedule 상태: Active | 유형: Informational | 작성일: 12Jul2021 개요 이 문서는 Python 3.11의 개발 및 릴리스 일정을 기술합니다. 일정은 주로 PEP 규모의 항목들을 다룹니다. 릴리스 매니저 및 팀 3.11 릴리스 매니저: Pablo Galindo "},{"id":"2025-09-27-pep-0665-a-file-format-to-list-python-dependencies-for-reproducibility-of-an-application","title":"[Rejected] PEP 665 - A file format to list Python dependencies for reproducibility of an application","excerpt":"Python Enhancement Proposal 665: 'A file format to list Python dependencies for reproducibility of an application'에 대한 한국어 번역입니다.","date":"2025-09-27 10:02:15+0900","permalink":"/python/pep/665","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 665 A file format to list Python dependencies for reproducibility of an application 상태: Rejected | 유형: Standards Track | 작성일: 29Jul2021 PEP 665: 애플리케이션 재현 가능성을 위한 Python 의존성 목록 파일 형식 (거부됨) "},{"id":"2025-09-27-pep-0666-reject-foolish-indentation","title":"[Rejected] PEP 666 - Reject Foolish Indentation","excerpt":"Python Enhancement Proposal 666: 'Reject Foolish Indentation'에 대한 한국어 번역입니다.","date":"2025-09-27 10:02:26+0900","permalink":"/python/pep/666","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 666 Reject Foolish Indentation 상태: Rejected | 유형: Standards Track | 작성일: 03Dec2001 PEP 666 – 어리석은 들여쓰기 거부 (Reject Foolish Indentation) 작성자: Laura Creighton <lac at strakt.com 상태: Rejected ("},{"id":"2025-09-27-pep-0667-consistent-views-of-namespaces","title":"[Final] PEP 667 - Consistent views of namespaces","excerpt":"Python Enhancement Proposal 667: 'Consistent views of namespaces'에 대한 한국어 번역입니다.","date":"2025-09-27 10:03:19+0900","permalink":"/python/pep/667","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 667 Consistent views of namespaces 상태: Final | 유형: Standards Track | 작성일: 30Jul2021 PEP 667 – Consistent views of namespaces (네임스페이스의 일관된 뷰) 작성자: Mark Shannon, Tian Gao 상태: Final (최종) 유형: S"},{"id":"2025-09-27-pep-0668-marking-python-base-environments-as-externally-managed","title":"[Accepted] PEP 668 - Marking Python base environments as “externally managed”","excerpt":"Python Enhancement Proposal 668: 'Marking Python base environments as “externally managed”'에 대한 한국어 번역입니다.","date":"2025-09-27 10:04:11+0900","permalink":"/python/pep/668","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 668 Marking Python base environments as “externally managed” 상태: Accepted | 유형: Standards Track | 작성일: 18May2021 PEP 668 – Python 기본 환경을 \"외부 관리\"로 표시 개요 (Abstract) 이 PEP는 OS 패키지 관리자와 와 같은 Py"},{"id":"2025-09-27-pep-0669-low-impact-monitoring-for-cpython","title":"[Final] PEP 669 - Low Impact Monitoring for CPython","excerpt":"Python Enhancement Proposal 669: 'Low Impact Monitoring for CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 10:04:48+0900","permalink":"/python/pep/669","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 669 Low Impact Monitoring for CPython 상태: Final | 유형: Standards Track | 작성일: 18Aug2021 PEP 669 – CPython을 위한 저영향 모니터링 (Low Impact Monitoring for CPython) 개요 (Abstract) CPython에서 프로파일러(profi"},{"id":"2025-09-27-pep-0670-convert-macros-to-functions-in-the-python-c-api","title":"[Final] PEP 670 - Convert macros to functions in the Python C API","excerpt":"Python Enhancement Proposal 670: 'Convert macros to functions in the Python C API'에 대한 한국어 번역입니다.","date":"2025-09-27 10:05:27+0900","permalink":"/python/pep/670","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 670 Convert macros to functions in the Python C API 상태: Final | 유형: Standards Track | 작성일: 19Oct2021 PEP 670 – Python C API의 매크로를 함수로 변환 개요 (Abstract) 이 PEP (Python Enhancement Proposal)는 P"},{"id":"2025-09-27-pep-0671-syntax-for-late-bound-function-argument-defaults","title":"[Draft] PEP 671 - Syntax for late-bound function argument defaults","excerpt":"Python Enhancement Proposal 671: 'Syntax for late-bound function argument defaults'에 대한 한국어 번역입니다.","date":"2025-09-27 10:05:44+0900","permalink":"/python/pep/671","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 671 Syntax for latebound function argument defaults 상태: Draft | 유형: Standards Track | 작성일: 24Oct2021 PEP 671 – 함수 인자 지연 바인딩 기본값 구문 초록 (Abstract) 함수 매개변수는 함수 정의 시점에 계산되어 저장되는 기본값을 가질 수 있습니다."},{"id":"2025-09-27-pep-0672-unicode-related-security-considerations-for-python","title":"[Active] PEP 672 - Unicode-related Security Considerations for Python","excerpt":"Python Enhancement Proposal 672: 'Unicode-related Security Considerations for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 10:06:17+0900","permalink":"/python/pep/672","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 672 Unicoderelated Security Considerations for Python 상태: Active | 유형: Informational | 작성일: 01Nov2021 PEP 672 – Python의 유니코드 관련 보안 고려사항 작성자: Petr Viktorin 상태: Active (활성) 유형: Informational "},{"id":"2025-09-27-pep-0673-self-type","title":"[Final] PEP 673 - Self Type","excerpt":"Python Enhancement Proposal 673: 'Self Type'에 대한 한국어 번역입니다.","date":"2025-09-27 10:07:14+0900","permalink":"/python/pep/673","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 673 Self Type 상태: Final | 유형: Standards Track | 작성일: 10Nov2021 PEP 673 – 타입 요약 (Abstract) 이 PEP는 메서드가 자신의 클래스 인스턴스를 반환할 때 이를 어노테이션(annotation)하는 간단하고 직관적인 방법을 제안합니다. 이 방식은 기존의 기반 접근 방식(PEP "},{"id":"2025-09-27-pep-0674-disallow-using-macros-as-l-values","title":"[Deferred] PEP 674 - Disallow using macros as l-values","excerpt":"Python Enhancement Proposal 674: 'Disallow using macros as l-values'에 대한 한국어 번역입니다.","date":"2025-09-27 10:07:42+0900","permalink":"/python/pep/674","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 674 Disallow using macros as lvalues 상태: Deferred | 유형: Standards Track | 작성일: 30Nov2021 PEP 674 – 매크로를 좌변값(lvalue)으로 사용하는 것을 금지 초록 (Abstract) 이 PEP는 C API에서 매크로를 좌변값(lvalue)으로 사용하는 것을 금지하는"},{"id":"2025-09-27-pep-0675-arbitrary-literal-string-type","title":"[Final] PEP 675 - Arbitrary Literal String Type","excerpt":"Python Enhancement Proposal 675: 'Arbitrary Literal String Type'에 대한 한국어 번역입니다.","date":"2025-09-27 10:08:22+0900","permalink":"/python/pep/675","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 675 Arbitrary Literal String Type 상태: Final | 유형: Standards Track | 작성일: 30Nov2021 PEP 675: 임의의 리터럴 문자열 타입 (Arbitrary Literal String Type) 개요 (Abstract) 현재 Python에서는 특정 리터럴 문자열(예: )이 아닌, \"어"},{"id":"2025-09-27-pep-0676-pep-infrastructure-process","title":"[Active] PEP 676 - PEP Infrastructure Process","excerpt":"Python Enhancement Proposal 676: 'PEP Infrastructure Process'에 대한 한국어 번역입니다.","date":"2025-09-27 10:08:50+0900","permalink":"/python/pep/676","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 676 PEP Infrastructure Process 상태: Active | 유형: Process | 작성일: 01Nov2021 파이썬 PEP 676 – PEP 인프라 프로세스 번역 및 설명 개요 PEP 676은 reStructuredText 파일에서 HTML 웹페이지로 PEP 파일을 렌더링하는 인프라를 다룹니다. 이 제안은 PEP 독"},{"id":"2025-09-27-pep-0677-callable-type-syntax","title":"[Rejected] PEP 677 - Callable Type Syntax","excerpt":"Python Enhancement Proposal 677: 'Callable Type Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:13+0900","permalink":"/python/pep/677","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 677 Callable Type Syntax 상태: Rejected | 유형: Standards Track | 작성일: 13Dec2021 요약 (Abstract) 이 PEP는 과 동일한 기능을 지원하면서도, 타입이 지정된 함수 시그니처에서 영감을 받은 화살표 문법으로 타입을 표현하는 간결하고 친숙한 문법을 도입합니다. 이를 통해 과 같은"},{"id":"2025-09-27-pep-0678-enriching-exceptions-with-notes","title":"[Final] PEP 678 - Enriching Exceptions with Notes","excerpt":"Python Enhancement Proposal 678: 'Enriching Exceptions with Notes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:33+0900","permalink":"/python/pep/678","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 678 Enriching Exceptions with Notes 상태: Final | 유형: Standards Track | 작성일: 20Dec2021 PEP 678 – 예외에 Notes 추가하기 초록 (Abstract) Python의 예외(Exception) 객체는 일반적으로 발생한 오류를 설명하는 메시지로 초기화됩니다. 하지만 예외가"},{"id":"2025-09-27-pep-0679-new-assert-statement-syntax-with-parentheses","title":"[Draft] PEP 679 - New assert statement syntax with parentheses","excerpt":"Python Enhancement Proposal 679: 'New assert statement syntax with parentheses'에 대한 한국어 번역입니다.","date":"2025-09-27 10:09:51+0900","permalink":"/python/pep/679","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 679 New assert statement syntax with parentheses 상태: Draft | 유형: Standards Track | 작성일: 07Jan2022 PEP 679 – 괄호가 있는 새로운 문법 개요 (Abstract) 이 PEP는 두 인수를 받는 문에서 괄호 사용을 허용하는 것을 제안합니다. 즉, 형태의 코드를 "},{"id":"2025-09-27-pep-0680-tomllib-support-for-parsing-toml-in-the-standard-library","title":"[Final] PEP 680 - tomllib: Support for Parsing TOML in the Standard Library","excerpt":"Python Enhancement Proposal 680: 'tomllib: Support for Parsing TOML in the Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:11+0900","permalink":"/python/pep/680","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 680 tomllib: Support for Parsing TOML in the Standard Library 상태: Final | 유형: Standards Track | 작성일: 01Jan2022 PEP 680 – : 표준 라이브러리에 TOML 파싱 지원 추가 요약 (Abstract) 이 PEP는 TOML(Tom's Obvious Mi"},{"id":"2025-09-27-pep-0681-data-class-transforms","title":"[Final] PEP 681 - Data Class Transforms","excerpt":"Python Enhancement Proposal 681: 'Data Class Transforms'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:35+0900","permalink":"/python/pep/681","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 681 Data Class Transforms 상태: Final | 유형: Standards Track | 작성일: 02Dec2021 PEP 681 – Data Class Transforms 개요 (Abstract) PEP 557은 Python 표준 라이브러리에 를 도입했습니다. 와 유사한 동작을 제공하지만 표준 타입 어노테이션으로는 설"},{"id":"2025-09-27-pep-0682-format-specifier-for-signed-zero","title":"[Final] PEP 682 - Format Specifier for Signed Zero","excerpt":"Python Enhancement Proposal 682: 'Format Specifier for Signed Zero'에 대한 한국어 번역입니다.","date":"2025-09-27 10:10:49+0900","permalink":"/python/pep/682","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 682 Format Specifier for Signed Zero 상태: Final | 유형: Standards Track | 작성일: 29Jan2022 PEP 682 – 부호 있는 0에 대한 형식 지정자 개요 이 문서는 Python의 및 타입이 부호 있는 0(예: )을 표현할 수 있음에도 불구하고, 많은 수학 분야에서 음수 0이 예상치"},{"id":"2025-09-27-pep-0683-immortal-objects-using-a-fixed-refcount","title":"[Final] PEP 683 - Immortal Objects, Using a Fixed Refcount","excerpt":"Python Enhancement Proposal 683: 'Immortal Objects, Using a Fixed Refcount'에 대한 한국어 번역입니다.","date":"2025-09-27 10:11:29+0900","permalink":"/python/pep/683","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 683 Immortal Objects, Using a Fixed Refcount 상태: Final | 유형: Standards Track | 작성일: 10Feb2022 PEP 683 – 고정된 참조 카운트()를 사용하는 불멸(Immortal) 객체 목표 이 PEP는 CPython 내부적으로 특정 객체들을 \"불멸(Immortal)\"로 지정"},{"id":"2025-09-27-pep-0684-a-per-interpreter-gil","title":"[Final] PEP 684 - A Per-Interpreter GIL","excerpt":"Python Enhancement Proposal 684: 'A Per-Interpreter GIL'에 대한 한국어 번역입니다.","date":"2025-09-27 10:11:59+0900","permalink":"/python/pep/684","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 684 A PerInterpreter GIL 상태: Final | 유형: Standards Track | 작성일: 08Mar2022 PEP 684 – 인터프리터별 GIL (A PerInterpreter GIL) 작성자 : Eric Snow 상태 : Final (최종) 유형 : Standards Track (표준 트랙) Python 버전 "},{"id":"2025-09-27-pep-0685-comparison-of-extra-names-for-optional-distribution-dependencies","title":"[Final] PEP 685 - Comparison of extra names for optional distribution dependencies","excerpt":"Python Enhancement Proposal 685: 'Comparison of extra names for optional distribution dependencies'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:15+0900","permalink":"/python/pep/685","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 685 Comparison of extra names for optional distribution dependencies 상태: Final | 유형: Standards Track | 작성일: 08Mar2022 PEP 685 – 선택적 배포 종속성() 비교를 위한 이름 정규화 개요 (Abstract) 이 PEP는 배포 을 비교할 때 어떻"},{"id":"2025-09-27-pep-0686-make-utf-8-mode-default","title":"[Accepted] PEP 686 - Make UTF-8 mode default","excerpt":"Python Enhancement Proposal 686: 'Make UTF-8 mode default'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:32+0900","permalink":"/python/pep/686","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 686 Make UTF8 mode default 상태: Accepted | 유형: Standards Track | 작성일: 18Mar2022 PEP 686 – UTF8 모드 기본값으로 설정 작성자 : Inada Naoki 상태 : Accepted (승인됨) 유형 : Standards Track (표준 트랙) 생성일 : 2022년 3월 1"},{"id":"2025-09-27-pep-0687-isolating-modules-in-the-standard-library","title":"[Accepted] PEP 687 - Isolating modules in the standard library","excerpt":"Python Enhancement Proposal 687: 'Isolating modules in the standard library'에 대한 한국어 번역입니다.","date":"2025-09-27 10:12:44+0900","permalink":"/python/pep/687","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 687 Isolating modules in the standard library 상태: Accepted | 유형: Standards Track | 작성일: 04Apr2022 요약 이 PEP는 표준 라이브러리의 확장 모듈(Extension Modules)을 다단계 초기화(multiphase initialization, PEP 489) 방"},{"id":"2025-09-27-pep-0688-making-the-buffer-protocol-accessible-in-python","title":"[Final] PEP 688 - Making the buffer protocol accessible in Python","excerpt":"Python Enhancement Proposal 688: 'Making the buffer protocol accessible in Python'에 대한 한국어 번역입니다.","date":"2025-09-27 10:13:02+0900","permalink":"/python/pep/688","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 688 Making the buffer protocol accessible in Python 상태: Final | 유형: Standards Track | 작성일: 23Apr2022 PEP 688 – Python에서 버퍼 프로토콜 접근성 확보 초록 (Abstract) 이 PEP는 현재 C 코드에서만 접근 가능한 버퍼 프로토콜(buffer "},{"id":"2025-09-27-pep-0689-unstable-c-api-tier","title":"[Final] PEP 689 - Unstable C API tier","excerpt":"Python Enhancement Proposal 689: 'Unstable C API tier'에 대한 한국어 번역입니다.","date":"2025-09-27 10:13:23+0900","permalink":"/python/pep/689","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 689 Unstable C API tier 상태: Final | 유형: Standards Track | 작성일: 22Apr2022 PEP 689 – 불안정한 C API 계층 (Unstable C API tier) 요약 (Abstract) 이 PEP는 CAPI의 일부 함수와 타입을 \"불안정(unstable)\"으로 지정할 것을 제안합니다. "},{"id":"2025-09-27-pep-0690-lazy-imports","title":"[Rejected] PEP 690 - Lazy Imports","excerpt":"Python Enhancement Proposal 690: 'Lazy Imports'에 대한 한국어 번역입니다.","date":"2025-09-27 10:19:51+0900","permalink":"/python/pep/690","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 690 Lazy Imports 상태: Rejected | 유형: Standards Track | 작성일: 29Apr2022 PEP 690 – Lazy Imports (지연 임포트) 개요 (Abstract) 이 PEP는 임포트된 모듈의 검색 및 실행을 임포트된 객체가 처음 사용되는 순간까지 투명하게 지연시키는 기능을 제안합니다. Pytho"},{"id":"2025-09-27-pep-0691-json-based-simple-api-for-python-package-indexes","title":"[Accepted] PEP 691 - JSON-based Simple API for Python Package Indexes","excerpt":"Python Enhancement Proposal 691: 'JSON-based Simple API for Python Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:21:04+0900","permalink":"/python/pep/691","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 691 JSONbased Simple API for Python Package Indexes 상태: Accepted | 유형: Standards Track | 작성일: 04May2022 초록 (Abstract) PEP 503에 정의된(그리고 그보다 훨씬 오래 사용되어 온) \"Simple Repository API\"는 오랫동안 상당히 잘 "},{"id":"2025-09-27-pep-0692-using-typeddict-for-more-precise-kwargs-typing","title":"[Final] PEP 692 - Using TypedDict for more precise **kwargs typing","excerpt":"Python Enhancement Proposal 692: 'Using TypedDict for more precise ** kwargs typing'에 대한 한국어 번역입니다.","date":"2025-09-27 10:21:57+0900","permalink":"/python/pep/692","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 692 Using TypedDict for more precise kwargs typing 상태: Final | 유형: Standards Track | 작성일: 29May2022 PEP 692 – 보다 정밀한 타입 지정을 위한 활용 개요 이 PEP는 에 포함된 키워드 인자들의 타입이 서로 다를 때, 를 사용하여 보다 정밀하게 의 타입을 "},{"id":"2025-09-27-pep-0693-python-3-12-release-schedule","title":"[Active] PEP 693 - Python 3.12 Release Schedule","excerpt":"Python Enhancement Proposal 693: 'Python 3.12 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 10:22:22+0900","permalink":"/python/pep/693","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 693 Python 3.12 Release Schedule 상태: Active | 유형: Informational | 작성일: 24May2022 PEP 693 – Python 3.12 릴리스 스케줄 작성자: Thomas Wouters <thomas at python.org 상태: Active (활성) 유형: Informational (정"},{"id":"2025-09-27-pep-0694-upload-2-0-api-for-python-package-indexes","title":"[Draft] PEP 694 - Upload 2.0 API for Python Package Indexes","excerpt":"Python Enhancement Proposal 694: 'Upload 2.0 API for Python Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 10:23:36+0900","permalink":"/python/pep/694","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 694 Upload 2.0 API for Python Package Indexes 상태: Draft | 유형: Standards Track | 작성일: 11Jun2022 PEP 694 – Python 패키지 인덱스를 위한 Upload 2.0 API 개요 (Abstract) 이 PEP는 PyPI(Python Package Index)와 같"},{"id":"2025-09-27-pep-0695-type-parameter-syntax","title":"[Final] PEP 695 - Type Parameter Syntax","excerpt":"Python Enhancement Proposal 695: 'Type Parameter Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 13:03:44+0900","permalink":"/python/pep/695","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 695 Type Parameter Syntax 상태: Final | 유형: Standards Track | 작성일: 15Jun2022 PEP 695 – 타입 매개변수 구문 작성자: Eric Traut 후원자: Guido van Rossum 상태: Final 타입: Standards Track (표준 트랙) 주제: Typing (타입 힌트"},{"id":"2025-09-27-pep-0696-type-defaults-for-type-parameters","title":"[Final] PEP 696 - Type Defaults for Type Parameters","excerpt":"Python Enhancement Proposal 696: 'Type Defaults for Type Parameters'에 대한 한국어 번역입니다.","date":"2025-09-27 13:04:16+0900","permalink":"/python/pep/696","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 696 Type Defaults for Type Parameters 상태: Final | 유형: Standards Track | 작성일: 14Jul2022 PEP 696 – 타입 파라미터의 기본값 (Type Defaults for Type Parameters) 개요 이 PEP (Python Enhancement Proposal)는 , ,"},{"id":"2025-09-27-pep-0697-limited-c-api-for-extending-opaque-types","title":"[Final] PEP 697 - Limited C API for Extending Opaque Types","excerpt":"Python Enhancement Proposal 697: 'Limited C API for Extending Opaque Types'에 대한 한국어 번역입니다.","date":"2025-09-27 13:04:45+0900","permalink":"/python/pep/697","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 697 Limited C API for Extending Opaque Types 상태: Final | 유형: Standards Track | 작성일: 23Aug2022 PEP 697 – 불투명 타입 확장을 위한 Limited C API 이 문서는 Python Enhancement Proposal (PEP) 697의 내용을 한국어 사용자가"},{"id":"2025-09-27-pep-0698-override-decorator-for-static-typing","title":"[Final] PEP 698 - Override Decorator for Static Typing","excerpt":"Python Enhancement Proposal 698: 'Override Decorator for Static Typing'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:19+0900","permalink":"/python/pep/698","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 698 Override Decorator for Static Typing 상태: Final | 유형: Standards Track | 작성일: 05Sep2022 PEP 698 – 정적 타이핑을 위한 Override 데코레이터 초록 (Abstract) 이 PEP는 Python 타입 시스템에 데코레이터를 추가할 것을 제안합니다. 이 데코레이"},{"id":"2025-09-27-pep-0699-remove-private-dict-version-field-added-in-pep-509","title":"[Accepted] PEP 699 - Remove private dict version field added in PEP 509","excerpt":"Python Enhancement Proposal 699: 'Remove private dict version field added in PEP 509'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:33+0900","permalink":"/python/pep/699","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 699 Remove private dict version field added in PEP 509 상태: Accepted | 유형: Standards Track | 작성일: 03Oct2022 PEP 699: PEP 509에서 추가된 비공개 딕셔너리 버전 필드 제거 작성자: Ken Jin 상태: Accepted (수락됨) 유형: Stand"},{"id":"2025-09-27-pep-0700-additional-fields-for-the-simple-api-for-package-indexes","title":"[Final] PEP 700 - Additional Fields for the Simple API for Package Indexes","excerpt":"Python Enhancement Proposal 700: 'Additional Fields for the Simple API for Package Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:05:58+0900","permalink":"/python/pep/700","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 700 Additional Fields for the Simple API for Package Indexes 상태: Final | 유형: Standards Track | 작성일: 21Oct2022 PEP 700: Simple Package Index를 위한 추가 필드 개요 PEP 700은 Python Package Index (PyPI)"},{"id":"2025-09-27-pep-0701-syntactic-formalization-of-f-strings","title":"[Accepted] PEP 701 - Syntactic formalization of f-strings","excerpt":"Python Enhancement Proposal 701: 'Syntactic formalization of f-strings'에 대한 한국어 번역입니다.","date":"2025-09-27 13:06:50+0900","permalink":"/python/pep/701","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 701 Syntactic formalization of fstrings 상태: Accepted | 유형: Standards Track | 작성일: 15Nov2022 PEP 701 – fstring의 문법적 형식화 (Syntactic formalization of fstrings) 저자: Pablo Galindo, Batuhan Taska"},{"id":"2025-09-27-pep-0702-marking-deprecations-using-the-type-system","title":"[Final] PEP 702 - Marking deprecations using the type system","excerpt":"Python Enhancement Proposal 702: 'Marking deprecations using the type system'에 대한 한국어 번역입니다.","date":"2025-09-27 13:07:08+0900","permalink":"/python/pep/702","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 702 Marking deprecations using the type system 상태: Final | 유형: Standards Track | 작성일: 30Dec2022 PEP 702 – 타입 시스템을 이용한 Deprecation 표시 개요 이 PEP (Python Enhancement Proposal) 702는 데코레이터를 추가하여 "},{"id":"2025-09-27-pep-0703-making-the-global-interpreter-lock-optional-in-cpython","title":"[Accepted] PEP 703 - Making the Global Interpreter Lock Optional in CPython","excerpt":"Python Enhancement Proposal 703: 'Making the Global Interpreter Lock Optional in CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:02+0900","permalink":"/python/pep/703","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 703 Making the Global Interpreter Lock Optional in CPython 상태: Accepted | 유형: Standards Track | 작성일: 09Jan2023 PEP 703: CPython에서 전역 인터프리터 락(GIL) 선택적 사용 가능하게 하기 초록 (Abstract) CPython의 전역 인터"},{"id":"2025-09-27-pep-0704-require-virtual-environments-by-default-for-package-installers","title":"[Withdrawn] PEP 704 - Require virtual environments by default for package installers","excerpt":"Python Enhancement Proposal 704: 'Require virtual environments by default for package installers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:22+0900","permalink":"/python/pep/704","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 704 Require virtual environments by default for package installers 상태: Withdrawn | 유형: Standards Track | 작성일: 16Jan2023 PEP 704: 패키지 설치 관리자를 위한 가상 환경 기본 요구 사항 (Require virtual environments "},{"id":"2025-09-27-pep-0705-typeddict-read-only-items","title":"[Final] PEP 705 - TypedDict: Read-only items","excerpt":"Python Enhancement Proposal 705: 'TypedDict: Read-only items'에 대한 한국어 번역입니다.","date":"2025-09-27 13:08:52+0900","permalink":"/python/pep/705","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 705 TypedDict: Readonly items 상태: Final | 유형: Standards Track | 작성일: 07Nov2022 PEP 705 – TypedDict: 읽기 전용 항목 (Readonly items) 초록 (Abstract) PEP 589는 고정된 키 집합을 가진 딕셔너리를 위한 구조적 타입인 를 정의합니다. 그"},{"id":"2025-09-27-pep-0706-filter-for-tarfile-extractall","title":"[Final] PEP 706 - Filter for tarfile.extractall","excerpt":"Python Enhancement Proposal 706: 'Filter for tarfile.extractall'에 대한 한국어 번역입니다.","date":"2025-09-27 13:09:29+0900","permalink":"/python/pep/706","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 706 Filter for tarfile.extractall 상태: Final | 유형: Standards Track | 작성일: 09Feb2023 PEP 706 Filter for tarfile.extractall Abstract: 모듈의 압축 해제 메서드에 인자가 추가됩니다. 이 인자는 아카이브를 추출할 때 파일을 거부하거나 메타데이"},{"id":"2025-09-27-pep-0707-a-simplified-signature-for-exit-and-aexit","title":"[Rejected] PEP 707 - A simplified signature for __exit__ and __aexit__","excerpt":"Python Enhancement Proposal 707: 'A simplified signature for __exit__ and __aexit__'에 대한 한국어 번역입니다.","date":"2025-09-27 13:09:56+0900","permalink":"/python/pep/707","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 707 A simplified signature for exit and aexit 상태: Rejected | 유형: Standards Track | 작성일: 18Feb2023 PEP 707: 및 메서드의 간소화된 시그니처 (거부됨) 거부 공지 이 PEP는 Python Steering Council(SC)에 의해 거부되었습니다 . SC는 "},{"id":"2025-09-27-pep-0708-extending-the-repository-api-to-mitigate-dependency-confusion-attacks","title":"[Provisional] PEP 708 - Extending the Repository API to Mitigate Dependency Confusion Attacks","excerpt":"Python Enhancement Proposal 708: 'Extending the Repository API to Mitigate Dependency Confusion Attacks'에 대한 한국어 번역입니다.","date":"2025-09-27 13:10:38+0900","permalink":"/python/pep/708","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 708 Extending the Repository API to Mitigate Dependency Confusion Attacks 상태: Provisional | 유형: Standards Track | 작성일: 20Feb2023 PEP 708: 의존성 혼란 공격 완화를 위한 저장소 API 확장 번역 및 요약 요약 (Abstract) 의"},{"id":"2025-09-27-pep-0709-inlined-comprehensions","title":"[Final] PEP 709 - Inlined comprehensions","excerpt":"Python Enhancement Proposal 709: 'Inlined comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:10:56+0900","permalink":"/python/pep/709","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 709 Inlined comprehensions 상태: Final | 유형: Standards Track | 작성일: 24Feb2023 PEP 709 – 인라인 컴프리헨션 (Inlined Comprehensions) 개요 (Abstract) 현재 Python의 리스트, 딕셔너리, 셋 컴프리헨션(comprehensions)은 내부적으로 중"},{"id":"2025-09-27-pep-0710-recording-the-provenance-of-installed-packages","title":"[Draft] PEP 710 - Recording the provenance of installed packages","excerpt":"Python Enhancement Proposal 710: 'Recording the provenance of installed packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:11:44+0900","permalink":"/python/pep/710","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 710 Recording the provenance of installed packages 상태: Draft | 유형: Standards Track | 작성일: 27Mar2023 PEP 710 – 설치된 패키지의 출처 기록 (Recording the provenance of installed packages) 초록 (Abstract) 이"},{"id":"2025-09-27-pep-0711-pybi-a-standard-format-for-distributing-python-binaries","title":"[Draft] PEP 711 - PyBI: a standard format for distributing Python Binaries","excerpt":"Python Enhancement Proposal 711: 'PyBI: a standard format for distributing Python Binaries'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:07+0900","permalink":"/python/pep/711","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 711 PyBI: a standard format for distributing Python Binaries 상태: Draft | 유형: Standards Track | 작성일: 06Apr2023 초록 (Abstract) 이 PEP는 Python 패키지를 위한 형식과 유사하게, 미리 빌드된 (prebuilt) Python 인터프리터를 배"},{"id":"2025-09-27-pep-0712-adding-a-converter-parameter-to-dataclasses-field","title":"[Rejected] PEP 712 - Adding a “converter” parameter to dataclasses.field","excerpt":"Python Enhancement Proposal 712: 'Adding a “converter” parameter to dataclasses.field'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:37+0900","permalink":"/python/pep/712","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 712 Adding a “converter” parameter to dataclasses.field 상태: Rejected | 유형: Standards Track | 작성일: 01Jan2023 PEP 712 – 에 \"converter\" 매개변수 추가 (거부됨) 개요 이 문서는 Python Enhancement Proposal (PEP) "},{"id":"2025-09-27-pep-0713-callable-modules","title":"[Rejected] PEP 713 - Callable Modules","excerpt":"Python Enhancement Proposal 713: 'Callable Modules'에 대한 한국어 번역입니다.","date":"2025-09-27 13:12:51+0900","permalink":"/python/pep/713","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 713 Callable Modules 상태: Rejected | 유형: Standards Track | 작성일: 20Apr2023 PEP 713 – 호출 가능한 모듈 (Callable Modules) 상태: 거부됨 (Rejected) 작성자: Amethyst Reese 후원자: Łukasz Langa 생성일: 2023년 4월 20일 Py"},{"id":"2025-09-27-pep-0714-rename-dist-info-metadata-in-the-simple-api","title":"[Accepted] PEP 714 - Rename dist-info-metadata in the Simple API","excerpt":"Python Enhancement Proposal 714: 'Rename dist-info-metadata in the Simple API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:16+0900","permalink":"/python/pep/714","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 714 Rename distinfometadata in the Simple API 상태: Accepted | 유형: Standards Track | 작성일: 06Jun2023 PEP 714 – Simple API에서 이름 변경 작성자: Donald Stufft PEP 담당자: Paul Moore 상태: Accepted (수락됨) 유형: "},{"id":"2025-09-27-pep-0715-disabling-bdist-egg-distribution-uploads-on-pypi","title":"[Final] PEP 715 - Disabling bdist_egg distribution uploads on PyPI","excerpt":"Python Enhancement Proposal 715: 'Disabling bdist_egg distribution uploads on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:33+0900","permalink":"/python/pep/715","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 715 Disabling bdistegg distribution uploads on PyPI 상태: Final | 유형: Standards Track | 작성일: 06Jun2023 PEP 715 – PyPI에서 배포 업로드 비활성화 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 PyPI에서 배포"},{"id":"2025-09-27-pep-0718-subscriptable-functions","title":"[Draft] PEP 718 - Subscriptable functions","excerpt":"Python Enhancement Proposal 718: 'Subscriptable functions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:13:52+0900","permalink":"/python/pep/718","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 718 Subscriptable functions 상태: Draft | 유형: Standards Track | 작성일: 23Jun2023 PEP 718 – Subscriptable functions (서브스크립트 가능한 함수) 개요 (Abstract) 이 PEP는 타이핑(typing) 목적으로 함수 객체(function objects)를"},{"id":"2025-09-27-pep-0719-python-3-13-release-schedule","title":"[Active] PEP 719 - Python 3.13 Release Schedule","excerpt":"Python Enhancement Proposal 719: 'Python 3.13 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 13:14:04+0900","permalink":"/python/pep/719","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 719 Python 3.13 Release Schedule 상태: Active | 유형: Informational | 작성일: 26May2023 PEP 719 – Python 3.13 릴리스 스케줄 작성자: Thomas Wouters 상태: Active (진행 중) 유형: Informational (정보성) 주제: 릴리스 생성일: 202"},{"id":"2025-09-27-pep-0720-cross-compiling-python-packages","title":"[Draft] PEP 720 - Cross-compiling Python packages","excerpt":"Python Enhancement Proposal 720: 'Cross-compiling Python packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:14:54+0900","permalink":"/python/pep/720","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 720 Crosscompiling Python packages 상태: Draft | 유형: Informational | 작성일: 01Jul2023 PEP 720 – Python 패키지 교차 컴파일 작성자: Filipe Laíns <lains at python.org PEP 담당자: 상태: 초안 (Draft) 유형: 정보성 (Informa"},{"id":"2025-09-27-pep-0721-using-tarfile-data-filter-for-source-distribution-extraction","title":"[Final] PEP 721 - Using tarfile.data_filter for source distribution extraction","excerpt":"Python Enhancement Proposal 721: 'Using tarfile.data_filter for source distribution extraction'에 대한 한국어 번역입니다.","date":"2025-09-27 13:15:15+0900","permalink":"/python/pep/721","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 721 Using tarfile.datafilter for source distribution extraction 상태: Final | 유형: Standards Track | 작성일: 12Jul2023 PEP 721: 소스 배포 추출을 위한 사용 개요 (Abstract) Python Source Distribution (sdist) 아카"},{"id":"2025-09-27-pep-0722-dependency-specification-for-single-file-scripts","title":"[Rejected] PEP 722 - Dependency specification for single-file scripts","excerpt":"Python Enhancement Proposal 722: 'Dependency specification for single-file scripts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:16:11+0900","permalink":"/python/pep/722","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 722 Dependency specification for singlefile scripts 상태: Rejected | 유형: Standards Track | 작성일: 19Jul2023 PEP 722 – 단일 파일 스크립트의 의존성(Dependency) 명세 (Rejected) 작성자: Paul Moore PEP 위임자: Brett Ca"},{"id":"2025-09-27-pep-0723-inline-script-metadata","title":"[Final] PEP 723 - Inline script metadata","excerpt":"Python Enhancement Proposal 723: 'Inline script metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:17:06+0900","permalink":"/python/pep/723","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 723 Inline script metadata 상태: Final | 유형: Standards Track | 작성일: 04Aug2023 PEP 723은 단일 파일 Python 스크립트에 메타데이터를 포함하는 표준 형식을 정의하여 런처(launcher), IDE 및 기타 외부 도구와 같은 도구가 스크립트와 상호 작용하는 데 도움을 주기 위"},{"id":"2025-09-27-pep-0724-stricter-type-guards","title":"[Withdrawn] PEP 724 - Stricter Type Guards","excerpt":"Python Enhancement Proposal 724: 'Stricter Type Guards'에 대한 한국어 번역입니다.","date":"2025-09-27 13:17:29+0900","permalink":"/python/pep/724","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 724 Stricter Type Guards 상태: Withdrawn | 유형: Standards Track | 작성일: 28Jul2023 PEP 724 – Stricter Type Guards (더 엄격한 Type Guard) 상태: 이 PEP는 철회되었습니다 (Withdrawn) . Typing Council이 이 제안에 대해 합의에"},{"id":"2025-09-27-pep-0725-specifying-external-dependencies-in-pyproject-toml","title":"[Draft] PEP 725 - Specifying external dependencies in pyproject.toml","excerpt":"Python Enhancement Proposal 725: 'Specifying external dependencies in pyproject.toml'에 대한 한국어 번역입니다.","date":"2025-09-27 13:18:53+0900","permalink":"/python/pep/725","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 725 Specifying external dependencies in pyproject.toml 상태: Draft | 유형: Standards Track | 작성일: 17Aug2023 Citations: PEP 725 – 에 외부 의존성 지정 초록 (Abstract) 이 PEP는 파이썬 프로젝트의 외부 또는 PyPI가 아닌 빌드 및 런"},{"id":"2025-09-27-pep-0726-module-setattr-and-delattr","title":"[Rejected] PEP 726 - Module__setattr__and__delattr__","excerpt":"Python Enhancement Proposal 726: 'Module__setattr__and__delattr__'에 대한 한국어 번역입니다.","date":"2025-09-27 13:19:13+0900","permalink":"/python/pep/726","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 726 Modulesetattranddelattr 상태: Rejected | 유형: Standards Track | 작성일: 24Aug2023 PEP 726 – 모듈 및 초록 (Abstract) 이 PEP는 모듈에 사용자 정의 및 메서드를 지원하여 PEP 562를 넘어선 모듈 속성 접근 사용자 정의를 확장할 것을 제안합니다. 동기 (Mo"},{"id":"2025-09-27-pep-0727-documentation-in-annotated-metadata","title":"[Withdrawn] PEP 727 - Documentation in Annotated Metadata","excerpt":"Python Enhancement Proposal 727: 'Documentation in Annotated Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:19:38+0900","permalink":"/python/pep/727","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 727 Documentation in Annotated Metadata 상태: Withdrawn | 유형: Standards Track | 작성일: 28Aug2023 PEP 727은 메타데이터에 문서화를 추가하는 방법을 표준화하려는 제안이었으나, 현재는 철회된(Withdrawn) 상태입니다. 이 문서는 를 사용하여 파이썬 심볼에 대한 문"},{"id":"2025-09-27-pep-0728-typeddict-with-typed-extra-items","title":"[Accepted] PEP 728 - TypedDict with Typed Extra Items","excerpt":"Python Enhancement Proposal 728: 'TypedDict with Typed Extra Items'에 대한 한국어 번역입니다.","date":"2025-09-27 13:20:42+0900","permalink":"/python/pep/728","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 728 TypedDict with Typed Extra Items 상태: Accepted | 유형: Standards Track | 작성일: 12Sep2023 PEP 728 – TypedDict에 타입이 지정된 추가 항목 개요 이 PEP는 에 추가 항목(extra items)의 타입을 지정하기 위한 두 가지 클래스 매개변수, 와 를 추가"},{"id":"2025-09-27-pep-0729-typing-governance-process","title":"[Active] PEP 729 - Typing governance process","excerpt":"Python Enhancement Proposal 729: 'Typing governance process'에 대한 한국어 번역입니다.","date":"2025-09-27 13:21:49+0900","permalink":"/python/pep/729","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 729 Typing governance process 상태: Active | 유형: Process | 작성일: 19Sep2023 PEP 729 – 타입 거버넌스 프로세스 초록 (Abstract) 이 PEP는 Python 타입 시스템을 관리하고 발전시키기 위한 새로운 방식, 즉 \"타이핑 위원회 (Typing Council)\"의 설립을 제안"},{"id":"2025-09-27-pep-0730-adding-ios-as-a-supported-platform","title":"[Final] PEP 730 - Adding iOS as a supported platform","excerpt":"Python Enhancement Proposal 730: 'Adding iOS as a supported platform'에 대한 한국어 번역입니다.","date":"2025-09-27 13:22:30+0900","permalink":"/python/pep/730","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 730 Adding iOS as a supported platform 상태: Final | 유형: Standards Track | 작성일: 09Oct2023 PEP 730은 CPython에 iOS를 지원 플랫폼으로 추가하는 내용을 제안합니다. 초기 목표는 Python 3.13에서 Tier 3 지원을 달성하는 것이며, 이 PEP는 iOS "},{"id":"2025-09-27-pep-0731-c-api-working-group-charter","title":"[Active] PEP 731 - C API Working Group Charter","excerpt":"Python Enhancement Proposal 731: 'C API Working Group Charter'에 대한 한국어 번역입니다.","date":"2025-09-27 13:22:44+0900","permalink":"/python/pep/731","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 731 C API Working Group Charter 상태: Active | 유형: Process | 작성일: 11Oct2023 PEP 731은 Python C API 워킹 그룹(Working Group)의 설립을 제안하는 문서로, 이 워킹 그룹은 Python C API의 개발 및 유지보수를 감독하고 조율하는 역할을 담당합니다. 개요"},{"id":"2025-09-27-pep-0732-the-python-documentation-editorial-board","title":"[Active] PEP 732 - The Python Documentation Editorial Board","excerpt":"Python Enhancement Proposal 732: 'The Python Documentation Editorial Board'에 대한 한국어 번역입니다.","date":"2025-09-27 13:23:00+0900","permalink":"/python/pep/732","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 732 The Python Documentation Editorial Board 상태: Active | 유형: Process | 작성일: 14Oct2023 PEP 732 – Python 문서 편집 위원회 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 Python 문서 편집 위원회(Python D"},{"id":"2025-09-27-pep-0733-an-evaluation-of-pythons-public-c-api","title":"[Final] PEP 733 - An Evaluation of Python’s Public C API","excerpt":"Python Enhancement Proposal 733: 'An Evaluation of Python’s Public C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:23:51+0900","permalink":"/python/pep/733","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 733 An Evaluation of Python’s Public C API 상태: Final | 유형: Informational | 작성일: 16Oct2023 PEP 733 – Python Public C API 평가 작성자: Erlend Egeberg Aasland 외 25명 상태: 최종 (Final) 유형: 정보 (Informati"},{"id":"2025-09-27-pep-0734-multiple-interpreters-in-the-stdlib","title":"[Final] PEP 734 - Multiple Interpreters in the Stdlib","excerpt":"Python Enhancement Proposal 734: 'Multiple Interpreters in the Stdlib'에 대한 한국어 번역입니다.","date":"2025-09-27 13:25:59+0900","permalink":"/python/pep/734","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 734 Multiple Interpreters in the Stdlib 상태: Final | 유형: Standards Track | 작성일: 06Nov2023 PEP 734 – Stdlib 내 다중 인터프리터 (Multiple Interpreters in the Stdlib) 이 문서는 역사적인 기록입니다. 최신 공식 문서는 에서 찾을 "},{"id":"2025-09-27-pep-0735-dependency-groups-in-pyproject-toml","title":"[Final] PEP 735 - Dependency Groups in pyproject.toml","excerpt":"Python Enhancement Proposal 735: 'Dependency Groups in pyproject.toml'에 대한 한국어 번역입니다.","date":"2025-09-27 13:26:47+0900","permalink":"/python/pep/735","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 735 Dependency Groups in pyproject.toml 상태: Final | 유형: Standards Track | 작성일: 20Nov2023 PEP 735 – 내 의존성 그룹 (Dependency Groups) 상태: Final (최종) 유형: Standards Track (표준 추적) 주제: Packaging (패키징"},{"id":"2025-09-27-pep-0736-shorthand-syntax-for-keyword-arguments-at-invocation","title":"[Rejected] PEP 736 - Shorthand syntax for keyword arguments at invocation","excerpt":"Python Enhancement Proposal 736: 'Shorthand syntax for keyword arguments at invocation'에 대한 한국어 번역입니다.","date":"2025-09-27 13:27:17+0900","permalink":"/python/pep/736","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 736 Shorthand syntax for keyword arguments at invocation 상태: Rejected | 유형: Standards Track | 작성일: 28Nov2023 PEP 736 – 함수 호출 시 키워드 인수에 대한 축약 문법 제안 (Shorthand syntax for keyword arguments at"},{"id":"2025-09-27-pep-0737-c-api-to-format-a-type-fully-qualified-name","title":"[Final] PEP 737 - C API to format a type fully qualified name","excerpt":"Python Enhancement Proposal 737: 'C API to format a type fully qualified name'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:04+0900","permalink":"/python/pep/737","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 737 C API to format a type fully qualified name 상태: Final | 유형: Standards Track | 작성일: 29Nov2023 PEP 737 – 타입의 Fully Qualified Name 포맷을 위한 C API 개요 이 문서는 PEP 737, \"C API to format a type fu"},{"id":"2025-09-27-pep-0738-adding-android-as-a-supported-platform","title":"[Final] PEP 738 - Adding Android as a supported platform","excerpt":"Python Enhancement Proposal 738: 'Adding Android as a supported platform'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:23+0900","permalink":"/python/pep/738","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 738 Adding Android as a supported platform 상태: Final | 유형: Standards Track | 작성일: 12Dec2023 PEP 738 – Android를 지원 플랫폼으로 추가 개요 (Abstract) 이 PEP는 CPython에 Android를 지원 플랫폼으로 추가할 것을 제안합니다. 초기 목"},{"id":"2025-09-27-pep-0739-build-details-json1-0-a-static-description-file-for-python-build-details","title":"[Accepted] PEP 739 - build-details.json1.0 — a static description file for Python build details","excerpt":"Python Enhancement Proposal 739: 'build-details.json1.0 — a static description file for Python build details'에 대한 한국어 번역입니다.","date":"2025-09-27 13:28:50+0900","permalink":"/python/pep/739","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 739 builddetails.json1.0 — a static description file for Python build details 상태: Accepted | 유형: Standards Track | 작성일: 19Dec2023 PEP 739 – 1.0 — Python 빌드 세부 정보를 위한 정적 설명 파일 개요 (Abstract) "},{"id":"2025-09-27-pep-0740-index-support-for-digital-attestations","title":"[Final] PEP 740 - Index support for digital attestations","excerpt":"Python Enhancement Proposal 740: 'Index support for digital attestations'에 대한 한국어 번역입니다.","date":"2025-09-27 13:31:46+0900","permalink":"/python/pep/740","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 740 Index support for digital attestations 상태: Final | 유형: Standards Track | 작성일: 08Jan2024 PEP 740: 디지털 증명(Digital Attestations)을 위한 인덱스 지원 개요 (Abstract) 이 PEP(Python Enhancement Proposal)"},{"id":"2025-09-27-pep-0741-python-configuration-c-api","title":"[Final] PEP 741 - Python Configuration C API","excerpt":"Python Enhancement Proposal 741: 'Python Configuration C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:32:38+0900","permalink":"/python/pep/741","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 741 Python Configuration C API 상태: Final | 유형: Standards Track | 작성일: 18Jan2024 PEP 741 – Python Configuration C API 번역 및 정리 개요 이 문서는 Python Enhancement Proposal (PEP) 741의 내용을 한국어 사용자가 이해하"},{"id":"2025-09-27-pep-0742-narrowing-types-with-typeis","title":"[Final] PEP 742 - Narrowing types with TypeIs","excerpt":"Python Enhancement Proposal 742: 'Narrowing types with TypeIs'에 대한 한국어 번역입니다.","date":"2025-09-27 13:33:17+0900","permalink":"/python/pep/742","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 742 Narrowing types with TypeIs 상태: Final | 유형: Standards Track | 작성일: 07Feb2024 PEP 742 – 를 사용한 타입 내로잉 저자: Jelle Zijlstra 상태: Final 타입: Standards Track Typing 생성일: 2024년 2월 7일 Python 버전: 3"},{"id":"2025-09-27-pep-0743-add-py-compat-api-version-to-the-python-c-api","title":"[Draft] PEP 743 - Add Py_COMPAT_API_VERSION to the Python C API","excerpt":"Python Enhancement Proposal 743: 'Add Py_COMPAT_API_VERSION to the Python C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:33:51+0900","permalink":"/python/pep/743","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 743 Add PyCOMPATAPIVERSION to the Python C API 상태: Draft | 유형: Standards Track | 작성일: 11Mar2024 PEP 743: Python C API에 PyCOMPATAPIVERSION 추가 초록 (Abstract) 이 PEP는 오래되었거나 (softdeprecated) 사용이"},{"id":"2025-09-27-pep-0744-jit-compilation","title":"[Draft] PEP 744 - JIT Compilation","excerpt":"Python Enhancement Proposal 744: 'JIT Compilation'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:20+0900","permalink":"/python/pep/744","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 744 JIT Compilation 상태: Draft | 유형: Informational | 작성일: 11Apr2024 PEP 744: JIT Compilation 요약 (Abstract) 최근 CPython의 메인 개발 브랜치에 실험적인 JIT(JustInTime) 컴파일러가 병합되었습니다. 이 PEP는 JIT 컴파일러의 도입 배경, "},{"id":"2025-09-27-pep-0745-python-3-14-release-schedule","title":"[Active] PEP 745 - Python 3.14 Release Schedule","excerpt":"Python Enhancement Proposal 745: 'Python 3.14 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:29+0900","permalink":"/python/pep/745","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 745 Python 3.14 Release Schedule 상태: Active | 유형: Informational | 작성일: 24Apr2024 PEP 745는 Python 3.14의 릴리스 스케줄을 다루는 정보성 문서입니다. 이 문서는 Python 3.14의 개발 및 릴리스 일정에 대해 상세히 설명하며, 릴리스 관리자 및 관련 팀원 정"},{"id":"2025-09-27-pep-0746-type-checking-annotated-metadata","title":"[Draft] PEP 746 - Type checking Annotated metadata","excerpt":"Python Enhancement Proposal 746: 'Type checking Annotated metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:34:43+0900","permalink":"/python/pep/746","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 746 Type checking Annotated metadata 상태: Draft | 유형: Standards Track | 작성일: 20May2024 PEP 746 – Annotated 메타데이터의 타입 체킹 요약 이 PEP는 타입을 사용하는 메타데이터에 대한 타입 체킹 메커니즘을 제안합니다. 새로운 프로토콜을 구현하는 메타데이터 객"},{"id":"2025-09-27-pep-0747-annotating-type-forms","title":"[Draft] PEP 747 - Annotating Type Forms","excerpt":"Python Enhancement Proposal 747: 'Annotating Type Forms'에 대한 한국어 번역입니다.","date":"2025-09-27 13:35:04+0900","permalink":"/python/pep/747","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 747 Annotating Type Forms 상태: Draft | 유형: Standards Track | 작성일: 27May2024 PEP 747 – 타입 폼 어노테이션 (Annotating Type Forms) 요약 (Abstract) PEP 747은 Python 타입 시스템에서 타입을 명시하는 표준화된 방법인 \"타입 표현식(type"},{"id":"2025-09-27-pep-0748-a-unified-tls-api-for-python","title":"[Draft] PEP 748 - A Unified TLS API for Python","excerpt":"Python Enhancement Proposal 748: 'A Unified TLS API for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 13:35:46+0900","permalink":"/python/pep/748","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 748 A Unified TLS API for Python 상태: Draft | 유형: Standards Track | 작성일: 27Jun2024 PEP 748 – Python을 위한 통합 TLS API 개요 (Abstract) 이 PEP는 프로토콜 클래스(protocol classes) 모음의 형태로 표준 TLS 인터페이스를 정의합니다"},{"id":"2025-09-27-pep-0749-implementing-pep-649","title":"[Accepted] PEP 749 - Implementing PEP 649","excerpt":"Python Enhancement Proposal 749: 'Implementing PEP 649'에 대한 한국어 번역입니다.","date":"2025-09-27 13:37:09+0900","permalink":"/python/pep/749","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 749 Implementing PEP 649 상태: Accepted | 유형: Standards Track | 작성일: 28May2024 PEP 749 – PEP 649 구현 이 문서는 PEP 649를 보완하는 Python Enhancement Proposal (PEP)입니다. PEP 649의 사양에 대한 다양한 조정 및 추가 사항을 제"},{"id":"2025-09-27-pep-0750-template-strings","title":"[Final] PEP 750 - Template Strings","excerpt":"Python Enhancement Proposal 750: 'Template Strings'에 대한 한국어 번역입니다.","date":"2025-09-27 13:38:50+0900","permalink":"/python/pep/750","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 750 Template Strings 상태: Final | 유형: Standards Track | 작성일: 08Jul2024 PEP 750 – Template Strings 작성자: Jim Baker 외 다수 논의처: Discourse thread 상태: Final 유형: Standards Track 생성일: 2024년 7월 8일 Pyt"},{"id":"2025-09-27-pep-0751-a-file-format-to-record-python-dependencies-for-installation-reproducibility","title":"[Final] PEP 751 - A file format to record Python dependencies for installation reproducibility","excerpt":"Python Enhancement Proposal 751: 'A file format to record Python dependencies for installation reproducibility'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:01+0900","permalink":"/python/pep/751","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 751 A file format to record Python dependencies for installation reproducibility 상태: Final | 유형: Standards Track | 작성일: 24Jul2024 PEP 751 – 설치 재현성을 위한 Python 종속성 기록 파일 형식 요약 (Abstract) 이 PE"},{"id":"2025-09-27-pep-0752-implicit-namespaces-for-package-repositories","title":"[Draft] PEP 752 - Implicit namespaces for package repositories","excerpt":"Python Enhancement Proposal 752: 'Implicit namespaces for package repositories'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:26+0900","permalink":"/python/pep/752","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 752 Implicit namespaces for package repositories 상태: Draft | 유형: Standards Track | 작성일: 13Aug2024 Python Enhancement Proposal (PEP) 752는 패키지 저장소에 대한 암묵적 네임스페이스(Implicit namespaces)를 도입하는 방안"},{"id":"2025-09-27-pep-0753-uniform-project-urls-in-core-metadata","title":"[Accepted] PEP 753 - Uniform project URLs in core metadata","excerpt":"Python Enhancement Proposal 753: 'Uniform project URLs in core metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 13:40:57+0900","permalink":"/python/pep/753","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 753 Uniform project URLs in core metadata 상태: Accepted | 유형: Standards Track | 작성일: 29Aug2024 PEP 753 – 코어 메타데이터 내 통일된 프로젝트 URL (Uniform project URLs in core metadata) 개요 (Abstract) PEP 753"},{"id":"2025-09-27-pep-0754-ieee-754-floating-point-special-values","title":"[Rejected] PEP 754 - IEEE 754 Floating Point Special Values","excerpt":"Python Enhancement Proposal 754: 'IEEE 754 Floating Point Special Values'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:12+0900","permalink":"/python/pep/754","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 754 IEEE 754 Floating Point Special Values 상태: Rejected | 유형: Standards Track | 작성일: 28Mar2003 PEP 754 – IEEE 754 부동 소수점 특수 값 상태: 거부됨 (Rejected) 유형: 표준 트랙 (Standards Track) 생성일: 2003년 3월 28"},{"id":"2025-09-27-pep-0755-implicit-namespace-policy-for-pypi","title":"[Draft] PEP 755 - Implicit namespace policy for PyPI","excerpt":"Python Enhancement Proposal 755: 'Implicit namespace policy for PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:27+0900","permalink":"/python/pep/755","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 755 Implicit namespace policy for PyPI 상태: Draft | 유형: Process | 작성일: 05Sep2024 PEP 755: PyPI에 대한 암묵적 네임스페이스 정책 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 PyPI에 대한 PEP 752의 구현을 체계화합니"},{"id":"2025-09-27-pep-0756-add-pyunicode-export-and-pyunicode-import-c-functions","title":"[Withdrawn] PEP 756 - Add PyUnicode_Export() and PyUnicode_Import() C functions","excerpt":"Python Enhancement Proposal 756: 'Add PyUnicode_Export() and PyUnicode_Import() C functions'에 대한 한국어 번역입니다.","date":"2025-09-27 13:41:47+0900","permalink":"/python/pep/756","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 756 Add PyUnicodeExport() and PyUnicodeImport() C functions 상태: Withdrawn | 유형: Standards Track | 작성일: 13Sep2024 PEP 756: 및 C 함수 추가 제안 번역 및 정리 개요 PEP 756은 Python 3.14의 Limited C API에 및 C 함수"},{"id":"2025-09-27-pep-0757-c-api-to-import-export-python-integers","title":"[Final] PEP 757 - C API to import-export Python integers","excerpt":"Python Enhancement Proposal 757: 'C API to import-export Python integers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:42:57+0900","permalink":"/python/pep/757","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 757 C API to importexport Python integers 상태: Final | 유형: Standards Track | 작성일: 13Sep2024 PEP 757 – C API를 이용한 Python 정수 import/export 개요 새로운 C API, 특히 및 함수를 추가하여 Python 정수( 객체)를 import하고 "},{"id":"2025-09-27-pep-0758-allowexceptandexceptexpressions-without-parentheses","title":"[Final] PEP 758 - Allowexceptandexcept*expressions without parentheses","excerpt":"Python Enhancement Proposal 758: 'Allowexceptandexcept*expressions without parentheses'에 대한 한국어 번역입니다.","date":"2025-09-27 13:43:13+0900","permalink":"/python/pep/758","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 758 Allowexceptandexceptexpressions without parentheses 상태: Final | 유형: Standards Track | 작성일: 30Sep2024 PEP 758 – 및 표현식에서 괄호 없이 사용 허용 저자: Pablo Galindo, Brett Cannon 상태: Final 타입: Standard"},{"id":"2025-09-27-pep-0759-external-wheel-hosting","title":"[Withdrawn] PEP 759 - External Wheel Hosting","excerpt":"Python Enhancement Proposal 759: 'External Wheel Hosting'에 대한 한국어 번역입니다.","date":"2025-09-27 13:43:53+0900","permalink":"/python/pep/759","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 759 External Wheel Hosting 상태: Withdrawn | 유형: Standards Track | 작성일: 01Oct2024 PEP 759 – External Wheel Hosting (외부 휠 호스팅) 작성자: Barry Warsaw, Emma Harper Smith PEP 위임자: Donald Stufft 논의: D"},{"id":"2025-09-27-pep-0760-no-more-bare-excepts","title":"[Withdrawn] PEP 760 - No More Bare Excepts","excerpt":"Python Enhancement Proposal 760: 'No More Bare Excepts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:07+0900","permalink":"/python/pep/760","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 760 No More Bare Excepts 상태: Withdrawn | 유형: Standards Track | 작성일: 02Oct2024 PEP 760 – Bare Except 금지 제안 번역 및 정리 개요 이 문서는 Python의 예외 처리 구문에서 (bare except) 절을 금지할 것을 제안합니다. 현재 Python은 절을 사용"},{"id":"2025-09-27-pep-0761-deprecating-pgp-signatures-for-cpython-artifacts","title":"[Active] PEP 761 - Deprecating PGP signatures for CPython artifacts","excerpt":"Python Enhancement Proposal 761: 'Deprecating PGP signatures for CPython artifacts'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:29+0900","permalink":"/python/pep/761","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 761 Deprecating PGP signatures for CPython artifacts 상태: Active | 유형: Process | 작성일: 08Oct2024 PEP 761 – CPython 아티팩트에 대한 PGP 서명 지원 중단 제안 이 문서는 CPython 배포 아티팩트(artifacts)에 대한 PGP(Pretty Goo"},{"id":"2025-09-27-pep-0762-repl-acing-the-default-repl","title":"[Final] PEP 762 - REPL-acing the default REPL","excerpt":"Python Enhancement Proposal 762: 'REPL-acing the default REPL'에 대한 한국어 번역입니다.","date":"2025-09-27 13:44:46+0900","permalink":"/python/pep/762","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 762 REPLacing the default REPL 상태: Final | 유형: Informational | 작성일: 11Oct2024 PEP 762 – 기본 REPL 교체 (REPLacing the default REPL) 초록 (Abstract) 이 PEP는 Python 3.13부터 도입될 새로운 REPL (ReadEvalPrin"},{"id":"2025-09-27-pep-0763-limiting-deletions-on-pypi","title":"[Draft] PEP 763 - Limiting deletions on PyPI","excerpt":"Python Enhancement Proposal 763: 'Limiting deletions on PyPI'에 대한 한국어 번역입니다.","date":"2025-09-27 13:45:17+0900","permalink":"/python/pep/763","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 763 Limiting deletions on PyPI 상태: Draft | 유형: Standards Track | 작성일: 24Oct2024 PEP 763 – PyPI에서 삭제 제한 (Limiting deletions on PyPI) 초록 (Abstract) 이 PEP(Python Enhancement Proposal)는 PyPI(Py"},{"id":"2025-09-27-pep-0764-inline-typed-dictionaries","title":"[Draft] PEP 764 - Inline typed dictionaries","excerpt":"Python Enhancement Proposal 764: 'Inline typed dictionaries'에 대한 한국어 번역입니다.","date":"2025-09-27 13:45:41+0900","permalink":"/python/pep/764","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 764 Inline typed dictionaries 상태: Draft | 유형: Standards Track | 작성일: 25Oct2024 PEP 764 – 인라인 TypedDict 제안 (Inline typed dictionaries) 개요 (Abstract) PEP 589는 를 생성하기 위한 클래스 기반 및 함수형(functiona"},{"id":"2025-09-27-pep-0765-disallow-returnbreakcontinue-that-exit-a-finally-block","title":"[Final] PEP 765 - Disallow return/break/continue that exit a finally block","excerpt":"Python Enhancement Proposal 765: 'Disallow return/break/continue that exit a finally block'에 대한 한국어 번역입니다.","date":"2025-09-27 13:46:10+0900","permalink":"/python/pep/765","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 765 Disallow return/break/continue that exit a finally block 상태: Final | 유형: Standards Track | 작성일: 15Nov2024 PEP 765 – 블록을 종료하는 // 금지 제안 저자: Irit Katriel, Alyssa Coghlan 상태: Final 유형: Stan"},{"id":"2025-09-27-pep-0766-explicit-priority-choices-among-multiple-indexes","title":"[Draft] PEP 766 - Explicit Priority Choices Among Multiple Indexes","excerpt":"Python Enhancement Proposal 766: 'Explicit Priority Choices Among Multiple Indexes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:46:56+0900","permalink":"/python/pep/766","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 766 Explicit Priority Choices Among Multiple Indexes 상태: Draft | 유형: Informational | 작성일: 18Nov2024 PEP 766: 여러 인덱스 간 명시적 우선순위 선택 (Explicit Priority Choices Among Multiple Indexes) 요약 (Abst"},{"id":"2025-09-27-pep-0767-annotating-read-only-attributes","title":"[Draft] PEP 767 - Annotating Read-Only Attributes","excerpt":"Python Enhancement Proposal 767: 'Annotating Read-Only Attributes'에 대한 한국어 번역입니다.","date":"2025-09-27 13:49:33+0900","permalink":"/python/pep/767","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 767 Annotating ReadOnly Attributes 상태: Draft | 유형: Standards Track | 작성일: 18Nov2024 PEP 767: 읽기 전용 속성 어노테이션 초록 (Abstract) PEP 705에서 타입 한정자(type qualifier)를 도입하여 항목을 읽기 전용으로 정의할 수 있게 했습니다. 이"},{"id":"2025-09-27-pep-0768-safe-external-debugger-interface-for-cpython","title":"[Accepted] PEP 768 - Safe external debugger interface for CPython","excerpt":"Python Enhancement Proposal 768: 'Safe external debugger interface for CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:50:05+0900","permalink":"/python/pep/768","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 768 Safe external debugger interface for CPython 상태: Accepted | 유형: Standards Track | 작성일: 25Nov2024 PEP 768 – CPython을 위한 안전한 외부 디버거 인터페이스 개요 (Abstract) 이 PEP는 디버거와 프로파일러가 실행 중인 Python 프로세"},{"id":"2025-09-27-pep-0769-add-a-default-keyword-argument-to-attrgetter-itemgetter-and-getitem","title":"[Rejected] PEP 769 - Add a ‘default’ keyword argument to ‘attrgetter’, ‘itemgetter’ and ‘getitem’","excerpt":"Python Enhancement Proposal 769: 'Add a ‘default’ keyword argument to ‘attrgetter’, ‘itemgetter’ and ‘getitem’'에 대한 한국어 번역입니다.","date":"2025-09-27 13:50:33+0900","permalink":"/python/pep/769","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 769 Add a ‘default’ keyword argument to ‘attrgetter’, ‘itemgetter’ and ‘getitem’ 상태: Rejected | 유형: Standards Track | 작성일: 22Dec2024 PEP 769 – , , 에 키워드 인자 추가 제안 작성자: Facundo Batista 토론: Di"},{"id":"2025-09-27-pep-0770-improving-measurability-of-python-packages-with-software-bill-of-materials","title":"[Accepted] PEP 770 - Improving measurability of Python packages with Software Bill-of-Materials","excerpt":"Python Enhancement Proposal 770: 'Improving measurability of Python packages with Software Bill-of-Materials'에 대한 한국어 번역입니다.","date":"2025-09-27 13:51:21+0900","permalink":"/python/pep/770","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 770 Improving measurability of Python packages with Software BillofMaterials 상태: Accepted | 유형: Standards Track | 작성일: 02Jan2025 PEP 770 – 소프트웨어 BOM(BillofMaterials)을 통한 Python 패키지 측정 가능성 개"},{"id":"2025-09-27-pep-0771-default-extras-for-python-software-packages","title":"[Draft] PEP 771 - Default Extras for Python Software Packages","excerpt":"Python Enhancement Proposal 771: 'Default Extras for Python Software Packages'에 대한 한국어 번역입니다.","date":"2025-09-27 13:52:58+0900","permalink":"/python/pep/771","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 771 Default Extras for Python Software Packages 상태: Draft | 유형: Standards Track | 작성일: 13Jan2025 PEP 771 – Python 소프트웨어 패키지를 위한 기본 Extras 초록 (Abstract) PEP 508은 패키지 의존성을 선언하기 위한 미니 언어(minil"},{"id":"2025-09-27-pep-0772-packaging-council-governance-process","title":"[Draft] PEP 772 - Packaging Council governance process","excerpt":"Python Enhancement Proposal 772: 'Packaging Council governance process'에 대한 한국어 번역입니다.","date":"2025-09-27 13:53:40+0900","permalink":"/python/pep/772","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 772 Packaging Council governance process 상태: Draft | 유형: Process | 작성일: 21Jan2025 PEP 772 – Packaging Council 거버넌스 프로세스 초록 (Abstract) 이 PEP(Python Enhancement Proposal)는 Python 패키징 표준, 도구 및"},{"id":"2025-09-27-pep-0773-a-python-installation-manager-for-windows","title":"[Accepted] PEP 773 - A Python Installation Manager for Windows","excerpt":"Python Enhancement Proposal 773: 'A Python Installation Manager for Windows'에 대한 한국어 번역입니다.","date":"2025-09-27 13:54:28+0900","permalink":"/python/pep/773","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 773 A Python Installation Manager for Windows 상태: Accepted | 유형: Standards Track | 작성일: 21Jan2025 PEP 773: Windows용 Python 설치 관리자 제안 초록 (Abstract) Windows에서 파이썬 배포판을 설치하는 것은 복잡합니다. 사용자 경험은 "},{"id":"2025-09-27-pep-0774-removing-the-llvm-requirement-for-jit-builds","title":"[Deferred] PEP 774 - Removing the LLVM requirement for JIT builds","excerpt":"Python Enhancement Proposal 774: 'Removing the LLVM requirement for JIT builds'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:06+0900","permalink":"/python/pep/774","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 774 Removing the LLVM requirement for JIT builds 상태: Deferred | 유형: Standards Track | 작성일: 27Jan2025 PEP 774: JIT 빌드 시 LLVM 요구사항 제거 제안 초록 (Abstract) Python 3.13부터 CPython은 (Linux 및 Mac) 또는 "},{"id":"2025-09-27-pep-0775-make-zlib-required-to-build-cpython","title":"[Withdrawn] PEP 775 - Make zlib required to build CPython","excerpt":"Python Enhancement Proposal 775: 'Make zlib required to build CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:17+0900","permalink":"/python/pep/775","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 775 Make zlib required to build CPython 상태: Withdrawn | 유형: Standards Track | 작성일: 24Feb2025 PEP 775 – CPython 빌드 시 zlib 필수화 (철회됨) 본 문서는 2025년 5월 7일에 작성자에 의해 철회(Withdrawn)되었습니다. 제안의 가치가 불충분"},{"id":"2025-09-27-pep-0776-emscripten-support","title":"[Draft] PEP 776 - Emscripten Support","excerpt":"Python Enhancement Proposal 776: 'Emscripten Support'에 대한 한국어 번역입니다.","date":"2025-09-27 13:55:47+0900","permalink":"/python/pep/776","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 776 Emscripten Support 상태: Draft | 유형: Informational | 작성일: 18Mar2025 PEP 776 – Emscripten 지원 개요 (Abstract) Emscripten은 C/C++ 코드를 WebAssembly/JavaScript 실행 파일로 컴파일하여 브라우저 및 Node.js를 포함한 Jav"},{"id":"2025-09-27-pep-0777-how-to-re-invent-the-wheel","title":"[Draft] PEP 777 - How to Re-invent the Wheel","excerpt":"Python Enhancement Proposal 777: 'How to Re-invent the Wheel'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:09+0900","permalink":"/python/pep/777","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 777 How to Reinvent the Wheel 상태: Draft | 유형: Standards Track | 작성일: 09Oct2024 PEP 777 – Wheel 재발명 방법 (How to Reinvent the Wheel) 번역 및 요약 개요 (Abstract) PEP 777은 10년 이상 사용된 현재의 wheel 1.0 사양에"},{"id":"2025-09-27-pep-0778-supporting-symlinks-in-wheels","title":"[Deferred] PEP 778 - Supporting Symlinks in Wheels","excerpt":"Python Enhancement Proposal 778: 'Supporting Symlinks in Wheels'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:34+0900","permalink":"/python/pep/778","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 778 Supporting Symlinks in Wheels 상태: Deferred | 유형: Standards Track | 작성일: 18May2024 PEP 778 – Wheels에서의 심볼릭 링크 지원 개요 (Abstract) 현재 Wheels는 심볼릭 링크(symlinks)를 제대로 처리하지 못하며, 설치 시 심볼릭 링크 대신 내"},{"id":"2025-09-27-pep-0779-criteria-for-supported-status-for-free-threaded-python","title":"[Accepted] PEP 779 - Criteria for supported status for free-threaded Python","excerpt":"Python Enhancement Proposal 779: 'Criteria for supported status for free-threaded Python'에 대한 한국어 번역입니다.","date":"2025-09-27 13:56:46+0900","permalink":"/python/pep/779","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 779 Criteria for supported status for freethreaded Python 상태: Accepted | 유형: Standards Track | 작성일: 13Mar2025 PEP 779 – 스레드프리 Python의 지원 상태 기준 초록 (Abstract) PEP 779는 PEP 703 (CPython에서 Glob"},{"id":"2025-09-27-pep-0780-abi-features-as-environment-markers","title":"[Draft] PEP 780 - ABI features as environment markers","excerpt":"Python Enhancement Proposal 780: 'ABI features as environment markers'에 대한 한국어 번역입니다.","date":"2025-09-27 13:57:13+0900","permalink":"/python/pep/780","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 780 ABI features as environment markers 상태: Draft | 유형: Standards Track | 작성일: 21Mar2025 PEP 780 – 환경 마커로서의 ABI 기능 개요 (Abstract) 이 PEP는 새로운 환경 마커를 통해 프로젝트 종속성에 ABI(Application Binary Interf"},{"id":"2025-09-27-pep-0781-maketype-checkinga-built-in-constant","title":"[Draft] PEP 781 - MakeTYPE_CHECKINGa built-in constant","excerpt":"Python Enhancement Proposal 781: 'MakeTYPE_CHECKINGa built-in constant'에 대한 한국어 번역입니다.","date":"2025-09-27 13:57:36+0900","permalink":"/python/pep/781","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 781 MakeTYPECHECKINGa builtin constant 상태: Draft | 유형: Standards Track | 작성일: 24Mar2025 PEP 781 – 을 내장(Builtin) 상수로 만들기 개요 이 문서는 Python 3.15에 도입될 예정인 에 대한 번역 및 요약본입니다. 이 PEP는 이라는 새로운 내장 변수를"},{"id":"2025-09-27-pep-0782-add-pybyteswriter-c-api","title":"[Final] PEP 782 - Add PyBytesWriter C API","excerpt":"Python Enhancement Proposal 782: 'Add PyBytesWriter C API'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:01+0900","permalink":"/python/pep/782","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 782 Add PyBytesWriter C API 상태: Final | 유형: Standards Track | 작성일: 27Mar2025 PEP 782 – PyBytesWriter C API 추가 저자: Victor Stinner 상태: Final 생성일: 2025년 3월 27일 Python 버전: 3.15 초록 (Abstract) PE"},{"id":"2025-09-27-pep-0783-emscripten-packaging","title":"[Draft] PEP 783 - Emscripten Packaging","excerpt":"Python Enhancement Proposal 783: 'Emscripten Packaging'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:16+0900","permalink":"/python/pep/783","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 783 Emscripten Packaging 상태: Draft | 유형: Standards Track | 작성일: 28Mar2025 PEP 783 – Emscripten 패키징 초록 (Abstract) 이 PEP는 Pyodide Python 런타임을 위한 바이너리 Python 패키지 배포판을 위한 새로운 플랫폼 태그 시리즈인 를 제안합니"},{"id":"2025-09-27-pep-0784-adding-zstandard-to-the-standard-library","title":"[Final] PEP 784 - Adding Zstandard to the standard library","excerpt":"Python Enhancement Proposal 784: 'Adding Zstandard to the standard library'에 대한 한국어 번역입니다.","date":"2025-09-27 13:58:47+0900","permalink":"/python/pep/784","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 784 Adding Zstandard to the standard library 상태: Final | 유형: Standards Track | 작성일: 06Apr2025 PEP 784 – 표준 라이브러리에 Zstandard 추가 개요 이 문서는 Python Enhancement Proposal (PEP) 784에 대한 한국어 번역 및 정리"},{"id":"2025-09-27-pep-0785-new-methods-for-easier-handling-ofexceptiongroups","title":"[Draft] PEP 785 - New methods for easier handling ofExceptionGroups","excerpt":"Python Enhancement Proposal 785: 'New methods for easier handling ofExceptionGroups'에 대한 한국어 번역입니다.","date":"2025-09-27 13:59:04+0900","permalink":"/python/pep/785","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 785 New methods for easier handling ofExceptionGroups 상태: Draft | 유형: Standards Track | 작성일: 08Apr2025 PEP 785는 을 더 쉽게 다룰 수 있는 새로운 메서드들을 제안합니다. 이 PEP는 Python 3.14 버전을 대상으로 하며, 와 라는 두 가지 새로운"},{"id":"2025-09-27-pep-0787-safer-subprocess-usage-using-t-strings","title":"[Deferred] PEP 787 - Safer subprocess usage using t-strings","excerpt":"Python Enhancement Proposal 787: 'Safer subprocess usage using t-strings'에 대한 한국어 번역입니다.","date":"2025-09-27 14:00:15+0900","permalink":"/python/pep/787","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 787 Safer subprocess usage using tstrings 상태: Deferred | 유형: Standards Track | 작성일: 13Apr2025 PEP 787 – tstring을 사용한 더 안전한 서브프로세스 활용 저자 : Nick Humrich, Alyssa Coghlan 상태 : Deffered (연기됨) Py"},{"id":"2025-09-27-pep-0788-pyinterpreterref-interpreter-references-in-the-c-api","title":"[Draft] PEP 788 - PyInterpreterRef: Interpreter References in the C API","excerpt":"Python Enhancement Proposal 788: 'PyInterpreterRef: Interpreter References in the C API'에 대한 한국어 번역입니다.","date":"2025-09-27 14:03:48+0900","permalink":"/python/pep/788","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 788 PyInterpreterRef: Interpreter References in the C API 상태: Draft | 유형: Standards Track | 작성일: 23Apr2025 PEP 788 – PyInterpreterRef: C API의 인터프리터 참조 (Interpreter References in the C API) "},{"id":"2025-09-27-pep-0789-preventing-task-cancellation-bugs-by-limiting-yield-in-async-generators","title":"[Draft] PEP 789 - Preventing task-cancellation bugs by limiting yield in async generators","excerpt":"Python Enhancement Proposal 789: 'Preventing task-cancellation bugs by limiting yield in async generators'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:23+0900","permalink":"/python/pep/789","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 789 Preventing taskcancellation bugs by limiting yield in async generators 상태: Draft | 유형: Standards Track | 작성일: 14May2024 PEP 789은 제너레이터에서 사용을 제한하여 태스크 취소 관련 버그를 방지하는 것을 제안합니다. 이 PEP의 목표는"},{"id":"2025-09-27-pep-0790-python-3-15-release-schedule","title":"[Active] PEP 790 - Python 3.15 Release Schedule","excerpt":"Python Enhancement Proposal 790: 'Python 3.15 Release Schedule'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:34+0900","permalink":"/python/pep/790","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 790 Python 3.15 Release Schedule 상태: Active | 유형: Informational | 작성일: 26Apr2025 PEP 790 – Python 3.15 릴리스 일정 작성자: Hugo van Kemenade 상태: Active (활성) 유형: Informational (정보성) 주제: Release (릴리스"},{"id":"2025-09-27-pep-0791-intmath-module-for-integer-specific-mathematics-functions","title":"[Draft] PEP 791 - intmath — module for integer-specific mathematics functions","excerpt":"Python Enhancement Proposal 791: 'intmath — module for integer-specific mathematics functions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:04:51+0900","permalink":"/python/pep/791","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 791 intmath — module for integerspecific mathematics functions 상태: Draft | 유형: Standards Track | 작성일: 12May2025 PEP 791: — 정수 전용 수학 함수 모듈 개요 (Abstract) 이 PEP는 또는 와 같이 정수 인수를 위해 정의된 수론, 조합론 "},{"id":"2025-09-27-pep-0792-project-status-markers-in-the-simple-index","title":"[Final] PEP 792 - Project status markers in the simple index","excerpt":"Python Enhancement Proposal 792: 'Project status markers in the simple index'에 대한 한국어 번역입니다.","date":"2025-09-27 14:05:09+0900","permalink":"/python/pep/792","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 792 Project status markers in the simple index 상태: Final | 유형: Standards Track | 작성일: 21May2025 PEP 792 – simple index에 프로젝트 상태 마커 추가 (Project status markers in the simple index) 초록 (Abstra"},{"id":"2025-09-27-pep-0793-pymodexport-a-new-entry-point-for-c-extension-modules","title":"[Draft] PEP 793 - PyModExport: A new entry point for C extension modules","excerpt":"Python Enhancement Proposal 793: 'PyModExport: A new entry point for C extension modules'에 대한 한국어 번역입니다.","date":"2025-09-27 14:07:12+0900","permalink":"/python/pep/793","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 793 PyModExport: A new entry point for C extension modules 상태: Draft | 유형: Standards Track | 작성일: 23May2025 PEP 793: PyModExport C 확장 모듈을 위한 새로운 진입점 작성자: Petr Viktorin 상태: Draft (초안) 생성일: 2"},{"id":"2025-09-27-pep-0794-import-name-metadata","title":"[Accepted] PEP 794 - Import Name Metadata","excerpt":"Python Enhancement Proposal 794: 'Import Name Metadata'에 대한 한국어 번역입니다.","date":"2025-09-27 14:07:32+0900","permalink":"/python/pep/794","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 794 Import Name Metadata 상태: Accepted | 유형: Standards Track | 작성일: 05Jun2025 PEP 794 – Import Name Metadata (가져오기 이름 메타데이터) 개요 (Abstract) 이 PEP는 Python 패키징을 위한 핵심 메타데이터 사양을 확장하여 과 라는 두 개의 새"},{"id":"2025-09-27-pep-0798-unpacking-in-comprehensions","title":"[Draft] PEP 798 - Unpacking in Comprehensions","excerpt":"Python Enhancement Proposal 798: 'Unpacking in Comprehensions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:08:32+0900","permalink":"/python/pep/798","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 798 Unpacking in Comprehensions 상태: Draft | 유형: Standards Track | 작성일: 19Jul2025 PEP 798 – Comprehension 내 언패킹 (Unpacking) 개요 (Abstract) 이 PEP는 , , Comprehension (컴프리헨션) 및 (제너레이터 표현식)을 확장하여"},{"id":"2025-09-27-pep-0799-a-dedicatedprofilingpackage-for-organizing-python-profiling-tools","title":"[Draft] PEP 799 - A dedicatedprofilingpackage for organizing Python profiling tools","excerpt":"Python Enhancement Proposal 799: 'A dedicatedprofilingpackage for organizing Python profiling tools'에 대한 한국어 번역입니다.","date":"2025-09-27 14:08:50+0900","permalink":"/python/pep/799","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 799 A dedicatedprofilingpackage for organizing Python profiling tools 상태: Draft | 유형: Standards Track | 작성일: 21Jul2025 PEP 799 – Python 프로파일링 도구 정리를 위한 전용 프로파일링 패키지 초록 (Abstract) 이 PEP는 Pyt"},{"id":"2025-09-27-pep-0800-disjoint-bases-in-the-type-system","title":"[Draft] PEP 800 - Disjoint bases in the type system","excerpt":"Python Enhancement Proposal 800: 'Disjoint bases in the type system'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:06+0900","permalink":"/python/pep/800","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 800 Disjoint bases in the type system 상태: Draft | 유형: Standards Track | 작성일: 21Jul2025 PEP 800: 타입 시스템 내의 (분리된 기본 클래스) 개요 (Abstract) 정확한 Python 프로그램 분석을 위해, 타입 체커(type checker)는 두 클래스가 공통 자"},{"id":"2025-09-27-pep-0801-reserved","title":"[Active] PEP 801 - Reserved","excerpt":"Python Enhancement Proposal 801: 'Reserved'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:12+0900","permalink":"/python/pep/801","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 801 Reserved 상태: Active | 유형: Informational | 작성일: 21Jun2018 PEP 801 – 예약됨 (Reserved) 번역 및 설명 PEP 801 은 현재 \"예약됨(Reserved)\" 상태의 문서이며, 향후 사용을 위해 지정된 placeholder(자리 표시자)입니다. 따라서 이 PEP는 특정 기술 제"},{"id":"2025-09-27-pep-0802-display-syntax-for-the-empty-set","title":"[Draft] PEP 802 - Display Syntax for the Empty Set","excerpt":"Python Enhancement Proposal 802: 'Display Syntax for the Empty Set'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:30+0900","permalink":"/python/pep/802","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 802 Display Syntax for the Empty Set 상태: Draft | 유형: Standards Track | 작성일: 08Aug2025 PEP 802 – 빈 집합(Empty Set)을 위한 표기법 제안 번역 및 정리 개요 (Abstract) PEP 802는 빈 집합(empty set)을 생성하고 표현하기 위한 새로운 표"},{"id":"2025-09-27-pep-0803-stable-abi-for-free-threaded-builds","title":"[Draft] PEP 803 - Stable ABI for Free-Threaded Builds","excerpt":"Python Enhancement Proposal 803: 'Stable ABI for Free-Threaded Builds'에 대한 한국어 번역입니다.","date":"2025-09-27 14:09:56+0900","permalink":"/python/pep/803","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 803 Stable ABI for FreeThreaded Builds 상태: Draft | 유형: Standards Track | 작성일: 19Aug2025 PEP 803 – 자유 스레드 빌드를 위한 Stable ABI 개요 (Abstract) 이 PEP (Python Enhancement Proposal)는 Python 3.15 버전의"},{"id":"2025-09-27-pep-0804-an-external-dependency-registry-and-name-mapping-mechanism","title":"[Draft] PEP 804 - An external dependency registry and name mapping mechanism","excerpt":"Python Enhancement Proposal 804: 'An external dependency registry and name mapping mechanism'에 대한 한국어 번역입니다.","date":"2025-09-27 14:10:24+0900","permalink":"/python/pep/804","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 804 An external dependency registry and name mapping mechanism 상태: Draft | 유형: Standards Track | 작성일: 03Sep2025 PEP 804: 외부 의존성 레지스트리 및 이름 매핑 메커니즘 초록 (Abstract) 는 패키징 도구들이 에서 소개된 외부 의존성 식별자"},{"id":"2025-09-27-pep-0806-mixed-syncasync-context-managers-with-precise-async-marking","title":"[Draft] PEP 806 - Mixed sync/async context managers with precise async marking","excerpt":"Python Enhancement Proposal 806: 'Mixed sync/async context managers with precise async marking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:10:48+0900","permalink":"/python/pep/806","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 806 Mixed sync/async context managers with precise async marking 상태: Draft | 유형: Standards Track | 작성일: 05Sep2025 PEP 806 – 혼합 동기/비동기 컨텍스트 관리자 (정확한 비동기 표기법) 초록 (Abstract) 현재 Python에서는 또는 문을"},{"id":"2025-09-27-pep-2026-calendar-versioning-for-python","title":"[Rejected] PEP 2026 - Calendar versioning for Python","excerpt":"Python Enhancement Proposal 2026: 'Calendar versioning for Python'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:21+0900","permalink":"/python/pep/2026","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 2026 Calendar versioning for Python 상태: Rejected | 유형: Process | 작성일: 11Jun2024 PEP 2026 – Python의 캘린더 버전 관리 (Calendar Versioning for Python) 상태: Rejected (거부됨) 유형: Process (프로세스) 생성일: 2024"},{"id":"2025-09-27-pep-3000-python-3000","title":"[Final] PEP 3000 - Python 3000","excerpt":"Python Enhancement Proposal 3000: 'Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:40+0900","permalink":"/python/pep/3000","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3000 Python 3000 상태: Final | 유형: Process | 작성일: 05Apr2006 PEP 3000 – Python 3000 작성자: Guido van Rossum <guido at python.org 상태: Final (최종) 유형: Process (프로세스) 생성일: 2006년 4월 5일 개요 (Abstract) "},{"id":"2025-09-27-pep-3001-procedure-for-reviewing-and-improving-standard-library-modules","title":"[Withdrawn] PEP 3001 - Procedure for reviewing and improving standard library modules","excerpt":"Python Enhancement Proposal 3001: 'Procedure for reviewing and improving standard library modules'에 대한 한국어 번역입니다.","date":"2025-09-27 14:11:53+0900","permalink":"/python/pep/3001","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3001 Procedure for reviewing and improving standard library modules 상태: Withdrawn | 유형: Process | 작성일: 05Apr2006 PEP 3001 – 표준 라이브러리 모듈 검토 및 개선 절차 작성자: Georg Brandl 상태: 철회됨 (Withdrawn) 유형: "},{"id":"2025-09-27-pep-3002-procedure-for-backwards-incompatible-changes","title":"[Final] PEP 3002 - Procedure for Backwards-Incompatible Changes","excerpt":"Python Enhancement Proposal 3002: 'Procedure for Backwards-Incompatible Changes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:06+0900","permalink":"/python/pep/3002","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3002 Procedure for BackwardsIncompatible Changes 상태: Final | 유형: Process | 작성일: 27Mar2006 PEP 3002 – 하위 호환성을 깨는 변경사항을 위한 절차 작성자: Steven Bethard 상태: Final (최종) 유형: Process (프로세스) 생성일: 2006년 "},{"id":"2025-09-27-pep-3003-python-language-moratorium","title":"[Final] PEP 3003 - Python Language Moratorium","excerpt":"Python Enhancement Proposal 3003: 'Python Language Moratorium'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:18+0900","permalink":"/python/pep/3003","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3003 Python Language Moratorium 상태: Final | 유형: Process | 작성일: 21Oct2009 PEP 3003 – Python 언어 변경 유예 (Moratorium) 개요 (Abstract) 이 PEP는 Python 3.1 출시일로부터 최소 2년 동안 Python 언어 문법, 의미론, 내장 함수(bui"},{"id":"2025-09-27-pep-3099-things-that-will-not-change-in-python-3000","title":"[Final] PEP 3099 - Things that will Not Change in Python 3000","excerpt":"Python Enhancement Proposal 3099: 'Things that will Not Change in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:12:36+0900","permalink":"/python/pep/3099","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3099 Things that will Not Change in Python 3000 상태: Final | 유형: Process | 작성일: 04Apr2006 PEP 3099 – Python 3000에서 변경되지 않을 것들 작성자: Georg Brandl 상태: Final 유형: Process 생성일: 2006년 4월 4일 개요 (Abs"},{"id":"2025-09-27-pep-3100-miscellaneous-python-3-0-plans","title":"[Final] PEP 3100 - Miscellaneous Python 3.0 Plans","excerpt":"Python Enhancement Proposal 3100: 'Miscellaneous Python 3.0 Plans'에 대한 한국어 번역입니다.","date":"2025-09-27 14:13:27+0900","permalink":"/python/pep/3100","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3100 Miscellaneous Python 3.0 Plans 상태: Final | 유형: Process | 작성일: 20Aug2004 PEP 3100 – Miscellaneous Python 3.0 Plans 작성자: Brett Cannon 상태: Final (최종) 유형: Process (프로세스) 생성일: 2004년 8월 20일 "},{"id":"2025-09-27-pep-3101-advanced-string-formatting","title":"[Final] PEP 3101 - Advanced String Formatting","excerpt":"Python Enhancement Proposal 3101: 'Advanced String Formatting'에 대한 한국어 번역입니다.","date":"2025-09-27 14:16:14+0900","permalink":"/python/pep/3101","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3101 Advanced String Formatting 상태: Final | 유형: Standards Track | 작성일: 16Apr2006 PEP 3101 – 고급 문자열 포매팅 초록 (Abstract) 이 PEP(Python Enhancement Proposal)는 기존의 '%' 문자열 포매팅 연산자를 대체하기 위한 새로운 내장 "},{"id":"2025-09-27-pep-3102-keyword-only-arguments","title":"[Final] PEP 3102 - Keyword-Only Arguments","excerpt":"Python Enhancement Proposal 3102: 'Keyword-Only Arguments'에 대한 한국어 번역입니다.","date":"2025-09-27 14:16:28+0900","permalink":"/python/pep/3102","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3102 KeywordOnly Arguments 상태: Final | 유형: Standards Track | 작성일: 22Apr2006 PEP 3102 – 키워드 전용 인자 (KeywordOnly Arguments) 초록 (Abstract) 이 PEP는 함수 인자가 명명된 매개변수 슬롯에 할당되는 방식에 대한 변경을 제안합니다. 특히 \""},{"id":"2025-09-27-pep-3103-a-switchcase-statement","title":"[Rejected] PEP 3103 - A Switch/Case Statement","excerpt":"Python Enhancement Proposal 3103: 'A Switch/Case Statement'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:03+0900","permalink":"/python/pep/3103","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3103 A Switch/Case Statement 상태: Rejected | 유형: Standards Track | 작성일: 25Jun2006 PEP 3103 – 문 제안 (거부됨) 거부 공지 PyCon 2007 기조연설 중 진행된 빠른 설문조사에서 이 제안이 대중적 지지를 받지 못했음이 확인되어, 결국 PEP 3103은 거부되었습니다"},{"id":"2025-09-27-pep-3104-access-to-names-in-outer-scopes","title":"[Final] PEP 3104 - Access to Names in Outer Scopes","excerpt":"Python Enhancement Proposal 3104: 'Access to Names in Outer Scopes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:40+0900","permalink":"/python/pep/3104","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3104 Access to Names in Outer Scopes 상태: Final | 유형: Standards Track | 작성일: 12Oct2006 PEP 3104 – 외부 스코프의 이름 접근 작성자: KaPing Yee <ping at zesty.ca 상태: Final 유형: Standards Track 생성일: 20061012 "},{"id":"2025-09-27-pep-3105-make-print-a-function","title":"[Final] PEP 3105 - Make print a function","excerpt":"Python Enhancement Proposal 3105: 'Make print a function'에 대한 한국어 번역입니다.","date":"2025-09-27 14:17:54+0900","permalink":"/python/pep/3105","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3105 Make print a function 상태: Final | 유형: Standards Track | 작성일: 19Nov2006 PEP 3105 – 를 함수로 만들기 작성자 : Georg Brandl 상태 : Final (최종) 유형 : Standards Track 생성일 : 2006년 11월 19일 Python 버전 : 3.0 "},{"id":"2025-09-27-pep-3106-revamping-dict-keys-values-and-items","title":"[Final] PEP 3106 - Revamping dict.keys(), .values() and .items()","excerpt":"Python Enhancement Proposal 3106: 'Revamping dict.keys(), .values() and .items()'에 대한 한국어 번역입니다.","date":"2025-09-27 14:18:13+0900","permalink":"/python/pep/3106","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3106 Revamping dict.keys(), .values() and .items() 상태: Final | 유형: Standards Track | 작성일: 19Dec2006 PEP 3106 – , , 메서드 개편 요약 (Abstract) 이 PEP(Python Enhancement Proposal)는 내장 타입의 , , 메서드가 더"},{"id":"2025-09-27-pep-3107-function-annotations","title":"[Final] PEP 3107 - Function Annotations","excerpt":"Python Enhancement Proposal 3107: 'Function Annotations'에 대한 한국어 번역입니다.","date":"2025-09-27 14:19:43+0900","permalink":"/python/pep/3107","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3107 Function Annotations 상태: Final | 유형: Standards Track | 작성일: 02Dec2006 PEP 3107 – 함수 Annotation (Function Annotations) 번역 및 설명 이 문서는 Python Enhancement Proposal (PEP) 3107의 내용을 한국어 사용자가"},{"id":"2025-09-27-pep-3108-standard-library-reorganization","title":"[Final] PEP 3108 - Standard Library Reorganization","excerpt":"Python Enhancement Proposal 3108: 'Standard Library Reorganization'에 대한 한국어 번역입니다.","date":"2025-09-27 14:20:43+0900","permalink":"/python/pep/3108","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3108 Standard Library Reorganization 상태: Final | 유형: Standards Track | 작성일: 01Jan2007 PEP 3108 – 표준 라이브러리 재편성 작성자 : Brett Cannon <brett at python.org 상태 : Final (최종) 유형 : Standards Track (표"},{"id":"2025-09-27-pep-3109-raising-exceptions-in-python-3000","title":"[Final] PEP 3109 - Raising Exceptions in Python 3000","excerpt":"Python Enhancement Proposal 3109: 'Raising Exceptions in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:02+0900","permalink":"/python/pep/3109","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3109 Raising Exceptions in Python 3000 상태: Final | 유형: Standards Track | 작성일: 19Jan2006 PEP 3109 – Python 3000에서의 예외 발생 (Raising Exceptions) 작성자: Collin Winter 상태: Final (최종) 유형: Standards "},{"id":"2025-09-27-pep-3110-catching-exceptions-in-python-3000","title":"[Final] PEP 3110 - Catching Exceptions in Python 3000","excerpt":"Python Enhancement Proposal 3110: 'Catching Exceptions in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:16+0900","permalink":"/python/pep/3110","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3110 Catching Exceptions in Python 3000 상태: Final | 유형: Standards Track | 작성일: 16Jan2006 파이썬 3000의 예외 처리 (PEP 3110) 개요 이 PEP (Python Enhancement Proposal)는 Python 3.0에서 예외(exception) 처리 구문의"},{"id":"2025-09-27-pep-3111-simple-input-built-in-in-python-3000","title":"[Final] PEP 3111 - Simple input built-in in Python 3000","excerpt":"Python Enhancement Proposal 3111: 'Simple input built-in in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:32+0900","permalink":"/python/pep/3111","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3111 Simple input builtin in Python 3000 상태: Final | 유형: Standards Track | 작성일: 13Sep2006 PEP 3111: Python 3000의 단순 내장 함수 작성자: Andre Roberge 상태: Final 유형: Standards Track 생성일: 2006년 9월 13일 "},{"id":"2025-09-27-pep-3112-bytes-literals-in-python-3000","title":"[Final] PEP 3112 - Bytes literals in Python 3000","excerpt":"Python Enhancement Proposal 3112: 'Bytes literals in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:21:44+0900","permalink":"/python/pep/3112","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3112 Bytes literals in Python 3000 상태: Final | 유형: Standards Track | 작성일: 23Feb2007 PEP 3112 – Python 3000의 바이트 리터럴 (Bytes literals in Python 3000) 초록 (Abstract) 이 PEP는 PEP 358에서 도입된 객체를 위한"},{"id":"2025-09-27-pep-3113-removal-of-tuple-parameter-unpacking","title":"[Final] PEP 3113 - Removal of Tuple Parameter Unpacking","excerpt":"Python Enhancement Proposal 3113: 'Removal of Tuple Parameter Unpacking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:03+0900","permalink":"/python/pep/3113","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3113 Removal of Tuple Parameter Unpacking 상태: Final | 유형: Standards Track | 작성일: 02Mar2007 PEP 3113 – 튜플 매개변수 언패킹 제거 개요 이 문서는 Python 3.0에서 튜플 매개변수 언패킹(Tuple Parameter Unpacking) 기능을 제거할 것을 "},{"id":"2025-09-27-pep-3114-renaming-iterator-next-to-iterator-next","title":"[Final] PEP 3114 - Renaming iterator.next() to iterator.__next__()","excerpt":"Python Enhancement Proposal 3114: 'Renaming iterator.next() to iterator.__next__()'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:21+0900","permalink":"/python/pep/3114","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3114 Renaming iterator.next() to iterator.next() 상태: Final | 유형: Standards Track | 작성일: 04Mar2007 PEP 3114 – 를 로 이름 변경 작성자: KaPing Yee 상태: Final (최종) 유형: Standards Track 작성일: 2007년 3월 4일 Py"},{"id":"2025-09-27-pep-3115-metaclasses-in-python-3000","title":"[Final] PEP 3115 - Metaclasses in Python 3000","excerpt":"Python Enhancement Proposal 3115: 'Metaclasses in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:22:47+0900","permalink":"/python/pep/3115","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3115 Metaclasses in Python 3000 상태: Final | 유형: Standards Track | 작성일: 07Mar2007 상태 : Final (최종) 유형 : Standards Track (표준 트랙) 작성자 : Talin 생성일 : 2007년 3월 7일 Python 버전 : 3.0 수정 이력 : 2007년 3월 "},{"id":"2025-09-27-pep-3116-new-io","title":"[Final] PEP 3116 - New I/O","excerpt":"Python Enhancement Proposal 3116: 'New I/O'에 대한 한국어 번역입니다.","date":"2025-09-27 14:24:50+0900","permalink":"/python/pep/3116","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3116 New I/O 상태: Final | 유형: Standards Track | 작성일: 26Feb2007 PEP 3116 – 새로운 I/O (New I/O) 작성자: Daniel Stutzbach, Guido van Rossum, Mike Verdone 상태: Final (최종) 유형: Standards Track (표준 트랙) 생"},{"id":"2025-09-27-pep-3117-postfix-type-declarations","title":"[Rejected] PEP 3117 - Postfix type declarations","excerpt":"Python Enhancement Proposal 3117: 'Postfix type declarations'에 대한 한국어 번역입니다.","date":"2025-09-27 14:25:21+0900","permalink":"/python/pep/3117","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3117 Postfix type declarations 상태: Rejected | 유형: Standards Track | 작성일: 01Apr2007 PEP 3117 – Postfix type declarations (후위 타입 선언) 개요 (Abstract) 이 PEP는 Python에 후위(postfix) 타입 선언 문법을 추가할 것을 "},{"id":"2025-09-27-pep-3118-revising-the-buffer-protocol","title":"[Final] PEP 3118 - Revising the buffer protocol","excerpt":"Python Enhancement Proposal 3118: 'Revising the buffer protocol'에 대한 한국어 번역입니다.","date":"2025-09-27 14:26:38+0900","permalink":"/python/pep/3118","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3118 Revising the buffer protocol 상태: Final | 유형: Standards Track | 작성일: 28Aug2006 PEP 3118 – 버퍼 프로토콜 재설계 (Revising the buffer protocol) 작성자: Travis Oliphant, Carl Banks 상태: Final 유형: Stand"},{"id":"2025-09-27-pep-3119-introducing-abstract-base-classes","title":"[Final] PEP 3119 - Introducing Abstract Base Classes","excerpt":"Python Enhancement Proposal 3119: 'Introducing Abstract Base Classes'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:33+0900","permalink":"/python/pep/3119","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3119 Introducing Abstract Base Classes 상태: Final | 유형: Standards Track | 작성일: 18Apr2007 PEP 3119 – 추상 기본 클래스 (Abstract Base Classes) 도입 개요 (Abstract) 이 문서는 Python 3000에 Abstract Base Class "},{"id":"2025-09-27-pep-3120-using-utf-8-as-the-default-source-encoding","title":"[Final] PEP 3120 - Using UTF-8 as the default source encoding","excerpt":"Python Enhancement Proposal 3120: 'Using UTF-8 as the default source encoding'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:43+0900","permalink":"/python/pep/3120","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3120 Using UTF8 as the default source encoding 상태: Final | 유형: Standards Track | 작성일: 15Apr2007 PEP 3120 – 기본 소스 인코딩으로 UTF8 사용 개요 이 문서는 Python 3.0부터 기본 소스 인코딩을 ASCII에서 UTF8로 변경할 것을 제안합니다. P"},{"id":"2025-09-27-pep-3121-extension-module-initialization-and-finalization","title":"[Final] PEP 3121 - Extension Module Initialization and Finalization","excerpt":"Python Enhancement Proposal 3121: 'Extension Module Initialization and Finalization'에 대한 한국어 번역입니다.","date":"2025-09-27 14:27:55+0900","permalink":"/python/pep/3121","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3121 Extension Module Initialization and Finalization 상태: Final | 유형: Standards Track | 작성일: 27Apr2007 PEP 3121 – 확장 모듈 초기화 및 종료 본 문서는 역사적 문서입니다. 최신 규범 문서는 및 에서 확인할 수 있습니다. 개요 (Abstract) 현재"},{"id":"2025-09-27-pep-3122-delineation-of-the-main-module","title":"[Rejected] PEP 3122 - Delineation of the main module","excerpt":"Python Enhancement Proposal 3122: 'Delineation of the main module'에 대한 한국어 번역입니다.","date":"2025-09-27 14:28:16+0900","permalink":"/python/pep/3122","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3122 Delineation of the main module 상태: Rejected | 유형: Standards Track | 작성일: 27Apr2007 PEP 3122 – 메인 모듈의 경계 설정 (Delineation of the main module) 작성자: Brett Cannon 상태: Rejected (거부됨) 유형: Sta"},{"id":"2025-09-27-pep-3123-making-pyobject-head-conform-to-standard-c","title":"[Final] PEP 3123 - Making PyObject_HEAD conform to standard C","excerpt":"Python Enhancement Proposal 3123: 'Making PyObject_HEAD conform to standard C'에 대한 한국어 번역입니다.","date":"2025-09-27 14:28:28+0900","permalink":"/python/pep/3123","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3123 Making PyObjectHEAD conform to standard C 상태: Final | 유형: Standards Track | 작성일: 27Apr2007 PEP 3123 – PyObjectHEAD를 표준 C에 맞게 변경 개요 (Abstract) Python은 현재 사용에서 C 표준에 정의되지 않은 동작에 의존하고 있습니"},{"id":"2025-09-27-pep-3124-overloading-generic-functions-interfaces-and-adaptation","title":"[Deferred] PEP 3124 - Overloading, Generic Functions, Interfaces, and Adaptation","excerpt":"Python Enhancement Proposal 3124: 'Overloading, Generic Functions, Interfaces, and Adaptation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:29:34+0900","permalink":"/python/pep/3124","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3124 Overloading, Generic Functions, Interfaces, and Adaptation 상태: Deferred | 유형: Standards Track | 작성일: 28Apr2007 PEP 3124 – 오버로딩, 제네릭 함수, 인터페이스 및 어댑테이션 작성자: Phillip J. Eby <pje at teleco"},{"id":"2025-09-27-pep-3125-remove-backslash-continuation","title":"[Rejected] PEP 3125 - Remove Backslash Continuation","excerpt":"Python Enhancement Proposal 3125: 'Remove Backslash Continuation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:29:48+0900","permalink":"/python/pep/3125","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3125 Remove Backslash Continuation 상태: Rejected | 유형: Standards Track | 작성일: 29Apr2007 PEP 3125 – Backslash Continuation 제거 제안 작성자: Jim J. Jewett 상태: Rejected (거부됨) 유형: Standards Track 생성일:"},{"id":"2025-09-27-pep-3126-remove-implicit-string-concatenation","title":"[Rejected] PEP 3126 - Remove Implicit String Concatenation","excerpt":"Python Enhancement Proposal 3126: 'Remove Implicit String Concatenation'에 대한 한국어 번역입니다.","date":"2025-09-27 14:30:11+0900","permalink":"/python/pep/3126","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3126 Remove Implicit String Concatenation 상태: Rejected | 유형: Standards Track | 작성일: 29Apr2007 PEP 3126 – 암시적 문자열 연결 제거 (Remove Implicit String Concatenation) 개요 이 문서는 Python 3000(Python 3의 "},{"id":"2025-09-27-pep-3127-integer-literal-support-and-syntax","title":"[Final] PEP 3127 - Integer Literal Support and Syntax","excerpt":"Python Enhancement Proposal 3127: 'Integer Literal Support and Syntax'에 대한 한국어 번역입니다.","date":"2025-09-27 14:30:46+0900","permalink":"/python/pep/3127","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3127 Integer Literal Support and Syntax 상태: Final | 유형: Standards Track | 작성일: 14Mar2007 PEP 3127 – 정수 리터럴 지원 및 구문 이 문서는 Python 3.0에 도입된 정수 리터럴(Integer Literal) 처리 방식의 변경 사항에 대한 Python Enha"},{"id":"2025-09-27-pep-3128-blist-a-faster-list-like-type","title":"[Rejected] PEP 3128 - BList: A Faster List-like Type","excerpt":"Python Enhancement Proposal 3128: 'BList: A Faster List-like Type'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:22+0900","permalink":"/python/pep/3128","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3128 BList: A Faster Listlike Type 상태: Rejected | 유형: Standards Track | 작성일: 30Apr2007 PEP 3128 – BList: 더 빠른 Listlike 타입 작성자: Daniel Stutzbach 논의처: Python3000 메일링 리스트 상태: Rejected (거부됨) 유형"},{"id":"2025-09-27-pep-3129-class-decorators","title":"[Final] PEP 3129 - Class Decorators","excerpt":"Python Enhancement Proposal 3129: 'Class Decorators'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:32+0900","permalink":"/python/pep/3129","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3129 Class Decorators 상태: Final | 유형: Standards Track | 작성일: 01May2007 PEP 3129 – 클래스 데코레이터 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 PEP 318에서 도입된 함수 및 메서드 데코레이터의 확장인 클래스 데코레이터를 제안"},{"id":"2025-09-27-pep-3130-access-to-current-moduleclassfunction","title":"[Rejected] PEP 3130 - Access to Current Module/Class/Function","excerpt":"Python Enhancement Proposal 3130: 'Access to Current Module/Class/Function'에 대한 한국어 번역입니다.","date":"2025-09-27 14:31:53+0900","permalink":"/python/pep/3130","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3130 Access to Current Module/Class/Function 상태: Rejected | 유형: Standards Track | 작성일: 22Apr2007 PEP 3130 – 현재 모듈/클래스/함수에 접근 작성자: Jim J. Jewett <jimjjewett at gmail.com 상태: Rejected (거부됨) 유"},{"id":"2025-09-27-pep-3131-supporting-non-ascii-identifiers","title":"[Final] PEP 3131 - Supporting Non-ASCII Identifiers","excerpt":"Python Enhancement Proposal 3131: 'Supporting Non-ASCII Identifiers'에 대한 한국어 번역입니다.","date":"2025-09-27 14:32:23+0900","permalink":"/python/pep/3131","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3131 Supporting NonASCII Identifiers 상태: Final | 유형: Standards Track | 작성일: 01May2007 PEP 3131 – 비(Non)ASCII 식별자 지원 (Supporting NonASCII Identifiers) 작성자: Martin von Löwis 상태: 최종 (Final) 유형"},{"id":"2025-09-27-pep-3132-extended-iterable-unpacking","title":"[Final] PEP 3132 - Extended Iterable Unpacking","excerpt":"Python Enhancement Proposal 3132: 'Extended Iterable Unpacking'에 대한 한국어 번역입니다.","date":"2025-09-27 14:32:37+0900","permalink":"/python/pep/3132","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3132 Extended Iterable Unpacking 상태: Final | 유형: Standards Track | 작성일: 30Apr2007 PEP 3132 – 확장된 이터러블 언패킹 (Extended Iterable Unpacking) 개요 (Abstract) 이 PEP는 이터러블(iterable) 언패킹(unpacking) 문법"},{"id":"2025-09-27-pep-3133-introducing-roles","title":"[Rejected] PEP 3133 - Introducing Roles","excerpt":"Python Enhancement Proposal 3133: 'Introducing Roles'에 대한 한국어 번역입니다.","date":"2025-09-27 14:33:16+0900","permalink":"/python/pep/3133","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3133 Introducing Roles 상태: Rejected | 유형: Standards Track | 작성일: 01May2007 PEP 3133 – 역할(Roles) 소개 (PEP 3133 – Introducing Roles) 작성자: Collin Winter 상태: 거부됨 (Rejected) 유형: 표준 트랙 (Standards "},{"id":"2025-09-27-pep-3134-exception-chaining-and-embedded-tracebacks","title":"[Final] PEP 3134 - Exception Chaining and Embedded Tracebacks","excerpt":"Python Enhancement Proposal 3134: 'Exception Chaining and Embedded Tracebacks'에 대한 한국어 번역입니다.","date":"2025-09-27 14:33:48+0900","permalink":"/python/pep/3134","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3134 Exception Chaining and Embedded Tracebacks 상태: Final | 유형: Standards Track | 작성일: 12May2005 PEP 3134 – 예외 체인(Exception Chaining) 및 임베디드 트레이스백(Embedded Tracebacks) 이 문서는 Python 3.0에 도입된"},{"id":"2025-09-27-pep-3135-new-super","title":"[Final] PEP 3135 - New Super","excerpt":"Python Enhancement Proposal 3135: 'New Super'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:00+0900","permalink":"/python/pep/3135","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3135 New Super 상태: Final | 유형: Standards Track | 작성일: 28Apr2007 PEP 3135 – 새로운 개요 이 PEP는 타입을 사용하여 메서드가 정의된 클래스와 현재 메서드가 작동하는 인스턴스(또는 클래스 메서드의 경우 클래스 객체)에 자동으로 바인딩되는 인스턴스를 구성하기 위한 \"구문 설탕(syn"},{"id":"2025-09-27-pep-3136-labeled-break-and-continue","title":"[Rejected] PEP 3136 - Labeled break and continue","excerpt":"Python Enhancement Proposal 3136: 'Labeled break and continue'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:24+0900","permalink":"/python/pep/3136","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3136 Labeled break and continue 상태: Rejected | 유형: Standards Track | 작성일: 30Jun2007 PEP 3136 – Labeled break 및 continue 개요 (Abstract) 이 PEP는 Python의 및 문에 레이블(label) 지원을 제안합니다. 이 아이디어는 다른 언어"},{"id":"2025-09-27-pep-3137-immutable-bytes-and-mutable-buffer","title":"[Final] PEP 3137 - Immutable Bytes and Mutable Buffer","excerpt":"Python Enhancement Proposal 3137: 'Immutable Bytes and Mutable Buffer'에 대한 한국어 번역입니다.","date":"2025-09-27 14:34:47+0900","permalink":"/python/pep/3137","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3137 Immutable Bytes and Mutable Buffer 상태: Final | 유형: Standards Track | 작성일: 26Sep2007 Python Enhancement Proposal (PEP) 3137은 Python 3.0에서 타입의 불변성(immutable)과 가변성(mutable)을 명확하게 분리하는 제안입"},{"id":"2025-09-27-pep-3138-string-representation-in-python-3000","title":"[Final] PEP 3138 - String representation in Python 3000","excerpt":"Python Enhancement Proposal 3138: 'String representation in Python 3000'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:12+0900","permalink":"/python/pep/3138","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3138 String representation in Python 3000 상태: Final | 유형: Standards Track | 작성일: 05May2008 PEP 3138 – Python 3000에서의 문자열 표현 (String representation in Python 3000) 작성자: Atsuo Ishimoto <ishim"},{"id":"2025-09-27-pep-3139-cleaning-out-sys-and-the-interpreter-module","title":"[Rejected] PEP 3139 - Cleaning out sys and the “interpreter” module","excerpt":"Python Enhancement Proposal 3139: 'Cleaning out sys and the “interpreter” module'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:24+0900","permalink":"/python/pep/3139","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3139 Cleaning out sys and the “interpreter” module 상태: Rejected | 유형: Standards Track | 작성일: 04Apr2008 PEP 3139 – 모듈 및 \"interpreter\" 모듈 정리 작성자: Benjamin Peterson 상태: Rejected (거부됨) 유형: Stan"},{"id":"2025-09-27-pep-3140-strcontainer-should-call-stritem-not-repritem","title":"[Rejected] PEP 3140 - str(container) should call str(item), not repr(item)","excerpt":"Python Enhancement Proposal 3140: 'str(container) should call str(item), not repr(item)'에 대한 한국어 번역입니다.","date":"2025-09-27 14:35:43+0900","permalink":"/python/pep/3140","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3140 str(container) should call str(item), not repr(item) 상태: Rejected | 유형: Standards Track | 작성일: 27May2008 PEP 3140 – 는 대신 을 호출해야 합니다. 작성자: Oleg Broytman, Jim J. Jewett 상태: Rejected (거절됨"},{"id":"2025-09-27-pep-3141-a-type-hierarchy-for-numbers","title":"[Final] PEP 3141 - A Type Hierarchy for Numbers","excerpt":"Python Enhancement Proposal 3141: 'A Type Hierarchy for Numbers'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:02+0900","permalink":"/python/pep/3141","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3141 A Type Hierarchy for Numbers 상태: Final | 유형: Standards Track | 작성일: 23Apr2007 PEP 3141: 숫자형을 위한 타입 계층 구조 (A Type Hierarchy for Numbers) 작성자: Jeffrey Yasskin 상태: Final (최종) 타입: Standard"},{"id":"2025-09-27-pep-3142-add-a-while-clause-to-generator-expressions","title":"[Rejected] PEP 3142 - Add a “while” clause to generator expressions","excerpt":"Python Enhancement Proposal 3142: 'Add a “while” clause to generator expressions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:16+0900","permalink":"/python/pep/3142","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3142 Add a “while” clause to generator expressions 상태: Rejected | 유형: Standards Track | 작성일: 12Jan2009 PEP 3142 – 제너레이터 표현식에 \"while\" 절 추가 제안 작성자 : Gerald Britton 상태 : 거부됨 (Rejected) 유형 : 표준"},{"id":"2025-09-27-pep-3143-standard-daemon-process-library","title":"[Deferred] PEP 3143 - Standard daemon process library","excerpt":"Python Enhancement Proposal 3143: 'Standard daemon process library'에 대한 한국어 번역입니다.","date":"2025-09-27 14:36:57+0900","permalink":"/python/pep/3143","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3143 Standard daemon process library 상태: Deferred | 유형: Standards Track | 작성일: 26Jan2009 초록 (Abstract) 잘 작동하는 유닉스(Unix) 데몬(daemon) 프로그램을 작성하는 것은 다소 복잡하고 제대로 구현하기 까다롭지만, 프로그램이 해야 할 다른 작업과는 관"},{"id":"2025-09-27-pep-3144-ip-address-manipulation-library-for-the-python-standard-library","title":"[Final] PEP 3144 - IP Address Manipulation Library for the Python Standard Library","excerpt":"Python Enhancement Proposal 3144: 'IP Address Manipulation Library for the Python Standard Library'에 대한 한국어 번역입니다.","date":"2025-09-27 14:37:11+0900","permalink":"/python/pep/3144","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3144 IP Address Manipulation Library for the Python Standard Library 상태: Final | 유형: Standards Track | 작성일: 06Feb2012 PEP 3144 – Python 표준 라이브러리를 위한 IP 주소 조작 라이브러리 초록 (Abstract) 이 PEP는 Pyth"},{"id":"2025-09-27-pep-3145-asynchronous-io-for-subprocess-popen","title":"[Withdrawn] PEP 3145 - Asynchronous I/O For subprocess.Popen","excerpt":"Python Enhancement Proposal 3145: 'Asynchronous I/O For subprocess.Popen'에 대한 한국어 번역입니다.","date":"2025-09-27 14:37:24+0900","permalink":"/python/pep/3145","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3145 Asynchronous I/O For subprocess.Popen 상태: Withdrawn | 유형: Standards Track | 작성일: 04Aug2009 PEP 3145 – 을 위한 비동기 I/O 작성자: Eric Pruitt, Charles R. McCreary, Josiah Carlson 상태: 철회됨 (Withdr"},{"id":"2025-09-27-pep-3146-merging-unladen-swallow-into-cpython","title":"[Withdrawn] PEP 3146 - Merging Unladen Swallow into CPython","excerpt":"Python Enhancement Proposal 3146: 'Merging Unladen Swallow into CPython'에 대한 한국어 번역입니다.","date":"2025-09-27 14:38:28+0900","permalink":"/python/pep/3146","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3146 Merging Unladen Swallow into CPython 상태: Withdrawn | 유형: Standards Track | 작성일: 01Jan2010 PEP 3146 – Unladen Swallow을 CPython으로 통합하기 (철회됨) 작성자: Collin Winter, Jeffrey Yasskin, Reid Kle"},{"id":"2025-09-27-pep-3147-pyc-repository-directories","title":"[Final] PEP 3147 - PYC Repository Directories","excerpt":"Python Enhancement Proposal 3147: 'PYC Repository Directories'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:02+0900","permalink":"/python/pep/3147","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3147 PYC Repository Directories 상태: Final | 유형: Standards Track | 작성일: 16Dec2009 PEP 3147 – PYC 저장소 디렉터리 요약 (Abstract) 이 PEP는 여러 다른 버전의 Python 인터프리터 간에 Python 소스 코드 파일을 공유하는 방법을 개선하는 Python"},{"id":"2025-09-27-pep-3148-futures-execute-computations-asynchronously","title":"[Final] PEP 3148 - futures - execute computations asynchronously","excerpt":"Python Enhancement Proposal 3148: 'futures - execute computations asynchronously'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:26+0900","permalink":"/python/pep/3148","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3148 futures execute computations asynchronously 상태: Final | 유형: Standards Track | 작성일: 16Oct2009 PEP 3148 – futures 비동기 연산 실행 개요 (Abstract) 이 PEP는 스레드와 프로세스를 활용하여 호출 가능한(callable) 객체의 연산을 "},{"id":"2025-09-27-pep-3149-abi-version-tagged-so-files","title":"[Final] PEP 3149 - ABI version tagged .so files","excerpt":"Python Enhancement Proposal 3149: 'ABI version tagged .so files'에 대한 한국어 번역입니다.","date":"2025-09-27 14:39:52+0900","permalink":"/python/pep/3149","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3149 ABI version tagged .so files 상태: Final | 유형: Standards Track | 작성일: 09Jul2010 PEP 3149 – ABI 버전 태그가 지정된 .so 파일 개요 (Abstract) 이 PEP(Python Enhancement Proposal)는 Python의 메커니즘을 확장하여 확장 모"},{"id":"2025-09-27-pep-3150-statement-local-namespaces-aka-given-clause","title":"[Deferred] PEP 3150 - Statement local namespaces (aka “given” clause)","excerpt":"Python Enhancement Proposal 3150: 'Statement local namespaces (aka “given” clause)'에 대한 한국어 번역입니다.","date":"2025-09-27 14:40:31+0900","permalink":"/python/pep/3150","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3150 Statement local namespaces (aka “given” clause) 상태: Deferred | 유형: Standards Track | 작성일: 09Jul2010 다음은 PEP 3150 문서의 내용을 한국어로 번역하고 정리한 것입니다. PEP 3150 – Statement local namespaces (일명 \""},{"id":"2025-09-27-pep-3151-reworking-the-os-and-io-exception-hierarchy","title":"[Final] PEP 3151 - Reworking the OS and IO exception hierarchy","excerpt":"Python Enhancement Proposal 3151: 'Reworking the OS and IO exception hierarchy'에 대한 한국어 번역입니다.","date":"2025-09-27 14:41:26+0900","permalink":"/python/pep/3151","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3151 Reworking the OS and IO exception hierarchy 상태: Final | 유형: Standards Track | 작성일: 21Jul2010 PEP 3151 – OS 및 IO 예외 계층 구조 재작업 ============================================ 이 문서는 Python E"},{"id":"2025-09-27-pep-3152-cofunctions","title":"[Rejected] PEP 3152 - Cofunctions","excerpt":"Python Enhancement Proposal 3152: 'Cofunctions'에 대한 한국어 번역입니다.","date":"2025-09-27 14:41:43+0900","permalink":"/python/pep/3152","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3152 Cofunctions 상태: Rejected | 유형: Standards Track | 작성일: 13Feb2009 개요 (Abstract) PEP 3152는 'cofunction'이라는 특수한 유형의 제너레이터(generator)를 정의하고 호출하기 위한 새로운 문법을 제안했습니다. 이 제안의 목표는 제너레이터 기반 코루틴(co"},{"id":"2025-09-27-pep-3153-asynchronous-io-support","title":"[Superseded] PEP 3153 - Asynchronous IO support","excerpt":"Python Enhancement Proposal 3153: 'Asynchronous IO support'에 대한 한국어 번역입니다.","date":"2025-09-27 14:42:18+0900","permalink":"/python/pep/3153","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3153 Asynchronous IO support 상태: Superseded | 유형: Standards Track | 작성일: 29May2011 PEP 3153 – 비동기 IO 지원 작성자: Laurens Van Houtven < at lvh.cc 상태: 대체됨 (Superseded) 유형: 표준 트랙 (Standards Track)"},{"id":"2025-09-27-pep-3154-pickle-protocol-version-4","title":"[Final] PEP 3154 - Pickle protocol version 4","excerpt":"Python Enhancement Proposal 3154: 'Pickle protocol version 4'에 대한 한국어 번역입니다.","date":"2025-09-27 19:20:26+0900","permalink":"/python/pep/3154","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3154 Pickle protocol version 4 상태: Final | 유형: Standards Track | 작성일: 11Aug2011 PEP 3154 – Pickle 프로토콜 버전 4 작성자: Antoine Pitrou 상태: Final (최종) 유형: Standards Track (표준 트랙) 생성일: 2011년 8월 11일 "},{"id":"2025-09-27-pep-3155-qualified-name-for-classes-and-functions","title":"[Final] PEP 3155 - Qualified name for classes and functions","excerpt":"Python Enhancement Proposal 3155: 'Qualified name for classes and functions'에 대한 한국어 번역입니다.","date":"2025-09-27 19:20:50+0900","permalink":"/python/pep/3155","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3155 Qualified name for classes and functions 상태: Final | 유형: Standards Track | 작성일: 29Oct2011 PEP 3155 – 클래스 및 함수를 위한 Qualified Name (정규화된 이름) 개요 PEP 3155는 Python의 클래스(class)와 함수(function)"},{"id":"2025-09-27-pep-3156-asynchronous-io-support-rebooted-the-asyncio-module","title":"[Final] PEP 3156 - Asynchronous IO Support Rebooted: the “asyncio” Module","excerpt":"Python Enhancement Proposal 3156: 'Asynchronous IO Support Rebooted: the “asyncio” Module'에 대한 한국어 번역입니다.","date":"2025-09-27 19:21:50+0900","permalink":"/python/pep/3156","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3156 Asynchronous IO Support Rebooted: the “asyncio” Module 상태: Final | 유형: Standards Track | 작성일: 12Dec2012 PEP 3156 – 비동기 I/O 지원 재정비: \"asyncio\" 모듈 작성자: Guido van Rossum 상태: Final 타입: Stan"},{"id":"2025-09-27-pep-3333-python-web-server-gateway-interface-v1-0-1","title":"[Final] PEP 3333 - Python Web Server Gateway Interface v1.0.1","excerpt":"Python Enhancement Proposal 3333: 'Python Web Server Gateway Interface v1.0.1'에 대한 한국어 번역입니다.","date":"2025-09-27 19:24:56+0900","permalink":"/python/pep/3333","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 3333 Python Web Server Gateway Interface v1.0.1 상태: Final | 유형: Informational | 작성일: 26Sep2010 PEP 3333 – Python Web Server Gateway Interface v1.0.1 번역 및 요약 서문 (PEP 333 독자를 위한) 이 문서는 기존 PEP"},{"id":"2025-09-27-pep-8000-python-language-governance-proposal-overview","title":"[Final] PEP 8000 - Python Language Governance Proposal Overview","excerpt":"Python Enhancement Proposal 8000: 'Python Language Governance Proposal Overview'에 대한 한국어 번역입니다.","date":"2025-09-27 19:25:09+0900","permalink":"/python/pep/8000","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8000 Python Language Governance Proposal Overview 상태: Final | 유형: Informational | 작성일: 24Aug2018 PEP 8000 – Python 언어 거버넌스 제안 개요 작성자: Barry Warsaw <barry at python.org 상태: Final (최종) 유형: In"},{"id":"2025-09-27-pep-8001-python-governance-voting-process","title":"[Final] PEP 8001 - Python Governance Voting Process","excerpt":"Python Enhancement Proposal 8001: 'Python Governance Voting Process'에 대한 한국어 번역입니다.","date":"2025-09-27 19:25:35+0900","permalink":"/python/pep/8001","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8001 Python Governance Voting Process 상태: Final | 유형: Process | 작성일: 24Aug2018 PEP 8001 – Python 거버넌스 투표 절차 (Python Governance Voting Process) 저자: Brett Cannon 외 다수 상태: Final (최종) 유형: Proce"},{"id":"2025-09-27-pep-8002-open-source-governance-survey","title":"[Final] PEP 8002 - Open Source Governance Survey","excerpt":"Python Enhancement Proposal 8002: 'Open Source Governance Survey'에 대한 한국어 번역입니다.","date":"2025-09-27 19:26:46+0900","permalink":"/python/pep/8002","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8002 Open Source Governance Survey 상태: Final | 유형: Informational | 작성일: 24Aug2018 PEP 8002 – 오픈 소스 거버넌스 설문조사 이 문서는 Python Enhancement Proposal (PEP) 8002의 내용을 한국어 사용자가 이해하기 쉽게 번역하고 정리한 것입니다"},{"id":"2025-09-27-pep-8010-the-technical-leader-governance-model","title":"[Rejected] PEP 8010 - The Technical Leader Governance Model","excerpt":"Python Enhancement Proposal 8010: 'The Technical Leader Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:27:12+0900","permalink":"/python/pep/8010","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8010 The Technical Leader Governance Model 상태: Rejected | 유형: Informational | 작성일: 24Aug2018 파이썬 개선 제안 (PEP) 8010: 기술 리더 거버넌스 모델 개요 이 문서는 Python Enhancement Proposal (PEP) 8010, 즉 \"기술 리더 거버"},{"id":"2025-09-27-pep-8011-python-governance-model-lead-by-trio-of-pythonistas","title":"[Rejected] PEP 8011 - Python Governance Model Lead by Trio of Pythonistas","excerpt":"Python Enhancement Proposal 8011: 'Python Governance Model Lead by Trio of Pythonistas'에 대한 한국어 번역입니다.","date":"2025-09-27 19:27:38+0900","permalink":"/python/pep/8011","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8011 Python Governance Model Lead by Trio of Pythonistas 상태: Rejected | 유형: Informational | 작성일: 24Aug2018 PEP 8011: 세 명의 파이써니스타가 이끄는 파이썬 거버넌스 모델 제안 (거절됨) 개요 이 문서는 파이썬 핵심 개발 커뮤니티를 위한 거버넌스(G"},{"id":"2025-09-27-pep-8012-the-community-governance-model","title":"[Rejected] PEP 8012 - The Community Governance Model","excerpt":"Python Enhancement Proposal 8012: 'The Community Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:28:04+0900","permalink":"/python/pep/8012","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8012 The Community Governance Model 상태: Rejected | 유형: Informational | 작성일: 03Oct2018 PEP 8012 – 커뮤니티 거버넌스 모델 작성자: Łukasz Langa <lukasz at python.org 상태: Rejected (거부됨) 유형: Informational (정"},{"id":"2025-09-27-pep-8013-the-external-council-governance-model","title":"[Rejected] PEP 8013 - The External Council Governance Model","excerpt":"Python Enhancement Proposal 8013: 'The External Council Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:28:36+0900","permalink":"/python/pep/8013","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8013 The External Council Governance Model 상태: Rejected | 유형: Informational | 작성일: 14Sep2018 PEP 8013 – 외부 위원회 거버넌스 모델 작성자: Steve Dower <steve.dower at python.org 상태: Rejected (거부됨) 유형: Inf"},{"id":"2025-09-27-pep-8014-the-commons-governance-model","title":"[Rejected] PEP 8014 - The Commons Governance Model","excerpt":"Python Enhancement Proposal 8014: 'The Commons Governance Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:29:07+0900","permalink":"/python/pep/8014","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8014 The Commons Governance Model 상태: Rejected | 유형: Informational | 작성일: 16Sep2018 PEP 8014 – 커먼즈 거버넌스 모델 요약 (Abstract) 이 PEP는 절차, 정의된 용어, 비율을 가능한 한 적게 사용하는 거버넌스 모델을 제안합니다. 이 모델은 '무정부주의 거버"},{"id":"2025-09-27-pep-8015-organization-of-the-python-community","title":"[Rejected] PEP 8015 - Organization of the Python community","excerpt":"Python Enhancement Proposal 8015: 'Organization of the Python community'에 대한 한국어 번역입니다.","date":"2025-09-27 19:29:53+0900","permalink":"/python/pep/8015","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8015 Organization of the Python community 상태: Rejected | 유형: Informational | 작성일: 04Oct2018 PEP 8015 – Python 커뮤니티의 조직 (Organization of the Python community) 작성자: Victor Stinner 상태: 거부됨 (Re"},{"id":"2025-09-27-pep-8016-the-steering-council-model","title":"[Accepted] PEP 8016 - The Steering Council Model","excerpt":"Python Enhancement Proposal 8016: 'The Steering Council Model'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:11+0900","permalink":"/python/pep/8016","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8016 The Steering Council Model 상태: Accepted | 유형: Informational | 작성일: 01Nov2018 PEP 8016 – 스티어링 위원회 모델 요약 PEP 8016은 Python 거버넌스 모델을 스티어링 위원회(Steering Council) 중심으로 제안하는 문서입니다. 이 위원회는 광범위한"},{"id":"2025-09-27-pep-8100-january-2019-steering-council-election","title":"[Final] PEP 8100 - January 2019 Steering Council election","excerpt":"Python Enhancement Proposal 8100: 'January 2019 Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:34+0900","permalink":"/python/pep/8100","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8100 January 2019 Steering Council election 상태: Final | 유형: Informational | 작성일: 03Jan2019 개요 이 문서는 PEP 13에 명시된 2019년 1월 Python 운영 위원회 선거의 일정 및 기타 세부 사항을 설명합니다. 이는 첫 번째 운영 위원회 선거입니다. 선거 관리관"},{"id":"2025-09-27-pep-8101-2020-term-steering-council-election","title":"[Final] PEP 8101 - 2020 Term Steering Council election","excerpt":"Python Enhancement Proposal 8101: '2020 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:30:45+0900","permalink":"/python/pep/8101","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8101 2020 Term Steering Council election 상태: Final | 유형: Informational | 작성일: 16Nov2019 PEP 8101은 2019년 12월에 실시된 Python 스티어링 카운슬(Steering Council) 선거에 대한 정보를 담고 있는 문서입니다. 이 문서는 선거의 배경, 절차, "},{"id":"2025-09-27-pep-8102-2021-term-steering-council-election","title":"[Final] PEP 8102 - 2021 Term Steering Council election","excerpt":"Python Enhancement Proposal 8102: '2021 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:31:05+0900","permalink":"/python/pep/8102","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8102 2021 Term Steering Council election 상태: Final | 유형: Informational | 작성일: 29Oct2020 PEP 8102 – 2021년 임기 스티어링 카운슬 선거 (2021 Term Steering Council election) 개요 이 문서는 PEP 13에 명시된 바와 같이, 202"},{"id":"2025-09-27-pep-8103-2022-term-steering-council-election","title":"[Final] PEP 8103 - 2022 Term Steering Council election","excerpt":"Python Enhancement Proposal 8103: '2022 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:31:17+0900","permalink":"/python/pep/8103","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8103 2022 Term Steering Council election 상태: Final | 유형: Informational | 작성일: 04Oct2021 PEP 8103 – 2022년도 Steering Council 선거 작성자: Ewa Jodlowska, Ee Durbin, Joe Carey 후원자: Barry Warsaw 상태: "},{"id":"2025-09-27-pep-8104-2023-term-steering-council-election","title":"[Final] PEP 8104 - 2023 Term Steering Council election","excerpt":"Python Enhancement Proposal 8104: '2023 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:34:01+0900","permalink":"/python/pep/8104","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8104 2023 Term Steering Council election 상태: Final | 유형: Informational | 작성일: 08Nov2022 PEP 8104는 \"2023 Term Steering Council election\"에 대한 정보성(Informational) PEP로, 2022년 12월에 진행된 Python 스티"},{"id":"2025-09-27-pep-8105-2024-term-steering-council-election","title":"[Final] PEP 8105 - 2024 Term Steering Council election","excerpt":"Python Enhancement Proposal 8105: '2024 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:34:28+0900","permalink":"/python/pep/8105","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8105 2024 Term Steering Council election 상태: Final | 유형: Informational | 작성일: 23Oct2023 PEP 8105 – 2024년 임기 Steering Council 선거 작성자: Ee Durbin <<ee at python.org 후원자: Thomas Wouters <<thoma"},{"id":"2025-09-27-pep-8106-2025-term-steering-council-election","title":"[Final] PEP 8106 - 2025 Term Steering Council election","excerpt":"Python Enhancement Proposal 8106: '2025 Term Steering Council election'에 대한 한국어 번역입니다.","date":"2025-09-27 19:35:42+0900","permalink":"/python/pep/8106","tags":["Python","PEP","Translation"],"text":"원문 링크: PEP 8106 2025 Term Steering Council election 상태: Final | 유형: Informational | 작성일: 21Oct2024 PEP 8106 – 2025년 임기 스티어링 카운슬 선거 번역 및 정리 1. 개요 (Abstract) 이 문서는 PEP 13에 명시된 바에 따라, Python 스티어링 카운슬(Ste"},{"id":"2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects","title":"[논문리뷰] A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects","excerpt":"Jennifer Ding이 arXiv에 게시한 'A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects","tags":["Review","Open Source AI","LLM Development","Open Collaboration","Governance Models","Developer Motivations","Community Engagement","AI Ecosystem"],"text":"링크: 논문 PDF로 바로 열기 저자: Jennifer Ding, Cailean Osborne, Johan Linåker, Ben Burtenshaw 핵심 연구 목표 오픈 대규모 언어 모델(LLM) 프로젝트에서 협업 방식, 동기, 거버넌스에 대한 포괄적인 이해를 구축하는 것이 목표입니다. 특히 LLM 개발 및 재사용 수명 주기 전반에 걸쳐 협업이 어떻게 시"},{"id":"2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models","title":"[논문리뷰] Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models","excerpt":"arXiv에 게시된 'Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models","tags":["Review","Reinforcement Learning","Process-Supervised RL","Large Language Models","Reasoning Models","Attention Mechanism","Efficient Exploration","Adaptive Sampling","Off-Policy Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai 핵심 연구 목표 본 논문은"},{"id":"2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective","title":"[논문리뷰] Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective","excerpt":"arXiv에 게시된 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective","tags":["Review","Reinforcement Learning","Large Language Models","Planning","Policy Gradient","Q-learning","Supervised Fine-Tuning","Diversity Collapse","Reward Hacking"],"text":"링크: 논문 PDF로 바로 열기 저자: Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, ShangHua Teng, Li Dong, Yaru Hao, Wei Chen 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)의 계획 능력 향상을 위한 강화 학습(RL) 방법론 의 이점과 한계를 이론적으로 분석하는 것을 목표로 합니다"},{"id":"2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software","title":"[논문리뷰] BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software","excerpt":"arXiv에 게시된 'BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software","tags":["Review","LLM Agents","Open-Source Software","Compilation","Benchmarking","Software Engineering","Error Resolution","Retrieval-Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zehua Zhang, Ati Priya Bajaj, Divij Handa, Siyu Liu, Arvind S Raj, Hongkai Chen, Hulin Wang, Yibo Liu, Zion Leonahenahe Basque, Souradip Nath, Vishal Juneja, Nikhil Chapre, Yan S"},{"id":"2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs","title":"[논문리뷰] Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs","excerpt":"normanpaulsen이 arXiv에 게시한 'Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs","tags":["Review","Large Language Models","Context Window","Effective Context Window","Model Performance","Hallucination Rates","RAG Systems","Token Limits"],"text":"링크: 논문 PDF로 바로 열기 저자: Norman Paulsen 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM) 공급자가 홍보하는 최대 컨텍스트 윈도우(MCW) 와 실제 사용 환경에서의 최대 유효 컨텍스트 윈도우(MECW) 간의 불일치를 해결하고자 합니다. 연구는 MECW 를 정의하고, 다양한 컨텍스트 크기 및 문제 유형에 따른 LLM의 효과성 저하"},{"id":"2025-10-1-DA2-Depth-Anything-in-Any-Direction","title":"[논문리뷰] DA^2: Depth Anything in Any Direction","excerpt":"arXiv에 게시된 'DA^2: Depth Anything in Any Direction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-DA2-Depth-Anything-in-Any-Direction","tags":["Review","Panoramic Depth Estimation","Zero-shot Generalization","Data Curation","SphereViT","Spherical Geometry","360-degree Imaging","Vision Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Haodong Li, Wangguandong Zheng, Jing He, Yuhao Liu, Xin Lin, Xin Yang, YingCong Chen, Chunchao Guo 핵심 연구 목표 파노라마 깊이 추정 분야에서 데이터 부족 , 제로샷 일반화 성능 저하 , 그리고 구형 왜곡 처리의 비효율성 이라는 세 가지 주"},{"id":"2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","title":"[논문리뷰] DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder","excerpt":"arXiv에 게시된 'DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","tags":["Review","Video Generation","Diffusion Models","Video Autoencoder","Deep Compression","Model Acceleration","Fine-tuning","Latent Space","Temporal Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyu Chen, Wenkun He, Yuchao Gu, Yuyang Zhao, Jincheng Yu, Junsong Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Muyang Li, Haocheng Xi, Ligeng Zhu, Enze Xie, Song Han, Han Cai 핵심"},{"id":"2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively","title":"[논문리뷰] DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively","excerpt":"arXiv에 게시된 'DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively","tags":["Review","AI Scientist","Autonomous Scientific Discovery","Bayesian Optimization","LLM-based Agents","SOTA-Surpassing","Findings Memory","Exploration-Exploitation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, Yue Zhang 핵심 연구 목표 본 논문은 기존 AI 과학자 시스템의 한계, 특히 인간이 정의한 문제에 대한 과학적으로 가치 있는 기여 부족을 해결하고자 합니다. 이를 위해 DeepScienti"},{"id":"2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","title":"[논문리뷰] Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention","excerpt":"arXiv에 게시된 'Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","tags":["Review","Audio-Visual Speech Separation","Deep Learning","Efficiency","Discrete Lip Semantics","Global-Local Attention","Lightweight Models","VQ-VAE"],"text":"링크: 논문 PDF로 바로 열기 저자: Kai Li, Kejun Gao, Xiaolin Hu 핵심 연구 목표 오디오비주얼 음성 분리(AVSS) 분야에서 기존 모델들의 높은 연산 비용과 파라미터 수로 인해 발생하는 실용적 배포의 한계를 해결하는 것을 목표로 합니다. 특히, 시각 인코더의 '경로 의존성' 문제와 반복적 분리기의 추론 지연 문제를 극복하면서 분리"},{"id":"2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting","title":"[논문리뷰] EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting","excerpt":"arXiv에 게시된 'EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting","tags":["Review","Time Series Forecasting","Transformer","Dynamic Patching","Entropy","Predictive Uncertainty","Adaptive Encoding","Attention Mechanisms","Causal Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Sachith Abeywickrama, Emadeldeen Eldele, Min Wu, Xiaoli Li, Chau Yuen 핵심 연구 목표 기존 Transformer 기반 시계열 예측 모델들이 사용하는 temporalagnostic 패칭 방식은 시간적 일관성을 해치고 단기 종속성을 파괴하며 훈련추론 불일치를 야기하는"},{"id":"2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning","title":"[논문리뷰] Estimating Time Series Foundation Model Transferability via In-Context Learning","excerpt":"Jun Qi이 arXiv에 게시한 'Estimating Time Series Foundation Model Transferability via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning","tags":["Review","Time Series Foundation Models","Transferability Estimation","In-Context Learning","Tabular Foundation Models","Model Selection","Entropy Profile","Meta-learning","Forecasting"],"text":"링크: 논문 PDF로 바로 열기 저자: Qingren Yao, Ming Jin, Chengqi Zhang, ChaoHan Huck Yang, Jun Qi, Shirui Pan 핵심 연구 목표 이 논문은 증가하는 시계열 파운데이션 모델(TSFM) 중에서 특정 하위 태스크에 가장 적합한 모델을 효율적으로 식별하는 문제를 해결하고자 합니다. 특히, 제한된 데이터"},{"id":"2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents","title":"[논문리뷰] Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents","excerpt":"arXiv에 게시된 'Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents","tags":["Review","GUI Agents","On-Device AI","Multimodal LLM","GUI Grounding","GUI Navigation","Reinforcement Learning","Supervised Fine-tuning","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhen Yang, ZiYi Dou, Di Feng, Forrest Huang, Anh Nguyen, Keen You, Omar Attia, Yuhao Yang, Michael Feng, Haotian Zhang, Ram Ramrakhya, Chao Jia, Jeffrey Nichols, Alexander Toshev"},{"id":"2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss","title":"[논문리뷰] Humanline: Online Alignment as Perceptual Loss","excerpt":"arXiv에 게시된 'Humanline: Online Alignment as Perceptual Loss' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss","tags":["Review","LLM Alignment","Online RLHF","Offline RLHF","Prospect Theory","Perceptual Loss","Human-Centric AI","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Sijia Liu, Niklas Muennighoff, Kawin Ethayarajh 핵심 연구 목표 본 논문은 온라인 정렬(예: GRPO )이 오프라인 정렬(예: DPO )보다 성능이 뛰어난 이유를 행동 경제학의 전망 이론(prospect theory) 에 기반한 인간 중심적 관점에서 설명하고자 합니다. 궁극적으로 "},{"id":"2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance","title":"[논문리뷰] IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance","excerpt":"arXiv에 게시된 'IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance","tags":["Review","Diffusion Models","Multimodal Alignment","MLLM","Image Re-generation","Preference Learning","Implicit Guidance","Text-to-Image"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiayi Guo, Chuanhao Yan, Xingqian Xu, Yulin Wang, Kai Wang, Gao Huang, Humphrey Shi 핵심 연구 목표 확산 모델(Diffusion Models)에서 생성된 이미지와 입력 프롬프트 간의 정확한 멀티모달 정렬(multimodal alignment) 부족 문제"},{"id":"2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents","title":"[논문리뷰] InfoAgent: Advancing Autonomous Information-Seeking Agents","excerpt":"arXiv에 게시된 'InfoAgent: Advancing Autonomous Information-Seeking Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents","tags":["Review","LLM Agents","Information Seeking","Reinforcement Learning","Data Synthesis","Web Search Tools","Tool Use","Deep Research Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Gongrui Zhang, Jialiang Zhu, Ruiqi Yang, Kai Qiu, Miaosen Zhang, Zhirong Wu, Qi Dai, Bei Liu, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Yuan Zhang, Xin Li, "},{"id":"2025-10-1-Knowledge-Homophily-in-Large-Language-Models","title":"[논문리뷰] Knowledge Homophily in Large Language Models","excerpt":"Nedim Lipka이 arXiv에 게시한 'Knowledge Homophily in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Knowledge-Homophily-in-Large-Language-Models","tags":["Review","LLM","Knowledge Homophily","Graph Neural Networks","Knowledge Graph","Knowledge Injection","Question Answering","Fine-tuning","Knowledge Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Utkarsh Sahu, Zhisheng Qi, Mahantesh Halappanavar, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt, Yu Zhang, Yao Ma, Yu Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 인간의 뇌와 유사하게 지식 동질성(Kno"},{"id":"2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers","title":"[논문리뷰] LayerD: Decomposing Raster Graphic Designs into Layers","excerpt":"Kota Yamaguchi이 arXiv에 게시한 'LayerD: Decomposing Raster Graphic Designs into Layers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers","tags":["Review","Graphic Design","Image Decomposition","Layer Extraction","Image Matting","Background Completion","Deep Learning","Creative AI","Dynamic Time Warping"],"text":"링크: 논문 PDF로 바로 열기 저자: Tomoyuki Suzuki, KangJun Liu, Naoto Inoue, Kota Yamaguchi 핵심 연구 목표 본 논문은 합성된 래스터 그래픽 디자인 이미지에서 레이어 정보를 복원하여 디자이너가 편집하기 어려운 문제를 해결하고자 합니다. 래스터 그래픽 디자인을 재편집 가능한 레이어 시퀀스로 자동 분해함으로써,"},{"id":"2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs","title":"[논문리뷰] Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs","excerpt":"arXiv에 게시된 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs","tags":["Review","AI-Generated Videos","Deepfake Detection","Multimodal LLMs","Human Perception","Video Generation Evaluation","Spatiotemporal Annotation","Reward Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, "},{"id":"2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training","title":"[논문리뷰] Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training","excerpt":"Koustuv Sinha이 arXiv에 게시한 'Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training","tags":["Review","LLM Visual Priors","Language Pre-training","Multimodal LLM","Data Mixture Optimization","Reasoning Prior","Perception Prior","VQA","MLE-Bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Koustuv Sinha, Yufan Ren, David Fan, Shengbang Tong, Junlin Han 핵심 연구 목표 본 논문은 텍스트 전용 사전 훈련을 통해 대규모 언어 모델(LLM)이 시각적 세계에 대해 습득하는 내재된 시각적 사전 지식(visual priors)의 구조와 기원 을 체계적으로 밝히는 것"},{"id":"2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification","title":"[논문리뷰] MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification","excerpt":"Zhiming Luo이 arXiv에 게시한 'MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification","tags":["Review","Adversarial Purification","Diffusion Models","Frequency Domain","Adaptive Noise Injection","Robustness","Image Security","Magnitude Spectrum"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoyi Huang, Junwei Wu, Kejia Zhang, Carl Yang, Zhiming Luo 핵심 연구 목표 기존 확산 모델 기반의 적대적 정화(Adversarial Purification, AP) 방식이 균일한 노이즈 주입으로 인해 이미지의 의미론적 구조를 손상시키고 강건성을 저해하는 문제를 해결하는"},{"id":"2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use","title":"[논문리뷰] MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use","excerpt":"arXiv에 게시된 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use","tags":["Review","LLM Agents","Model Context Protocol","Benchmark","Tool Use","CRUD Operations","Workflow Automation","Stress Testing","Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijian Wu, Xiangyan Liu, Xinyuan Zhang, Lingjun Chen, Fanqing Meng, Lingxiao Du, Yiran Zhao, Fanshi Zhang, Yaoqi Ye, Jiawei Wang, Zirui Wang, Jinjie Ni, Yufan Yang, Arvin Xu, Mic"},{"id":"2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning","title":"[논문리뷰] Mem-α: Learning Memory Construction via Reinforcement Learning","excerpt":"Yuzhen Mao이 arXiv에 게시한 'Mem-α: Learning Memory Construction via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning","tags":["Review","LLM Agents","External Memory","Reinforcement Learning","Memory Management","Long-Context Understanding","Tool Learning","RAG","Memory Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, Xiaojian Wu 핵심 연구 목표 대규모 언어 모델(LLM) 에이전트의 제한된 컨텍스트 윈도우 문제를 해결하기 위해, 기존의 외부 메모리 시스템이 사전에 정의된 규칙에만 의"},{"id":"2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models","title":"[논문리뷰] More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models","excerpt":"Fabian Waschkowski이 arXiv에 게시한 'More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models","tags":["Review","Vision-Language Models","Multimodal Reasoning","Reasoning","Visual Forgetting","Perceptual Grounding","Reinforcement Learning","Policy Optimization","Visual Anchors"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, Jing Zhang 핵심 연구 목표 이 논문은 VisionLanguage Models (VLMs)의 추론이 논리적 추론을 강화하지만, 기본적인 시각적 질"},{"id":"2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation","title":"[논문리뷰] MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation","excerpt":"Limin Wang이 arXiv에 게시한 'MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation","tags":["Review","Image-to-Video Generation","Motion Transfer","Retrieval-Augmented Generation (RAG)","In-Context Learning","Diffusion Models","Video Diffusion","Motion Realism"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang 핵심 연구 목표 본 연구는 기존 이미지투비디오(ImagetoVideo) 생성 모델이 시각적 충실도는 높지만, 물리적으로 그럴듯하고 의미론적으로 일관된 동작을 생성하는 데 어려움을 겪는 문제를 해결하는 것을 목표로 합"},{"id":"2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents","title":"[논문리뷰] OceanGym: A Benchmark Environment for Underwater Embodied Agents","excerpt":"arXiv에 게시된 'OceanGym: A Benchmark Environment for Underwater Embodied Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents","tags":["Review","Underwater Robotics","Embodied AI","Benchmark Environment","Multi-modal Large Language Models","Autonomous Underwater Vehicles","Perception","Decision-Making","Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen 핵심 연구 목표 본 연구는 해저 환경의 낮은 가시성, "},{"id":"2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always","title":"[논문리뷰] OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!","excerpt":"arXiv에 게시된 'OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always","tags":["Review","Large Language Models (LLMs)","Operational Safety","Out-of-Domain (OOD)","Prompt Steering","Jailbreak Attacks","Evaluation Benchmark","Refusal Rate"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 운영 안전성(operational safety) 이라는 중요한 측면을 다룹니다. 이는 LLM 기"},{"id":"2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark","title":"[논문리뷰] Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark","excerpt":"Penghao Zhu이 arXiv에 게시한 'Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark","tags":["Review","AI Reasoning","Physics Research","LLM Evaluation","Scientific Benchmark","Frontier Physics","Problem Solving","Model Reliability","Auto-grading"],"text":"링크: 논문 PDF로 바로 열기 저자: Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, et al. 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)이 고등학교 수준의 수학 및 코딩 과제에서는 진전을 보였지만, 현대 물리학 연구에서 발생하는 복잡하고 개방형의 난제들을 얼마나 효과적"},{"id":"2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation","title":"[논문리뷰] ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation","excerpt":"Antonio Liotta이 arXiv에 게시한 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation","tags":["Review","Video-Language Model","Proficiency Estimation","Multi-View Video","Action Quality Assessment","Lightweight Model","Generative Feedback"],"text":"링크: 논문 PDF로 바로 열기 저자: Edoardo Bianchi, Jacopo Staiano, Antonio Liotta 핵심 연구 목표 본 논문은 기존의 블랙박스 비디오 분류기가 다중 시점(multiview) 컨텍스트를 무시하고 설명 가능성이 부족하다는 문제점을 해결하고자 합니다. 궁극적으로는 사람의 기술 숙련도를 예측할 뿐만 아니라, 전문가 수준의 "},{"id":"2025-10-1-Regression-Language-Models-for-Code","title":"[논문리뷰] Regression Language Models for Code","excerpt":"arXiv에 게시된 'Regression Language Models for Code' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Regression-Language-Models-for-Code","tags":["Review","Regression Language Model","Code Performance Prediction","Static Analysis","Neural Architecture Search","Text-to-Text Regression","Multi-task Learning","T5Gemma","ONNX"],"text":"링크: 논문 PDF로 바로 열기 저자: Yash Akhauri, Xingyou Song, Arissa Wongpanich, Bryan Lewandowski, Mohamed S. Abdelfattah 핵심 연구 목표 본 논문은 다양한 프로그래밍 언어 및 컴파일 수준의 코드 실행으로부터 메모리 사용량, 지연 시간, 신경망 정확도 와 같은 수치적 메트릭을 예측하"},{"id":"2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models","title":"[논문리뷰] Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models","excerpt":"arXiv에 게시된 'Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models","tags":["Review","Test-Time Training (TTT)","Foundation Models","Underparameterization","Sparse Autoencoders (SAE)","Linear Representation Hypothesis (LRH)","Specialization","Scaling Laws","In-Distribution Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Jonas Hübotter, Patrik Wolf, Alexander Shevchenko, Dennis Jüni, Andreas Krause, Gil Kur 핵심 연구 목표 본 논문은 대규모 파운데이션 모델에서 TestTime Training (TTT) 의 효과를 심층적으로 이해하고, 특히 모델이 이미 학습한 indi"},{"id":"2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation","title":"[논문리뷰] Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation","excerpt":"arXiv에 게시된 'Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation","tags":["Review","Video Generation","Evaluation Framework","Cinematic Control","Taxonomy","Human Annotation","Vision-Language Models","Text-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Agneet Chatterjee, Rahim Entezari, Max Lapin, Reshinth Adithyan, Maksym Zhuravinskyi, Chitta Baral, Yezhou Yang, Amit Raj, Varun Jampani 핵심 연구 목표 본 논문은 기존 비디오 생성 모델 및 벤치마크가 전문적인 "},{"id":"2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics","title":"[논문리뷰] TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics","excerpt":"Szu-Chi Chen이 arXiv에 게시한 'TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics","tags":["Review","Audio Language Models","Cultural Sound Understanding","Localized Benchmark","Non-semantic Audio","Human-in-the-loop","Multimodal AI","Taipei Soundscape"],"text":"링크: 논문 PDF로 바로 열기 저자: SzuChi Chen, YuehHsuan Huang, JiaKai Dong, YuHua Chen, YiCheng Lin 핵심 연구 목표 AI 모델이 지역별 문화적 맥락을 이해하고 비의미론적(nonsemantic) 음향 신호를 해석하는 능력의 부족을 해결하는 것을 목표로 합니다. 특히, 전역적으로 수집된 데이터셋의 한계"},{"id":"2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training","title":"[논문리뷰] TTT3R: 3D Reconstruction as Test-Time Training","excerpt":"Anpei Chen이 arXiv에 게시한 'TTT3R: 3D Reconstruction as Test-Time Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training","tags":["Review","3D Reconstruction","Test-Time Training (TTT)","Recurrent Neural Networks (RNN)","Online Learning","Length Generalization","Associative Memory","State Update Rule"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen 핵심 연구 목표 본 논문은 최신 RNN 기반 3D 재구성 모델 이 긴 시퀀스에 적용될 때 발생하는 길이 일반화(length generalization) 부족 과 재앙적 망각(catastrophic forge"},{"id":"2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs","title":"[논문리뷰] Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs","excerpt":"Yao Shu이 arXiv에 게시한 'Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs","tags":["Review","Large Language Models","Multi-turn Interaction","Test-Time Adaptation","Reinforcement Learning from Human Feedback","Policy Optimization","Online Learning","Self-Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenxing Wei, Hong Wang, Ying He, Fei Yu, Yao Shu 핵심 연구 목표 논문은 LLM이 정적, 단일 턴 데이터로 훈련되어 확장된 다중 턴 상호작용에서 성능이 저하되고 실시간 사용자 피드백에 적응하기 어려운 문제를 해결하고자 합니다. 이를 위해 추론 시 사용자 선호도에 맞춰 정책을 동적"},{"id":"2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain","title":"[논문리뷰] The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain","excerpt":"arXiv에 게시된 'The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain","tags":["Review","Large Language Models","Brain-Inspired AI","Graph Neural Networks","Hebbian Learning","Scale-Free Networks","Model Interpretability","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Adrian Kosowski, Przemysław Uznański, Jan Chorowski, Zuzanna Stamirowska, Michał Bartoszkiewicz 핵심 연구 목표 본 논문은 기존 Transformer 모델이 CoT (ChainofThought) 추론 의 일반화와 뇌 기능에 대한 미시적 해석을 "},{"id":"2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training","title":"[논문리뷰] Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training","excerpt":"arXiv에 게시된 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training","tags":["Review","Mechanistic Interpretability","Attention Heads","Post-Training","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Circuit Analysis","Reasoning Models","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Yein Park, Minbyul Jeong, Jaewoo Kang 핵심 연구 목표 대규모 추론 모델의 후처리 훈련(PostTraining) 기법(SFT, RL 등)이 모델의 추론 능력 향상에 기여하는 내부 아키텍처 메커니즘의 불투명성을 해소하는 것이 주요 목표입니다. 특히, 이러한 훈련 과정에서 기능적으로 특화된 어"},{"id":"2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning","title":"[논문리뷰] TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning","excerpt":"arXiv에 게시된 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning","tags":["Review","LLM Hallucination","Truthfulness","Reinforcement Learning","Ternary Reward","Abstention","Knowledge Boundary","GRPO","RLHF"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhepei Wei, Xiao Yang, Kai Sun, Jiaqi Wang, Rulin Shao, Sean Chen, Mohammad Kachuee, Teja Gollapudi, Tony Liao, Nicolas Scheffer, Rakesh Wanga, Anuj Kumar, Yu Meng, Wentau Yih, X"},{"id":"2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play","title":"[논문리뷰] Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play","excerpt":"Jing Shi이 arXiv에 게시한 'Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play","tags":["Review","Vision-Language Models (VLMs)","Self-Play","Reinforcement Learning","Gamification","Data Efficiency","Strategic Reasoning","Multimodal AI","Self-Improvement"],"text":"링크: 논문 PDF로 바로 열기 저자: Qinsi Wang, Bo Liu, Tianyi Zhou, Jing Shi, Yueqian Lin, Yiran Chen, Hai Helen Li, Kun Wan, Wentian Zhao 핵심 연구 목표 VisionLanguage Models (VLMs)의 훈련이 고비용의 수동 주석 데이터셋 에 과도하게 의존하여 확장성"},{"id":"2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes","title":"[논문리뷰] VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes","excerpt":"Muhammad Huzaifa이 arXiv에 게시한 'VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes","tags":["Review","Visual Question Answering","Multimodal Models","Dense Scenes","Fine-Grained Perception","Benchmark","Error Analysis","Counting","OCR"],"text":"링크: 논문 PDF로 바로 열기 저자: Paul Gavrikov, Wei Lin, M. Jehanzeb Mirza, Soumya Jahagirdar, Muhammad Huzaifa, Sivan Doveh, Serena YeungLevy, James Glass, Hilde Kuehne 핵심 연구 목표 현재 시각 언어 모델(VLM) 벤치마크가 밀집된 고해상도 "},{"id":"2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications","title":"[논문리뷰] VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications","excerpt":"arXiv에 게시된 'VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications","tags":["Review","LLM Agents","Benchmarking","Interactive Tasks","Real-world Applications","Tool Use","Multi-turn Conversation","Task Complexity"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengcheng Han, Dengchang Zhao, Hongyan Hao, Hui Su, Kefeng Zhang, Man Gao, Qi Gu, Wei He, Xi Su, Xiaodong Cai, Xueyuan Hao, Xunliang Cai, Yu Yang, Yueqing Sun, Yunke Zhao, Zhika"},{"id":"2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap","title":"[논문리뷰] Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap","excerpt":"Hengfan Zhang이 arXiv에 게시한 'Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap","tags":["Review","Voice AI","LLM","Reasoning","Benchmark","Modality Gap","Latency","Speech Recognition","Generative AI","Real-time Systems","Conversational AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yueqian Lin, Zhengmian Hu, Qinsi Wang, Yudong Liu, Hengfan Zhang, Jayakumar Subramanian, Nikos Vlassis, Hai “Helen” Li, Yiran Chen 핵심 연구 목표 본 논문은 실시간 대화 제약 조건 하에서 음성 대화형 시스템의 추론 "},{"id":"2025-10-1-Who-invented-deep-residual-learning","title":"[논문리뷰] Who invented deep residual learning?","excerpt":"Juergen Schmidhuber이 arXiv에 게시한 'Who invented deep residual learning?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Who-invented-deep-residual-learning","tags":["Review","Deep Learning History","Residual Connections","Recurrent Neural Networks (RNN)","Long Short-Term Memory (LSTM)","Feedforward Neural Networks (FNN)","Highway Networks","ResNet","Vanishing Gradient"],"text":"링크: 논문 PDF로 바로 열기 저자: Juergen Schmidhuber 핵심 연구 목표 이 논문은 깊은 잔여 학습(deep residual learning) 의 발명 및 진화에 대한 명확한 연대기를 확립하고, 그 핵심 원리와 주요 개발을 주로 Schmidhuber 연구실의 연구, 특히 Sepp Hochreiter의 1991년 학위 논문 과 이후의 LST"},{"id":"2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments","title":"[논문리뷰] Who's Your Judge? On the Detectability of LLM-Generated Judgments","excerpt":"arXiv에 게시된 'Who's Your Judge? On the Detectability of LLM-Generated Judgments' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments","tags":["Review","LLM-as-a-judge","Judgment Detection","Bias Quantification","Feature Engineering","Interpretability","Peer Review","AI Ethics","Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Dawei Li, Zhen Tan, Chengshuai Zhao, Bohan Jiang, Baixiang Huang, Pingchuan Ma, Abdullah Alnaibari, Kai Shu, Huan Liu 핵심 연구 목표 본 논문은 LLM이 생성한 평가(judgment)를 인간의 평가와 구별하는 판단 탐지(jud"},{"id":"2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning","title":"[논문리뷰] Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning","excerpt":"Yue Min이 arXiv에 게시한 'Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning","tags":["Review","LLM SFT","Data Pruning","Sample Pruning","Token Pruning","Error-Uncertainty Plane","Q-Tuning","Data Efficiency","Dynamic Pruning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaobo Wang, Jiaming Wang, Jiajun Zhang, Cong Wang, Yue Min, Zichen Wen, Fei Huang, Huiqiang Jiang, Junyang Lin, Dayiheng Liu, Linfeng Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 Super"},{"id":"2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching","title":"[논문리뷰] d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching","excerpt":"Jiarui Wang이 arXiv에 게시한 'd^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching","tags":["Review","Diffusion Models","Large Language Models (LLMs)","Inference Acceleration","KV Cache","Bidirectional Attention","Adaptive Caching","Token Selection"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, Xu Yang 핵심 연구 목표 확산 기반 대규모 언어 모델(dLLM)은 양방향 어텐션 구조 때문에 표준 KeyValue(KV) 캐시 의 이점을 활용하지 못해 추론 효율성이 떨어진다는 문"},{"id":"2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs","title":"[논문리뷰] dParallel: Learnable Parallel Decoding for dLLMs","excerpt":"arXiv에 게시된 'dParallel: Learnable Parallel Decoding for dLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs","tags":["Review","Diffusion Language Models","Parallel Decoding","Inference Acceleration","Certainty Distillation","Self-Distillation","Masked Language Models","LLaDA"],"text":"링크: 논문 PDF로 바로 열기 저자: Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang 핵심 연구 목표 본 연구는 확산 언어 모델(dLLMs)이 가진 병렬 디코딩 잠재력 을 충분히 활용하지 못하는 문제, 즉 기존 dLLMs가 성능 유지를 위해 거의 토큰 길이만큼의 디코딩 스텝을 요구하는 병목 현"},{"id":"2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking","title":"[논문리뷰] jina-reranker-v3: Last but Not Late Interaction for Document Reranking","excerpt":"arXiv에 게시된 'jina-reranker-v3: Last but Not Late Interaction for Document Reranking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-01 14:04:08+0900","permalink":"/ai/review/2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking","tags":["Review","Document Reranking","Last but Not Late Interaction","Multilingual","Transformer Architecture","Cross-Encoder","InfoNCE Loss","Contextual Embedding","Qwen3"],"text":"링크: 논문 PDF로 바로 열기 저자: Feng Wang, Yuqing Li, Han Xiao 핵심 연구 목표 본 논문은 문서 리랭킹에서 효율성과 효과성 사이의 근본적인 트레이드오프를 해결하고자 합니다. 특히, 기존의 Late Interaction 모델들이 인코딩 이후 상호작용을 지연시키는 문제를 극복하고, 질의와 문서 간의 풍부한 상호작용을 인코딩 과정에"},{"id":"2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning","title":"[논문리뷰] A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning","excerpt":"arXiv에 게시된 'A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning","tags":["Review","Question Answering","Reinforcement Learning","Large Language Models","Ambiguity Resolution","Multi-hop QA","Automated Data Generation","Tool-Augmented LLMs","AnsF1 Reward"],"text":"링크: 논문 PDF로 바로 열기 저자: Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Fan Zhou, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin 핵심 연구 목표 본 논문은 기존 QA 모델들이 여러 유효한 답변을 허용하는 모호한 질"},{"id":"2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation","title":"[논문리뷰] ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation","excerpt":"arXiv에 게시된 'ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation","tags":["Review","3D Reconstruction","Monocular SLAM","Gaussian Splatting","Level of Detail (LoD)","Feed-Forward Models","Structured Scene Representation","Real-time","High-Fidelity"],"text":"링크: 논문 PDF로 바로 열기 저자: Guanghao Li, Kerui Ren, Linning Xu, Zhewen Zheng, Changjian Jiang, Xin Gao, Bo Dai, Jian Pu, Mulin Yu, Jiangmiao Pang 핵심 연구 목표 본 논문은 단안 이미지 시퀀스에서 고효율 및 고품질의 실시간 3D 재구성 을 달성하는 것을 "},{"id":"2025-10-10-Agent-Learning-via-Early-Experience","title":"[논문리뷰] Agent Learning via Early Experience","excerpt":"arXiv에 게시된 'Agent Learning via Early Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Agent-Learning-via-Early-Experience","tags":["Review","Language Agents","Early Experience","Reward-Free Learning","World Modeling","Self-Reflection","Imitation Learning","Reinforcement Learning","Out-of-Domain Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianw"},{"id":"2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization","title":"[논문리뷰] Beyond Outliers: A Study of Optimizers Under Quantization","excerpt":"arXiv에 게시된 'Beyond Outliers: A Study of Optimizers Under Quantization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization","tags":["Review","Quantization","Optimizers","LLM","Post-Training Quantization (PTQ)","Quantization-Aware Training (QAT)","Error Propagation","Scaling Laws","Shampoo"],"text":"링크: 논문 PDF로 바로 열기 저자: Georgios Vlassis, Saleh Ashkboos, Alexandra Volkova, Torsten Hoefler, Dan Alistarh 핵심 연구 목표 대규모 언어 모델(LLMs)의 효율적인 배포를 위해 Quantization 이 필수가 됨에 따라, 옵티마이저 선택 이 양자화 성능에 미치는 영향을 체계적으"},{"id":"2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window","title":"[논문리뷰] Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window","excerpt":"Yaojie Lu이 arXiv에 게시한 'Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window","tags":["Review","Deep Search Agents","Dynamic Context Window","Reinforcement Learning","Long-horizon Interaction","Context Management","High-difficulty Tasks","Multi-turn Reasoning","Web Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiaoyu Tang, Hao Xiang, Le Yu, Bowen Yu, Yaojie Lu 핵심 연구 목표 본 논문은 기존의 다중 턴 에이전트가 낮은 태스크 복잡도와 컨텍스트 관리의 한계로 인해 장기적인 상호작용에서 깊은 추론 능력을 발휘하지 못하는 문제를 해결하고자 합니다. 특히, 기존 데이터셋이 얕은 정보 검색에 "},{"id":"2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards","title":"[논문리뷰] CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards","excerpt":"Yijiang Li이 arXiv에 게시한 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards","tags":["Review","Multi-Agent Systems","LLM Agents","Self-Evolution","Reinforcement Learning","Interaction Rewards","LLM-as-a-Judge","Decentralized Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트들이 외부 감독 없이 에이"},{"id":"2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy","title":"[논문리뷰] DeepPrune: Parallel Scaling without Inter-trace Redundancy","excerpt":"arXiv에 게시된 'DeepPrune: Parallel Scaling without Inter-trace Redundancy' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy","tags":["Review","Parallel Scaling","Chain-of-Thought","LLM Reasoning","Dynamic Pruning","Inter-trace Redundancy","Judge Model","Resource Efficiency","Answer Diversity"],"text":"링크: 논문 PDF로 바로 열기 저자: Shangqing Tu, Yaxuan Li, Yushi Bai, Lei Hou, Juanzi Li 핵심 연구 목표 논문은 LLM의 병렬 추론(parallel reasoning)에서 발생하는 심각한 intertrace redundancy 문제 를 해결하고, 높은 성능을 유지하면서도 계산 효율성을 대폭 향상 시키는 것을 "},{"id":"2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model","title":"[논문리뷰] DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model","excerpt":"Li Yi이 arXiv에 게시한 'DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model","tags":["Review","Dexterous Manipulation","In-Hand Rotation","Sim-to-Real Transfer","Neural Dynamics Model","Joint-Wise Learning","Autonomous Data Collection","Reinforcement Learning","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Xueyi Liu, He Wang, Li Yi, Y. Ma, Roberto Calandra, Jitendra Malik 핵심 연구 목표 본 연구는 컨택트(contact)가 풍부한 인핸드 객체 회전(inhand object rotation) 태스크에서 발생하는 심투리얼(simtoreal) 격차 의 근본적인 문제를 해결하"},{"id":"2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints","title":"[논문리뷰] Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints","excerpt":"Huazhe Xu이 arXiv에 게시한 'Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints","tags":["Review","Entropy Regularization","Activation Functions","Continuous Control","Large Language Models","Image Classification","Reinforcement Learning","Policy Stochasticity","Entropy Constraints"],"text":"링크: 논문 PDF로 바로 열기 저자: Zilin Kang, Chonghua Liao, Tingqiang Xu, Huazhe Xu 핵심 연구 목표 논문은 기존의 엔트로피 정규화 방식들이 최적화 목표를 왜곡하거나 특정 도메인에만 적용 가능한 한계를 지적하며, 범용적이고 비침습적이며 이론적으로 근거 있는 새로운 엔트로피 제약 패러다임을 제안하는 것을 목표로 합"},{"id":"2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization","title":"[논문리뷰] Fidelity-Aware Data Composition for Robust Robot Generalization","excerpt":"Liliang Chen이 arXiv에 게시한 'Fidelity-Aware Data Composition for Robust Robot Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization","tags":["Review","Robot Generalization","Data Augmentation","Out-of-Distribution (OOD)","Shortcut Learning","Information Fidelity","Data Composition","Diffusion Models","Multi-View Video Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, Ling Shao 핵심 연구 목표 본 논문은 대규모 시각적으로 균질한 데이터셋으로 훈련된 로봇 정책이 Shortcut Learning 에 취약하여 O"},{"id":"2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models","title":"[논문리뷰] First Try Matters: Revisiting the Role of Reflection in Reasoning Models","excerpt":"Wee Sun Lee이 arXiv에 게시한 'First Try Matters: Revisiting the Role of Reflection in Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models","tags":["Review","Large Language Models (LLMs)","Reasoning","Chain-of-Thought (CoT)","Reflection","Early Stopping","Supervised Fine-tuning (SFT)","Token Efficiency","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Liwei Kang, Yue Deng, Yao Xiao, Zhanfeng Mo, Wee Sun Lee, Lidong Bing 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 과정에서 \"반영(reflection)\"의 실제 기여도를 체계적으로 분석하는 것을 목표로 합니다. 특히, 모델이 이미 후보 답변을 생성"},{"id":"2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning","title":"[논문리뷰] From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning","excerpt":"Feiwei Qin이 arXiv에 게시한 'From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning","tags":["Review","Multi-Agent System","Chemical Reaction Prediction","Explainable AI","Evidence-Based Reasoning","Large Language Models","Tool-Augmented LLMs","Scientific Discovery"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Yang¹, Jiaxuan Lu², Haiyuan Wan²³, Junchi Yu⁴, Feiwei Qin¹ 핵심 연구 목표 본 논문은 화학 반응 조건 추천에서 단순히 \"무엇(what)\"을 예측하는 것을 넘어 \"왜(why)\" 특정 조건이 적절한지에 대한 설명 가능한 근거 를 제공하는 것을 목표로 합니다. 기존 "},{"id":"2025-10-10-GCPO-When-Contrast-Fails-Go-Gold","title":"[논문리뷰] GCPO: When Contrast Fails, Go Gold","excerpt":"arXiv에 게시된 'GCPO: When Contrast Fails, Go Gold' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-GCPO-When-Contrast-Fails-Go-Gold","tags":["Review","Reinforcement Learning","LLMs Reasoning","Policy Optimization","Contrastive Learning","Chain of Thought","Reference Answers","Math Reasoning","Gold-Standard Answer"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Wu, Wei Liu 핵심 연구 목표 본 논문은 기존 강화 학습 방법론, 특히 Group Relative Policy Optimization (GRPO) 이 모델의 추론 한계에 갇혀 샘플 활용 효율성이 떨어지는 문제점을 해결하고자 합니다. 모델이 문제를 해결하지 못해 올바른 샘플이 생성되지 않을 때 발생하는 정"},{"id":"2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense","title":"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense","excerpt":"arXiv에 게시된 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense","tags":["Review","Reinforcement Learning","Reward Modeling","Large Language Models (LLMs)","Mathematical Reasoning","Sparse Rewards","Dense Rewards","Hybrid Reinforcement","Verifier-based Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu, Yixuan Li, Jason E Weston, Ping Yu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 추론 훈련에서 결정론적 검증기(deterministic checkers) 의 이진(0"},{"id":"2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance","title":"[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance","excerpt":"Xinghui Li이 arXiv에 게시한 'InstructX: Towards Unified Visual Editing with MLLM Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance","tags":["Review","Visual Editing","MLLM Guidance","Diffusion Models","Image Editing","Video Editing","Unified Framework","Multimodal AI","Instruction-based Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He 핵심 연구 목표 컴퓨터 비전 분야에서 Multimodal Large Language Models (MLLM) 의 강력한 시각 이해 및 추론 능력을 활용하여"},{"id":"2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions","title":"[논문리뷰] LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions","excerpt":"arXiv에 게시된 'LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions","tags":["Review","LLM Misalignment","Dishonesty","Deception","Finetuning","Human-AI Interaction","Biased Feedback","Emergent Behavior"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuhao Hu, Peng Wang, Xiaoya Lu, Dongrui Liu, Xuanjing Huang, Jing Shao 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)에서 발생하는 \" emergent misalignment\" 현상이 윤리적 또는 규범적 행동을 넘어 고위험 시나리오에서의 비정직성(dishon"},{"id":"2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency","title":"[논문리뷰] Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency","excerpt":"Jintao Zhang이 arXiv에 게시한 'Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency","tags":["Review","Diffusion Distillation","Consistency Models","Score Regularization","Large-Scale Generative Models","Text-to-Image","Text-to-Video","Model Acceleration","JVP"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, MingYu Liu, Jun Zhu, Qinsheng Zhang 핵심 연구 목표 본 논문은 연속 시간 일관성 증류 (sCM) 를 대규모 텍스트투이미지 (T2"},{"id":"2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks","title":"[논문리뷰] Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks","excerpt":"arXiv에 게시된 'Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks","tags":["Review","LLM Agents","Continuous Learning","Self-Evolving","Memory Module","Long-Horizon Planning","Productivity Tasks","Test-Time Learning","Experience Replay"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li 핵심 연구 목표 본 논문은 실세계의 복잡한 장기(long"},{"id":"2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs","title":"[논문리뷰] Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs","excerpt":"Franck Dernoncourt이 arXiv에 게시한 'Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs","tags":["Review","LLM Routing","Contextual Bandits","Bandit Feedback","Multi-objective Optimization","Preference-tuning","Policy Gradient","Cost-efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Wang Wei, Tiankai Yang, Hongjie Chen, Yue Zhao, Franck Dernoncourt, Ryan A. Rossi, Hoda Eldardiry 핵심 연구 목표 대규모 LLM 배포 환경에서 각 쿼리당 최적의 LLM을 효율적으로 선택하는 문제를 해결하는 것이 목표입니다. 기존 라우터들이 배"},{"id":"2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling","title":"[논문리뷰] LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling","excerpt":"arXiv에 게시된 'LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling","tags":["Review","Reward Model","Long Context","LLM Alignment","Multi-stage Training","Context Window Scaling","Preference Learning","Long-RewardBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Zecheng Tang, Baibei Ji, Quantong Qiu, Haitian Wang, Xiaobo Liang, Juntao Li, Min Zhang 핵심 연구 목표 현재의 Reward Model (RM)은 주로 짧은 컨텍스트에 국한되며 응답의 유용성이나 안전성과 같은 표면적인 속성에만 집중하고 있습니다. 본 "},{"id":"2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward","title":"[논문리뷰] Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward","excerpt":"arXiv에 게시된 'Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward","tags":["Review","Reinforcement Learning","LLM Exploration","Verifiable Reward","Low-Probability Regularization","Reasoning Sparks","Policy Entropy","KL Divergence","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou 핵심 연구 목표 본 논문은 Verifiable Reward를 사용하는 RL(RLVR) 환경에서 Large Lan"},{"id":"2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization","title":"[논문리뷰] MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization","excerpt":"vanilla1116이 arXiv에 게시한 'MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization","tags":["Review","Multimodal LLMs","Reflective Reasoning","Long-Chain Reasoning","Benchmark","Policy Optimization","Data Generation","Reinforcement Learning","Backtracking"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangyu Zhao, Junming Lin, Tianhao Liang, Yifan Zhou, Wenhao Chai, Yuzhe Gu, Weiyun Wang, Kai Chen, Gen Luo, Wenwei Zhang, Junchi Yan, Hua Yang, Haodong Duan, Xue Yang 핵심 연구 목표 현"},{"id":"2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model","title":"[논문리뷰] MemMamba: Rethinking Memory Patterns in State Space Model","excerpt":"Xiao Sun이 arXiv에 게시한 'MemMamba: Rethinking Memory Patterns in State Space Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model","tags":["Review","State Space Models","Mamba","Long-sequence modeling","Memory decay","State summarization","Cross-layer attention","Perplexity","Linear complexity"],"text":"링크: 논문 PDF로 바로 열기 저자: Youjin Wang, Jiahao Yan, Yangjingyi Chen, Jiaxuan Lu, Xiao Sun 핵심 연구 목표 본 논문은 기존 Mamba와 같은 State Space Model (SSM) 이 가지는 장거리 메모리 지수적 감쇠 문제를 체계적으로 분석하고, 이러한 한계를 극복하여 선형 복잡도를 유지하면서"},{"id":"2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens","title":"[논문리뷰] Memory Retrieval and Consolidation in Large Language Models through Function Tokens","excerpt":"arXiv에 게시된 'Memory Retrieval and Consolidation in Large Language Models through Function Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens","tags":["Review","Large Language Models","LLM Interpretability","Function Tokens","Memory Retrieval","Memory Consolidation","Sparse Autoencoders","Pre-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaohua Zhang, Yuan Lin, Hang Li 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs) 내에서 기억 검색(memory retrieval) 및 기억 통합(memory consolidation) 메커니즘이 어떻게 작동하는지에 대한 이해 부족을 해결하는 것을 목표로 합니다. 특히, 함수 토큰(fu"},{"id":"2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning","title":"[논문리뷰] Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning","excerpt":"arXiv에 게시된 'Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning","tags":["Review","Meta-Awareness","Reinforcement Learning","Self-Alignment","LLM Reasoning","Training Efficiency","Generalization","Predictive Gating"],"text":"링크: 논문 PDF로 바로 열기 저자: Yoonjeon Kim, Doohyuk Jang, Eunho Yang 핵심 연구 목표 대규모 언어 모델(LLM)의 메타 인식(metaawareness) 능력 부족으로 인한 심각한 불일치(misalignment) 문제를 해결하고, 메타 예측(metaprediction)과 실제 롤아웃(rollout) 간의 정렬을 통해 추"},{"id":"2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints","title":"[논문리뷰] NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints","excerpt":"arXiv에 게시된 'NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints","tags":["Review","Multimodal Large Language Models","Native MLLMs","Scaling Laws","Data Constraints","Visual Encoder","LLM Initialization","Mixture-of-Experts","End-to-end Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai 핵심 연구 목표 본 논문은 "},{"id":"2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents","title":"[논문리뷰] NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents","excerpt":"Baixuan Xu이 arXiv에 게시한 'NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents","tags":["Review","LLM Agents","Scientific Law Discovery","Benchmarking","Metaphysical Shifts","Interactive Environments","Exploration-Exploitation","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianshi Zheng, Kelvin KiuWai Tam, Newt HueNam K. Nguyen, Baixuan Xu, Zhaowei Wang, et al. 핵심 연구 목표 기존 과학 법칙 발견 벤치마크들이 겪는 과학적 관련성, 확장성, 암기 저항성 간의 방법론적 딜레마 를 해결하고, 정적인 함수 피팅을 넘어 복잡"},{"id":"2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation","title":"[논문리뷰] R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation","excerpt":"Zheng Zhu이 arXiv에 게시한 'R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation","tags":["Review","Robotic Manipulation","Data Augmentation","Spatial Generalization","3D Data Generation","Imitation Learning","Point Cloud","Real-to-Real","Mobile Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiuwei Xu, Angyuan Ma, Hankun Li, Bingyao Yu, Zheng Zhu, Jie Zhou, Jiwen Lu 핵심 연구 목표 본 연구는 로봇 매니퓰레이션에서 공간적 일반화 를 위한 방대한 인간 시연 데이터 의 필요성을 해결하고자 합니다. 기존 데이터 생성 방법론들이 겪는 SimtoReal 갭"},{"id":"2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training","title":"[논문리뷰] Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training","excerpt":"Peng Cheng이 arXiv에 게시한 'Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training","tags":["Review","Mixture-of-Experts","Large Language Models","Checkpoint Recycling","Model Growth","Efficient Pretraining","Depth Growth","Width Growth","Sunk Cost"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruizhe Wang, Yucheng Ding, Xiao Liu, Peng Cheng, Baining Guo, Zhengjun Zha, Yaoxiang Wang, Yeyun Gong 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 사전 훈련의 급증하는 계산 비용 문제를 해결하기 위해, 기존의 사전 훈련된 체크포인"},{"id":"2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization","title":"[논문리뷰] Reinforcing Diffusion Models by Direct Group Preference Optimization","excerpt":"Jing Tang이 arXiv에 게시한 'Reinforcing Diffusion Models by Direct Group Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization","tags":["Review","Diffusion Models","Reinforcement Learning","Preference Optimization","Group Preference","Direct Preference Optimization","ODE Samplers","Efficient Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Yihong Luo, Tianyang Hu, Jing Tang 핵심 연구 목표 본 논문은 효율적인 확산 모델 을 그룹 상대 선호도 에 기반하여 정렬하는 과정에서 발생하는 핵심적인 문제를 해결합니다. 기존 GRPO(Group Relative Policy Optimization) 와 같은 강화 학습 방법론은 확률적 정책을"},{"id":"2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation","title":"[논문리뷰] SViM3D: Stable Video Material Diffusion for Single Image 3D Generation","excerpt":"arXiv에 게시된 'SViM3D: Stable Video Material Diffusion for Single Image 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation","tags":["Review","Single Image 3D Reconstruction","Material Prediction","Video Diffusion Models","Physically Based Rendering (PBR)","Inverse Rendering","Novel View Synthesis","Camera Control","Latent Diffusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Andreas Engelhardt, Mark Boss, Vikram Voletti, ChunHan Yao, Hendrik P. A. Lensch, Varun Jampani 핵심 연구 목표 본 논문은 단일 이미지로부터 다중 시점 일관성 있는 PBR(Physically Based Rendering) 재질(알베도, 러프니스"},{"id":"2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models","title":"[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models","excerpt":"Mohit Bansal이 arXiv에 게시한 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models","tags":["Review","Video Reasoning","Multimodal AI","Scientific Research","Large Multimodal Models","Benchmark","Quantitative Reasoning","Domain Knowledge","Visual Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena YeungLevy, Xiaohan Wang 핵심 연구 목표 기존 비디오 벤치마크들이 일반 시나리오와 단순 추론에 집중하여 최신 대규모 멀티모달 모델(LMM)"},{"id":"2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models","title":"[논문리뷰] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models","excerpt":"James Cheng이 arXiv에 게시한 'Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models","tags":["Review","Large Language Models","Reinforcement Learning","Sentence Embedding","Retrieval-Augmented Generation","Chain-of-Thought","Information Retrieval","Supervised Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuntao Gui, James Cheng 핵심 연구 목표 본 논문은 Large Language Models (LLMs)의 강력한 추론 능력이 검색(retrieval) 작업에서 충분히 활용되지 못하는 문제를 해결하고자 합니다. LLM의 추론 프로세스와 검색 임베딩 생성 과정을 통합하여, 복잡한 의미 분석을 통해 보다 "},{"id":"2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction","title":"[논문리뷰] Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction","excerpt":"arXiv에 게시된 'Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction","tags":["Review","Text-to-Sounding Video Generation","Diffusion Models","Dual-tower Architecture","Cross-modal Fusion","Visual Grounding","Hierarchical Captioning","Cross-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaisi Guan, Xi Hua, Zhengfeng Lai, Xin Cheng, Peng Zhang, Kieran Liu, Ruihua Song, Meng Cao. 핵심 연구 목표 본 논문은 텍스트로부터 사운딩 비디오를 생성하는 TexttoSounding Video (T2SV) 연구에서 발생하는 두 가지 근본적인 문"},{"id":"2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety","title":"[논문리뷰] The Alignment Waltz: Jointly Training Agents to Collaborate for Safety","excerpt":"arXiv에 게시된 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety","tags":["Review","LLM Safety","Multi-agent Reinforcement Learning","Safety Alignment","Overrefusal","Adversarial Attacks","Feedback Agent","Conversation Agent","Dynamic Improvement Reward"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, Hongyuan Zhan 핵심 연구 목표 대규모 언어 모델(LLM)이 "},{"id":"2025-10-10-Towards-Scalable-and-Consistent-3D-Editing","title":"[논문리뷰] Towards Scalable and Consistent 3D Editing","excerpt":"Pan Zhou이 arXiv에 게시한 'Towards Scalable and Consistent 3D Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Towards-Scalable-and-Consistent-3D-Editing","tags":["Review","3D Editing","Generative Models","Transformer Architecture","Dataset Generation","Multimodal Learning","Conditional Generation","Image-to-3D"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruihao Xia, Yang Tang, Pan Zhou 핵심 연구 목표 3D 에셋의 기하학적 형태나 외관을 로컬하게 수정하는 3D 편집 태스크에서 발생하는 주요 난제들을 해결하는 것을 목표로 합니다. 특히, 기존 방법들의 느린 처리 속도, 기하학적 왜곡 발생 가능성, 그리고 오류에 취약한 수동 3D 마스크 의존성 등"},{"id":"2025-10-10-Training-Free-Group-Relative-Policy-Optimization","title":"[논문리뷰] Training-Free Group Relative Policy Optimization","excerpt":"arXiv에 게시된 'Training-Free Group Relative Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization","tags":["Review","LLM Agents","Reinforcement Learning","Parameter-Free Optimization","Experiential Knowledge","Token Prior","Group Relative Policy Optimization","In-Context Learning","Cost-Effective AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, Xing Sun 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트가 외부 "},{"id":"2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG","title":"[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG","excerpt":"arXiv에 게시된 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG","tags":["Review","Multimodal RAG","Document AI","Benchmark","Information Retrieval","Large Language Models","Multimodal Embeddings","PDF Processing","Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangyu Peng, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, ChienSheng Wu 핵심 연구 목표 본 논문은 문서 중심의 멀티모달 RAG(RetrievalAugmented Generation) 시스템 평가를 위한 기존 벤치마크들의 한계(파편화된 평가, 단순화된 멀티모달 "},{"id":"2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections","title":"[논문리뷰] UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections","excerpt":"Boqian Li이 arXiv에 게시한 'UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections","tags":["Review","3D Human Reconstruction","Unconstrained Photos","Data Rectifier","Multi-View Generation","Pose-Correlated Feature Aggregation","SMPL-X","Diffusion Models","Virtual Try-On"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyu Cai, Ziyang Li, Xiaoben Li, Boqian Li, Zeyu Wang, Zhenyu Zhang, Yuliang Xiu 핵심 연구 목표 논문은 제약 없는(unconstrained) 2D 사진 컬렉션 으로부터 고품질의 3D 의상 착용 인물 재구성 을 위한 튜닝프리(tuningfree) 솔루션을 "},{"id":"2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution","title":"[논문리뷰] UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution","excerpt":"arXiv에 게시된 'UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution","tags":["Review","Video Super-Resolution","Multi-Modal Generation","Latent Diffusion Models","Cascaded Framework","Condition Injection","Text-to-Video","Video Editing","4K Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji 핵심 연구 목표 본 논문은 기존의 캐스케이드(cascaded) 비디오 초해상화(VSR) 모델이 텍스트투비디오(texttovideo) 작업에 한정되어 다양한 생성 조건을"},{"id":"2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos","title":"[논문리뷰] UniVideo: Unified Understanding, Generation, and Editing for Videos","excerpt":"Xintao Wang이 arXiv에 게시한 'UniVideo: Unified Understanding, Generation, and Editing for Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos","tags":["Review","Unified Multimodal Model","Video Generation","Video Editing","MLLM","Diffusion Transformer","In-Context Learning","Zero-shot Generalization","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen 핵심 연구 목표 기존의 통합 멀티모달 모델들이 이미지 도메인에 주로 한정되어 있고, 비디오 관련 작업은 태스크별 전문 모델에 의존하는 한계를 극복하고자 합"},{"id":"2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning","title":"[논문리뷰] VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning","excerpt":"Quande Liu이 arXiv에 게시한 'VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning","tags":["Review","Video Completion","Spatio-Temporal Control","In-Context Conditioning","Video Diffusion Models","RoPE Interpolation","VAE","Unified Framework","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minghong Cai, Qiulin Wang, Zongli Ye, Wenze Liu, Quande Liu, Weicai Ye, Xintao Wang, Pengfei Wan, Kun Gai, Xiangyu Yue 핵심 연구 목표 본 논문은 사용자가 지정한 임의의 공간 및 시간 위치에 패치를 배치하여 비디오를 생성하는 "},{"id":"2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs","title":"[논문리뷰] When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs","excerpt":"arXiv에 게시된 'When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-10 13:53:45+0900","permalink":"/ai/review/2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs","tags":["Review","Long-Context LMs","Multi-hop Reasoning","Thought Templates","Retrieval-Augmented Generation","Natural Language Feedback","Knowledge-intensive QA","Reasoning Reuse"],"text":"링크: 논문 PDF로 바로 열기 저자: Soyeong Jeong, Taehee Jung, Sung Ju Hwang, JooKyung Kim, Dongyeop Kang 핵심 연구 목표 본 논문은 LongContext Language Models (LCLMs) 이 방대한 문맥을 처리할 수 있음에도 불구하고, 복잡한 다중 홉(multihop) 추론을 위해 증거를"},{"id":"2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks","title":"[논문리뷰] A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks","excerpt":"Fanchao Qi이 arXiv에 게시한 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks","tags":["Review","Long-Horizon Tasks","LLM Agents","Global Planning","Reinforcement Learning","Supervised Fine-tuning","Homologous Consensus Filtering","Executor Capability Gain Reward","Plan-and-Execute"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuzheng Si, Haozhe Zhao, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트가 긴 시간 범위의 태스크에서 글로벌 플래닝 능력 부족 으로 인해 겪"},{"id":"2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall","title":"[논문리뷰] ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall","excerpt":"Jiaqi Tang이 arXiv에 게시한 'ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall","tags":["Review","Knowledge Editing","LLMs","Multi-hop Reasoning","Mechanistic Interpretability","Neuron-level Attribution","Factual Recall","Transformer Networks"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiayu YANG, Songning LAI, Yuxuan FAN, Shengen WU, Jiaqi TANG, Chun KANG, Zhijiang GUO, Yutao YUE 핵심 연구 목표 대규모 언어 모델(LLMs)의 지식 편집(KE) 과정에서 다중 홉 사실 회상(multihop factual recall) 성능이 "},{"id":"2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping","title":"[논문리뷰] ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping","excerpt":"Wenbo Hu이 arXiv에 게시한 'ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping","tags":["Review","Multimodal Reasoning","Adaptive Learning","Reinforcement Learning","Entropy Shaping","Difficulty-Aware","Chain-of-Thought","Token-Level Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Shawn Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, Nanyun Peng 핵심 연구 목표 멀티모달 대규모 추론 모델(MLRMs)이 쉬운 문제에 대해 과도하게 추론하여 비효율적인 반면, 어"},{"id":"2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols","title":"[논문리뷰] Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols","excerpt":"Maksym Andriushchenko이 arXiv에 게시한 'Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols","tags":["Review","AI Control Protocols","LLM Monitors","Adaptive Attacks","Prompt Injection","Jailbreaking","Red Teaming","Scalable Oversight"],"text":"링크: 논문 PDF로 바로 열기 저자: Mikhail Terekhov, Alexander Panfilov, Daniil Dzenhaliou, Caglar Gulcehre, Maksym Andriushchenko, Ameya Prabhu, Jonas Geiping 핵심 연구 목표 본 연구는 신뢰할 수 없는 LLM 에이전트가 안전 메커니즘을 우회하여 AI 제어"},{"id":"2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion","title":"[논문리뷰] AutoPR: Let's Automate Your Academic Promotion!","excerpt":"Yixin Yuan이 arXiv에 게시한 'AutoPR: Let's Automate Your Academic Promotion!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion","tags":["Review","Academic Promotion","Large Language Models","Multi-Agent Systems","Scholarly Communication","Multimodal Processing","Benchmark","Content Generation","Social Media Marketing"],"text":"링크: 논문 PDF로 바로 열기 저자: Yixin Yuan, Libo Qin, Mingda Yang, Zheng Yan, Qiguang Chen 핵심 연구 목표 최근 학술 연구의 양이 급증하면서 연구자들은 자신의 논문을 효과적으로 홍보하고 가시성 및 인용을 확보하는 데 상당한 시간과 노력을 투자해야 합니다. 이 논문은 이러한 수동적인 홍보 과정의 비효율성을"},{"id":"2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models","title":"[논문리뷰] Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models","excerpt":"arXiv에 게시된 'Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models","tags":["Review","Unpaired Multimodal Learning","Unimodal Representation","Weight Sharing","Cross-modal Transfer","Fisher Information","Self-supervised Learning","Multimodal Neurons","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka, Phillip Isola 핵심 연구 목표 본 논문은 기존 멀티모달 학습이 paired datasets 에 크게 의존하는 한계를 해결하고자 합니다. unpaired auxiliary multimodal da"},{"id":"2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution","title":"[논문리뷰] BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution","excerpt":"Hange Liu이 arXiv에 게시한 'BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution","tags":["Review","Code Generation","Human Preference","LLM Evaluation","Execution Feedback","Benchmarking","Crowdsourcing","Software Engineering","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Terry Yue Zhuo, Xiaolong Jin, Hange Liu, Ahsen Khaliq 핵심 연구 목표 코드 생성 대형 언어 모델(LLM)의 품질을 평가하는 기존 방법론의 한계를 해결하는 것이 이 연구의 핵심 목표입니다. 특히, 단순히 코드 스니펫을 읽거나 정적 분석에 의존하는 방식으로는 코드의 실제 기능성,"},{"id":"2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization","title":"[논문리뷰] Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization","excerpt":"Mahdi Ghaznavai이 arXiv에 게시한 'Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization","tags":["Review","Complexity OoD Generalization","System-1 Thinking","System-2 Reasoning","Kolmogorov Complexity","Inductive Biases","Large Language Models (LLMs)","Reasoning Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mahdi Samiei, Arash Marioriyad, Arman TahmasebiZadeh, Mohamadreza Fereydooni, Mahdi Ghaznavai, Mahdieh Soleymani Baghshah 핵심 연구 목표 본 논문은 AI, 특히 System2 유형의 추론 능력 을 정의하고 측정할 명확한 프"},{"id":"2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI","title":"[논문리뷰] D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI","excerpt":"Haebin Seong이 arXiv에 게시한 'D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI","tags":["Review","Embodied AI","Vision-Action Pretraining","Desktop Data","Inverse Dynamics Model (IDM)","Pseudo-labeling","Robotics","Generalization","Data Compression"],"text":"링크: 논문 PDF로 바로 열기 저자: Suhwan Choi, Jaeyoon Jung, Haebin Seong, Minchan Kim, Minyeong Kim, Yongjun Cho, Yoonshik Kim, Yubeen Park, Youngjae Yu†, Yunsung Lee† 핵심 연구 목표 본 논문은 물리적 상호작용 데이터 수집의 높은 비용으로 인해 "},{"id":"2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation","title":"[논문리뷰] DISCO: Diversifying Sample Condensation for Efficient Model Evaluation","excerpt":"arXiv에 게시된 'DISCO: Diversifying Sample Condensation for Efficient Model Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation","tags":["Review","Efficient Evaluation","Sample Condensation","Model Disagreement","Predictive Diversity","Performance Prediction","Large Language Models","Model Signatures","Meta-modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexander Rubinstein, Benjamin Raible, Martin Gubri, Seong Joon Oh 핵심 연구 목표 최신 머신러닝 모델, 특히 대규모 언어 모델(LLM) 의 평가에 소요되는 막대한 시간과 비용(수천 시간의 GPU 사용) 문제를 해결하는 것을 목표로 합니다. 기존 샘플 선택 방식이 복"},{"id":"2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting","title":"[논문리뷰] Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting","excerpt":"Julia Kempe이 arXiv에 게시한 'Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting","tags":["Review","Reinforcement Learning","Large Language Models","Reasoning Tasks","GRPO","Negative Samples","Reward Modeling","Confidence Reweighting","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunzhen Feng, Parag Jain, Anthony Hartshorn, Yaqi Duan, Julia Kempe 핵심 연구 목표 본 논문은 Group Relative Policy Optimization (GRPO) 기반의 LLM(대규모 언어 모델) 추론 학습 과정에서 \"음성 그룹\"(모든 샘플이 오답인 경우)이"},{"id":"2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents","title":"[논문리뷰] Dyna-Mind: Learning to Simulate from Experience for Better AI Agents","excerpt":"Qianhui Wu이 arXiv에 게시한 'Dyna-Mind: Learning to Simulate from Experience for Better AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents","tags":["Review","AI Agents","Reinforcement Learning","World Models","Simulation","Reasoning","Language Models","Planning","Interactive AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiao Yu, Baolin Peng, Michel Galley, Hao Cheng, Qianhui Wu, Janardhan Kulkarni, Suman Nath, Zhou Yu, Jianfeng Gao 핵심 연구 목표 AI 에이전트가 복잡하고 장기적인 대화형 태스크에서 \"대리 시행착오(vicarious trial a"},{"id":"2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare","title":"[논문리뷰] GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare","excerpt":"arXiv에 게시된 'GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare","tags":["Review","Large Language Models","LLM Alignment","Game Theory","Reinforcement Learning","Mutual Welfare","Payoff Matrix","Strategic Decision Making","Human-AI Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Siqi Zhu, David Zhang, Pedro CisnerosVelarde, Jiaxuan You 핵심 연구 목표 본 논문은 LLM이 사용자에게 최적화되지 않은 응답을 생성하여 개별적인 합리적 선택이 사회적으로 최적화되지 않은 결과를 초래하는 프리저너스 딜레마(prisoner's dilemma) 와 유사한 문제를"},{"id":"2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation","title":"[논문리뷰] Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation","excerpt":"Zekun Qi이 arXiv에 게시한 'Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation","tags":["Review","Self-supervised Monocular Depth Estimation","Foundation Models","CLIP","DINO","Language Guidance","Coarse-to-fine Learning","Feature Aggregation","3D Perception"],"text":"링크: 논문 PDF로 바로 열기 저자: Zekun Qi, Jiawei He, Bohan Li, Hongsi Liu, Wenyao Zhang 핵심 연구 목표 이 논문은 자기 지도(selfsupervised) 단안 깊이 추정(MDE)에서 기존 방법론의 한계를 극복하고자 합니다. 특히, CLIP 과 DINO 와 같은 파운데이션 모델의 잠재력을 최대한 활용하여, "},{"id":"2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes","title":"[논문리뷰] Instant4D: 4D Gaussian Splatting in Minutes","excerpt":"Li Lu이 arXiv에 게시한 'Instant4D: 4D Gaussian Splatting in Minutes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes","tags":["Review","4D Gaussian Splatting","Dynamic View Synthesis","Monocular Reconstruction","Visual SLAM","Grid Pruning","Real-time Rendering","GPU Memory Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Li Lu, Haoxi Ran, Zhanpeng Luo 핵심 연구 목표 본 논문은 보정되지 않은 단안 비디오 에서 동적 3D 장면을 재구성하는 데 있어 느린 최적화와 복잡한 파라미터 추정으로 인한 문제를 해결하는 것을 목표로 합니다. 4D 표현 을 활용하여 수분 내에 캐주얼 비디오 시퀀스를 효율적으로 처리하고, 교정된"},{"id":"2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone","title":"[논문리뷰] KORMo: Korean Open Reasoning Model for Everyone","excerpt":"arXiv에 게시된 'KORMo: Korean Open Reasoning Model for Everyone' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone","tags":["Review","Large Language Model","Korean","Bilingual","Synthetic Data","Fully Open Model","Tokenizer","Reasoning","Pretraining","Instruction Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Minjun Kim, Hyeonseok Lim, Hangyeol Yoo, Inho Won, Seungwoo Song, Minkyung Cho, Junghun Yuk, Changsu Choi, Dongjae Shin, Huije Lee, Hoyun Song, Alice Oh, Kyung Tae Lim 핵심 연구 목표 본"},{"id":"2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval","title":"[논문리뷰] MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval","excerpt":"Tingyu Song이 arXiv에 게시한 'MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval","tags":["Review","Multimodal Retrieval","Benchmark","Reasoning","Multidisciplinary","Expert-Level","Image-Text Interleaving","Contradiction Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyue Zhang, Yuan Gao, Xiao Zhou, Yilun Zhao, Tingyu Song, Arman Cohan, Anh Tuan Luu, Chen Zhao 핵심 연구 목표 기존 멀티모달 검색 벤치마크의 한계(일반 도메인, 단순 의미 매칭, 단일 이미지/단일 모달 문서)를 극복하고, 전문가 수준의 다학제"},{"id":"2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping","title":"[논문리뷰] Mitigating Overthinking through Reasoning Shaping","excerpt":"Wen Luo이 arXiv에 게시한 'Mitigating Overthinking through Reasoning Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping","tags":["Review","Large Reasoning Models (LRMs)","RLVR","Overthinking Mitigation","Reasoning Shaping","Segment-level Penalization","Computational Efficiency","Training Stability","Length-aware Weighting"],"text":"링크: 논문 PDF로 바로 열기 저자: Feifan Song, Shaohang Wei, Bofei Gao, Yejie Wang, Wen Luo, Wei Li, Linli Yao, Weimin Xiong, Liang Chen, Tianyu Liu, Houfeng Wang 핵심 연구 목표 본 논문은 Reinforcement Learning from Verifi"},{"id":"2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs","title":"[논문리뷰] Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs","excerpt":"arXiv에 게시된 'Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs","tags":["Review","Multimodal AI","Prompt Optimization","MLLMs","Bayesian Optimization","Cross-modal Alignment","Prompt Engineering","Generative AI","Exploration-Exploitation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yumin Choi, Dongki Kim, Jinheon Baek, Sung Ju Hwang 핵심 연구 목표 본 논문은 기존 프롬프트 최적화 방법론이 텍스트 모달리티에만 국한되어 Multimodal Large Language Models (MLLMs) 의 잠재력을 완전히 활용하지 못하는 한계를 해결하고자 합니다. 텍스"},{"id":"2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework","title":"[논문리뷰] One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework","excerpt":"Giuseppe Amato이 arXiv에 게시한 'One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework","tags":["Review","Zero-Shot Captioning","Region-Level Captioning","Vision Transformers","DINOv2","Patch-Centric","Modality Gap Mitigation","Visual-Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Lorenzo Bianchi, Giacomo Pacini, Fabio Carrara, Nicola Messina, Giuseppe Amato, Fabrizio Falchi 핵심 연구 목표 본 논문은 기존의 이미지 전체 기반(imagecentric) 제로샷 캡셔닝 모델이 지역 단위 캡셔닝에서 낮은 성능을 보이는 문제를 "},{"id":"2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models","title":"[논문리뷰] Parallel Test-Time Scaling for Latent Reasoning Models","excerpt":"arXiv에 게시된 'Parallel Test-Time Scaling for Latent Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models","tags":["Review","Latent Reasoning","Test-Time Scaling","Parallel Inference","Stochastic Sampling","Monte Carlo Dropout","Additive Gaussian Noise","Latent Reward Model","Trajectory Aggregation"],"text":"링크: 논문 PDF로 바로 열기 저자: Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li 핵심 연구 목표 본 논문은 latent reasoning models 가 연속적인 벡터 공간에서 추론을 수행함에도 불구하고, 기존 tokenbased models 처럼 parallel TestT"},{"id":"2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs","title":"[논문리뷰] PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs","excerpt":"Xu Zheng이 arXiv에 게시한 'PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs","tags":["Review","Multimodal Large Language Models (MLLMs)","Physical Tool Understanding","Benchmarking","Embodied AI","Visual Question Answering (VQA)","Tool Affordances","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zixin Zhang, Kanghao Chen, Xingwang Lin, Lutao Jiang, Xu Zheng, Yuanhuiyi Lyu, Litao Guo, Yinchuan Li, YingCong Chen 핵심 연구 목표 본 논문은 현대 다중 모달 대규모 언어 모델(MLLMs) 이 물리적 도구를 얼마나 깊이 이해하"},{"id":"2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction","title":"[논문리뷰] Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction","excerpt":"danxuhk이 arXiv에 게시한 'Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction","tags":["Review","3D Occupancy Prediction","Open Vocabulary","Gaussian Splatting","Transformer","Progressive Densification","Anisotropy-aware Sampling","Autonomous Driving"],"text":"링크: 논문 PDF로 바로 열기 저자: Chi Yan, Dan Xu 핵심 연구 목표 본 논문은 기존 3D 점유 예측 방법론이 고정된 카테고리에 국한되거나, 희소한 가우시안 표현이 세밀한 객체 묘사에 한계가 있고, 조밀한 표현은 높은 연산 비용을 수반하는 문제를 해결하고자 합니다. 특히 오픈 보케블러리(OpenVocabulary) 환경에서 효율적이면서도 세밀"},{"id":"2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition","title":"[논문리뷰] Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition","excerpt":"Shang-Tse Chen이 arXiv에 게시한 'Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition","tags":["Review","ASR","Pseudo-labeling","Domain Adaptation","Task Arithmetic","Correction Vector","Accent Adaptation","Speaker Clustering","Model Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: YiCheng Lin, YuHsuan Li Liang, Hsuan Su, TzuQuan Lin, ShangTse Chen, YunNung Chen, Hungyi Lee 핵심 연구 목표 본 논문은 ASR 도메인 적응 시 타겟 도메인의 실제 레이블(ground truth)이 없는 상황에서 pseudolabeling 으로 "},{"id":"2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth","title":"[논문리뷰] R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?","excerpt":"arXiv에 게시된 'R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth","tags":["Review","Long-Horizon Reasoning","Query Composition","Large Reasoning Models","Reinforcement Learning","Benchmark Evaluation","Thinking Budget","Performance Degradation","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Lu, Jianing Wang, Linsen Guo, Wei He, Hongyin Tang, Tao Gui, Xuanjing Huang, Xuezhi Cao, Wei Wang, Xunliang Cai 핵심 연구 목표 이 논문은 기존 벤치마크가 대규모 추론 모델(LRMs)의 복잡하고 상호 의존적인 장기 추론 능력을"},{"id":"2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review","title":"[논문리뷰] ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review","excerpt":"Christopher Pal이 arXiv에 게시한 'ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review","tags":["Review","Peer Review","AI-Assisted Review","Large Language Models","LLM Agents","Meta-Review","Conference Submissions","Reviewer Personas","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Gaurav Sahu, Hugo Larochelle, Laurent Charlin, Christopher Pal 핵심 연구 목표 과학 출판의 핵심인 피어 리뷰 과정에서 발생하는 불일치, 주관성, 확장성 문제를 해결하고, AI가 인간의 판단을 보완하는 체계적이고 일관된 평가를 제공할 수 있도록 AI 기반 피어 리뷰 시스"},{"id":"2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km","title":"[논문리뷰] SpaceVista: All-Scale Visual Spatial Reasoning from mm to km","excerpt":"Kaituo Feng이 arXiv에 게시한 'SpaceVista: All-Scale Visual Spatial Reasoning from mm to km' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km","tags":["Review","Spatial Reasoning","Multi-Scale Vision","MLLM","Dataset","Scale Experts","Reinforcement Learning","Computer Vision","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Peiwen Sun, Shiqiang Lang◇, Dongming Wu, Yi Ding, Kaituo Feng, Huadai Liu, Zhen Ye, Rui Liu♠, YunHui Liu, Jianan Wang†, Xiangyu Yue♠ 핵심 연구 목표 본 논문은 기존 공간 추론 모델들이 실내 3D 스캔 및 수동 어노"},{"id":"2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation","title":"[논문리뷰] Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation","excerpt":"Han Shi이 arXiv에 게시한 'Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation","tags":["Review","Autoregressive Models","Text-to-Image Generation","Inference Acceleration","Jacobi Decoding","Denoising Diffusion Models","Speculative Decoding","Multi-token Prediction","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yao Teng, Fuyun Wang, Xian Liu, Zhekai Chen, Yu Wang, Zhenguo Li, Weiyang Liu, Difan Zou, Han Shi, Xihui Liu 핵심 연구 목표 본 논문은 순차적인 토큰별 디코딩 과정으로 인해 수천 번의 모델 포워드 패스를 요구하는 자율회귀 텍스트투이미"},{"id":"2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics","title":"[논문리뷰] StatEval: A Comprehensive Benchmark for Large Language Models in Statistics","excerpt":"arXiv에 게시된 'StatEval: A Comprehensive Benchmark for Large Language Models in Statistics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics","tags":["Review","Statistical Reasoning","LLM Benchmark","Statistics Education","Proof Verification","Multi-agent Pipeline","Automated Extraction","Evaluation Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchen Lu, Run Yang, Yichen Zhang, Shuguang Yu, Runpeng Dai, Ziwei Wang, Jiayi Xiang, Wenxin E, Siran Gao, Xinyao Ruan, Yirui Huang, Chenjing Xi, Haibo Hu, Yueming Fu, Qinglan Yu"},{"id":"2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams","title":"[논문리뷰] StreamingVLM: Real-Time Understanding for Infinite Video Streams","excerpt":"Kelly Peng이 arXiv에 게시한 'StreamingVLM: Real-Time Understanding for Infinite Video Streams' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams","tags":["Review","Video Stream Understanding","Real-Time VLM","Attention Sink","KV Cache Management","Contiguous RoPE","Supervised Fine-tuning","Long-Context Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han 핵심 연구 목표 본 논문은 nearinfinite 비디오 스트림 을 이해하는 데 있어 기존 VLM이 겪는 높은 지연 시간과 메모리 사용량 증가 문제를 해결하는 것을 목표로 합니다"},{"id":"2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control","title":"[논문리뷰] TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control","excerpt":"Adityan Jothi이 arXiv에 게시한 'TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control","tags":["Review","Diffusion Models","Conditional Generation","LoRA","Hypernetwork","Dynamic Weight Adaptation","Generative AI","Controllable Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minkyoung Cho, Ruben Ohana, Christian Jacobsen, Adityan Jothi, MinHung Chen, Z. Morley Mao, Ethem Can 핵심 연구 목표 기존의 controllable diffusion model이 고정된 아키텍처와 정적인 컨디셔닝 전략을 사용하여 동적인 d"},{"id":"2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation","title":"[논문리뷰] Temporal Prompting Matters: Rethinking Referring Video Object Segmentation","excerpt":"Sifei Liu이 arXiv에 게시한 'Temporal Prompting Matters: Rethinking Referring Video Object Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation","tags":["Review","Referring Video Object Segmentation","Foundation Models","Prompt Engineering","Object Tracking","SAM","Video Analysis","Prompt Preference Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: CiSiang Lin, MinHung Chen, IJieh Liu, ChienYi Wang, Sifei Liu, YuChiang Frank Wang 핵심 연구 목표 논문은 Referring Video Object Segmentation (RVOS) 의 높은 계산 비용과 확장성 문제를 해결하고자 합니다. 특히, 기존 R"},{"id":"2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation","title":"[논문리뷰] Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation","excerpt":"Linyi Jin이 arXiv에 게시한 'Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation","tags":["Review","Unified Multimodal Model","Camera-Centric","Image Understanding","Image Generation","Spatial Reasoning","Camera Parameters","Instruction Tuning","Multimodal Spatial Intelligence"],"text":"링크: 논문 PDF로 바로 열기 저자: Kang Liao, Size Wu, Zhonghua Wu, Linyi Jin, Chao Wang, Yikai Wang, Fei Wang, Wei Li, Chen Change Loy 핵심 연구 목표 카메라 중심의 장면 이해와 생성을 별개의 문제로 다루던 기존 방식의 한계를 극복하고, 이를 단일 멀티모달 모델 로 통합하는"},{"id":"2025-10-13-Understanding-DeepResearch-via-Reports","title":"[논문리뷰] Understanding DeepResearch via Reports","excerpt":"Chengen Huang이 arXiv에 게시한 'Understanding DeepResearch via Reports' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports","tags":["Review","DeepResearch Agents","LLM-as-a-Judge","Report Evaluation","Agentic AI","Factuality","Redundancy","Research Automation","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyu Fan, Xinyao Niu, Yuxiang Zheng, Fengji Zhang, Chengen Huang, Bei Chen, Junyang Lin, Chao Huang 핵심 연구 목표 본 논문은 지식 집약적 연구 작업을 수행하는 DeepResearch 에이전트 의 복합적인 평가 문제에 주목합니다. 기존 "},{"id":"2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels","title":"[논문리뷰] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels","excerpt":"arXiv에 게시된 'Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels","tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Data Pipeline","Web-scale Data","Question-Answering (QA)","Data Generation","Data Diversity","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhepeng Cen, Haolin Chen, Shiyu Wang, Zuxin Liu, Zhiwei Liu, Ding Zhao, Silvio Savarese, Caiming Xiong, Huan Wang, Weiran Yao 핵심 연구 목표 대규모 언어 모델(LLM)이 모방 학습의 한계(훈련추론 격차, 견고한 추론 능"},{"id":"2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression","title":"[논문리뷰] Which Heads Matter for Reasoning? RL-Guided KV Cache Compression","excerpt":"Huan Wang이 arXiv에 게시한 'Which Heads Matter for Reasoning? RL-Guided KV Cache Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-13 13:44:18+0900","permalink":"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression","tags":["Review","KV Cache Compression","Large Language Models (LLMs)","Reinforcement Learning (RL)","Reasoning Models","Attention Heads","Chain-of-Thought (CoT)","Memory Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenjie Du, Li Jiang, Keda Tao, Xue Liu, Huan Wang 핵심 연구 목표 추론(reasoning) 기반 대규모 언어 모델(LLM)은 긴 CoT(ChainofThought) 생성을 통해 막대한 KV(KeyValue) 캐시 오버헤드를 발생시킵니다. 기존 KV 캐시 압축 방식이 추론 모델에서"},{"id":"2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models","title":"[논문리뷰] A Survey of Vibe Coding with Large Language Models","excerpt":"arXiv에 게시된 'A Survey of Vibe Coding with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models","tags":["Review","Vibe Coding","Large Language Models","Coding Agents","Human-AI Collaboration","Software Engineering","Development Models","Context Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuyao Ge, Lingrui Mei, Zenghao Duan, Tianhao Li, Yujia Zheng, Yiwei Wang, Lexin Wang, Jiayu Yao, Tianyu Liu, Yujun Cai, Baolong Bi, Fangda Guo, Jiafeng Guo, Shenghua Liu, Xueqi C"},{"id":"2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training","title":"[논문리뷰] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training","excerpt":"arXiv에 게시된 'Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training","tags":["Review","Pixel-space Generative Models","Diffusion Models","Consistency Models","Self-supervised Pre-training","End-to-end Training","Image Generation","FID","Representation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, Xiangxiang Chu 핵심 연구 목표 본 연구는 픽셀 공간(pixelspace) 기반 생성 모델이 잠재 공간(latentspace) 기반 모델에 비해 훈련이 어렵고 성능이 낮은"},{"id":"2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models","title":"[논문리뷰] Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models","excerpt":"arXiv에 게시된 'Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models","tags":["Review","Diffusion Large Language Models","Reinforcement Learning","Memory Efficiency","Monte Carlo Sampling","Log-Likelihood Approximation","Policy Optimization","ELBO"],"text":"링크: 논문 PDF로 바로 열기 저자: Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li, Tsinghua University 핵심 연구 목표 본 논문은 확산 대규모 언어 모델(dLLMs)에 강화 학습(RL)을 적용할 때 발생하는 주요 문제점, 즉 RL 목표에 필수적인 우도 함수의 계산 불가능성을 해결하는 것을 목표로 합니다."},{"id":"2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation","title":"[논문리뷰] DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation","excerpt":"arXiv에 게시된 'DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation","tags":["Review","Machine Translation Evaluation","Large Language Models (LLMs)","Web Novel Translation","Multi-Agent Systems","Cultural Nuance","Benchmark Dataset","Natural Language Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Enze Zhang, Jiaying Wang, Mengxi Xiao, Jifei Liu, Ziyan Kuang, Rui Dong, Eric Dong, Sophia Ananiadou, Min Peng, Qianqian Xie 핵심 연구 목표 본 연구는 웹 소설 번역에 대한 기존 기계 번역(MT) 평가 벤치마크들이 표면적"},{"id":"2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search","title":"[논문리뷰] DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search","excerpt":"arXiv에 게시된 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search","tags":["Review","Multimodal LLM","Web Search","Visual Question Answering","Reinforcement Learning","Image Cropping","Self-Correction","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan 핵심 연구 목표 기존 MLLM이 지식 집약적 시각 질의응답(VQA)에서 겪는 정보 부족, 정체된"},{"id":"2025-10-15-Detect-Anything-via-Next-Point-Prediction","title":"[논문리뷰] Detect Anything via Next Point Prediction","excerpt":"arXiv에 게시된 'Detect Anything via Next Point Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Detect-Anything-via-Next-Point-Prediction","tags":["Review","Multimodal Large Language Models","Object Detection","Coordinate Prediction","Reinforcement Learning","Supervised Fine-tuning","Visual Perception","Zero-shot Learning","Spatial Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, Lei Zhang 핵심 연구 목표 본 논문은 MLLM(Multimodal Large Language Model) 기반 객체 감지에서 발생하는 낮"},{"id":"2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs","title":"[논문리뷰] Dr.LLM: Dynamic Layer Routing in LLMs","excerpt":"arXiv에 게시된 'Dr.LLM: Dynamic Layer Routing in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs","tags":["Review","Dynamic Routing","LLMs","Adaptive Depth","Computational Efficiency","Monte Carlo Tree Search (MCTS)","Retrofittable Framework","Supervised Learning","Accuracy Improvement"],"text":"링크: 논문 PDF로 바로 열기 저자: Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh 핵심 연구 목표 대규모 언어 모델(LLM)이 모든 입력 토큰을 고정된 모든 레이어에 통과시키면서 발생하는 비효율성(쉬운 작업 시 연산 낭비)과 복잡한 추론 작업 시 유연성 부족 문제를 해결하는 것을 "},{"id":"2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning","title":"[논문리뷰] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning","excerpt":"arXiv에 게시된 'ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning","tags":["Review","Embodied AI","Vision Language Models (VLMs)","Reinforcement Learning (RL)","Prior Learning","Supervised Fine-tuning (SFT)","Embodied Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanyang Chen, Mark Zhao, Rui Yang, Qinwei Ma, Ke Yang, Jiarui Yao, Kangrui Wang, Hao Bai, Zhenhailong Wang, Rui Pan, Mengchao Zhang, Jose Barreiros, Aykut Onol, ChengXiang Zhai, "},{"id":"2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning","title":"[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding & Reasoning","excerpt":"arXiv에 게시된 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning","tags":["Review","Experiment Video Understanding","Multimodal Large Language Models (MLLMs)","Scientific Reasoning","Benchmark","Wet-Lab Experiments","Procedural Understanding","Fine-grained Perception","Video QA"],"text":"링크: 논문 PDF로 바로 열기 저자: Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Tianxiang Jiang, Yinan He, Manabu Okumura, Yi Wang 핵심 연구 목표 본 연구의 목표는 실제 과학 실험 영상, 특히 습식 "},{"id":"2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution","title":"[논문리뷰] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution","excerpt":"Yihao Liu이 arXiv에 게시한 'FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution","tags":["Review","Video Super-Resolution (VSR)","Diffusion Models","Real-time VSR","Streaming VSR","Sparse Attention","Distillation","Conditional Decoder","High-resolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue 핵심 연구 목표 본 논문은 확산 모델 기반 비디오 초해상도(VSR) 기술을 현실 세계에 적용 가능하도록 효율성, 확장성 및 실시간 성능을 확보하는 것을 목표로 합니다. 특히 높은"},{"id":"2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners","title":"[논문리뷰] HoneyBee: Data Recipes for Vision-Language Reasoners","excerpt":"arXiv에 게시된 'HoneyBee: Data Recipes for Vision-Language Reasoners' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners","tags":["Review","Vision-Language Models","Data Curation","Chain-of-Thought","VL Reasoning","Dataset Scaling","Supervised Finetuning","HONEYBEE","Test-Time Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Hritik Bansal, Devandra Singh Sachan, KaiWei Chang, Aditya Grover, Gargi Ghosh, Wentau Yih, Ramakanth Pasunuru 핵심 연구 목표 본 연구는 고성능 시각언어(VL) 추론 훈련 데이터셋 구축의 원리를 규명하고, 다양한 데이터 큐레이션 접"},{"id":"2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation","title":"[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation","excerpt":"arXiv에 게시된 'Information-Preserving Reformulation of Reasoning Traces for Antidistillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation","tags":["Review","Antidistillation","Reasoning Traces","Large Language Models","Knowledge Distillation","Information Preservation","Trace Reformulation","Supervised Fine-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiayu Ding, Lei Cui, Li Dong, Nanning Zheng, Furu Wei 핵심 연구 목표 대규모 언어 모델(LLMs)의 추론 흔적(reasoning traces)이 복잡한 작업에서 성능을 향상시키지만, 무단 지식 증류(distillation)에 취약하다는 문제를 해결하고자 합니다. 기존의 증류 "},{"id":"2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens","title":"[논문리뷰] LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens","excerpt":"arXiv에 게시된 'LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens","tags":["Review","Large Language Models (LLMs)","Machine Translation (MT)","Chain-of-Thought (CoT)","Knowledge Distillation","Fine-tuning","Prompt Engineering","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Armel Zebaze, Rachel Bawden & Benoît Sagot 핵심 연구 목표 대규모 추론 모델(LRM)의 \"사고 토큰\" 생성이 기계 번역(MT) 성능에 미치는 영향을 탐구하고, 표준 CoT 증류 방식과 MT 특정 모듈식 프롬프트 전략을 비교하여 어떤 형태의 중간 정보가 MT에 유익한지 밝히는 것을 목표"},{"id":"2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces","title":"[논문리뷰] MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces","excerpt":"Sungchul Kim이 arXiv에 게시한 'MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces","tags":["Review","Multimodal LLMs","UI Evaluation","Human Perception","Benchmarking","UX Research","MLLM-as-a-Judge","Cognitive Factors","Pairwise Comparison"],"text":"링크: 논문 PDF로 바로 열기 저자: Sungchul Kim, Samyadeep Basu, Ryan Rossi, Reuben A. Luera, Franck Dernoncourt 핵심 연구 목표 본 논문은 사용자 인터페이스(UI) 디자인 평가 과정에서 발생하는 리소스 제약을 해결하기 위해 Multimodal Large Language Models (MLLM"},{"id":"2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks","title":"[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks","excerpt":"Xueyuan Lin이 arXiv에 게시한 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks","tags":["Review","Long-Horizon Tasks","Agentic AI","Context Curation","Working Memory","Reinforcement Learning","Policy Optimization","Large Language Models","Memory-as-Action"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, Jitao Sang 핵심 연구 목표 본 논문은 LLM 기반 에이전트가 긴 작업(longhorizon tasks)을 수행할 때 제한된 작업 메모리 가 불필요하거나 관련 없는 컨텍스트에 의해 쉽게 과부하되는 문제"},{"id":"2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration","title":"[논문리뷰] One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration","excerpt":"Mohit Bansal이 arXiv에 게시한 'One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration","tags":["Review","Symbolic World Models","Stochastic Environments","Unguided Exploration","Probabilistic Programming","Law Synthesis","Crafter-OO","Program Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Zaid Khan, Archiki Prasad, Elias StengelEskin, Jaemin Cho, Mohit Bansal 핵심 연구 목표 본 논문은 복잡하고 확률적인 환경에서 제한된 상호작용 예산(\"one life\")과 인간의 보상/목표와 같은 외부 안내 없이 기호적 월드 모델을 학습하는 어려운 문제를 해결하는"},{"id":"2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability","title":"[논문리뷰] ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability","excerpt":"Tsui-Wei Weng이 arXiv에 게시한 'ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability","tags":["Review","Trustworthy AI","Large Reasoning Models (LRMs)","Interpretability","Faithfulness","Reliability","Chain-of-Thought (CoT)","Supervised Fine-tuning (SFT)","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: ChungEn Sun, Ge Yan, Akshay Kulkarni, TsuiWei Weng 핵심 연구 목표 논문은 기존 Long ChainofThought (CoT) 추론 모델 들이 답변 정확도와 토큰 효율성에만 집중하여 신뢰성(trustworthiness) 을 간과하는 문제를 해결하고자 합니다. 특히, 인간이 쉽게 "},{"id":"2025-10-15-Robot-Learning-A-Tutorial","title":"[논문리뷰] Robot Learning: A Tutorial","excerpt":"arXiv에 게시된 'Robot Learning: A Tutorial' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial","tags":["Review","Robot Learning","Reinforcement Learning","Imitation Learning","Behavioral Cloning","Vision-Language-Action Models","Diffusion Models","Transformers","LeRobot"],"text":"링크: 논문 PDF로 바로 열기 저자: Francesco Capuano, Caroline Pascal, Adil Zouitine, Thomas Wolf, Michel Aractingi 핵심 연구 목표 이 튜토리얼은 현대 로봇 학습의 발전 과정을 종합적으로 안내하여, 연구자와 실무자가 로봇 학습 분야의 개념적 이해와 실제 도구를 습득하도록 돕는 것을 목표로 "},{"id":"2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model","title":"[논문리뷰] SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model","excerpt":"arXiv에 게시된 'SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model","tags":["Review","Omni-modal Embedding","Multimodal Learning","Recommendation Systems","Hard Negative Mining","Contrastive Learning","Large Language Models (LLMs)","Data Balancing","Multitask Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: ByteDance Douyin SAIL Team, CUHK MMLab 핵심 연구 목표 기존 멀티모달 임베딩 모델의 한계인 제한된 모달리티 지원, 불안정한 학습 메커니즘, 산업 도메인 간극을 해결하는 것을 목표로 합니다. 이를 통해 다양한 실세계 시나리오에서 효과적인 옴니모달 임베딩(omnimodal embedding)"},{"id":"2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models","title":"[논문리뷰] SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models","excerpt":"arXiv에 게시된 'SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models","tags":["Review","Unified Multimodal Models","Self-Rewarding","Text-to-Image Generation","Image Understanding","Post-Training","Global-Local Reward","Compositional Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Weiyang Jin, Yuwei Niu, Chengqi Duan, Aoxue Li, Jiaqi Liao, Shenghua Gao, Xihui Liu 핵심 연구 목표 본 논문은 Unified Multimodal Models ( UMMs )이 이미지 이해 능력에 비해 이미지 생성 능력에서 현저한 격차를 보이는 문제에 주"},{"id":"2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning","title":"[논문리뷰] Scaling Language-Centric Omnimodal Representation Learning","excerpt":"arXiv에 게시된 'Scaling Language-Centric Omnimodal Representation Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning","tags":["Review","Multimodal Embeddings","MLLMs","Contrastive Learning","Cross-modal Alignment","Generative Pretraining","Representation Learning","Scaling Laws"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong 핵심 연구 목표 본 논문은 MLLM(Multimodal Large Language Model) 기반 임베딩 모델의 우수한 성능이 전통적인 CLIP스타일 모델 에 비해 가지는 근본적인"},{"id":"2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model","title":"[논문리뷰] Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model","excerpt":"arXiv에 게시된 'Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model","tags":["Review","Vision-Language-Action Models","Spatial Perception","Implicit Representation Alignment","3D Foundation Models","Robotics","Data Efficiency","Representation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li 핵심 연구 목표 본 논문은 2D 데이터로 사전 훈련된 VLA 모델이 3D 물리 세계에서 정확한 동작을 수행하는 데 필요한 공간 인식이 부족하다는"},{"id":"2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale","title":"[논문리뷰] SynthID-Image: Image watermarking at internet scale","excerpt":"arXiv에 게시된 'SynthID-Image: Image watermarking at internet scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale","tags":["Review","Image Watermarking","AI-Generated Content","Provenance","Robustness","Security","Deep Learning","Internet Scale","Post-hoc"],"text":"링크: 논문 PDF로 바로 열기 저자: Sven Gowal, Rudy Bunel, Florian Stimberg, David Stutz, Guillermo OrtizJimenez, et al. 핵심 연구 목표 본 논문은 AI 생성 이미지의 출처(provenance)를 인터넷 규모로 확립하기 위한 SynthIDImage 라는 딥러닝 기반의 비가시적 이미지 워"},{"id":"2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models","title":"[논문리뷰] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models","excerpt":"arXiv에 게시된 'Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models","tags":["Review","Diffusion Models","Generative Models","Guidance","On-Manifold Sampling","Temporal Alignment","Score Approximation Error","Training-Free Guidance"],"text":"링크: 논문 PDF로 바로 열기 저자: Youngrok Park, Hojung Jung, Sangmin Bae, SeYoung Yun 핵심 연구 목표 논문은 Diffusion 모델이 외부 가이던스(guidance)를 적용할 때 발생하는 'offmanifold' 현상으로 인해 생성된 샘플이 실제 데이터 manifold에서 벗어나 품질이 저하되는 문제를 해결하"},{"id":"2025-10-15-Tensor-Logic-The-Language-of-AI","title":"[논문리뷰] Tensor Logic: The Language of AI","excerpt":"Pedro Domingos이 arXiv에 게시한 'Tensor Logic: The Language of AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-Tensor-Logic-The-Language-of-AI","tags":["Review","Tensor Logic","Neurosymbolic AI","Logic Programming","Tensor Algebra","Deep Learning","Automated Reasoning","Embedding Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Pedro Domingos 핵심 연구 목표 AI 분야의 발전이 프로그래밍 언어의 한계로 인해 저해되고 있다는 문제의식에서 출발합니다. PyTorch나 TensorFlow와 같은 라이브러리가 자동 미분과 GPU 가속을 제공하지만, 자동 추론 및 지식 습득 기능이 부족하며, LISP나 Prolog 같은 심볼릭 AI 언어는"},{"id":"2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation","title":"[논문리뷰] UniFusion: Vision-Language Model as Unified Encoder in Image Generation","excerpt":"arXiv에 게시된 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation","tags":["Review","Vision-Language Model","Unified Encoder","Image Generation","Diffusion Models","Multimodal Learning","Text-to-Image","Image Editing","Zero-shot Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kevin (YuTeng) Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale (Adobe Applied Research) 핵심 연구 목표 기존 이미지 생성 모델들이 이미지와 텍스트에 대해 분리된 인코더를 사용하는 한계를 극복하고, 크로스모달 추론 및 지식 전이"},{"id":"2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution","title":"[논문리뷰] ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution","excerpt":"arXiv에 게시된 'ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution","tags":["Review","Multimodal Large Language Models (MLLMs)","Dynamic Resolution","Token Compression","Semantic Awareness","Visual Consistency Learning (ViCO)","Visual Resolution Router (ViR)","Inference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang 핵심 연구 목표 본 논문은 MLLM의 이미지 입력으로 인한 추론 비용 증가 문제를 해결하고, 이미지의 의미론적 복잡성 에 따라 가변"},{"id":"2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions","title":"[논문리뷰] What If : Understanding Motion Through Sparse Interactions","excerpt":"arXiv에 게시된 'What If : Understanding Motion Through Sparse Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-15 13:01:40+0900","permalink":"/ai/review/2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions","tags":["Review","Motion Understanding","Sparse Interactions","Multimodal Prediction","Flow Poke Transformer","Physical Scene Dynamics","Uncertainty Quantification","Generative Models","Computer Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Stefan Andreas Baumann, Nick Stracke, Timy Phan, Björn Ommer 핵심 연구 목표 논문은 물리적 장면의 동역학을 이해하는 것을 목표로 하며, 특히 국부적인 상호작용(\"pokes\")의 결과로 발생할 수 있는 잠재적인 변화의 다중 모드 분포 를 예측하고자 합니다. 기존의 단일 결"},{"id":"2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization","title":"[논문리뷰] Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization","excerpt":"arXiv에 게시된 'Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization","tags":["Review","LLM Reasoning","Attention Mechanisms","Reinforcement Learning","Credit Assignment","Policy Optimization","Interpretability","Preplan-and-Anchor Rhythm","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Li, Zhichen Dong, Yuhan Sun, Weixun Wang, Shaopan Xiong, Yijia Luo, Jiashun Liu, Han Lu, Jiamang Wang, Wenbo Su, Bo Zheng, Junchi Yan 핵심 연구 목표 본 논문은 LLM의 불투명한 추론 과정을 명확히 이해하"},{"id":"2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs","title":"[논문리뷰] Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs","excerpt":"arXiv에 게시된 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs","tags":["Review","Multimodal Large Language Models","Data Curation","Supervised Fine-tuning","Chain-of-Thought","Open-source AI","Data Quality","MLLM Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Zhang, Bolin Ni, XinSheng Chen, HengRui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, MengHao Guo, ShiMin Hu 핵심 연구 목표 본 논문은 데이터 품질 격차로 인해 독점 모델에 뒤처지는 Fully Open MLLM 의"},{"id":"2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","title":"[논문리뷰] CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving","excerpt":"Jingcheng Ni이 arXiv에 게시한 'CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","tags":["Review","Autonomous Driving","Video Generation","Diffusion Models","Spatial-Temporal Reconstruction","3D Gaussian Splatting","Variational Autoencoder","World Modeling","Multi-View Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni 핵심 연구 목표 자율 주행을 위한 포괄적인 세계 모델을 구축하기 위해, 다양한 제어 입력 하에 장기간의 다중 시점 비디오를 생성하고 동시에 4D 장면 재구성 기능을 제공하는 것을 목표로 합니다. 특히, 기존 "},{"id":"2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving","title":"[논문리뷰] CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving","excerpt":"arXiv에 게시된 'CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving","tags":["Review","Autonomous Driving","Imitation Learning","Reinforcement Learning","World Models","Latent Space","Dual-Policy","Competitive Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoji Zheng, Ziyuan Yang, Yanhao Chen, Yuhang Peng, Yuanrong Tang, Gengyuan Liu, Bokui Chen, Jiangtao Gong 핵심 연구 목표 본 논문은 모방 학습(IL)에만 의존하는 자율주행 모델이 겪는 일반화 성능 저하 및 롱테일 시나리오 대응 문제"},{"id":"2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs","title":"[논문리뷰] Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs","excerpt":"arXiv에 게시된 'Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs","tags":["Review","LLM","NPC","Game Dialogue","Persona-Grounded Dialogue","Task Execution","Prompt Engineering","Fine-tuning","Deflanderization"],"text":"링크: 논문 PDF로 바로 열기 저자: Pasin Buakhaw, Kun Kerdthaisong, Phuree Phenhiran, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot 핵심 연구 목표 LLM 기반 비플레이어 캐릭터(NPC"},{"id":"2025-10-16-Direct-Multi-Token-Decoding","title":"[논문리뷰] Direct Multi-Token Decoding","excerpt":"Xifeng Yan이 arXiv에 게시한 'Direct Multi-Token Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Direct-Multi-Token-Decoding","tags":["Review","LLM Inference","Multi-token Decoding","Transformer Architecture","Layer Specialization","Cyclical Refilling","Inference Speedup","Model Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuan Luo, Weizhi Wang, Xifeng Yan 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 비효율적인 계층 활용을 해결하여 추론 속도를 가속화하는 것을 목표로 합니다. 특히, 기존 디코더온리 트랜스포머가 단일 토큰 생성을 위해 모든 계층을 반복적으로 사용하는 비효율성을 개선하고, 추가적인 파라"},{"id":"2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling","title":"[논문리뷰] EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling","excerpt":"Ahmet Üstün이 arXiv에 게시한 'EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling","tags":["Review","LLM","Inference-Time Scaling","Entropy-Aware Generation","Adaptive Budget Allocation","Reasoning Benchmarks","Computational Efficiency","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Daniel Scalena, Leonidas Zotos, Elisabetta Fersini, Malvina Nissim, Ahmet Üstün 핵심 연구 목표 본 논문은 추론 언어 모델(LLM)에서 여러 추론 경로를 탐색할 때 발생하는 불필요한 계산 오버헤드 를 줄이고자 합니다. 특히, 다양한 프롬프트가 서로 다른 복"},{"id":"2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model","title":"[논문리뷰] FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model","excerpt":"Dawei Liang이 arXiv에 게시한 'FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model","tags":["Review","Vision-Language Alignment","Fine-grained Understanding","Bilingual Model","Contrastive Learning","Multimodal Retrieval","Open-Vocabulary Detection","Region-Text Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang 핵심 연구 목표 기존 비전언어 모델(VLM)이 대규모 전역 정렬에는 능숙하지만, 객체 속성, 공간 관계, 미묘한 언어 표현 등 세분화된 디테일 을 포착하고 비영어권 환경(특히 중국어) 에서 다국어 지원이 부족하"},{"id":"2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds","title":"[논문리뷰] FlashWorld: High-quality 3D Scene Generation within Seconds","excerpt":"Chunchao Guo이 arXiv에 게시한 'FlashWorld: High-quality 3D Scene Generation within Seconds' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds","tags":["Review","3D Scene Generation","Diffusion Models","Multi-View Synthesis","3D Gaussian Splatting","Knowledge Distillation","Real-time Generation","High-Quality Rendering","Cross-modal Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, Liujuan Cao 핵심 연구 목표 논문은 기존 3D 장면 생성 방법론의 한계인 긴 생성 시간(수분수시간)과 시각적 품질 저하, 3D 일관성 부족 문제를 해결하고자 합니다. 단일 이미지 또는 텍"},{"id":"2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner","title":"[논문리뷰] Generative Universal Verifier as Multimodal Meta-Reasoner","excerpt":"arXiv에 게시된 'Generative Universal Verifier as Multimodal Meta-Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner","tags":["Review","Multimodal AI","Visual Verification","Generative Models","Self-Refinement","Vision-Language Models","Test-Time Scaling","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang 핵심 연구 목표 본 논문은 차세대 멀티모달 추론 및 통합 모델을 위한 생성형 범용 검증기(Generative Universal Veri"},{"id":"2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search","title":"[논문리뷰] GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search","excerpt":"Zijian Zhang이 arXiv에 게시한 'GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search","tags":["Review","LLM Agents","Multi-Agent Systems","Failure Tracing","Root Cause Analysis","Information Dependency Graph","Reinforcement Learning","Deep Search"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijian Zhang, Haochen You, Xiaodong Gu, Heng Zhang, YerbaPage 핵심 연구 목표 본 논문은 다중 에이전트 LLM 시스템에서 발생하는 복잡한 다중 턴 심층 탐색 시나리오 의 실패에 대한 정확한 원인 추론(failure attribution) 문제를 해결하는 것을 목표로 합니"},{"id":"2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math","title":"[논문리뷰] Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math","excerpt":"arXiv에 게시된 'Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math","tags":["Review","LLM Verification","Math Reasoning","Step-Level Verification","Benchmark","Open-Ended Problems","Process Reward Models","Generative Critics"],"text":"링크: 논문 PDF로 바로 열기 저자: Shrey Pandit, Austin Xu, XuanPhi Nguyen, Yifei Ming, Caiming Xiong, Shafiq Joty 핵심 연구 목표 본 논문은 LLM 기반 추론 시스템의 수학적 증명 단계별 검증 능력을 평가하기 위한 새로운 벤치마크, Hard2Verify 를 제시합니다. 기존 벤치마크가 프론"},{"id":"2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain","title":"[논문리뷰] Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain","excerpt":"Lingxi Lu이 arXiv에 게시한 'Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain","tags":["Review","Large Language Models","Syntactic Structure","Human Brain","Frequency Tagging","Neuroscience","Model Interpretability","Representational Similarity Analysis","Intracranial EEG"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingmin An, Yilong Song, Ruolin Yang, Nai Ding, Lingxi Lu, Yuxuan Wang, Wei Wang, Chu Zhuang, Qian Wang, Fang Fang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 인간 수준의 언어 능력을 보여주지만 구문 구조를 모델링하는"},{"id":"2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication","title":"[논문리뷰] HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication","excerpt":"Haochen You이 arXiv에 게시한 'HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication","tags":["Review","Large Language Model","Multi-agent Systems","Multi-agent Communication","Graph Neural Networks","Hypergraph","Topology Optimization","Variational Autoencoder","Sparsity Regularization"],"text":"링크: 논문 PDF로 바로 열기 저자: Heng Zhang, Zijian Zhang, Yilei Yuan, Yuling Shi, Haochen You, Xiaodong Gu, Lubin Gan, Jin Huang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 멀티 에이전트 시스템에서 발생하는 비효율적인 그룹 협업 모델링(단순한 쌍별 관계) 및 "},{"id":"2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue","title":"[논문리뷰] InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue","excerpt":"Dongchuan Ran이 arXiv에 게시한 'InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue","tags":["Review","Omni-modal LLM","Audio-Visual Dialogue","Multi-turn Interaction","Speech Generation","Long-term Memory","Multimodal Understanding","End-to-end Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiando"},{"id":"2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy","title":"[논문리뷰] InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy","excerpt":"Yilun Chen이 arXiv에 게시한 'InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy","tags":["Review","Robotics","Vision-Language-Action (VLA)","Spatial Grounding","Generalist Policy","Multimodal Learning","Instruction Following","Simulation-to-Real","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yilun Chen, Ning Gao, Jiangmiao Pang, et al. (Intern Robotics, Shanghai AI Laboratory) 핵심 연구 목표 본 논문은 로봇이 지시를 이해하고 3D 공간에서 행동하는 데 필요한 본질적인 격차를 해소하여, 확장 가능하고 범용적인 지능을 갖춘 지시추종 로봇을 "},{"id":"2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models","title":"[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models","excerpt":"arXiv에 게시된 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models","tags":["Review","Vision-Language-Action Models","Robotics","Robustness Analysis","Generalization","Perturbations","Benchmark","LIBERO-Plus","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu 핵심 연구 목표 본 연구는 VisualLang"},{"id":"2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model","title":"[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model","excerpt":"Wieland Brendel이 arXiv에 게시한 'MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model","tags":["Review","Reinforcement Learning (RL)","Mathematical Reasoning","Benchmark","Large Language Models (LLMs)","Exploration","Boundary Expansion","MATH-Beyond"],"text":"링크: 논문 PDF로 바로 열기 저자: Prasanna Mayilvahanan, Ricardo DominguezOlmedo, Thaddäus Wiedemer, Wieland Brendel 핵심 연구 목표 기존 RL 기반 LLM들이 수학적 추론 능력을 확장하기보다 기존 지식을 정교화하는 데 그치는 한계를 극복하고, 실제 모델의 추론 능력 경계를 확장 시키는 "},{"id":"2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training","title":"[논문리뷰] MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training","excerpt":"arXiv에 게시된 'MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training","tags":["Review","Multi-turn Text-to-SQL","Agentic Training","Reinforcement Learning","Large Language Models","Dialogue Systems","Semantic Parsing","Database Interaction","Self-correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Taicheng Guo, Hai Wang, ChaoChun Liu, Mohsen Golalikhani, Xin Chen, Xiangliang Zhang, Chandan K. Reddy 핵심 연구 목표 본 논문은 기존 Multiturn TexttoSQL 시스템들이 단기적인 추론 패러다임에 머물러 실행 가능하거나 일관성 "},{"id":"2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention","title":"[논문리뷰] NOSA: Native and Offloadable Sparse Attention","excerpt":"Zhiyuan Liu이 arXiv에 게시한 'NOSA: Native and Offloadable Sparse Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention","tags":["Review","Sparse Attention","KV Cache Offloading","LLMs","Decoding Throughput","Locality Constraint","Memory Optimization","Trainable Sparse Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 긴 컨텍스트 디코딩 시 발생하는 메모리 병목 현상, 특히 KV 캐시 크기 가 배치 크기 및 디코딩 처리량을 제한하는 문제를 해결하는 것을 목표로 합니다. 기존 학습 가능한 희소"},{"id":"2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs","title":"[논문리뷰] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs","excerpt":"arXiv에 게시된 'ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs","tags":["Review","Diffusion LLMs","Parallel Decoding","Speed-Quality Trade-off","Benchmark","Token Dependencies","Unmasking Strategies","Information Theory"],"text":"링크: 논문 PDF로 바로 열기 저자: Wonjun Kang, Kevin Galim, Seunghyuk Oh, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Minjae Lee, Hyung Il Koo, Nam Ik Cho, Kangwook Lee, Yuezhou Hu 핵심 연구 목표 본 논문은 Diffusion LLM (d"},{"id":"2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","title":"[논문리뷰] PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning","excerpt":"Hengshuang Zhao이 arXiv에 게시한 'PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","tags":["Review","Video Generation","Physical Plausibility","Reinforcement Learning","Direct Preference Optimization","Physical Representation","Diffusion Models","World Models","Image-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, Hengshuang Zhao 핵심 연구 목표 본 논문은 최신 비디오 생성 모델들이 시각적으로 사실적인 비디오를 생성하지만 물리 법칙을 준수하지 못하는 문제를 해결하는 것을 목표로 합니다. 물리적 지식을 비디오 생성 모델에 통합하여 물리적으로 그럴"},{"id":"2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models","title":"[논문리뷰] Point Prompting: Counterfactual Tracking with Video Diffusion Models","excerpt":"Andrew Owens이 arXiv에 게시한 'Point Prompting: Counterfactual Tracking with Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models","tags":["Review","Video Diffusion Models","Point Tracking","Zero-Shot Learning","Counterfactual Modeling","Visual Prompting","SDEdit","Negative Prompting","Object Permanence"],"text":"링크: 논문 PDF로 바로 열기 저자: Ayush Shrivastava, Sanyam Mehta, Daniel Geng, Andrew Owens 핵심 연구 목표 본 논문은 사전 학습된 비디오 확산 모델(video diffusion models) 이 추가 훈련 없이 제로샷(zeroshot) 방식으로 시점 추적(point tracking)을 수행할 수 있는지 "},{"id":"2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World","title":"[논문리뷰] Reasoning in Space via Grounding in the World","excerpt":"Li Zhang이 arXiv에 게시한 'Reasoning in Space via Grounding in the World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World","tags":["Review","3D Visual Grounding","Spatial Reasoning","Large Language Models (LLMs)","Chain-of-Thought (CoT)","Hybrid Representation","Multi-modal LLMs","Point Clouds"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, Peidong Liu 핵심 연구 목표 기존 3D LLM이 통일된 3D 표현 부재 및 외부 모듈 의존으로 인해 3D 시각적 그라운딩과 공간 추론을 원활하게 통합하지 못하는 문제를 해결하는 것이 목표입니다. 본 연구는 LL"},{"id":"2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning","title":"[논문리뷰] Revisiting Model Interpolation for Efficient Reasoning","excerpt":"arXiv에 게시된 'Revisiting Model Interpolation for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning","tags":["Review","Model Interpolation","Efficient Reasoning","Large Language Models","Chain-of-Thought","Model Merging","Performance Dynamics","Ablation Study"],"text":"링크: 논문 PDF로 바로 열기 저자: Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Ngai Wong 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)의 복잡한 연쇄적 사고(ChainofThought, CoT) 추론에서 발생하는 과도한 사고(overthinking) 및 높은 지연 시간 문제를 해결하기 위한 효율"},{"id":"2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs","title":"[논문리뷰] Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs","excerpt":"Hao Zhang이 arXiv에 게시한 'Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs","tags":["Review","Large Language Models (LLMs)","Reinforcement Learning (RL)","Multi-Agent Systems (MAS)","On-Policy RL","Collaborative AI","Agentic LLMs","Group-based Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujie Zhao, Lanxiang Hu, Hao Zhang, Ke Ding, Minmin Hou, Yang Wang, Jishen Zhao 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트의 성능 향상을 위해 다중 에이전트 시스템(MAS) 과 강화 학습(RL) 을 통합하는 것을 목표로 합니다. 특히, "},{"id":"2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs","title":"[논문리뷰] The Art of Scaling Reinforcement Learning Compute for LLMs","excerpt":"arXiv에 게시된 'The Art of Scaling Reinforcement Learning Compute for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs","tags":["Review","Reinforcement Learning","LLMs","Scaling Laws","Compute Efficiency","Predictability","Sigmoidal Curves","ScaleRL","Off-Policy RL"],"text":"링크: 논문 PDF로 바로 열기 저자: Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit S. Dhillon, David Brandfonbrener, Rishabh Agarwal 핵심 연구 목표 본 연구는 LLM 훈련에 "},{"id":"2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research","title":"[논문리뷰] The Role of Computing Resources in Publishing Foundation Model Research","excerpt":"Zhenwen Liang이 arXiv에 게시한 'The Role of Computing Resources in Publishing Foundation Model Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research","tags":["Review","Foundation Models","Computing Resources","GPU Disparity","AI Research","Publication Bias","Resource Allocation","Research Transparency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuexing Hao, Yue Huang, Haoran Zhang, Chenyang Zhao, Zhenwen Liang, Paul Pu Liang, Yue Zhao, Lichao Sun, Saleh Kalantari, Xiangliang Zhang, Marzyeh Ghassemi 핵심 연구 목표 본 논문은 GPU, 데"},{"id":"2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields","title":"[논문리뷰] Trace Anything: Representing Any Video in 4D via Trajectory Fields","excerpt":"arXiv에 게시된 'Trace Anything: Representing Any Video in 4D via Trajectory Fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields","tags":["Review","4D Video Representation","Trajectory Fields","Neural Networks","Spatio-temporal Modeling","3D Point Tracking","Motion Forecasting","Computer Vision","B-splines"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinhang Liu, Yuxi Xiao, Donny Y. Chen, Jiashi Feng, YuWing Tai, ChiKeung Tang, Bingyi Kang 핵심 연구 목표 본 논문은 비디오의 동적 장면을 모델링하고 이해하는 데 필수적인 효과적인 시공간 표현 문제를 해결하고자 합니다. 기존의 4D 재구성 방식이 "},{"id":"2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark","title":"[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark","excerpt":"arXiv에 게시된 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark","tags":["Review","Multimodal AI","Unified Models","Benchmark","Generation","Understanding","Reasoning","Evaluation","Cross-modal Synergy"],"text":"링크: 논문 PDF로 바로 열기 저자: Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu 핵심 연구 목표 본 논문은 통합 멀티모달 모델의 생성(Generation) 및 이해(Understanding) 능력 간의 "},{"id":"2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning","title":"[논문리뷰] UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning","excerpt":"Ziyong Feng이 arXiv에 게시한 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning","tags":["Review","Multimodal Embeddings","MLLM-as-a-Judge","Hard Negative Mining","Semantic Alignment","Representation Learning","Reranking","Contrastive Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing 핵심 연구 목표 기존 multimodal 임베딩 모델의 한계인 hard negative 샘플의 다양성 부"},{"id":"2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE","title":"[논문리뷰] UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE","excerpt":"arXiv에 게시된 'UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE","tags":["Review","Mixture of Experts","Speech Generation","Music Generation","Multimodal AI","Dynamic Routing","Training Curriculum","Data Imbalance","Audio Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian "},{"id":"2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification","title":"[논문리뷰] Universal Image Restoration Pre-training via Masked Degradation Classification","excerpt":"arXiv에 게시된 'Universal Image Restoration Pre-training via Masked Degradation Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification","tags":["Review","Universal Image Restoration","Pre-training","Masked Image Modeling","Degradation Classification","Deep Learning","Computer Vision","Self-supervised Learning","Low-level Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu 핵심 연구 목표 본 논문은 다양한 종류의 이미지 손상(degradation)을 복원하는 단일 모델(universal image restoration)의 성능을 향상시키기 위해, 기존 사전 훈련 방법론의 한계를 극"},{"id":"2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model","title":"[논문리뷰] X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model","excerpt":"Xirui Kang이 arXiv에 게시한 'X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-16 13:09:51+0900","permalink":"/ai/review/2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model","tags":["Review","Vision-Language-Action (VLA) Models","Soft Prompts","Transformer","Cross-Embodiment","Robotics","Pretraining","Domain Adaptation","Flow Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, YaQin Zhang, Jiangmiao Pang, Jingjing Liu, Tai Wan"},{"id":"2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses","title":"[논문리뷰] AI for Service: Proactive Assistance with AI Glasses","excerpt":"arXiv에 게시된 'AI for Service: Proactive Assistance with AI Glasses' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses","tags":["Review","AI for Service","Proactive AI","AI Glasses","Multi-agent System","Human-AI Interaction","Context-aware AI","Wearable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Zichen Wen, Yiyu Wang, Chenfei Liao, Boxue Yang, Junxian Li, Weifeng Liu, Haocong He, Bolong Feng, Xuyang Liu, Yuanhuiyi Lyu, Xu Zheng, Xuming Hu, Linfeng Zhang 핵심 연구 목표 기존의 수동적이"},{"id":"2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization","title":"[논문리뷰] Agentic Entropy-Balanced Policy Optimization","excerpt":"arXiv에 게시된 'Agentic Entropy-Balanced Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization","tags":["Review","Agentic Reinforcement Learning","Web Agents","Tool Learning","Entropy Balancing","Policy Optimization","Rollout Strategy","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, JiRong Wen, Zhicheng Do"},{"id":"2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs","title":"[논문리뷰] Attention Is All You Need for KV Cache in Diffusion LLMs","excerpt":"arXiv에 게시된 'Attention Is All You Need for KV Cache in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs","tags":["Review","Diffusion LLMs","KV Cache","Adaptive Caching","Inference Optimization","Attention Mechanism","Latency Reduction","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Quan NguyenTri, Mukul Ranjan, Zhiqiang Shen 핵심 연구 목표 본 논문은 확산 대규모 언어 모델(DLMs)의 추론 과정에서 발생하는 과도한 KeyValue (KV) 캐시 재계산으로 인한 높은 지연 시간을 해결하는 것을 목표로 합니다. KV 상태 변화가 미미함에도 불구하고 모든 토큰과 레"},{"id":"2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures","title":"[논문리뷰] Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures","excerpt":"arXiv에 게시된 'Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures","tags":["Review","Subjective Preference Learning","Writing Evaluation","Reward Models","RLHF","Cross-Cultural AI","Generative Models","Language Model Judges","Genre Instability"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuangshuang Ying, Yunwen Li, Xingwei Qu, et al. 핵심 연구 목표 본 논문은 기존 RLHF 보상 모델이 객관적인 품질 신호(문법 오류, 사실 정확성 등)를 제거했을 때 주관적인 쓰기 선호도 평가에서 성능이 크게 저하되는 문제를 해결하고자 합니다. 특히, 객관적 오류 탐지를 넘어 창"},{"id":"2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts","title":"[논문리뷰] Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts","excerpt":"arXiv에 게시된 'Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts","tags":["Review","Role-playing LLMs","Multiversal Consistency","Character Benchmarking","Moral Dilemmas","Canon Events","Reasoning-Acting Alignment","Chain-of-Thought","Superheroes"],"text":"링크: 논문 PDF로 바로 열기 저자: Perapard Ngokpol, Kun Kerdthaisong, Pasin Buakhaw, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot 핵심 연구 목표 본 연구는 대규모 언어 모델(LLMs"},{"id":"2025-10-17-BitNet-Distillation","title":"[논문리뷰] BitNet Distillation","excerpt":"arXiv에 게시된 'BitNet Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-BitNet-Distillation","tags":["Review","Low-bit Quantization","LLM Compression","Knowledge Distillation","Ternary Weights","Inference Optimization","Memory Efficiency","SubLN","Continual Pre-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Xun Wu, Shaohan Huang, Wenhui Wang, Ting Song, Li Dong, Yan Xia, Furu Wei 핵심 연구 목표 본 논문은 기존의 풀정밀도 LLM (예: Qwen )을 특정 다운스트림 태스크를 위해 1.58비트 정밀도 (삼진 가중치: {1, 0, 1}) 로 미세 조정하여, 최소한의 "},{"id":"2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes","title":"[논문리뷰] COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes","excerpt":"arXiv에 게시된 'COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes","tags":["Review","Chinese Creative Writing","Process Supervision","LLM Training","Dataset Creation","Cross-Lingual Transfer","Narrative Logic","Linguistic Expression","Type-Token Ratio"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunwen Li, Shuangshuang Ying, Xingwei Qu, et al. 핵심 연구 목표 대규모 언어 모델(LLM)이 비영어권, 특히 중국어 창의적 글쓰기에서 겪는 체계적인 결함(예: 예측 가능한 내러티브, 스타일 다양성 부족, 문화적 비정합성)을 해결하는 것을 목표로 합니다. 이를 위해, 내러티브 논리"},{"id":"2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation","title":"[논문리뷰] DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation","excerpt":"arXiv에 게시된 'DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation","tags":["Review","Multimodal Generation","Dialect Robustness","Text-to-Image","Text-to-Video","Benchmarking","Diffusion Models","Text Encoder Tuning","Low-Resource Dialects"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, ChoJui Hsieh, KaiWei Chang, Nanyun Peng 핵심 연구 목표 현재 다중 모달 생성 모델이 다양한 영어 방언 텍스트 입력에 대해 효과적으로 콘텐츠를 생성할 수 있는지 평가하고, 방언 사용자들이 겪는"},{"id":"2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models","title":"[논문리뷰] Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models","excerpt":"arXiv에 게시된 'Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models","tags":["Review","Recurrent-Depth Models","Diffusion Forcing","Parallel Sampling","LLM Inference Acceleration","Transformer Architectures","Generative AI","Latent Space Diffusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Jonas Geiping, Guinan Su, Xinyu Yang 핵심 연구 목표 본 논문은 반복적 깊이(recurrentdepth)를 가진 언어 모델의 느린 추론 속도를 해결하기 위해, 이러한 모델과 확산(diffusion) 언어 모델 간의 유사성을 활용한 효율적인 병렬 샘플링 기법을 개발하는 것을 목표로 합니다. "},{"id":"2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning","title":"[논문리뷰] Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning","excerpt":"Sijia Gu이 arXiv에 게시한 'Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning","tags":["Review","Vision-Language-Action (VLA)","Mixture of Experts (MoE)","Robotic Manipulation","Expert Specialization","Decoupled Routing","Load Balancing","Transfer Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Sijia Gu, Zhixuan Liang, Yuhao Wu, Yitian Liu, Weijie Shen 핵심 연구 목표 본 연구는 VisionLanguageAction (VLA) 모델 스케일링의 두 가지 주요 과제, 즉 사전 훈련된 VLA 모델 가중치 활용을 통한 효율적인 스케일업과 실시간 제어를 위한 모델 용량 및"},{"id":"2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report","title":"[논문리뷰] Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report","excerpt":"arXiv에 게시된 'Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report","tags":["Review","ColBERT","Retrieval Models","Small Models","Distillation","Long Context","Edge AI","Information Retrieval","RAG"],"text":"링크: 논문 PDF로 바로 열기 저자: Rikiya Takehi, Benjamin Clavié, Sean Lee, Aamir Shakir 핵심 연구 목표 본 연구는 클라우드부터 엣지 기기까지 모든 스케일에서 정보 검색을 지원하기 위해, 현대적인 아키텍처와 높은 효율성을 갖춘 소형 ColBERT 모델(mxbaiedgecolbertv0) 을 개발하는 것을 목표"},{"id":"2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale","title":"[논문리뷰] From Pixels to Words -- Towards Native Vision-Language Primitives at Scale","excerpt":"arXiv에 게시된 'From Pixels to Words -- Towards Native Vision-Language Primitives at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale","tags":["Review","Vision-Language Models","Native VLMs","Early Fusion","Multimodal Learning","Transformer Architecture","Rotary Position Embeddings","Pixel-Word Alignment","End-to-End Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu 핵심 연구 목표 본 논문은 기존의 모듈형 VisionLanguage Models (VLMs)이 가진 강한 시각적 인코딩 편향과 "},{"id":"2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","title":"[논문리뷰] ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints","excerpt":"arXiv에 게시된 'ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","tags":["Review","Video Generation","Test-Time Search","Diffusion Models","Semantic Dependency","Adaptive Reward","Evaluation Benchmark","Prompt-Guided"],"text":"링크: 논문 PDF로 바로 열기 저자: Meiqi Wu, Jiashu Zhu, Xiaokun Feng, Chubin Chen, Bingze Song, Fangyuan Mao, Jiahong Wu, Xiangxiang Chu, Chen Zhu, Kaiqi Huang 핵심 연구 목표 본 연구는 기존 비디오 생성 모델들이 상상적인 시나리오 나 장거리 의미론적 관"},{"id":"2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents","title":"[논문리뷰] Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents","excerpt":"arXiv에 게시된 'Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents","tags":["Review","LLM Agents","Reinforcement Learning","Multi-Turn Interactions","Reward Sparsity","Information Gain","Policy Optimization","Ground-Truth Awareness","Sample Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, and Zhenzhe Ying 핵심 연구 목표 이 논문은 다중 턴(multiturn) 대규모 언어 모델(LLM) 에이전트 훈련 시 발생하는 희소한 보상(sparse rewar"},{"id":"2025-10-17-LLM-guided-Hierarchical-Retrieval","title":"[논문리뷰] LLM-guided Hierarchical Retrieval","excerpt":"arXiv에 게시된 'LLM-guided Hierarchical Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval","tags":["Review","Information Retrieval","Large Language Models","Hierarchical Retrieval","Semantic Tree","Tree Traversal","Zero-shot Performance","Reasoning-based Retrieval","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Nilesh Gupta, WeiCheng Chang, Ngot Bui, ChoJui Hsieh, Inderjit S. Dhillon 핵심 연구 목표 기존 LLM 기반 정보 검색(IR) 시스템이 직면한 패러다임의 초기 검색 단계 한계와 의 확장성 문제를 해결하는 것이 목표입니다. 특히, 대규모 문서 코퍼스에서 LLM이 "},{"id":"2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training","title":"[논문리뷰] LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training","excerpt":"arXiv에 게시된 'LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training","tags":["Review","LLM","Digital Agents","UI Simulation","Synthetic Data Generation","Targeted Data Synthesis","World Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, KaiWei Chang 핵심 연구 목표 본 논문은 디지털 에이전트 훈련에 필요한 대규모, 고품질 UI 환경 훈련 궤"},{"id":"2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding","title":"[논문리뷰] LaSeR: Reinforcement Learning with Last-Token Self-Rewarding","excerpt":"arXiv에 게시된 'LaSeR: Reinforcement Learning with Last-Token Self-Rewarding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding","tags":["Review","Reinforcement Learning","LLM","Self-Verification","Last-Token","Reward Modeling","Efficiency","Reasoning","RLVR"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 추론 능력을 강화하는 검증 가능한 보상 강화 학습(RLVR) 의 한계, 즉 테스트 시점에서의 검증 신호 부족과 기존 "},{"id":"2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know","title":"[논문리뷰] Large Language Models Do NOT Really Know What They Don't Know","excerpt":"arXiv에 게시된 'Large Language Models Do NOT Really Know What They Don't Know' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know","tags":["Review","LLMs","Hallucination Detection","Mechanistic Interpretability","Internal States","Knowledge Recall","Refusal Tuning","Factual Associations","Associated Hallucinations"],"text":"링크: 논문 PDF로 바로 열기 저자: Chi Seng Cheang, Hou Pong Chan, Wenxuan Zhang, Yang Deng 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 사실적 오류를 생성할 때 내부적으로 어떻게 처리하는지 기계적으로 분석하여, LLMs가 진정으로 \"무엇을 모르는지 아는지\" 여부를 밝히는 것을 목표로 합니다. 특"},{"id":"2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs","title":"[논문리뷰] Learning an Image Editing Model without Image Editing Pairs","excerpt":"arXiv에 게시된 'Learning an Image Editing Model without Image Editing Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs","tags":["Review","Image Editing","Diffusion Models","Vision-Language Models (VLMs)","No-Pair Training","Few-step Generation","Distribution Matching","Gradient-based Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Nupur Kumari, ShengYu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, JunYan Zhu, Xun Huang 핵심 연구 목표 본 논문은 대규모 입력편집 쌍 데이터 에 대한 의존성"},{"id":"2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning","title":"[논문리뷰] LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning","excerpt":"arXiv에 게시된 'LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning","tags":["Review","Layer Skipping","Multi-stage Reasoning","Latency Optimization","Early Exit","Small Language Models (LLMs)","Adaptive Computation","Confidence-based Decoding"],"text":"링크: 논문 PDF로 바로 열기 저자: Beomseok Kang, Jiwon Song, JaeJoon Kim 핵심 연구 목표 본 연구는 소규모 LLM에서 다단계 추론 시 발생하는 높은 레이턴시 문제를 해결하고자 합니다. 기존 계층 건너뛰기(layer skipping) 기법이 각 추론 단계의 상이한 민감도와 불필요한 토큰 생성으로 인해 효율성과 정확도 균형을"},{"id":"2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning","title":"[논문리뷰] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning","excerpt":"Ke Wang이 arXiv에 게시한 'MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning","tags":["Review","Multimodal Reasoning","Visual Chain-of-Thought (VCoT)","Large Multimodal Models (LMMs)","Geometric Reasoning","Diagram Generation","Dataset","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li 핵심 연구 목표 본 논문은"},{"id":"2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems","title":"[논문리뷰] MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems","excerpt":"Feiyu Xiong이 arXiv에 게시한 'MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems","tags":["Review","Retrieval-Augmented Generation (RAG)","Document Memory","Text Chunking","Small Language Models (SLMs)","Large Language Models (LLMs)","Scenario-Aware Processing","Multi-Layer Retrieval","Cognitive Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jihao Zhao, Zhiyuan Ji, Simin Niu, Hanyu Wang, Feiyu Xiong, Zhiyu Li 핵심 연구 목표 기존 RAG 패러다임의 수동적인 텍스트 청킹 방식이 지식 내부화 및 추론 능력을 제한하는 문제를 해결합니다. 인간의 인지 과정을 모방하여 텍스트 처리를 수동적인 청킹에서 사전 이해"},{"id":"2025-10-17-On-Pretraining-for-Project-Level-Code-Completion","title":"[논문리뷰] On Pretraining for Project-Level Code Completion","excerpt":"arXiv에 게시된 'On Pretraining for Project-Level Code Completion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-On-Pretraining-for-Project-Level-Code-Completion","tags":["Review","Code LLMs","Project-level Context","Code Completion","Context Window Extension","RoPE Scaling","Repository Pretraining","Long Code Arena"],"text":"링크: 논문 PDF로 바로 열기 저자: Maksim Sapronov, Evgeniy Glukhov 핵심 연구 목표 본 연구는 코드 언어 모델(Code LLMs)이 코드베이스 전체의 컨텍스트를 활용하여 정확하고 컨텍스트를 인지하는 코드 완성을 생성하도록 돕기 위해, 저장소 수준(repositorylevel) 사전 훈련 전략이 OpenCoder 1.5B 모델의"},{"id":"2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model","title":"[논문리뷰] PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model","excerpt":"arXiv에 게시된 'PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model","tags":["Review","Document Parsing","Vision-Language Model","Multilingual OCR","Layout Analysis","Resource-Efficient AI","Table Recognition","Formula Recognition","Chart Recognition"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Ju"},{"id":"2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation","title":"[논문리뷰] Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation","excerpt":"arXiv에 게시된 'Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation","tags":["Review","Human-human Interaction","Pose Animation","Diffusion Models","Generative AI","Motion Synthesis","Interactive Poses","Temporal Priors","Spatial Priors"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaowei Liu, Chuan Guo†, Bing Zhou²†, Jian Wang²† 핵심 연구 목표 본 연구는 기존 상호작용 애니메이션 모델이 근접 상호작용의 동적 맥락을 파악하고 다양한 입력 유형에 대한 일반화 능력이 부족하다는 문제점을 해결하고자 합니다. 상호작용 포즈에 내재된 다이내믹스 사전 지식(dynam"},{"id":"2025-10-17-Qwen3Guard-Technical-Report","title":"[논문리뷰] Qwen3Guard Technical Report","excerpt":"arXiv에 게시된 'Qwen3Guard Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-Qwen3Guard-Technical-Report","tags":["Review","LLM Safety","Guardrail Models","Multilingual AI","Real-time Moderation","Tri-class Classification","Instruction Tuning","Streaming Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Qwen Team 핵심 연구 목표 본 연구는 기존 가드레일 모델의 이진 분류 한계와 스트리밍 LLM 추론과의 비호환성 문제를 해결하는 것을 목표로 합니다. 구체적으로는 Qwen3Guard 를 통해 safe, controversial, unsafe 의 세분화된 3단계 안전성 판단을 제공하고, 증분 텍스트 생성 중 토큰 "},{"id":"2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems","title":"[논문리뷰] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems","excerpt":"arXiv에 게시된 'RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems","tags":["Review","Large Language Models","Retrieval Augmented Generation","Agentic Systems","Benchmarking","Intermediate Tasks","Error Analysis","LLM Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingru Lin, Chen Zhang, Stephen Y. Liu, Haizhou Li 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM) 기반 에이전트형 검색 증강 생성(RAG) 시스템의 한계, 특히 복잡한 다단계 질문 처리 능력 및 중간 추론 능력 부족 문제를 해결하고자 합니다. 기존의 종단 간 QA 기반 평"},{"id":"2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","title":"[논문리뷰] RealDPO: Real or Not Real, that is the Preference","excerpt":"Chenyang Si이 arXiv에 게시한 'RealDPO: Real or Not Real, that is the Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","tags":["Review","Video Generation","Diffusion Models","Direct Preference Optimization","Preference Learning","Real Data","Human Motion Synthesis","RealDPO","RealAction-5K"],"text":"링크: 논문 PDF로 바로 열기 저자: Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu 핵심 연구 목표 본 연구는 기존 비디오 생성 모델들이 복잡한 동작, 특히 사람 중심의 일상 활동에서 자연스럽고 부드러우며 맥락적으로 일관된 움직임을 생성하는 데 겪는 문제를 해결하고자 합니다. "},{"id":"2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models","title":"[논문리뷰] RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models","excerpt":"arXiv에 게시된 'RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models","tags":["Review","RAG Systems","Selective Refusal","Generative Evaluation","Linguistic Perturbations","LLM Evaluation","Informational Uncertainty","Model Calibration","AI Safety"],"text":"링크: 논문 PDF로 바로 열기 저자: Aashiq Muhamed, Leonardo F. R. Ribeiro, Markus Dreyer, Virginia Smith, Mona T. Diab 핵심 연구 목표 이 논문은 RAG(RetrievalAugmented Generation) 시스템 에서 언어 모델이 불충분하거나 신뢰할 수 없는 정보 를 기반으로 답변을 "},{"id":"2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis","title":"[논문리뷰] SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis","excerpt":"arXiv에 게시된 'SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis","tags":["Review","4D Novel View Synthesis","Dynamic Scenes","3D Gaussian Splatting","Cascaded Optimization","Deformation Modeling","Point Tracking","Object Segmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jipeng Lyu, Jiahua Dong, YuXiong Wang 핵심 연구 목표 본 연구는 동적 3D 장면 모델링에서 정확한 변형을 포착하면서도 계산 효율성을 유지하는 데 따른 어려움을 해결합니다. 특히, 3D Gaussian Splatting (3DGS) 의 내재된 구조적 패턴을 활용하는 새로운 캐스케이드 최적화"},{"id":"2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models","title":"[논문리뷰] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models","excerpt":"arXiv에 게시된 'The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models","tags":["Review","German Commons","Large Language Models","Training Data","Openly Licensed Text","Data Curation","German NLP","Corpus Construction","Quality Filtering"],"text":"링크: 논문 PDF로 바로 열기 저자: Lukas Gienapp, Christopher Schröder, Ferdinand Schlatt, Arden Zimmermann, Stefan Schweter, Philippe Genêt, Christopher Akiki, Martin Potthast 핵심 연구 목표 이 논문은 대규모 독일어 언어 모델 개발 을 위한"},{"id":"2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar","title":"[논문리뷰] TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar","excerpt":"arXiv에 게시된 'TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar","tags":["Review","Code LLMs","Subword Tokenization","Grammar-aware Tokenization","Semantic Preservation","Rewrite Rules","Model Robustness","Tokenization Misalignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinxi Li, Yuntian Deng, Pengyu Nie 핵심 연구 목표 본 논문은 Code LLM 이 사용하는 서브워드 토크나이저 와 프로그래밍 언어(PL) 문법 간의 불일치 문제를 해결하고자 합니다. 통계에 기반한 서브워드 토크나이저가 공백이나 식별자 명명과 같은 표면적 요인에 따라 동일한 의미의 코드 스니펫"},{"id":"2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator","title":"[논문리뷰] VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator","excerpt":"Federico Tombari이 arXiv에 게시한 'VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator","tags":["Review","Text-to-3D","Model Stitching","Multi-view Reconstruction","Video Generation","Latent Diffusion Models","Gaussian Splats","Pointmaps","Reward Finetuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Truong, Federico Tombari, Konrad Schindler 핵심 연구 목표 본 논문은 기존 텍스트투3D(Textto3D) 모델의 느린 최적화 및 오류 축적 문제를 해결하기 위해, 강력한 텍스트투비디오(texttov"},{"id":"2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification","title":"[논문리뷰] VLA-0: Building State-of-the-Art VLAs with Zero Modification","excerpt":"arXiv에 게시된 'VLA-0: Building State-of-the-Art VLAs with Zero Modification' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification","tags":["Review","Vision-Language-Action Models","VLA-0","Zero Modification","Text-based Action Prediction","Robot Manipulation","Large Language Models","Fine-tuning","State-of-the-Art"],"text":"링크: 논문 PDF로 바로 열기 저자: Ankit Goyal, Hugo Hadfield, Xuning Yang, Valts Blukis, Fabio Ramos 핵심 연구 목표 본 논문은 VisionLanguage Model (VLM)의 아키텍처나 어휘를 변경하지 않고 순수한 텍스트 생성 능력만을 활용하여 로봇 행동을 예측하는 단순한 VLA(VisionLan"},{"id":"2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation","title":"[논문리뷰] VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation","excerpt":"arXiv에 게시된 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation","tags":["Review","Vision-Language-Action Models","Agentic Framework","Unseen Concept Manipulation","Out-of-Distribution Generalization","Tool Use","Web Retrieval","Object Detection","LIBERO Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang 핵심 연구 목표 본 논문은 기존 VLA 모델이 훈련 데이터 외부의 미확인 객체 개념(unseen concepts) 에 직면했을 때 급격히 성능이 저하되는 문제, 즉 OOD(OutofDistribut"},{"id":"2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning","title":"[논문리뷰] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning","excerpt":"arXiv에 게시된 'VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning","tags":["Review","Video Reward Models","Multimodal Reasoning","Thinking-with-Image","Visual Reasoning","Reinforcement Learning","Chain-of-Thought","Context Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Qunzhong Wang, Jie Liu, Jiajun Liang, Yilei Jiang, Yuanxing Zhang, Jinyuan Chen, Yaozhi Zheng, Xintao Wang, Pengfei Wan, Xiangyu Yue, Jiaheng Liu 핵심 연구 목표 본 논문은 시각적 생성 모델의 후속 훈련을"},{"id":"2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA","title":"[논문리뷰] When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA","excerpt":"Artem Vazhentsev이 arXiv에 게시한 'When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA","tags":["Review","Hallucination Detection","Multilingual LLMs","Span-Level Annotation","Synthetic Data Generation","Question Answering (QA)","Encoder Models","Uncertainty Quantification","GPT-4o"],"text":"링크: 논문 PDF로 바로 열기 저자: Elisei Rykov, Kseniia Petrushina, Maksim Savkin, Valerii Olisov, Artem Vazhentsev, Kseniia Titova, Alexander Panchenko, Vasily Konovalov, Julia Belikova 핵심 연구 목표 대규모 언어 모델(LLM)의 "},{"id":"2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation","title":"[논문리뷰] WithAnyone: Towards Controllable and ID Consistent Image Generation","excerpt":"arXiv에 게시된 'WithAnyone: Towards Controllable and ID Consistent Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation","tags":["Review","Identity-Consistent Generation","Text-to-Image Diffusion","Copy-Paste Artifacts","Contrastive Learning","Multi-Identity Dataset","Controllable Generation","ID-Preservation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, YuGang Jiang 핵심 연구 목표 본 논문은 텍스트투이미지 생성 모델에서 레퍼런스 인물의 ID(Iden"},{"id":"2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation","title":"[논문리뷰] pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation","excerpt":"arXiv에 게시된 'pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-17 13:09:57+0900","permalink":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation","tags":["Review","Diffusion Models","Flow Matching","Generative Models","Model Distillation","Imitation Learning","Few-Step Generation","Policy-Based AI","Text-to-Image"],"text":"링크: 논문 PDF로 바로 열기 저자: Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi 핵심 연구 목표 이 논문은 기존 fewstep 확산 및 흐름 기반 생성 모델의 증류 과정에서 발생하는 품질다양성 트레이드오프 와 복잡한 훈련 절차 문제를 해결하고자 합니다. 특히, "},{"id":"2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents","title":"[논문리뷰] ACON: Optimizing Context Compression for Long-horizon LLM Agents","excerpt":"arXiv에 게시된 'ACON: Optimizing Context Compression for Long-horizon LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents","tags":["Review","LLM Agents","Context Compression","Long-horizon Tasks","Prompt Optimization","Knowledge Distillation","Memory Efficiency","Task Performance","Failure Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Minki Kang, WeiNing Chen, Dongge Han, Huseyin A. Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, Saravan Rajmohan 핵심 연구 목표 본 논문은 장기(longhorizon) LLM 에이전트 태스크 에서 발생하는 컨텍스트 길이 증가 문"},{"id":"2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications","title":"[논문리뷰] An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications","excerpt":"Bram Adams이 arXiv에 게시한 'An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications","tags":["Review","AI Agent","LLM Agent","Testing","Empirical Study","Software Quality","Agent Frameworks","Agentic Applications","Non-Determinism"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohammed Mehedi Hasan, Hao Li, Emad Fallahzadeh, Gopi Krishnan Rajbahadur, Bram Adams, Ahmed E. Hassan 핵심 연구 목표 본 연구는 FM(Foundation Model) 기반 AI 에이전트 의 본질적인 비결정론적 특성과 재현 불가능성으로 인"},{"id":"2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum","title":"[논문리뷰] Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum","excerpt":"Hanghang Tong이 arXiv에 게시한 'Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum","tags":["Review","Supervised Fine-tuning (SFT)","Large Language Models (LLMs)","Training Objectives","Negative Log Likelihood (NLL)","Model Capability Continuum","Generalization","Probability-based Loss Functions"],"text":"링크: 논문 PDF로 바로 열기 저자: Gaotang Li, Ruizhong Qiu, Xiusi Chen, Heng Ji, Hanghang Tong 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM) 의 지도 미세 조정(SFT) 에서 흔히 발생하는 일반화 한계를 해결하고자 합니다. 특히, 기본 훈련 목표인 음의 로그 가능도(NLL) 가 사전 훈련된 모델의"},{"id":"2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses","title":"[논문리뷰] BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses","excerpt":"Julian McAuley이 arXiv에 게시한 'BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses","tags":["Review","LLM Bias Mitigation","Benchmark","Evaluation Metrics","Prompt Engineering","Fine-tuning","Bias-Free Score","Fairness"],"text":"링크: 논문 PDF로 바로 열기 BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses Julian McAuley, Ruizhe Chen, Churan Zhi, Xunzhi He, Xin Xu 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 편향 완화(bias "},{"id":"2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","title":"[논문리뷰] BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration","excerpt":"Xiangyang Xia이 arXiv에 게시한 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","tags":["Review","Video Generation","Subject Consistency","Cross-Modal Integration","Diffusion Models","Multimodal LLM","Diffusion Transformer","Text-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, Zehuan Yuan 핵심 연구 목표 기존 비디오 생성 모델들이 복잡한 공간 관계, 시간적 논리, 다중 주체 상호작용을 포함하는 프롬프"},{"id":"2025-10-2-Boolean-Satisfiability-via-Imitation-Learning","title":"[논문리뷰] Boolean Satisfiability via Imitation Learning","excerpt":"Xiangyu Xu이 arXiv에 게시한 'Boolean Satisfiability via Imitation Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Boolean-Satisfiability-via-Imitation-Learning","tags":["Review","Boolean Satisfiability","Imitation Learning","CDCL Solvers","Branching Policy","KeyTrace","Transformer Architecture","Perceiver AR"],"text":"링크: 논문 PDF로 바로 열기 저자: Zewei Zhang, Huan Liu, Yuanhao Yu, Jun Chen, Xiangyu Xu 핵심 연구 목표 본 논문은 CDCL(ConflictDriven Clause Learning) SAT solver 의 핵심 구성 요소인 브랜칭 정책의 비효율성을 개선하는 것을 목표로 합니다. 기존 강화 학습 기반 방법의 "},{"id":"2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration","title":"[논문리뷰] BroRL: Scaling Reinforcement Learning via Broadened Exploration","excerpt":"arXiv에 게시된 'BroRL: Scaling Reinforcement Learning via Broadened Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration","tags":["Review","Reinforcement Learning","LLMs","Scaling Laws","Exploration","Rollout Size","Verifiable Rewards","PPO","Mass Balance Equation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Hu, Mingjie Liu, Ximing Lu, Fang Wu, Zaid Harchaoui, Shizhe Diao, Yejin Choi, Pavlo Molchanov, Jun Yang, Jan Kautz, Yi Dong 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)의 복잡한 추론 능력을 향상시키기 위"},{"id":"2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation","title":"[논문리뷰] Code2Video: A Code-centric Paradigm for Educational Video Generation","excerpt":"arXiv에 게시된 'Code2Video: A Code-centric Paradigm for Educational Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation","tags":["Review","Educational Video Generation","Code-centric AI","Multi-agent Framework","Manim","Vision-Language Models","Knowledge Transfer","Code Generation","MMMC Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanzhe Chen, Kevin Qinghong Lin, Mike Zheng Shou 핵심 연구 목표 최근 픽셀 기반 생성 모델들은 전문적인 교육용 비디오 제작에 어려움을 겪습니다. 특히 학문적 지식, 정밀한 시각 구조, 일관된 전환이 필요한데, Code2Video는 이러한 한계를 극복하기 위해 실행 가능한 Pyth"},{"id":"2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs","title":"[논문리뷰] CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs","excerpt":"Hengyi Cai이 arXiv에 게시한 'CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs","tags":["Review","Curriculum Learning","LLMs","Reasoning","Gradient Optimization","Reinforcement Learning","Bayesian Inference","Sample Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yongcheng Zeng, Zexu Sun, Bokai Ji, Erxue Min, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Haifeng Zhang, Xu Chen, Jun Wang 핵심 연구 목표 본 연구는 추론 태스크에서 대규모 언어 모델( LLMs )의 훈련 효율성을 향상시키는 것"},{"id":"2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search","title":"[논문리뷰] DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search","excerpt":"arXiv에 게시된 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search","tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Monte Carlo Tree Search (MCTS)","Mathematical Reasoning","Large Language Models (LLMs)","Systematic Exploration","Adaptive Training","Tree-GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi 핵심 연구 목표 논문은 LLM의 추론 능력 향상을 위한 Verifiable Rewards 기반의 강화 학습(RLVR) 에서 발생하는 훈련 정체(training plateaus) 및 불"},{"id":"2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models","title":"[논문리뷰] Eliciting Secret Knowledge from Language Models","excerpt":"Neel Nanda이 arXiv에 게시한 'Eliciting Secret Knowledge from Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models","tags":["Review","Language Models","Secret Elicitation","Mechanistic Interpretability","Black-box Methods","White-box Methods","AI Auditing","Model Organisms","Prefill Attacks"],"text":"링크: 논문 PDF로 바로 열기 저자: Bartosz Cywiński, Emil Ryd, Rowan Wang, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy, Samuel Marks 핵심 연구 목표 이 논문은 AI 모델이 명시적으로 표현하지 않는 내재된 지식, 즉 \"비밀 지식(secret knowledge)\"을 "},{"id":"2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution","title":"[논문리뷰] Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution","excerpt":"arXiv에 게시된 'Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution","tags":["Review","LLM Agents","Parallel Execution","DAG-based Planning","Tool Orchestration","Web Agents","Reasoning Framework","Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: OPPO AI Agent Team (Wangchunshu Zhou, Xitong Gao 등) 핵심 연구 목표 본 논문은 기존 LLM 에이전트 프레임워크의 고질적인 문제인 비효율적인 순차적 처리 방식 을 해결하여, 복잡한 웹 기반 추론 작업에서 발생하는 과도한 실행 단계와 긴 지연 시간을 단축하는 것을 목표로 합니다. "},{"id":"2025-10-2-GEM-A-Gym-for-Agentic-LLMs","title":"[논문리뷰] GEM: A Gym for Agentic LLMs","excerpt":"arXiv에 게시된 'GEM: A Gym for Agentic LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-GEM-A-Gym-for-Agentic-LLMs","tags":["Review","Agentic LLMs","Reinforcement Learning","Environment Simulator","Multi-turn Interactions","Return Batch Normalization","Tool Integration","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Zichen Liu, Anya Sims, Keyu Duan, Changyu Chen, Simon Yu, Xiangxin Zhou, Haotian Xu, Shaopan Xiong, Bo Liu, Chenmien Tan, Chuen Yang Beh, Weixun Wang, Hao Zhu, Weiyan Shi, Diyi Y"},{"id":"2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness","title":"[논문리뷰] GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness","excerpt":"Chien-Sheng Wu이 arXiv에 게시한 'GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness","tags":["Review","GUI Agents","KV Cache Compression","Spatio-Temporal Awareness","Vision-Language Models","Efficiency","Attention Sparsity","QR Decomposition"],"text":"링크: 논문 PDF로 바로 열기 저자: KungHsiang Huang, Haoyi Qiu, Yutong Dai, Caiming Xiong, ChienSheng Wu 핵심 연구 목표 본 논문은 VisionLanguage Model (VLM) 기반 GUI 에이전트가 고해상도 스크린샷 시퀀스 및 장기 작업을 처리할 때 발생하는 비효율성 문제를 해결하는 것을 목표"},{"id":"2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures","title":"[논문리뷰] Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures","excerpt":"Andrea Passerini이 arXiv에 게시한 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures","tags":["Review","LLM Interpretability","Vector Symbolic Architectures","Neural Probing","Information Decoding","Hyperdimensional Computing","Latent Representations"],"text":"링크: 논문 PDF로 바로 열기 저자: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini 핵심 연구 목표 대규모 언어 모델(LLM)의 불투명한 내부 표현에 대한 제한적인 이해를 극복하고, LLM 벡터 공간 에서 사람이 해석할 수 있는 정보를 디코딩 하는 새로운 패러다임"},{"id":"2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning","title":"[논문리뷰] In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning","excerpt":"Chaehyeon Chung이 arXiv에 게시한 'In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning","tags":["Review","LLM Feedback","Multi-turn Reasoning","In-place Editing","Token Efficiency","Error Correction","Human-AI Interaction","Reasoning Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaehyeon Chung, Seunghyuk Cho, Saemi Moon, Minjong Lee, Youngbin Choi 핵심 연구 목표 본 연구는 다중 턴(multiturn) 추론 과정에서 대규모 언어 모델(LLMs)이 사용자 피드백을 신뢰성 있게 통합하지 못하는 문제를 해결하는 것을 목표로 합니다. 기존 피드"},{"id":"2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents","title":"[논문리뷰] Infusing Theory of Mind into Socially Intelligent LLM Agents","excerpt":"arXiv에 게시된 'Infusing Theory of Mind into Socially Intelligent LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents","tags":["Review","Theory of Mind","Large Language Models","Social Agents","Dialogue Systems","Mental State Modeling","Look-ahead Planning","Supervised Fine-tuning","Sotopia Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: EunJeong Hwang, Yuwei Yin, Giuseppe Carenini, Peter West, Vered Shwartz 핵심 연구 목표 본 논문은 대화형 LLM(Large Language Model) 기반 소셜 에이전트가 타인의 정신 상태 이해 능력(Theory of Mind, ToM) 을 통합함으로써 사회적"},{"id":"2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA","title":"[논문리뷰] JoyAgent-JDGenie: Technical Report on the GAIA","excerpt":"arXiv에 게시된 'JoyAgent-JDGenie: Technical Report on the GAIA' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA","tags":["Review","Generalist Agent","Multi-Agent System","Plan-Execute","ReAct","Hierarchical Memory","Tool Integration","GAIA Benchmark","LLM Agent"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiarun Liu, Shiyue Xu, Shangkun Liu, Yang Li, Wen Liu, Min Liu, Xiaoqing Zhou, Hanmin Wang, Shilin Jia, zhen Wang, Shaohua Tian, Hanhao Li, Junbo Zhang, Yongli Yu, Peng Cao (Jing"},{"id":"2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation","title":"[논문리뷰] Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation","excerpt":"arXiv에 게시된 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation","tags":["Review","Large Language Models (LLMs)","Reinforcement Learning (RL)","Exploration Budget Allocation","Knapsack Problem","Group Relative Policy Optimization (GRPO)","Mathematical Reasoning","Resource Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziniu Li, Congliang Chen, Tianyun Yang, Ding Tian, Ruoyu Sun, Ge Zhang, Wenhao Huang, ZhiQuan Luo 핵심 연구 목표 본 연구는 LLM의 RL 기반 자기 개선 과정에서 발생하는 높은 연산 비용과 비효율적인 탐색 예산 할당 문제를 해결하고자 합니다"},{"id":"2025-10-2-Making-not-Taking-the-Best-of-N","title":"[논문리뷰] Making, not Taking, the Best of N","excerpt":"arXiv에 게시된 'Making, not Taking, the Best of N' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Making-not-Taking-the-Best-of-N","tags":["Review","LLM Aggregation","Generative Fusion","Best-of-N","Synthetic Data Generation","Test-Time Scaling","Multilingual Models","Ensemble Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Ammar Khairi, Daniel D'souza, Marzieh Fadaee, Julia Kreutzer 핵심 연구 목표 본 논문은 기존 BestofN (BON) 방식이 여러 LLM 생성물 중 하나만을 선택하여 잠재적으로 유용한 정보를 버리는 제로섬 게임이라는 문제점을 지적합니다. 대신, 모든 후보 생성물이 최종 "},{"id":"2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models","title":"[논문리뷰] On Predictability of Reinforcement Learning Dynamics for Large Language Models","excerpt":"Yuqing Huang이 arXiv에 게시한 'On Predictability of Reinforcement Learning Dynamics for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models","tags":["Review","Reinforcement Learning","Large Language Models","Parameter Dynamics","Rank-1 Dominance","Linear Dynamics","SVD","Model Acceleration","Predictability"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchen Cai, Ding Cao, Xin Xu, Zijun Yao, Yuqing Huang, Zhenyu Tan, Benyi Zhang, Guiquan Liu, Junfeng Fang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 강화 학습(RL) 훈련 과정에서 발생하는 파라미터 업데이트 동역학 에 대한"},{"id":"2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning","title":"[논문리뷰] PIPer: On-Device Environment Setup via Online Reinforcement Learning","excerpt":"arXiv에 게시된 'PIPer: On-Device Environment Setup via Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning","tags":["Review","Environment Setup","LLMs","Reinforcement Learning","Supervised Fine-tuning","On-device AI","Software Engineering","Verifiable Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexander Kovrigin, Aleksandra Eliseeva, Konstantin Grotov, Egor Bogomolov, Yaroslav Zharov 핵심 연구 목표 소프트웨어 엔지니어링(SE)에서 환경 설정(environment setup)은 지속적인 과제로 남아 있으며, 기존 대규모 언어 모델(LLM"},{"id":"2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction","title":"[논문리뷰] ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction","excerpt":"arXiv에 게시된 'ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction","tags":["Review","Sliced Wasserstein Distance","Reservoir Sampling","Variance Reduction","Distribution Matching","Diffusion Guidance","Color Correction","Monte Carlo Estimation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mark Boss, Andreas Engelhardt, Simon Donné, Varun Jampani 핵심 연구 목표 본 논문은 분포 매칭(distribution matching)에서 널리 사용되는 Sliced Wasserstein Distance (SWD) 의 Monte Carlo 추정기가 겪는 높은 분산 문제를 "},{"id":"2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned","title":"[논문리뷰] Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned","excerpt":"arXiv에 게시된 'Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned","tags":["Review","Vision-Language Models (VLMs)","Process Reward Models (PRMs)","Multimodal Reasoning","Test-Time Scaling (TTS)","Process Supervision","Dataset Construction","Perception Errors","MCTS"],"text":"링크: 논문 PDF로 바로 열기 저자: Brandon Ong, Tej Deep Pala, Vernon Toh, William Chandra Tjhi, Soujanya Poria 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)의 추론 신뢰성을 향상시키는 프로세스 보상 모델(PRM)을 시각언어 모델(VLM) 영역으로 확장하고자 합니다. 특히, 기존 VLM"},{"id":"2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators","title":"[논문리뷰] VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators","excerpt":"Zirui Ge이 arXiv에 게시한 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators","tags":["Review","Vision-Language-Action Models","Reinforcement Learning","World Models","Fine-tuning","Embodied AI","Robotics","Reward Design","Distribution Shift"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, Weihua Su 핵심 연구 목표 본 논문은 모방 학습의 한계점(오류 누적, 분포 변화"},{"id":"2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs","title":"[논문리뷰] VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs","excerpt":"arXiv에 게시된 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs","tags":["Review","Vision-Language Models","Object Grounding","Fine-grained Perception","Hybrid Region Encoder","Plug-and-play","Two-stage Training","Visual Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Peng Liu, Haozhan Shen, Chunxin Fang, Zhicheng Sun, Jiajia Liao, Tiancheng Zhao 핵심 연구 목표 본 논문은 기존 VLM(VisionLanguage Models)이 고수준 장면 이해에는 뛰어나지만, 정밀한 공간적 지역화가 필요한 미세 조정 지각(finegra"},{"id":"2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls","title":"[논문리뷰] Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls","excerpt":"Stuart Shieber이 arXiv에 게시한 'Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-02 13:30:22+0900","permalink":"/ai/review/2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls","tags":["Review","Transformers","Multiplication","Long-Range Dependencies","Implicit Chain-of-Thought","Attention Mechanisms","Inductive Bias","Reverse Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoyan Bai, Itamar Pres, Yuntian Deng, Chenhao Tan, Stuart Shieber, Fernanda Viégas, Martin Wattenberg, Andrew Lee 핵심 연구 목표 본 논문은 Transformer 기반 언어 모델이 다중 자릿수 곱셈과 같은 겉보기에 간단한 알고"},{"id":"2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning","title":"[논문리뷰] A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning","excerpt":"arXiv에 게시된 'A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning","tags":["Review","Adaptive Agent","Foundation Model","Hybrid Reasoning","Tool-Aware LLM","Mode Selection","Reinforcement Learning","Cost Efficiency","LLM Agent"],"text":"링크: 논문 PDF로 바로 열기 저자: OPPO AI Agent Team 핵심 연구 목표 이 논문은 추론 중심 LLM(도구 사용 불가)과 에이전트 중심 LLM(추론 능력 부족) 간의 근본적인 격차를 해결하고자 합니다. 기존 LLM들이 단순 질의에 대해 불필요하게 과도한 추론이나 도구 호출을 수행하는 비효율성을 해소하며, 단일 백본 아래에서 에이전틱, 추론,"},{"id":"2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation","title":"[논문리뷰] BLIP3o-NEXT: Next Frontier of Native Image Generation","excerpt":"arXiv에 게시된 'BLIP3o-NEXT: Next Frontier of Native Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation","tags":["Review","Image Generation","Image Editing","Autoregressive Model","Diffusion Model","Reinforcement Learning","Multimodal AI","Foundation Model","Open-source"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Silvio Savarese, Caiming Xiong, Ran Xu 핵심 연구 목표 본 "},{"id":"2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation","title":"[논문리뷰] Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation","excerpt":"Cat Yan이 arXiv에 게시한 'Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation","tags":["Review","Multiagent Systems","Science Automation","Dynamic Workflows","Workspace-based Communication","Context Compaction","Human-in-the-loop AI","Open-source Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Cat Yan, Xintian Pan, Junyu Ren, zhuoranyang, edli 핵심 연구 목표 과학 자동화를 위한 기존 에이전트 시스템의 고정된 워크플로우 와 불충분한 컨텍스트 관리 라는 한계를 극복하는 것입니다. 궁극적으로는 동적이고 상호작용적인 다중 에이전트 프레임워크를 통해 지속적인 장기 연구 프로그"},{"id":"2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning","title":"[논문리뷰] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning","excerpt":"arXiv에 게시된 'DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Length Penalty","Reasoning Efficiency","Large Language Models","RL Optimization","Accuracy-Efficiency Trade-off","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: ShihYang Liu¹, Xin Dong, Ximing Lu, Shizhe Diao, Mingjie Liu, MinHung Chen, Hongxu Yin, YuChiang Frank Wang, KwangTing Cheng¹, Yejin Choi, Jan Kautz, Pavlo Molchanov 핵심 연구 목표 본 논"},{"id":"2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion","title":"[논문리뷰] DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion","excerpt":"arXiv에 게시된 'DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion","tags":["Review","Driving Scene Generation","Video Diffusion","3D Reconstruction","Gaussian Splatting","Feed-Forward Models","Temporal Coherence","Multimodal Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Weijie Wang, Jiagang Zhu, Zheng Zhu, Zeyu Zhang, Xiaofeng Wang, Chaojun Ni, Haoxiao Wang, Guan Huang, Xinze Chen, Yukun Zhou, Wenkang Qin, Duochao Shi, Haoyun Li, Guanghong Jia, "},{"id":"2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models","title":"[논문리뷰] ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models","excerpt":"Sean O'Brien이 arXiv에 게시한 'ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models","tags":["Review","Multi-turn Conversation","Large Language Models (LLMs)","Context Management","Entropy-guided Resetting","Uncertainty Quantification","Performance Degradation","Prompt Engineering","Conversational AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Haziq Mohammad Khalid, Athikash Jeyaganthan, Timothy Do, Yicheng Fu, Sean O'Brien, Vasu Sharma, Kevin Zhu 핵심 연구 목표 논문은 다중 턴 대화에서 Large Language Models (LLMs) 의 성능이 저하되는 문제를 해결하는 "},{"id":"2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs","title":"[논문리뷰] Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs","excerpt":"Kevin Zhu이 arXiv에 게시한 'Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs","tags":["Review","Emergent Misalignment","In-Context Learning","LLM Safety","Persona Rationalization","Prompt Engineering","Model Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Nikita Afonin, Nikita Andriyanov, Nikhil Bageshpura, Kyle Liu, Kevin Zhu, Sunishchal Dev, Ashwinee Panda, Alexander Panchenko, Oleg Rogov, Elena Tutubalina, Mikhail Seleznyov 핵심 "},{"id":"2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents","title":"[논문리뷰] Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents","excerpt":"Jianshu Zhang이 arXiv에 게시한 'Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents","tags":["Review","Web Agents","Information Aggregation","Data Synthesis","Online Exploration","Foundation Models","Multi-hop QA","Deep Research"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianshu Zhang, JunYu Ma, Ce Zhang, Rui Wang, tqfang229 핵심 연구 목표 기존 웹 에이전트 시스템들이 정보 탐색 기능에만 중점을 두고 정보 집계 능력을 간과하여 심층적인 연구 결과 생성을 제한하는 문제를 해결하고자 합니다. 복잡한 정보 집계 능력을 갖춘 웹 에이전트 훈련을 위한"},{"id":"2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain","title":"[논문리뷰] FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain","excerpt":"Arman Cohan이 arXiv에 게시한 'FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain","tags":["Review","LLM Trustworthiness","Finance Domain","Benchmark","Alignment Evaluation","Financial AI","Hallucination","Privacy","Fairness"],"text":"링크: 논문 PDF로 바로 열기 저자: Tiansheng Hu, Tongyan Hu, Arman Cohan, Liuyang Bai, Yilun Zhao, Chen Zhao 핵심 연구 목표 본 논문은 금융 도메인에서 대규모 언어 모델(LLM)의 신뢰성을 종합적으로 평가하기 위한 FINTRUST 벤치마크를 제시합니다. 금융 분야의 높은 위험도와 엄격한 신뢰성 "},{"id":"2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition","title":"[논문리뷰] Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition","excerpt":"arXiv에 게시된 'Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition","tags":["Review","Foundation Models","Scientific Discovery","Paradigm Shift","Human-AI Collaboration","Autonomous Agents","Meta-Science","Experimental Design","Hypothesis Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Fan Liu, Jindong Han, Tengfei Lyu, Weijia Zhang, ZheRui Yang, Lu Dai, Cancheng Liu, Hao Liu 핵심 연구 목표 본 논문은 GPT4 및 AlphaFold와 같은 파운데이션 모델(FMs) 이 과학 연구의 기존 방법론을 단순히 개선하는 것을 넘어, 새로운"},{"id":"2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation","title":"[논문리뷰] Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation","excerpt":"Junsheng Yu이 arXiv에 게시한 'Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation","tags":["Review","3D Scene Layout Generation","Vision-guided","Diffusion Models","Scene Graph","Asset Retrieval","Pose Estimation","High-Quality Assets","AI Content Creation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoming Zhu, Xu Huang, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan, Zhongyuan Liu, Lin Zhu, Qijun Zhao, Ligang Liu, Long Zeng 핵심 연구 목표 본 논문은 기존의 수동 최적화 방법론, 심층 생성 모델, 대규"},{"id":"2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training","title":"[논문리뷰] InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training","excerpt":"Congkai Xie이 arXiv에 게시한 'InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training","tags":["Review","LLMs","Reinforcement Learning","Rubric-Based Training","Medical Dialogue","Open-Ended Tasks","HealthBench","RAG"],"text":"링크: 논문 PDF로 바로 열기 저자: Congkai Xie, Zhijie Sang, Pengwei Liu, Qi Zuo, Pengkai Wang 핵심 연구 목표 본 논문은 보상 함수가 모호하고 주관적인 개방형 AI 태스크 , 특히 의료 상담 과 같은 고위험 시나리오에서 LLM의 성능 향상을 목표로 합니다. 기존 강화 학습(RL) 방법론의 한계를 극복하고,"},{"id":"2025-10-20-Language-Models-Model-Language","title":"[논문리뷰] Language Models Model Language","excerpt":"arXiv에 게시된 'Language Models Model Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Language-Models-Model-Language","tags":["Review","Large Language Models","Linguistics","Witold Mańczak","Frequency Hypothesis","Empirical Validation","Usage-Based Linguistics","Semantic Embeddings"],"text":"링크: 논문 PDF로 바로 열기 저자: Łukasz Borchmann 핵심 연구 목표 전통적인 언어학적 비판(예: Chomsky, de Saussure)에 맞서 LLM이 언어를 모델링하는 능력을 재평가하고, Witold Mańczak의 경험주의적 원칙에 기반한 대안적인 이론적 프레임워크를 제시하는 것을 목표로 합니다. 이를 통해 LLM의 설계, 평가 및 해"},{"id":"2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder","title":"[논문리뷰] Latent Diffusion Model without Variational Autoencoder","excerpt":"arXiv에 게시된 'Latent Diffusion Model without Variational Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder","tags":["Review","Latent Diffusion Model","Variational Autoencoder","Self-supervised Learning","DINO Features","Generative Models","Image Generation","Training Efficiency","Unified Representation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, Jiwen Lu 핵심 연구 목표 기존 잠재 확산 모델(LDM)이 VAE(Variational Autoencoder) 의 한계로 인해 훈련"},{"id":"2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal","title":"[논문리뷰] LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal","excerpt":"arXiv에 게시된 'LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal","tags":["Review","Lens Flare Removal","Diffusion Models","Image Outpainting","Deep Learning","Image Restoration","Preprocessing","LoRA"],"text":"링크: 논문 PDF로 바로 열기 저자: ShrRuei Tsai, WeiCheng Chang, JieYing Lee, ChihHai Su, YuLun Liu 핵심 연구 목표 본 연구는 불완전하거나 프레임 외부의 광원이 존재할 때 기존 단일 이미지 플레어 제거(SIFR) 모델 의 성능이 저하되는 문제를 해결하고자 합니다. 완전한 광원 정보를 재구성함으로써 SI"},{"id":"2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning","title":"[논문리뷰] MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning","excerpt":"arXiv에 게시된 'MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning","tags":["Review","LLM Evaluation","Reasoning Benchmark","Difficulty Adaptation","Multimodal AI","Proof Graph","Agent Recognition","Automated Question Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xukai Wang, Xuanbo Liu, Mingrui Chen, Haitian Zhong, Xuanlin Yang, Bohan Zeng, Jinbo Hu, Hao Liang, Junbo Niu, Xuchen Li, Ruitao Wu, Ruichuan An, Yang Shi, Liu Liu, XuYao Zhang, "},{"id":"2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks","title":"[논문리뷰] NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks","excerpt":"Hongyu Yan이 arXiv에 게시한 'NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks","tags":["Review","3D Object Editing","Training-Free","FlowEdit","Mask-Free","Deep Generative Models","TRELLIS","Data Generation","Geometric Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Junliang Ye, Shenghao Xie, Ruowen Zhao, Zhengyi Wang, Hongyu Yan 핵심 연구 목표 본 논문은 기존 3D 객체 편집 방법들이 비효율적이고 일관성이 부족하며, 편집되지 않은 영역을 보존하는 데 실패하는 문제를 해결하고자 합니다. 특히, 마스크나 시간 소모적인 최적화 없이도"},{"id":"2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM","title":"[논문리뷰] OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM","excerpt":"arXiv에 게시된 'OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM","tags":["Review","Omni-Modal LLM","Multimodal Understanding","Vision-Audio Alignment","Temporal Reasoning","Data Curation","Foundation Models","Contrastive Learning","Rotary Time Embedding"],"text":"링크: 논문 PDF로 바로 열기 OmniVinci: Enhancing Architecture and Data for OmniModal Understanding LLM 저자: Hanrong Ye, ChaoHan Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, AnChieh Cheng"},{"id":"2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive","title":"[논문리뷰] Paper2Web: Let's Make Your Paper Alive!","excerpt":"Yao Wan이 arXiv에 게시한 'Paper2Web: Let's Make Your Paper Alive!' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive","tags":["Review","Academic Webpage Generation","Multi-Agent Systems","Large Language Models","Model Context Protocol","Interactive Content","Multimedia Dissemination","Evaluation Benchmark","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhang Chen, Tianpeng Lv, Siyi Zhang, Yixiang Yin, Yao Wan 핵심 연구 목표 이 논문은 학술 논문을 레이아웃 인식적이고 상호작용적이며 멀티미디어 가 풍부한 웹 페이지로 변환하는 PAPER2WEB 이라는 새로운 태스크를 제안합니다. 기존 LLM 기반 또는 템플릿 방식의 웹 페"},{"id":"2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models","title":"[논문리뷰] Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models","excerpt":"Shiwei Liu이 arXiv에 게시한 'Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models","tags":["Review","Mixture-of-Experts (MoE)","Online Adaptation","Test-Time Adaptation (TTA)","Expert Routing","Large Language Models (LLMs)","Self-Supervision","Computational Efficiency","Context Shift Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Guinan Su, Yanwu Yang, Li Shen, Lu Yin, Shiwei Liu, Jonas Geiping 핵심 연구 목표 MoE(MixtureofExperts) 모델이 배포 시 발생하는 분포 변화(distribution shifts) 로 인해 차선적인 라우팅 결정(suboptimal routing deci"},{"id":"2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning","title":"[논문리뷰] Robust Layerwise Scaling Rules by Proper Weight Decay Tuning","excerpt":"arXiv에 게시된 'Robust Layerwise Scaling Rules by Proper Weight Decay Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning","tags":["Review","Weight Decay Scaling","Maximal-Update Parameterization (µP)","AdamW","Transformer","Hyperparameter Transfer","Scaling Laws","Singular Value Spectrum","Steady State Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Fan, Yifeng Liu, Qingyue Zhao, Angela Yuan, Quanquan Gu 핵심 연구 목표 본 논문은 이 현대 스케일 불변 아키텍처에서 훈련의 에 도달했을 때 발생하는 학습률 전이(transfer) 저하 문제를 해결하고자 합니다. 특히, 옵티마이저를 사용할 때 발생하는 문제로, "},{"id":"2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset","title":"[논문리뷰] Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset","excerpt":"Hao Ouyang이 arXiv에 게시한 'Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset","tags":["Review","Video Editing","Instruction-Based Editing","Synthetic Data Generation","Dataset","Curriculum Learning","Diffusion Models","Vision-Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Qingyan Bai, Qiuyu Wang, Hao Ouyang, Yue Yu, Hanlin Wang, Wen Wang, Ka Leong Cheng, Shuailei Ma, Yanhong Zeng, Zichen Liu, Yinghao Xu, Yujun Shen, Qifeng Chen 핵심 연구 목표 지시 기반 비디오 "},{"id":"2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery","title":"[논문리뷰] Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery","excerpt":"Chung-Ho Wu이 arXiv에 게시한 'Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery","tags":["Review","3D Scene Synthesis","Gaussian Splatting","Satellite Imagery","Diffusion Models","Urban Modeling","Novel View Synthesis","Curriculum Learning","Real-time Rendering"],"text":"링크: 논문 PDF로 바로 열기 저자: JieYing Lee, YiRuei Liu, ShrRuei Tsai, WeiCheng Chang, ChungHo Wu, Jiewen Chan, Zhenjun Zhao, Chieh Hubert Lin, YuLun Liu 핵심 연구 목표 본 논문은 대규모의 탐색 가능하며 기하학적으로 정확한 3D 도시 장면을 합성하는 문제"},{"id":"2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data","title":"[논문리뷰] Train a Unified Multimodal Data Quality Classifier with Synthetic Data","excerpt":"Ritesh Sarkhel이 arXiv에 게시한 'Train a Unified Multimodal Data Quality Classifier with Synthetic Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data","tags":["Review","Multimodal Data Quality","MLLM","Synthetic Data","Data Filtering","Image-Text Captioning","Interleaved Document Analysis","Pre-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Ritesh Sarkhel, Colin Lockard, Shiyang Li, Rongmei Lin, weizhiwang 핵심 연구 목표 멀티모달 대규모 언어 모델(MLLM) 사전 학습에 사용되는 이미지텍스트 캡션 및 인터리브된 문서 데이터의 고품질 필터링 방법이 미흡하다는 문제를 해결하고자 합니다. 기존 CLIPSco"},{"id":"2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent","title":"[논문리뷰] VISTA: A Test-Time Self-Improving Video Generation Agent","excerpt":"Tomas Pfister이 arXiv에 게시한 'VISTA: A Test-Time Self-Improving Video Generation Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-20 13:04:24+0900","permalink":"/ai/review/2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent","tags":["Review","Text-to-Video Generation","Prompt Optimization","Multi-Agent System","Test-Time Improvement","MLLM-as-a-Judge","Video Evaluation","Audio-Video Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Do Xuan Long, Xingchen Wan, Hootan Nakhost, ChenYu Lee, Tomas Pfister, Sercan Ö. Arık 핵심 연구 목표 본 논문은 텍스트투비디오(T2V) 생성 모델이 사용자 프롬프트에 매우 민감 하여 고품질 비디오를 얻기 위한 반복적인 프롬프트 수정과 필터링이 필요하다"},{"id":"2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe","title":"[논문리뷰] Agentic Reinforcement Learning for Search is Unsafe","excerpt":"arXiv에 게시된 'Agentic Reinforcement Learning for Search is Unsafe' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe","tags":["Review","Agentic Reinforcement Learning","LLM Safety","Tool Use","Search Models","Jailbreaking","Instruction Tuning","Vulnerability"],"text":"링크: 논문 PDF로 바로 열기 저자: Yushi Yang, Shreyansh Padarha, Andrew Lee, Adam Mahdi 핵심 연구 목표 본 논문은 에이전트형 강화 학습(RL)으로 훈련된 검색 모델의 안전성, 특히 유해한 요청에 대한 거부 능력과 기존 지시 튜닝(Instruction Tuning)으로부터 물려받은 안전성 속성이 어떻게 변화하는"},{"id":"2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment","title":"[논문리뷰] Annotation-Efficient Universal Honesty Alignment","excerpt":"Jingtong Wu이 arXiv에 게시한 'Annotation-Efficient Universal Honesty Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment","tags":["Review","LLM Honesty Alignment","Confidence Calibration","Annotation Efficiency","Self-Consistency","Elicitation-Then-Calibration (EliCal)","HonestyBench","LoRA","Trustworthy AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Shiyu Ni, Keping Bi, Jiafeng Guo, Minghao Tang, Jingtong Wu, Zengxin Han, Xueqi Cheng 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 지식 경계를 인식하고 보정된 자신감을 표현하는 Honesty Alignment 를 달성하는 것을 목표로 합니다"},{"id":"2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning","title":"[논문리뷰] AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning","excerpt":"Nikos Vlassis이 arXiv에 게시한 'AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning","tags":["Review","Real-Time Interaction","Asynchronous Agents","LLM Explanation","Human-AI Collaboration","Voice Interface","Planning and Reasoning","Context Management","Interruption Handling"],"text":"링크: 논문 PDF로 바로 열기 저자: Yueqian Lin, Zhengmian Hu, Jayakumar Subramanian, Qinsi Wang, Nikos Vlassis, Hai “Helen” Li, Yiran Chen 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 복잡한 추론 과정(ChainofThought, CoT)이 현재 모놀리식 텍스트"},{"id":"2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training","title":"[논문리뷰] Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training","excerpt":"Aditya Vir이 arXiv에 게시한 'Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training","tags":["Review","Satellite Image Classification","Multi-Task Attention","From-Scratch Training","EuroSAT Dataset","Squeeze-Excitation Networks","Coordinate Attention","CNN","Deep Learning Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Aditya Vir 핵심 연구 목표 이 논문은 사전 훈련된 모델 없이 위성 이미지 분류를 위한 맞춤형 CNN 아키텍처 를 체계적으로 연구하여 EuroSAT 데이터셋 에서 높은 정확도를 달성하는 것을 목표로 합니다. 위성 이미지 분류의 특정 실패 모드를 식별하고 해결하며, 공간 및 스펙트럼 특징 모달리티에 대한 균형 잡"},{"id":"2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting","title":"[논문리뷰] Chronos-2: From Univariate to Universal Forecasting","excerpt":"arXiv에 게시된 'Chronos-2: From Univariate to Universal Forecasting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting","tags":["Review","Time Series Forecasting","Foundation Models","Pretrained Models","Transformer","In-Context Learning","Multivariate Forecasting","Covariates","Group Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Abdul Fatir Ansari, Oleksandr Shchur, Jaris Küken, Andreas Auer, Boran Han, Pedro Mercado, Syama Sundar Rangapuram, Huibin Shen, Lorenzo Stella, Xiyuan Zhang, Mononito Goswami, S"},{"id":"2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing","title":"[논문리뷰] ConsistEdit: Highly Consistent and Precise Training-free Visual Editing","excerpt":"Xili Dai이 arXiv에 게시한 'ConsistEdit: Highly Consistent and Precise Training-free Visual Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing","tags":["Review","Image Editing","Video Editing","Diffusion Transformer","Attention Control","Training-free","Multi-modal Diffusion Transformer (MM-DiT)","Consistency Preservation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zixin Yin, LingHao Chen, Lionel Ni, Xili Dai 핵심 연구 목표 본 논문은 기존의 훈련 없이(trainingfree) 텍스트 기반 시각 편집 방법론이 겪는 한계, 즉 강한 편집 강도를 유지하면서도 원본과의 일관성을 보존하기 어렵다는 문제를 해결하고자 합니다. 특히 UNet 기반 모델 에"},{"id":"2025-10-21-Deep-Self-Evolving-Reasoning","title":"[논문리뷰] Deep Self-Evolving Reasoning","excerpt":"arXiv에 게시된 'Deep Self-Evolving Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Deep-Self-Evolving-Reasoning","tags":["Review","Deep Self-Evolving Reasoning","LLMs","Iterative Reasoning","Markov Chain","Self-Verification","Self-Refinement","Mathematical Reasoning","AIME Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihan Liu1, Shun Zheng†2, Xumeng Wen2, Yang Wang2, Jiang Bian², Mao Yang2 핵심 연구 목표 본 연구는 개방형 소형 언어 모델(LLM)이 어려운 추론 작업에서 취약한 검증 및 교정 능력으로 인해 한계에 부딪히는 문제를 해결하고자 합니다. Deep SelfEvolv"},{"id":"2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science","title":"[논문리뷰] DeepAnalyze: Agentic Large Language Models for Autonomous Data Science","excerpt":"arXiv에 게시된 'DeepAnalyze: Agentic Large Language Models for Autonomous Data Science' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science","tags":["Review","Autonomous Data Science","Agentic LLM","Curriculum Learning","Reinforcement Learning","Data Agents","End-to-end Data Science"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaolei Zhang, Ju Fan, Meihao Fan, Guoliang Li, Xiaoyong Du 핵심 연구 목표 본 논문은 원시 데이터부터 분석가 수준의 심층 연구 보고서에 이르는 완전히 자율적인 데이터 과학 을 달성하는 것을 목표로 합니다. 기존 워크플로우 기반 데이터 에이전트들이 사전 정의된 워크플로우에"},{"id":"2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense","title":"[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense","excerpt":"arXiv에 게시된 'Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense","tags":["Review","Large Reasoning Models (LRMs)","Prompt Injection","Adversarial Attack","Reasoning Distraction","Chain-of-Thought","Robustness","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhehao Zhang¹, Weijie Xu¹, Shixian Cui¹, Chandan K. Reddy¹ 핵심 연구 목표 본 논문은 대규모 추론 모델(LRMs)에서 '추론 방해(Reasoning Distraction)' 라는 새로운 취약점을 식별하고 체계적으로 분석하는 것을 목표로 합니다. 이는 프롬프트에 삽입된 관련"},{"id":"2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset","title":"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset","excerpt":"arXiv에 게시된 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset","tags":["Review","3D Motion Dataset","Multimodal Data","Human Behavior","Pose Tracking","Hand Tracking","Audio-Visual Data","Large-scale Dataset","SMPL-X"],"text":"링크: 논문 PDF로 바로 열기 저자: Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Ju"},{"id":"2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics","title":"[논문리뷰] Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics","excerpt":"arXiv에 게시된 'Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics","tags":["Review","Multi-Agent Systems","Deep Research","Enterprise AI","Human-in-the-Loop","Steerable AI","LLM Agents","Context Engineering","Enterprise Analytics"],"text":"링크: 논문 PDF로 바로 열기 저자: Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, Weiran Yao 핵심 연구 목표 본 논문은 기업이 비정형 데이터를 실용적인 통찰력으로 전환하는 과정에서 직면하는 어려움, 특히 기존 자율"},{"id":"2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research","title":"[논문리뷰] Executable Knowledge Graphs for Replicating AI Research","excerpt":"arXiv에 게시된 'Executable Knowledge Graphs for Replicating AI Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research","tags":["Review","AI Research Replication","Large Language Models (LLMs)","Knowledge Graphs (KGs)","Executable Code Generation","Retrieval-Augmented Generation (RAG)","PaperBench","Automated AI Research"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujie Luo, Zhuoyun Yu, Xuehai Wang, Yuqi Zhu, Ningyu Zhang, Lanning Wei, Lun Du, Da Zheng, Huajun Chen 핵심 연구 목표 AI 연구 재현은 LLM 에이전트 에게 중요한 도전 과제이며, 기존 방법론은 불충분한 배경 지식, RAG 방식의 한계,"},{"id":"2025-10-21-FineVision-Open-Data-Is-All-You-Need","title":"[논문리뷰] FineVision: Open Data Is All You Need","excerpt":"arXiv에 게시된 'FineVision: Open Data Is All You Need' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-FineVision-Open-Data-Is-All-You-Need","tags":["Review","Multimodal Datasets","VLM","Data Curation","Data Hygiene","De-duplication","Human-in-the-loop","GUI Automation","Test-set Decontamination"],"text":"링크: 논문 PDF로 바로 열기 저자: Luis Wiedmann, Leandro von Werra, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Aritra Roy Gosthipaty, Andrés Marafioti 핵심 연구 목표 파편화되고 일관성 없으며 오염된 공개 데이터셋으로 인해 저해되는"},{"id":"2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression","title":"[논문리뷰] Glyph: Scaling Context Windows via Visual-Text Compression","excerpt":"Wenyi Hong이 arXiv에 게시한 'Glyph: Scaling Context Windows via Visual-Text Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression","tags":["Review","Long-Context Modeling","Visual Compression","Vision-Language Models","Token Efficiency","Genetic Algorithms","Multimodal AI","LLM Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong 핵심 연구 목표 논문은 대규모 언어 모델(LLM)의 컨텍스트 창을 수백만 토큰 수준으로 확장할 때 발생하는 막대한 계산 및 메모리 비용 문제를 해결하는 것을 목표로 합니다. 이는 기존 토큰 기반 컨텍스트 확장 방식"},{"id":"2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer","title":"[논문리뷰] GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer","excerpt":"arXiv에 게시된 'GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer","tags":["Review","3D Appearance Transfer","Rectified Flow","Generative Models","Optimization-Guided Sampling","Neural Latent Representations","Training-Free","GPT-Based Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Sayan Deb Sarkar, Vincent Lepetit, Sinisa Stekovic, Iro Armeni 핵심 연구 목표 본 논문은 입력 3D 객체와 외형 객체 간의 기하학적 차이가 클 때, 기존 3D 외형 전이 방법론이 실패하는 문제를 해결하고자 합니다. 특히, 기존 3D 생성 모델 을 직접 적용하는 방식의 "},{"id":"2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering","title":"[논문리뷰] Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering","excerpt":"arXiv에 게시된 'Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering","tags":["Review","Visual Question Answering","Retrieval-Augmented Generation","Multimodal AI","Reinforcement Learning","Knowledge Base","Tool Learning","Information Filtering"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye 핵심 연구 목표 본 논문은 지식 기반 시각 질문 답변(KBVQA) 태스크에서 멀티모달 쿼리의 품질과 검색 결과의 관련성 이 부족하여 발생하는 문"},{"id":"2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models","title":"[논문리뷰] MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models","excerpt":"arXiv에 게시된 'MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models","tags":["Review","Multi-Turn Conversation","VLM Evaluation","Benchmark","Vision and Language Models","Contextual Understanding","Checklist-based Evaluation","Interactive AI"],"text":"링크: 논문 PDF로 바로 열기 저자: YoungJun Lee, ByungKwan Lee, Jianshu Zhang, Yechan Hwang, Byungsoo Ko, HanGyu Kim, Dongyu Yao, Xuankun Rong, Eojin Joo, SeungHo Han, Bowon Ko, HoJin Choi 핵심 연구 목표 기존 VisionandLan"},{"id":"2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators","title":"[논문리뷰] On Non-interactive Evaluation of Animal Communication Translators","excerpt":"Adam Tauman Kalai이 arXiv에 게시한 'On Non-interactive Evaluation of Animal Communication Translators' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators","tags":["Review","Machine Translation Quality Evaluation","Reference-Free Evaluation","Animal Communication","Language Models","Shuffle Test","Conlangs","Non-interactive Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Orr Paradise, David F. Gruber, Adam Tauman Kalai 핵심 연구 목표 이 논문은 AI 기반 동물 언어 번역기(예: 고래영어 번역기)의 작동 여부를 상호작용 없이 검증하는 방법을 제시하는 것을 목표로 합니다. 특히, 참조 번역이 전혀 없는 상황(referencefree)에서 번역 품질을"},{"id":"2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing","title":"[논문리뷰] PICABench: How Far Are We from Physically Realistic Image Editing?","excerpt":"Kaiwen Zhu이 arXiv에 게시한 'PICABench: How Far Are We from Physically Realistic Image Editing?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing","tags":["Review","Image Editing","Physical Realism","Benchmark","VLM-as-a-Judge","Synthetic Data","Physics-Aware AI","Diffusion Models","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuandong Pu, Le Zhuo, Songhao Han, Jinbo Xing, Kaiwen Zhu, Shuo Cao, Xi Chen, Yihao Liu, et al. 핵심 연구 목표 이미지 편집 모델이 지시 사항을 따르는 것을 넘어, 물리 법칙을 준수하는 현실적인 편집 결과 를 얼마나 잘 생성하는지 평가하고 개선"},{"id":"2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems","title":"[논문리뷰] QueST: Incentivizing LLMs to Generate Difficult Problems","excerpt":"arXiv에 게시된 'QueST: Incentivizing LLMs to Generate Difficult Problems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems","tags":["Review","LLM","Problem Generation","Competitive Programming","Synthetic Data","Difficulty Estimation","Rejection Fine-tuning","Graph Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanxu Hu, Xingxing Zhang, Jannis Vamvas, Rico Sennrich, Furu Wei 핵심 연구 목표 본 논문은 LLM 학습에 있어 인간이 주석을 단 고품질의 어려운 코딩 문제 데이터셋이 부족하여 확장성이 제한되는 문제를 해결하고자 합니다. 특히, LLM 생성기가 더욱 도전적인 경쟁 프로"},{"id":"2025-10-21-RL-makes-MLLMs-see-better-than-SFT","title":"[논문리뷰] RL makes MLLMs see better than SFT","excerpt":"arXiv에 게시된 'RL makes MLLMs see better than SFT' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-RL-makes-MLLMs-see-better-than-SFT","tags":["Review","Multimodal Language Models","Reinforcement Learning","Supervised Finetuning","Vision Encoder","Visual Representations","Direct Preference Optimization","Preference Alignment","PIVOT"],"text":"링크: 논문 PDF로 바로 열기 저자: Junha Song, Sangdoo Yun, Dongyoon Han, Jaegul Choo, Byeongho Heo 핵심 연구 목표 본 논문은 MLLM(Multimodal Language Model) 연구에서 LLM 백본 에 대한 지배적인 가정으로 인해 비전 인코더 의 역할이 간과되어 왔다는 문제의식에서 출발합니다. "},{"id":"2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation","title":"[논문리뷰] Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation","excerpt":"arXiv에 게시된 'Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation","tags":["Review","Universal RAG","Multimodal Retrieval","Mixed-Modal Data Generation","Vision-Language Models","Contrastive Learning","Matryoshka Representation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenghao Zhang, Guanting Dong, Xinyu Yang, Zhicheng Dou 핵심 연구 목표 본 연구는 기존 RAG 시스템이 단일 모드 텍스트나 제한된 다중 모드 설정에만 초점을 맞춰, 실제 환경의 혼합 모드(mixedmodal) 질의 및 문서 처리에 한계가 있다는 문제를 해결하고자 합니다. 궁"},{"id":"2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action","title":"[논문리뷰] UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action","excerpt":"arXiv에 게시된 'UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action","tags":["Review","Computer Use Agents","Hybrid Action","Foundation Models","Reinforcement Learning","Supervised Fine-tuning","Synthetic Data Generation","Tool Learning","GUI Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhao Yang, Zhen Yang, ZiYi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan 핵심 연구 목표 본 논문은 기존 "},{"id":"2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback","title":"[논문리뷰] Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback","excerpt":"arXiv에 게시된 'Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback","tags":["Review","Image Editing","Diffusion Models","Reinforcement Learning","MLLM","Policy Optimization","Finetuning","Reward Modeling","Human Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: UniWorld Team 핵심 연구 목표 본 논문은 지도 미세 조정(supervised finetuning)만으로는 학습 분포를 넘어선 이미지 편집 모델의 일반화 및 제어 능력 부족 문제를 해결하는 것을 목표로 합니다. 특히, 모델이 주석 처리된 패턴에 과적합되는 경향을 극복하고, 다양한 편집 지침과 작업에 대한 범용"},{"id":"2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling","title":"[논문리뷰] Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling","excerpt":"Dim P. Papadopoulos이 arXiv에 게시한 'Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling","tags":["Review","Visual Autoregressive Models","Diffusion Models","Inference Time Scaling","Beam Search","Image Generation","Text-to-Image Synthesis","Discrete Latent Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Erik Riise, Mehmet Onurcan Kaya, Dim P. Papadopoulos 핵심 연구 목표 본 연구는 대규모 언어 모델(LLMs)에서 성공적인 추론 시간 스케일링(search) 전략이 연속적인 잠재 공간을 사용하는 확산 모델(Diffusion Models)에서는 제한적인 이점을 보이는 문제를 해결하"},{"id":"2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling","title":"[논문리뷰] When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling","excerpt":"arXiv에 게시된 'When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-21 13:08:30+0900","permalink":"/ai/review/2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling","tags":["Review","LLM Ensembling","Token-level Ensembling","Speculative Decoding","Tokenization Mismatch","Probability Sharpening","Long-form Generation","KV Cache Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Heecheol Yun, Kwangmin Ki, Junghyun Lee, Eunho Yang 핵심 연구 목표 본 논문은 LLM(Large Language Model) 앙상블이 장문(longform) 생성에서 겪는 불안정성과 비효율성 문제를 해결하는 것을 목표로 합니다. 특히, 기존 앙상블 방식이 토큰화 불일치와 높은 "},{"id":"2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading","title":"[논문리뷰] AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading","excerpt":"Jiashu Wang이 arXiv에 게시한 'AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading","tags":["Review","Automated Trading","Reinforcement Learning","LLM Agents","Tool Orchestration","Financial Markets","Algorithmic Trading","Interpretable AI","ReAct"],"text":"링크: 논문 PDF로 바로 열기 저자: Zheye Deng, Jiashu Wang 핵심 연구 목표 본 논문은 기존 대규모 언어 모델(LLM) 기반 자동화된 주식 거래 시스템의 비효율성, 신호 불일치, 전략 학습의 비일관성 등의 한계를 해결하고자 합니다. 투명하고 도구가 증강된 의사결정 워크플로우를 통해 단일 에이전트가 도구를 자율적으로 조율하고 정보를 능동"},{"id":"2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist","title":"[논문리뷰] Chem-R: Learning to Reason as a Chemist","excerpt":"arXiv에 게시된 'Chem-R: Learning to Reason as a Chemist' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist","tags":["Review","Chemical Reasoning","Large Language Models","Chem-R","Structured Reasoning","Multi-task Optimization","Chain-of-Thought","Chemical Discovery"],"text":"링크: 논문 PDF로 바로 열기 저자: Weida Wang, Benteng Chen, Di Zhang, Wanhao Liu, Shuchen Pu, Ben Gao, Jin Zeng, Lei Bai, Wanli Ouyang, Xiaoyong Wei, Tianshu Yu, Tianfan Fu, Shuzhou Sun, Jiatong Li, Zifu Wang, Yu"},{"id":"2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence","title":"[논문리뷰] DSI-Bench: A Benchmark for Dynamic Spatial Intelligence","excerpt":"arXiv에 게시된 'DSI-Bench: A Benchmark for Dynamic Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence","tags":["Review","Dynamic Spatial Reasoning","Vision-Language Models (VLMs)","Benchmark","Video Understanding","Motion Perception","3D Spatial Intelligence","Hallucinations","Bias"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziang Zhang, Zehan Wang, Guanghao Zhang, Weilong Dai, Yan Xia, Ziang Yan, Minjie Hong, Zhou Zhao 핵심 연구 목표 논문은 관찰자와 객체가 동시에 움직이는 동적 3D 시나리오 에서 최신 VisionLanguage Models (VLMs)의 제한적"},{"id":"2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning","title":"[논문리뷰] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning","excerpt":"Qipeng Guo이 arXiv에 게시한 'EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning","tags":["Review","Verifiable Learning","Data Synthesis","Evolutionary Algorithm","Large Language Models","Reinforcement Learning","Model Distillation","Test Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao 핵심 연구 목표 본 논문은 환각(hallucination) 문제와 부실한 검증 아티팩트로 인해 신뢰성 있는 합성 검증 데이터를 생성하기 어렵다는 문제를 해결하고자 합니다. 기존의 태스크별 휴리스틱 방식의 "},{"id":"2025-10-22-Extracting-alignment-data-in-open-models","title":"[논문리뷰] Extracting alignment data in open models","excerpt":"arXiv에 게시된 'Extracting alignment data in open models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models","tags":["Review","Alignment Data Extraction","Large Language Models","Memorization","Neural Embeddings","Semantic Similarity","Chat Templates","Model Distillation","Reinforcement Learning","Supervised Finetuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Federico Barbero, Xiangming Gu, Christopher A. ChoquetteChoo, Chawin Sitawarin, Matthew Jagielski, Itay Yona, Petar Veličković, Ilia Shumailov, Jamie Hayes 핵심 연구 목표 본 논문은 오픈 모델에서"},{"id":"2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs","title":"[논문리뷰] Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs","excerpt":"arXiv에 게시된 'Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs","tags":["Review","Multimodal LLMs","Region Understanding","Contextual Pixel Understanding","RoI-aligned Feature Replay","Compositional Reasoning","GAR-Bench","Zero-shot Video Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, "},{"id":"2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions","title":"[논문리뷰] IF-VidCap: Can Video Caption Models Follow Instructions?","excerpt":"arXiv에 게시된 'IF-VidCap: Can Video Caption Models Follow Instructions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions","tags":["Review","Video Captioning","Instruction Following","MLLMs","Benchmark","Controllable Generation","Multimodal Evaluation","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi"},{"id":"2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues","title":"[논문리뷰] MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues","excerpt":"arXiv에 게시된 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues","tags":["Review","Multimodal LLMs","Video Understanding","Benchmark","Multi-Turn Dialogues","Perceptivity","Interactivity","Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, Jiaheng L"},{"id":"2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models","title":"[논문리뷰] MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models","excerpt":"arXiv에 게시된 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models","tags":["Review","Video Generation","Diffusion Transformer","Large-scale Training","Megatron-Core","Video VAE","E-commerce AI","High-efficiency Pipeline","Preference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yongshun Zhang, Zhongyi Fan, Yonghang Zhang, Zhangzikang Li, Weifeng Chen, Zhongwei Feng, Chaoyue Wang, Peng Hou, Anxiang Zeng (LLM Team, Shopee Pte. Ltd.) 핵심 연구 목표 본 논문은 대규모 비디오"},{"id":"2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation","title":"[논문리뷰] MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation","excerpt":"arXiv에 게시된 'MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation","tags":["Review","Long Video Generation","Sparse Attention","Diffusion Transformers","Mixture-of-Groups Attention","Token Routing","Computational Efficiency","Context Length"],"text":"링크: 논문 PDF로 바로 열기 저자: Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao 핵심 연구 목표 본 논문은 Diffusion Transformers (DiTs) 기반의 긴 비디오 생성에서 발생하는 전"},{"id":"2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies","title":"[논문리뷰] PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies","excerpt":"James Glass이 arXiv에 게시한 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies","tags":["Review","Large Multimodal Models (LMMs)","Scientific Document Analysis","Multimodal Inconsistencies","Peer Review","Benchmark","Debiasing","JSON-based Representation","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Lukas Selch, Yufang Hou, M. Jehanzeb Mirza, Sivan Doveh, James Glass, Rogerio Feris, Wei Lin 핵심 연구 목표 과학 논문 내 텍스트, 그림, 표, 수식 등 다양한 모달리티 간의 불일치(inconsistencies) 를 LMM이 얼마나 신뢰성 있게 "},{"id":"2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold","title":"[논문리뷰] PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold","excerpt":"arXiv에 게시된 'PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold","tags":["Review","Deep Research Agent","Reinforcement Learning from AI Feedback","RLOO Algorithm","Large Language Models","Tool Use","Self-Correction","Reasoning Scaffold","Agent Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Wan, Jiuqi Wang, Liam Li, Jinsong Liu, Ruihao Zhu, Zheqing Zhu (Pokee AI) 핵심 연구 목표 이 논문은 기존 도구 증강 LLM 기반 에이전트의 얕은 검색 능력, 약한 정렬 메트릭, 불안정한 도구 사용의 한계를 극복하고자 합니다. 궁극적으로 사실적 정확도, 인"},{"id":"2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder","title":"[논문리뷰] ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder","excerpt":"Zonghao Guo이 arXiv에 게시한 'ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder","tags":["Review","Vision-Language Models","CLIP","LLM-based Embedder","Knowledge Distillation","Contrastive Learning","Curriculum Learning","Multimodal Alignment","Progressive Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoxing Hu, Kaicheng Yang, Ziyong Gong, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang 핵심 연구 목표 기존 CLIP 텍스트 인코더의 77토큰 길이 제한 , 영어 전용 지원, 미흡한 세분화된 의미 이해 능력이라는 한"},{"id":"2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning","title":"[논문리뷰] Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning","excerpt":"Yuchen Eleanor Jiang이 arXiv에 게시한 'Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning","tags":["Review","LLM Personalization","Reinforcement Learning","Generative Reward Model","Critique-Post-Edit","Reward Hacking","Controllable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenghao Zhu, Meiling Tao, Dongyi Ding, Tiannan Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 개인화가 사용자의 개별적인 선호도에 충실하게 부합하도록 하는 도전적인 문제를 해결하고자 합니다. "},{"id":"2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention","title":"[논문리뷰] UltraGen: High-Resolution Video Generation with Hierarchical Attention","excerpt":"Ran Yi이 arXiv에 게시한 'UltraGen: High-Resolution Video Generation with Hierarchical Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention","tags":["Review","Video Generation","High-Resolution","Diffusion Transformer","Hierarchical Attention","Global-Local Attention","Computational Efficiency","4K Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Teng Hu, Jiangning Zhang, Zihan Su, Ran Yi 핵심 연구 목표 기존 Diffusion Transformer 기반 비디오 생성 모델들이 출력 해상도(예: <720P)에 따라 attention 메커니즘의 제곱 복잡도 로 인해 발생하는 높은 연산 비용 문제를 해결하는 것이 목표입니다. 이를 통"},{"id":"2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation","title":"[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation","excerpt":"Yujie Zhou이 arXiv에 게시한 'UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Semantic Evaluation","Benchmark","Multilingual Evaluation","Fine-grained Assessment","Large Language Models","Model Evaluation","Prompt Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Yibin Wang, Yujie Zhou, Jiazi Bu, Yuhang Zang, Zhimin Li, et al. 핵심 연구 목표 기존 TexttoImage(T2I) 모델 평가 벤치마크의 한계점들을 해결하고, T2I 모델의 정교한 의미론적 일관성 및 실세계 적용 능력 을 종합적이고 효율적으로 평가하는 통합 벤치마크를"},{"id":"2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism","title":"[논문리뷰] Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism","excerpt":"Shuang Gu이 arXiv에 게시한 'Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism","tags":["Review","Scientific Reasoning","Bio-experimental Protocol Generation","LLM","Structured Reward","SciRecipe Dataset","Sketch-and-Fill","Reinforcement Learning","Thoth"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuang Gu, Yaning Pan, Zhenyu Tang, Yankai Jiang, Haoran Sun 외 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 생물 실험 프로토콜을 생성할 때 발생하는 불완전성 및 비일관성 문제를 해결하고, 정밀하고 논리적으로 정렬되며 실행 가능한 프로토콜을 자율적으로 생성하는"},{"id":"2025-10-22-Video-Reasoning-without-Training","title":"[논문리뷰] Video Reasoning without Training","excerpt":"arXiv에 게시된 'Video Reasoning without Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-Video-Reasoning-without-Training","tags":["Review","Video Reasoning","Large Multimodal Models (LMMs)","Inference-Time Optimization","Entropy-Based Objective","Training-Free","KV-Cache Steering","Micro-Exploration","Macro-Exploitation"],"text":"링크: 논문 PDF로 바로 열기 저자: Deepak Sridhar, Kartikeya Bhardwaj, Jeya Pradha Jeyaraj, Nuno Vasconcelos, Ankita Nayak, Harris Teague 핵심 연구 목표 본 논문은 Large Multimodal Models (LMMs) 기반 비디오 추론 시 발생하는 높은 연산 비용과 추론"},{"id":"2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World","title":"[논문리뷰] World-in-World: World Models in a Closed-Loop World","excerpt":"Arda Uzunoglu이 arXiv에 게시한 'World-in-World: World Models in a Closed-Loop World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-22 13:07:20+0900","permalink":"/ai/review/2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World","tags":["Review","World Models","Embodied AI","Closed-Loop Evaluation","Online Planning","Data Scaling","Controllability","Robotic Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahan Zhang, Muqing Jiang, Nanru Dai, Taiming Lu, Arda Uzunoglu, Shunchi Zhang, Yana Wei, Jiahao Wang, Vishal M. Patel, Paul Pu Liang, Daniel Khashabi, Cheng Peng, Rama Chellapp"},{"id":"2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library","title":"[논문리뷰] AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library","excerpt":"Chonghe Jiang이 arXiv에 게시한 'AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library","tags":["Review","Optimization Modeling","Large Language Models (LLMs)","Experience Library","Self-Improving Systems","Continual Learning","Out-of-Distribution Generalization","Operations Research","Knowledge Representation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minwei Kong, Ao Qu, Xiaotong Guo, Wenbin Ouyang, Han Zheng, Yining Ma, Dingyi Zhuang, Cathy Wu, Jinhua Zhao, Chonghe Jiang, Yuhan Tang, Junyi Li 핵심 연구 목표 본 논문은 최적화 모델링 자동화의 어려움, "},{"id":"2025-10-23-Attention-Sinks-in-Diffusion-Language-Models","title":"[논문리뷰] Attention Sinks in Diffusion Language Models","excerpt":"Simone Scardapane이 arXiv에 게시한 'Attention Sinks in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Attention-Sinks-in-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Attention Sinks","Transformer Architecture","Masked Language Modeling","Bidirectional Attention","Generative Models","Robustness","Dynamic Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Maximo Eduardo Rulli, Simone Petruzzi, Edoardo Michielon, Fabrizio Silvestri, Simone Scardapane, Alessio Devoto 핵심 연구 목표 Diffusion Language Models (DLMs)의 내부 메커니즘, 특히 다른 트랜스포머 아키"},{"id":"2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping","title":"[논문리뷰] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping","excerpt":"Junrui Shen이 arXiv에 게시한 'BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping","tags":["Review","Off-Policy Reinforcement Learning","Large Language Models","Adaptive Clipping","Policy Optimization","PPO","Entropy Preservation","RL Stabilization"],"text":"링크: 논문 PDF로 바로 열기 저자: Junrui Shen, Enyu Zhou, Yang Nan, Xin Guo, Zhiheng Xi 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)을 위한 오프폴리시(offpolicy) 강화 학습(RL)의 불안정성 문제를 해결하고자 합니다. 오프폴리시 RL은 정책 엔트로피 급감, 불안정한 최적화, 그리고 훈련 붕괴"},{"id":"2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent","title":"[논문리뷰] ColorAgent: Building A Robust, Personalized, and Interactive OS Agent","excerpt":"Weiming Zhang이 arXiv에 게시한 'ColorAgent: Building A Robust, Personalized, and Interactive OS Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent","tags":["Review","OS Agent","Reinforcement Learning","Multi-agent Systems","Personalization","Proactive Interaction","GUI Agents","Self-Evolving Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Ning Li, Qiqiang Lin, Zheng Wu, Xiaoyun Mo, Weiming Zhang, Yin Zhao, Xiangmou Qu, Jiamu Zhou, Jun Wang, Congmin Zheng, Yuanyi Song, Hongjiang Chen, Heyuan Huang, Jihong Wang, Jia"},{"id":"2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents","title":"[논문리뷰] DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents","excerpt":"arXiv에 게시된 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents","tags":["Review","Multimodal LLMs","Fine-tuning","Data Mixing Optimization","Mobile Phone Agents","Downstream Task Prediction","Benchmark","Neural Networks"],"text":"링크: 논문 PDF로 바로 열기 저자: Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie†, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu 핵심 연구 목표 본 논문은 Multimodal Large Language Models (MLLMs)의 다중 작업 지도 미세 조정("},{"id":"2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models","title":"[논문리뷰] DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models","excerpt":"Roi Reichart이 arXiv에 게시한 'DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models","tags":["Review","Semantic Leakage","Text-to-Image Models","Attention Control","Inference-time Mitigation","Diffusion Models","Evaluation Dataset","Self-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Mor Ventura, Michael Toker, Or Patashnik, Yonatan Belinkov, Roi Reichart 핵심 연구 목표 본 논문은 TexttoImage (T2I) 모델에서 발생하는 의도치 않은 의미적 누출(semantic leakage) 문제를 해결하는 것을 목표로 합니다. 이는 서로 다른 "},{"id":"2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs","title":"[논문리뷰] Directional Reasoning Injection for Fine-Tuning MLLMs","excerpt":"Jialian Wu이 arXiv에 게시한 'Directional Reasoning Injection for Fine-Tuning MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs","tags":["Review","Multimodal LLMs","Reasoning Transfer","Gradient-based Fine-tuning","Model Merging","Parameter-Efficient Learning","Supervised Fine-tuning","Directional Prior"],"text":"링크: 논문 PDF로 바로 열기 저자: Chao Huang, Zeliang Zhang, Jiang Liu, Ximeng Sun, Jialian Wu, Xiaodong Yu, Ze Wang, Chenliang Xu, Emad Barsoum, Zicheng Liu 핵심 연구 목표 논문은 멀티모달 대규모 언어 모델(MLLM)의 추론 능력이 텍스트 전용 LLM에 "},{"id":"2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","title":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning","excerpt":"arXiv에 게시된 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","tags":["Review","Long-Context LLM","Hybrid Attention","Linear Attention","Mixture-of-Experts","FP8 Training","GPU Optimization","Training-Inference Alignment","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, W"},{"id":"2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research","title":"[논문리뷰] FinSight: Towards Real-World Financial Deep Research","excerpt":"Yutao Zhu이 arXiv에 게시한 'FinSight: Towards Real-World Financial Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research","tags":["Review","Financial Research","Multi-Agent System","Code Generation","Multimodal Reports","Iterative Visualization","Variable Memory","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiajie Jin, Yuyao Zhang, Yimeng Xu, Hongjin Qian, Yutao Zhu, Zhicheng Dou 핵심 연구 목표 본 논문은 기존 AI 시스템이 완전 자동화하기 어려웠던 전문 금융 보고서 생성의 문제를 해결하는 것을 목표로 합니다. 특히, 노동 집약적이고 지적인 노력이 많이 드는 금융"},{"id":"2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models","title":"[논문리뷰] From Charts to Code: A Hierarchical Benchmark for Multimodal Models","excerpt":"Dongxing Mao이 arXiv에 게시한 'From Charts to Code: A Hierarchical Benchmark for Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models","tags":["Review","Chart-to-Code","Multimodal Models","Hierarchical Benchmark","Chart Understanding","Code Generation","Evaluation Metrics","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahao Tang, Henry Hengyuan Zhao, Lijian Wu, Yifei Tao, Dongxing Mao, Yang Wan, Jingru Tan, Min Zeng, Min Li, Alex Jinpeng Wang 핵심 연구 목표 기존 차트코드(charttocode) 벤치마크가 단순한 재현 작업에 치중하"},{"id":"2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model","title":"[논문리뷰] GigaBrain-0: A World Model-Powered Vision-Language-Action Model","excerpt":"arXiv에 게시된 'GigaBrain-0: A World Model-Powered Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model","tags":["Review","Vision-Language-Action Model","World Model","Data Augmentation","Robot Generalization","Embodied AI","RGBD","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang"},{"id":"2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints","title":"[논문리뷰] KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints","excerpt":"Jinhe Bi이 arXiv에 게시한 'KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints","tags":["Review","Knowledge Injection","Large Multimodal Models","Catastrophic Forgetting","Data Augmentation","Parameter-Efficient Fine-Tuning","Null Space","Continual Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kailin Jiang, Hongbo Jiang, Ning Jiang, Zhi Gao, Jinhe Bi 핵심 연구 목표 대규모 멀티모달 모델(LMM)의 고정적이고 제한적인 지식 문제를 해결하고, 새로운 지식 주입 시 발생하는 치명적 망각(Catastrophic Forgetting)을 완화하는 것을 목표로 합니다. 연구"},{"id":"2025-10-23-Language-Models-are-Injective-and-Hence-Invertible","title":"[논문리뷰] Language Models are Injective and Hence Invertible","excerpt":"arXiv에 게시된 'Language Models are Injective and Hence Invertible' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Language-Models-are-Injective-and-Hence-Invertible","tags":["Review","Language Models","Injectivity","Invertibility","Transformer","Representation Learning","Exact Recovery","SIPIT Algorithm","Real Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Donato Crisostomi, Giorgos Nikolaou, Tommaso Mencattini, Andrea Santilli, Yannis Panagakis, Emanuele Rodolà 핵심 연구 목표 논문은 비선형 활성화 함수와 정규화 등으로 인해 Transformer 언어 모델이 정보를 손실하고, 입력 텍스"},{"id":"2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection","title":"[논문리뷰] Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection","excerpt":"Yi Cheng이 arXiv에 게시한 'Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection","tags":["Review","Data Selection","Large Language Models (LLMs)","Data Diversity","Data Quality","Principal Component Analysis (PCA)","Orthogonal Dimensions","Pre-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongyi He, Xiao Liu, Zhenghao Lin, Mingni Tang, Yi Cheng, Jintao Wang, Wenjie Li, Peng Cheng, Yeyun Gong 핵심 연구 목표 대규모 언어 모델(LLMs) 사전 훈련 시, 기존의 점수 기반 데이터 선택 방식이 다양성 부족으로 인해 성능 저하를"},{"id":"2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts","title":"[논문리뷰] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts","excerpt":"arXiv에 게시된 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts","tags":["Review","Reinforcement Learning","Long Context Reasoning","Large Language Models","Multi-hop QA","Data Synthesis","Retrieval-Augmented Generation","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyuan Wang, Gaokai Zhang, Li Lyna Zhang, Ning Shang, Fan Yang, Dongyao Chen, Mao Yang 핵심 연구 목표 대규모 언어 모델(LLMs)이 긴 컨텍스트에 대한 고급 추론 능력을 갖추도록 하는 것이 목표입니다. 기존 RL 방법론들이 주로 짧은 컨텍스트 추론에"},{"id":"2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models","title":"[논문리뷰] MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models","excerpt":"Yifan Gao이 arXiv에 게시한 'MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models","tags":["Review","Large Multimodal Models (LMMs)","Time-Sensitive Knowledge","Temporal Reasoning","Knowledge Editing","Multimodal Benchmarking","Temporal Awareness","Dynamic Knowledge"],"text":"링크: 논문 PDF로 바로 열기 저자: Kailin Jiang, Ning Jiang, Yuchen Ren, Yuchen Li, Yifan Gao, Jinhe Bi, Yunpu Ma, Qingqing Liu, Xianhao Wang, Yifan Jia, Hongbo Jiang, Yaocong Hu, Bin Li, Lei Liu, Yuntao Du 핵심 연구 "},{"id":"2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks","title":"[논문리뷰] Machine Text Detectors are Membership Inference Attacks","excerpt":"Naoaki Okazaki이 arXiv에 게시한 'Machine Text Detectors are Membership Inference Attacks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks","tags":["Review","Membership Inference Attacks","Machine-Generated Text Detection","Transferability","Likelihood Ratio Test","Large Language Models","Zero-Shot Detection","Model Security","AI Safety"],"text":"링크: 논문 PDF로 바로 열기 저자: Ryuto Koike, Liam Dugan, Masahiro Kaneko, Chris CallisonBurch, Naoaki Okazaki 핵심 연구 목표 본 연구는 멤버십 추론 공격(MIAs)과 기계 생성 텍스트 감지(MGTD)라는 두 가지 관련 연구 분야가 독립적으로 연구되어 발생하는 비효율성을 해결하고자 합니다."},{"id":"2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models","title":"[논문리뷰] OmniNWM: Omniscient Driving Navigation World Models","excerpt":"Zhujin Liang이 arXiv에 게시한 'OmniNWM: Omniscient Driving Navigation World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models","tags":["Review","Autonomous Driving","World Models","Multi-modal Generation","3D Occupancy","Plücker Ray-maps","Action Control","Dense Rewards","Long-term Forecasting"],"text":"링크: 논문 PDF로 바로 열기 저자: Bohan Li, Zhuang Ma, Dalong Du, Baorui Peng, Zhujin Liang, Zhenqiang Liu, Chao Ma, Yueming Jin, Hao Zhao, Wenjun Zeng, Xin Jin 핵심 연구 목표 본 논문은 기존 자율주행 월드 모델이 가진 제한된 상태 모달리티, 짧은 시퀀"},{"id":"2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing","title":"[논문리뷰] Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing","excerpt":"arXiv에 게시된 'Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing","tags":["Review","Text-Guided Image Editing","Large-Scale Dataset","Multimodal Models","Dataset Curation","Quality Control","Prompt Engineering","Preference Learning","Multi-Turn Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Yusu Qian, Eli BocekRivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan 핵심 연구 목표 본 논문은 대규모, 고품질, 공개적으로 접근 가능한 텍스트 기반 이미지 편집 데이터셋의 부족으로 인해 제한되었던 연구 발전을"},{"id":"2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge","title":"[논문리뷰] ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge","excerpt":"arXiv에 게시된 'ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge","tags":["Review","LLM Evaluation","Rubric-based Benchmark","Professional Knowledge","Multi-domain Tasks","LLM-Judge Bias Mitigation","Cost Reduction","Reasoning Assessment","Open-weight Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhilin Wang, Jaehun Jung, Ximing Lu, Shizhe Diao, Ellie Evans, Jiaqi Zeng, Pavlo Molchanov, Yejin Choi, Jan Kautz, Yi Dong 핵심 연구 목표 본 논문은 기존 LLM 평가 벤치마크가 쉬운 검증 태스크에 국한되어 있다는 한계를 "},{"id":"2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling","title":"[논문리뷰] RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling","excerpt":"Mandip Goswami이 arXiv에 게시한 'RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling","tags":["Review","Room Impulse Response","Dataset","Room Acoustics","Machine Learning","Dereverberation","Speech Recognition","Simulation","Hugging Face"],"text":"링크: 논문 PDF로 바로 열기 저자: Mandip Goswami 핵심 연구 목표 본 논문은 반향음 제거, 강건한 음성 인식, 음원 위치 추정, 음향 환경 추정 등 다양한 AI/ML 태스크를 위한 대규모 시뮬레이션된 Room Impulse Response (RIR) 데이터셋의 부족 문제를 해결하는 것을 목표로 합니다. 연구자들에게 일관된 경로 처리, 스키마"},{"id":"2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models","title":"[논문리뷰] Unified Reinforcement and Imitation Learning for Vision-Language Models","excerpt":"arXiv에 게시된 'Unified Reinforcement and Imitation Learning for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models","tags":["Review","Vision-Language Models","Reinforcement Learning","Imitation Learning","Model Distillation","Lightweight VLMs","LLM-as-a-Judge","Multimodal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: ByungKwan Lee, Ryo Hachiuma, YuChiang Frank Wang, Yong Man Ro, YuehHua Wu 핵심 연구 목표 본 논문은 대규모 VisionLanguage Models (VLMs) 의 비효율성을 해결하기 위해, 리소스가 제한된 환경에서도 강력하고 경량화된 VLM을 구축하는 효율적인"},{"id":"2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos","title":"[논문리뷰] VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos","excerpt":"Xinyuan Wang이 arXiv에 게시한 'VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos","tags":["Review","GUI Agents","Video Pretraining","Inverse Dynamics","Action Recognition","Computer Use Automation","Data Synthesis","Multimodal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyuan Wang, Haoyuan Wu, Junli Wang, Yiheng Xu, Dunjie Lu 핵심 연구 목표 본 연구는 GUI(Graphical User Interface) 에이전트 훈련에 필요한 대규모의 수동 주석된 상호작용 데이터 확보의 어려움을 해결하고자 합니다. 이를 위해 레이블이 없는 공개 웹 스"},{"id":"2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR","title":"[논문리뷰] olmOCR 2: Unit Test Rewards for Document OCR","excerpt":"arXiv에 게시된 'olmOCR 2: Unit Test Rewards for Document OCR' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-23 13:08:59+0900","permalink":"/ai/review/2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR","tags":["Review","Document OCR","Vision Language Model","Reinforcement Learning","Unit Tests","Synthetic Data Generation","RLVR","Document Parsing","State-of-the-Art OCR"],"text":"링크: 논문 PDF로 바로 열기 저자: Jake Poznanski, Luca Soldaini, Kyle Lo 핵심 연구 목표 본 논문은 인쇄된 문서를 깨끗하고 자연스럽게 정렬된 일반 텍스트로 변환하는 OCR 시스템인 OLMOCR 2 를 제안합니다. 특히, 강화 학습(RL) 과 검증 가능한 보상(RLVR) 을 활용하여 수학 공식, 테이블 파싱, 다단 레이아웃"},{"id":"2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model","title":"[논문리뷰] ARGenSeg: Image Segmentation with Autoregressive Image Generation Model","excerpt":"arXiv에 게시된 'ARGenSeg: Image Segmentation with Autoregressive Image Generation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model","tags":["Review","Image Segmentation","Autoregressive Generation","Multimodal Large Language Models (MLLMs)","Visual Understanding","VQ-VAE","Multi-scale Prediction","Referring Expression Segmentation","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou 핵심 연구 목표 본 논문은 기존 MLLM 기반 분할 방법론이 픽셀 수준의 미세한 시각적 디테일을 포착하는 데 한계가 있음을 지적하며, Autoregress"},{"id":"2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders","title":"[논문리뷰] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders","excerpt":"arXiv에 게시된 'AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders","tags":["Review","Speculative Decoding","Knowledge Distillation","LLM Inference","Model Acceleration","Token Filtering","Draft Model","Acceptance Rate"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 추론 속도 향상을 위한 Speculative Decoding (SD) 과정에서 드래프트 모델과 타겟 모델 간의 불일치 문제를 해결하는 것을 목표로 합니다. 기존 Knowledge Dis"},{"id":"2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models","title":"[논문리뷰] AlphaFlow: Understanding and Improving MeanFlow Models","excerpt":"arXiv에 게시된 'AlphaFlow: Understanding and Improving MeanFlow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models","tags":["Review","Generative Models","Flow Matching","Consistency Models","MeanFlow","Curriculum Learning","Few-Step Generation","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov 핵심 연구 목표 본 논문은 MeanFlow 모델의 성공 원리를 심층적으로 분석하고, MeanFlow 훈련 목표 내"},{"id":"2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature","title":"[논문리뷰] ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature","excerpt":"arXiv에 게시된 'ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature","tags":["Review","Multi-agent Systems","Large Language Models (LLMs)","Information Extraction","Scientific Literature","Materials Science","Data Curation","Piezoelectric Materials","RAG (Retrieval-Augmented Generation)"],"text":"링크: 논문 PDF로 바로 열기 저자: Aritra Roy, Enrico Grisan, John Buckeridge, Chiara Gattinoni 핵심 연구 목표 본 논문은 과학 문헌에서 화학 조성물성 구조 데이터와 합성 정보를 추출하기 위한 자동화되고 사용자 친화적인 멀티 에이전트 기반 프레임워크 를 개발하는 것을 목표로 합니다. 기존 LLM의 발전에도"},{"id":"2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence","title":"[논문리뷰] Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence","excerpt":"arXiv에 게시된 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence","tags":["Review","Video Reasoning","Multimodal Large Language Models (MLLMs)","Reinforcement Learning (RLVR)","Evidence Grounding","Multi-step Reasoning","Frame Retrieval","Dataset Construction","Progressive Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLMs)이 순수 텍스트 추론이나 부정확한 증거 지역화로 인해 종종 발생시키는 근거 없는/환각적 결론의 "},{"id":"2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding","title":"[논문리뷰] Diff-XYZ: A Benchmark for Evaluating Diff Understanding","excerpt":"arXiv에 게시된 'Diff-XYZ: A Benchmark for Evaluating Diff Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding","tags":["Review","Diff Understanding","Code Diff","Benchmark","LLMs","Code Editing","Software Engineering","Unified Diff Format","Search-Replace"],"text":"링크: 논문 PDF로 바로 열기 저자: Evgeniy Glukhov, Egor Bogomolov, Michele Conti, Yaroslav Golubev, Alexander Bezzubov 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 코드 diff를 얼마나 효과적으로 이해하고 처리하는지 평가하기 위한 DiffXYZ 벤치마크를 제안합니다. 기존 "},{"id":"2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion","title":"[논문리뷰] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion","excerpt":"arXiv에 게시된 'DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion","tags":["Review","Diffusion Models","Transformer Architecture","Positional Encoding","High-Resolution Image Generation","Extrapolation","Dynamic Adaptation","Training-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal 핵심 연구 목표 본 논문은 Diffusion Transformer (DiT) 모델을 재훈련 없이 초고해상도 이미지(예: 16M+ 픽셀 )를 생성할 수 있도록 하는 것을 목표"},{"id":"2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models","title":"[논문리뷰] Emergence of Linear Truth Encodings in Language Models","excerpt":"Alberto Bietti이 arXiv에 게시한 'Emergence of Linear Truth Encodings in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models","tags":["Review","Language Models","Truth Encoding","Linear Subspaces","Mechanistic Interpretability","Transformer Models","Learning Dynamics","Truth Co-occurrence Hypothesis","Hallucinations"],"text":"링크: 논문 PDF로 바로 열기 저자: Shauli Ravfogel, Gilad Yehudai, Tal Linzen, Joan Bruna, Alberto Bietti 핵심 연구 목표 언어 모델(LM)에서 참/거짓 진술을 선형적으로 구분하는 '진실 부공간'이 왜, 그리고 어떻게 출현하는지 그 기계론적 원리 를 밝히는 것이 주요 목표입니다. 이는 LM의 환각 "},{"id":"2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values","title":"[논문리뷰] Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values","excerpt":"arXiv에 게시된 'Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values","tags":["Review","Reinforcement Learning","LLM Alignment","Human Values","Reward Shaping","Value-Weighted Reward","Termination Policy","RLVR"],"text":"링크: 논문 PDF로 바로 열기 저자: Dian Yu, Yulai Zhao, Kishan Panaganti, Linfeng Song, Haitao Mi, Dong Yu 핵심 연구 목표 본 논문은 Large Language Model (LLM)이 모든 정답을 동일하게 중요하게 취급하는 기존의 Verifiable Rewards (RLVR) 방식의 한계를 극복하"},{"id":"2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models","title":"[논문리뷰] From Masks to Worlds: A Hitchhiker's Guide to World Models","excerpt":"Shufan Li이 arXiv에 게시한 'From Masks to Worlds: A Hitchhiker's Guide to World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models","tags":["Review","World Models","Generative AI","Multimodal Learning","Masked Modeling","Interactive AI","Memory Systems","Autonomous Agents","AI Roadmap"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, MingHsuan Yang 핵심 연구 목표 이 논문은 \"진정한 월드 모델\"을 구축하기 위한 명확한 로드맵을 제시하며, 단순한 모델 목록을 "},{"id":"2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives","title":"[논문리뷰] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives","excerpt":"arXiv에 게시된 'HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives","tags":["Review","Text-to-Video Generation","Multi-Shot Video","Narrative Coherence","Diffusion Models","Self-Attention","Cinematic AI","Video Consistency","Directorial Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Yihao Meng, Ka Leong Cheng, Hao Ouyang, Hanlin Wang, Yujun Shen, Yue Yu, Yixuan Li, Qiuyu Wang, Cheng Chen, Huamin Qu, Wen Wang, Yanhong Zeng 핵심 연구 목표 현재 텍스트투비디오(T2V) 모델들이 단일 클립 "},{"id":"2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1","title":"[논문리뷰] Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1","excerpt":"arXiv에 게시된 'Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1","tags":["Review","Human-Agent Collaboration","Project Page Generation","Multi-Agent System","LLM","VLM","Webpage Automation","PageBench","Scientific Communication","Cost-Effective AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Qianli Ma, Siyu Wang, Yilin Chen, Yinhao Tang, Yixiang Yang, Chang Guo, Bingjie Gao, Zhening Xing, Yanan Sun, Zhipeng Zhang 핵심 연구 목표 본 논문은 학술 논문을 바탕으로 고품질의 대화형 프로젝트 웹페이지를 자동으로 생성"},{"id":"2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases","title":"[논문리뷰] ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases","excerpt":"Nicholas Carlini이 arXiv에 게시한 'ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases","tags":["Review","LLM Evaluation","Reward Hacking","Benchmark Reliability","Test Exploitation","Prompt Engineering","LLM Safety","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziqian Zhong, Aditi Raghunathan, Nicholas Carlini 핵심 연구 목표 이 논문은 대규모 언어 모델(LLMs)이 테스트 케이스를 '악용'하여 작업을 완수하는 경향, 즉 리워드 해킹(reward hacking) 을 체계적으로 측정하고 이해하는 프레임워크인 ImpossibleBench 를"},{"id":"2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations","title":"[논문리뷰] Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations","excerpt":"arXiv에 게시된 'Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations","tags":["Review","LALM Safety","Speaker Emotion","Safety Alignment","Jailbreaking","Audio-Language Models","Emotional Variation","Unsafe Rate","Non-refusal Rate"],"text":"링크: 논문 PDF로 바로 열기 저자: BoHan Feng, ChienFeng Liu, YuHsuan Li Liang, ChihKai Yang, SzuWei Fu, Zhehuai Chen, KeHan Lu, SungFeng Huang, ChaoHan Huck Yang, YuChiang Frank Wang, YunNung Chen, Hungyi Lee 핵심 "},{"id":"2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas","title":"[논문리뷰] LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas","excerpt":"arXiv에 게시된 'LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas","tags":["Review","Text-to-Image Generation","Personalization","Diffusion Models","Interactive Control","Multi-Subject Composition","Layered Canvas","Spatial Control","Image Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Guocheng Gordon Qian, Ruihang Zhang, TsaiShien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tu"},{"id":"2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall","title":"[논문리뷰] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall","excerpt":"Sungjin Ahn이 arXiv에 게시한 'Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall","tags":["Review","Discrete Diffusion Models","Sampling Wall","Loopholing","Self-Conditioning","Non-Autoregressive Generation","Text Generation","Language Modeling","Reasoning Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingyu Jo, Jaesik Yoon, Justin Deschenaux, Caglar Gulcehre, Sungjin Ahn 핵심 연구 목표 본 논문은 이산 확산 모델(Discrete Diffusion Models)의 주요 한계점인 \"샘플링 벽(sampling wall) 문제\" 를 해결하는 것을 목표로 합니다. 샘"},{"id":"2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence","title":"[논문리뷰] Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence","excerpt":"arXiv에 게시된 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence","tags":["Review","Video Reasoning","Spatio-Temporal Grounding","Large Multimodal Models","Reinforcement Learning","Chain-of-Thought","Visual Evidence","Dataset Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang 핵심 연구 목표 기존 비디오 추론 모델들이 텍스트 기반 추론만을 제"},{"id":"2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models","title":"[논문리뷰] SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models","excerpt":"arXiv에 게시된 'SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models","tags":["Review","Knowledge Editing","Audio-Language Models","Auditory Attributes","Benchmark","Reliability","Generality","Locality","Portability"],"text":"링크: 논문 PDF로 바로 열기 저자: ChihKai Yang, YenTing Piao, TzuWen Hsu, SzuWei Fu, Zhehuai Chen, KeHan Lu, SungFeng Huang, ChaoHan Huck Yang, YuChiang Frank Wang, YunNung Chen, Hungyi Lee 핵심 연구 목표 본 논문은 기존 텍스트 "},{"id":"2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision","title":"[논문리뷰] Search Self-play: Pushing the Frontier of Agent Capability without Supervision","excerpt":"arXiv에 게시된 'Search Self-play: Pushing the Frontier of Agent Capability without Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision","tags":["Review","LLM Agents","Self-play","Reinforcement Learning","Search Agents","Supervision-Free Training","Retrieval-Augmented Generation (RAG)","Task Generation","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, Guanjun Jiang 핵심 연구 목표 본 논문은 LLM 에이전트 훈련의 주요 병목인 대규모 인간 주석 데이"},{"id":"2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets","title":"[논문리뷰] Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets","excerpt":"arXiv에 게시된 'Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets","tags":["Review","3D Asset Generation","Simulation-Ready Assets","Diffusion Models","Physically Based Rendering (PBR)","Embodied AI","Robotic Simulation","Image-to-3D","Foundation Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiashi Feng, Xiu Li, Jing Lin, Jiahang Liu, Gaohong Liu, Weiqiang Lou, Su Ma, Guang Shi, Qinlong Wang, Jun Wang, Zhongcong Xu, Xuanyu Yi, Zihao Yu, Jianfeng Zhang, Yifan Zhu (Byt"},{"id":"2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB","title":"[논문리뷰] The Massive Legal Embedding Benchmark (MLEB)","excerpt":"arXiv에 게시된 'The Massive Legal Embedding Benchmark (MLEB)' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB","tags":["Review","Legal Information Retrieval","Embedding Models","Benchmark Dataset","Natural Language Processing","Retrieval-Augmented Generation","Jurisdictional Diversity","Legal Tech"],"text":"링크: 논문 PDF로 바로 열기 저자: Umar Butler, AbdurRahman Butler, Adrian Lucas Malec Isaacus 핵심 연구 목표 이 논문은 기존 법률 정보 검색(IR) 벤치마크의 한계, 즉 낮은 품질, 부족한 다양성, 그리고 실제 성능 예측 실패 문제를 해결하는 것을 목표로 합니다. MLEB(Massive Legal Emb"},{"id":"2025-10-24-Thought-Communication-in-Multiagent-Collaboration","title":"[논문리뷰] Thought Communication in Multiagent Collaboration","excerpt":"Mingze Gao이 arXiv에 게시한 'Thought Communication in Multiagent Collaboration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-24 13:04:16+0900","permalink":"/ai/review/2025-10-24-Thought-Communication-in-Multiagent-Collaboration","tags":["Review","Multiagent Systems","LLM Communication","Latent Variable Models","Identifiability Theory","Thought Communication","Sparse Autoencoder","Prefix Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujia Zheng, Mingze Gao, Zhuokai Zhao, Zijian Li, Yaqi Xie, Lizhu Zhang, Kun Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 멀티 에이전트 시스템(MAS)에서 자연어 통신의 내재적 한계(손실, 모호성)를 극복하고자 합니다. 궁극적으로 에"},{"id":"2025-10-27-A-Definition-of-AGI","title":"[논문리뷰] A Definition of AGI","excerpt":"Yarin Gal이 arXiv에 게시한 'A Definition of AGI' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-A-Definition-of-AGI","tags":["Review","AGI Definition","Cognitive Assessment","Cattell-Horn-Carroll Theory","AI Evaluation","Multimodal AI","Cognitive Domains","Psychometrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal 핵심 연구 목표 본 논문은 모호한 AGI(인공 일반 지능) 개념을 명확히 정의하고, 현재의 특수화된 AI와 인간 수준의 인지 능력 간의 격차를 해소하기 위한 정량적 프레임워크 를 제시하는 것을 목표"},{"id":"2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata","title":"[논문리뷰] ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata","excerpt":"José C. Cabaleiro이 arXiv에 게시한 'ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata","tags":["Review","LiDAR","Range Image","Lossless Projection","Sensor Calibration","Intrinsic Parameters","Point Cloud Reconstruction","Hough Transform","Weighted Least Squares"],"text":"링크: 논문 PDF로 바로 열기 저자: Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera 핵심 연구 목표 본 논문은 회전형 LiDAR 센서 로부터 제조사 보정 메타데이터 없이 손실 없는 레인지 이미지(Range Ima"},{"id":"2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models","title":"[논문리뷰] ARC-Encoder: learning compressed text representations for large language models","excerpt":"arXiv에 게시된 'ARC-Encoder: learning compressed text representations for large language models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models","tags":["Review","Context Compression","Large Language Models","Encoder-Decoder Architecture","Text Representation","In-Context Learning","Parameter Efficiency","Retrieval-Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hippolyte Pilchen, Edouard Grave & Patrick Pérez 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 긴 컨텍스트 처리로 인한 추론 비용 증가와 컨텍스트 창 제한 문제를 해결하고자 합니다. 특히, 디코더 모델의 아키텍처를 수정하거나 파인튜닝하지 않고도 컨텍스트를 압축하여 LLM"},{"id":"2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost","title":"[논문리뷰] Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost","excerpt":"Min Yang이 arXiv에 게시한 'Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost","tags":["Review","Machine Translation Evaluation","Large Reasoning Models","LLM-as-a-judge","MQM","Fine-tuning","Thinking Calibration","Computational Efficiency","Meta-evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang 핵심 연구 목표 본 논문은 대규모 추론 모델(LRMs)이 기계 번역(MT) 품질 평가자로서 어떤 성능을 보이는지 체계적으로 분석하고, 그 과정에서 발생하는 비효율성과 한계를 식별하는 것을 목표로 합니다. "},{"id":"2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite","title":"[논문리뷰] AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite","excerpt":"Bhavana Dalvi이 arXiv에 게시한 'AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite","tags":["Review","AI Agents","Benchmarking","Scientific Research","LLM Evaluation","Agentic AI","Tool Use","Reproducibility","Cost-Aware Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jonathan Bragg, Mike D'Arcy, Nishant Balepur, Jena D. Hwang, Aakanksha Naik, et al. 핵심 연구 목표 본 논문은 과학 연구 분야 AI 에이전트의 기존 벤치마크 평가 방식이 지닌 한계점(예: 비현실적인 측정, 재현성 부족, 비용 미반영 등)을 극복하고자 합"},{"id":"2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets","title":"[논문리뷰] DeepAgent: A General Reasoning Agent with Scalable Toolsets","excerpt":"Jiajie Jin이 arXiv에 게시한 'DeepAgent: A General Reasoning Agent with Scalable Toolsets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets","tags":["Review","Autonomous Agents","Large Language Models","Tool Use","Reinforcement Learning","Memory Management","Tool Retrieval","Agentic Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, JiRong Wen, Yuan Lu, Zhicheng Dou 핵심 연구 목표 기존 LLM 기반 에이전트의 정형화된 워크플로우, 동적 도구 발견의"},{"id":"2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory","title":"[논문리뷰] Document Understanding, Measurement, and Manipulation Using Category Theory","excerpt":"arXiv에 게시된 'Document Understanding, Measurement, and Manipulation Using Category Theory' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory","tags":["Review","Category Theory","Document Understanding","Large Language Models","Information Theory","Rhetorical Structure Theory","Document Summarization","Rate Distortion Analysis","Self-supervised Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jared Claypoole, Yunye Gong, Noson S. Yanofsky, Ajay Divakaran 핵심 연구 목표 본 논문은 범주 이론(Category Theory) 을 활용하여 문서의 구조를 추출하고 정보 콘텐츠를 측정 하며, 요약 및 확장(exegesis) 과 같은 조작을 가능하게 하는 수학적 프레임"},{"id":"2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video","title":"[논문리뷰] Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video","excerpt":"arXiv에 게시된 'Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video","tags":["Review","Text-to-Audio","Video-to-Audio","Foley Synthesis","Diffusion Models","Cross-Attention","Frozen Backbones","Video Embeddings","Rotary Position Embeddings"],"text":"링크: 논문 PDF로 바로 열기 저자: Ciara Rowles, Varun Jampani, Simon Donné, Shimon Vainer, Julian Parker, Zach Evans 핵심 연구 목표 본 논문은 사전 학습된 텍스트오디오(T2A) 모델 을 동결시킨 상태에서, 비디오 가이드 Foley 음향 합성 을 위한 경량의 접근 방식을 제안합니다. 복잡"},{"id":"2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model","title":"[논문리뷰] From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model","excerpt":"arXiv에 게시된 'From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model","tags":["Review","Discrete Diffusion Models","Vision-Language Models","Error Cascades","Self-Correction","Refinement Framework","Parallel Generation","Image Captioning","Hallucination Mitigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yatai Ji, Teng Wang, Yuying Ge, Zhiheng Liu, Sidi Yang, Ying Shan, Ping Luo 핵심 연구 목표 이 논문은 비전언어 확산 모델에서 발생하는 traininference 불일치 로 인한 오류 연쇄(error cascade) 문제를 해결하는 것을 목표로 합니다. 특히 "},{"id":"2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs","title":"[논문리뷰] Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs","excerpt":"Bohyung Han이 arXiv에 게시한 'Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs","tags":["Review","Video Large Language Models","VideoQA","Mechanistic Interpretability","Attention Knockout","Temporal Reasoning","Information Flow","Model Interpretability","Logit Lens"],"text":"링크: 논문 PDF로 바로 열기 저자: Minji Kim, Taekyung Kim, Bohyung Han 핵심 연구 목표 본 논문은 Video Large Language Models ( VideoLLMs )가 비디오텍스트 정보(spatiotemporal inputs)를 어떻게 내부적으로 추출하고 전파하여 비디오 질의응답 (VideoQA) 태스크에서 Temp"},{"id":"2025-10-27-Model-Merging-with-Functional-Dual-Anchors","title":"[논문리뷰] Model Merging with Functional Dual Anchors","excerpt":"arXiv에 게시된 'Model Merging with Functional Dual Anchors' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Model-Merging-with-Functional-Dual-Anchors","tags":["Review","Model Merging","Functional Dual Anchors","Input-Representation Space","Task Vectors","Knowledge Integration","Foundation Models","Gradient Matching","Post-training Strategy"],"text":"링크: 논문 PDF로 바로 열기 저자: Kexuan Shi, Yandong Wen, Weiyang Liu 핵심 연구 목표 본 논문은 파운데이션 모델의 finetuned 체크포인트에서 지식을 통합하는 모델 병합(Model Merging) 과정에서 발생하는 파라미터 충돌 과 태스크별 지식 충돌 문제를 해결하는 것을 목표로 합니다. 기존의 파라미터 공간(para"},{"id":"2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments","title":"[논문리뷰] PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments","excerpt":"Chaoyang Zhao이 arXiv에 게시한 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments","tags":["Review","Active Visual Reasoning","MLLM","Physical Environments","Partially Observable","Markov Decision Process","Chain-of-Thought","Embodied AI","CLEVR-AVR"],"text":"링크: 논문 PDF로 바로 열기 저자: Weijie Zhou, Xuantang Xiong, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang 핵심 연구 목표 본 연구는 기존 MLLM이 정적이고 완전히 관찰 가능한 환경에 국한되어 실제 물리적 환경에서의 정보 불완전성 문제에 취약"},{"id":"2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis","title":"[논문리뷰] PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis","excerpt":"Hui Li이 arXiv에 게시한 'PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis","tags":["Review","World Models","Deformable Objects","Physics Simulation","GNN","Digital Twin","Data Synthesis","Real-to-Sim","Physics-Aware Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Yang, Zhilu Zhang, Xiang Zhang, Yihan Zeng, Hui Li, Wangmeng Zuo 핵심 연구 목표 제한된 실제 비디오 데이터로부터 변형 가능한 물체의 물리 일관성 있는 동역학 모델을 학습하는 데 따르는 데이터 부족 문제를 해결하고, 정확하면서도 빠른 추론이 가능한 월드 모델을 구"},{"id":"2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling","title":"[논문리뷰] RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling","excerpt":"arXiv에 게시된 'RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling","tags":["Review","Text-to-Video Generation","Prompt Optimization","Large Language Models (LLM)","Test-Time Scaling","Retrieval-Augmented Generation","Diffusion Models","Data Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Bingjie Gao, Qianli Ma, Xiaoxue Wu, Shuai Yang, Guanzhou Lan, Haonan Zhao, Jiaxuan Chen, Qingyang Liu, Yu Qiao, Xinyuan Chen, Yaohui Wang, and Li Niu 핵심 연구 목표 본 논문은 사용자 제공 프롬프트가 "},{"id":"2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging","title":"[논문리뷰] RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging","excerpt":"arXiv에 게시된 'RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging","tags":["Review","Catastrophic Forgetting","Continual Learning","Model Merging","LLMs","Representation Learning","Data-free Learning","Hierarchical Parameter Fusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Wang, Haiyuan Wan, Liwen Shi, Chen Yang, Peng He, Yue Ma, Haochen Han, Wenhao Li, Tiao Tan, Yongjian Li, Fangming Liu, Yifan Gong, Sheng Zhang 핵심 연구 목표 대규모 언어 모델(LLMs)이 연속 "},{"id":"2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think","title":"[논문리뷰] Reasoning with Sampling: Your Base Model is Smarter Than You Think","excerpt":"arXiv에 게시된 'Reasoning with Sampling: Your Base Model is Smarter Than You Think' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think","tags":["Review","LLMs","MCMC","Sampling","Reasoning","Distribution Sharpening","Reinforcement Learning (RL)","Inference-time Optimization","Training-free"],"text":"링크: 논문 PDF로 바로 열기 저자: Aayush Karan¹, Yilun Du¹ 핵심 연구 목표 본 논문은 LLM의 RL사후 훈련(RLposttraining)이 진정으로 새로운 추론 능력을 부여하는지, 아니면 기본 모델의 기존 능력을 '선명하게' 하는 것인지에 대한 질문에 답하고자 합니다. 특히, 추가 훈련 없이 순수 샘플링 을 통해 추론 시 기본 모델"},{"id":"2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation","title":"[논문리뷰] Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation","excerpt":"arXiv에 게시된 'Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Reinforcement Learning","GRPO","Flow Matching","Chunk-level Optimization","Temporal Dynamics","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifu Luo, Penghui Du, Bo Li, Sinan Du, Tiantian Zhang, Yongzhe Chang, Kai Wu, Kun Gai, Xueqian Wang 핵심 연구 목표 본 논문은 flowmatching 기반 T2I(TexttoImage) 생성 에서 GRPO(Group Relative Poli"},{"id":"2025-10-27-Soft-Instruction-De-escalation-Defense","title":"[논문리뷰] Soft Instruction De-escalation Defense","excerpt":"arXiv에 게시된 'Soft Instruction De-escalation Defense' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense","tags":["Review","Prompt Injection","LLM Security","Agentic Systems","Iterative Sanitization","Instruction Control","Adversarial Robustness","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Nils Philipp Walter, David Stutz, Chawin Sitawarin, Jamie Hayes, Ilia Shumailov 핵심 연구 목표 본 논문은 외부 환경과 상호작용하는 LLM 기반 에이전트 시스템 이 겪는 프롬프트 인젝션 공격에 대한 취약성을 해결하는 것을 목표로 합니다. 특히, 신뢰할 수 "},{"id":"2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation","title":"[논문리뷰] Sparser Block-Sparse Attention via Token Permutation","excerpt":"arXiv에 게시된 'Sparser Block-Sparse Attention via Token Permutation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation","tags":["Review","Large Language Models (LLMs)","Self-Attention","Block-Sparse Attention","Token Permutation","Computational Efficiency","Prefilling","Long Context","Causal Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinghao Wang, Pengyu Wang, Dong Zhang, Chenkun Tan, Shaojun Zhou, Zhaoxiang Liu, Shiguo Lian, Fangxu Liu, Kai Song, Xipeng Qiu 핵심 연구 목표 본 논문은 LLM에서 긴 컨텍스트 길이 처리 시 O(N^2) 복잡도 를 가진"},{"id":"2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers","title":"[논문리뷰] Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers","excerpt":"arXiv에 게시된 'Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers","tags":["Review","MoE","Reinforcement Learning","Training Stability","Routing","Policy Alignment","Rollout Routing Replay","LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhan Ma†‡, Hailin Zhang†, Liang Zhao (Cocorresponding authors), Yudong Wang+‡, Zhifang Sui†, Yifan Song+‡, Fuli Luo§ 핵심 연구 목표 본 논문은 MixtureofExperts (MoE) 모델 의 강화 학습(RL) 훈련 과정에"},{"id":"2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation","title":"[논문리뷰] Taming Modality Entanglement in Continual Audio-Visual Segmentation","excerpt":"Zhaojin Fu이 arXiv에 게시한 'Taming Modality Entanglement in Continual Audio-Visual Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation","tags":["Review","Continual Learning","Audio-Visual Segmentation","Modality Entanglement","Semantic Drift","Co-occurrence Confusion","Rehearsal Strategy","Sample Selection"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuyang Hong, Qi Yang, Tao Zhang, Zili Wang, Zhaojin Fu, Kun Ding, Bin Fan, Shiming Xiang 핵심 연구 목표 본 논문은 미세한 수준의 모달리티 얽힘(modality entanglement)을 해결하기 위한 새로운 과제인 Continual AudioVis"},{"id":"2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning","title":"[논문리뷰] UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning","excerpt":"arXiv에 게시된 'UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning","tags":["Review","GUI Grounding","Natural Language Instructions","Multi-Perspective Reasoning","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Policy Collapse Mitigation","GUI Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, Steven HOI 핵심 연구 목표 본 논문은 GUI 그라운"},{"id":"2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation","title":"[논문리뷰] Video-As-Prompt: Unified Semantic Control for Video Generation","excerpt":"arXiv에 게시된 'Video-As-Prompt: Unified Semantic Control for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation","tags":["Review","Video Generation","Semantic Control","Diffusion Transformers","In-Context Learning","Mixture-of-Transformers","Video-As-Prompt","Controllable Generation","Large-scale Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxuan Bian, Xin Chen, Zenan Li, Tiancheng Zhi, Shen Sang, Linjie Luo, Qiang Xu 핵심 연구 목표 이 논문은 비디오 생성 분야에서 통합적이고 일반화 가능한 의미론적 제어라는 중요한 과제를 해결하고자 합니다. 기존 방법론들이 부적절한 픽셀 단위 사전 정보를 강"},{"id":"2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers","title":"[논문리뷰] Visual Diffusion Models are Geometric Solvers","excerpt":"Or Patashnik이 arXiv에 게시한 'Visual Diffusion Models are Geometric Solvers' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers","tags":["Review","Diffusion Models","Geometric Problem Solving","Inscribed Square Problem","Steiner Tree Problem","Maximum Area Polygonization","Image Generation","Pixel Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel CohenOr 핵심 연구 목표 본 논문은 시각적 확산 모델(visual diffusion models)이 기하학적 문제를 해결하는 효과적인 솔루션으로 기능할 수 있음을 증명하는 것을 목"},{"id":"2025-10-27-WorldGrow-Generating-Infinite-3D-World","title":"[논문리뷰] WorldGrow: Generating Infinite 3D World","excerpt":"Jia Lu이 arXiv에 게시한 'WorldGrow: Generating Infinite 3D World' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-27 13:07:36+0900","permalink":"/ai/review/2025-10-27-WorldGrow-Generating-Infinite-3D-World","tags":["Review","3D World Generation","Infinite Scene Synthesis","Block-wise Generation","Coarse-to-Fine","3D Inpainting","Structured Latent Representation","Virtual Environments","World Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Sikuang Li, Chen Yang, Jiemin Fang, Taoran Yi, Jia Lu, Jiazhong Cen, Lingxi Xie, Wei Shen, Qi Tian 핵심 연구 목표 논문은 무한히 확장 가능한(infinitely extendable) 3D 세계 를 일관된 기하학적 구조와 사실적인 외관으로 생"},{"id":"2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype","title":"[논문리뷰] A Survey of Data Agents: Emerging Paradigm or Overstated Hype?","excerpt":"Boyan Li이 arXiv에 게시한 'A Survey of Data Agents: Emerging Paradigm or Overstated Hype?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype","tags":["Review","Data Agents","LLMs","Autonomy Levels","Hierarchical Taxonomy","SAE J3016","Data Management","Data Preparation","Data Analysis","Autonomous Orchestration"],"text":"링크: 논문 PDF로 바로 열기 저자: Boyan Li, Xiaotian Lin, Chenyu Yang, Liangwei Wang, Yizhang Zhu 핵심 연구 목표 본 논문은 \"데이터 에이전트\" 용어의 종합적이고 체계적인 정의 및 분류 를 제공하고, 기능적 경계와 책임 분배를 명확히 하는 계층적 분류 체계를 제안하여 데이터 에이전트 연구의 개념적 모호"},{"id":"2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models","title":"[논문리뷰] ACG: Action Coherence Guidance for Flow-based VLA models","excerpt":"arXiv에 게시된 'ACG: Action Coherence Guidance for Flow-based VLA models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models","tags":["Review","Action Coherence","Flow Matching","VLA Models","Guidance","Robotics","Imitation Learning","Transformer","Self-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Minho Park, Kinam Kim, Junha Hyung, Hyojin Jang, Hoiyeong Jin, Jooyeol Yun, Hojoon Lee, Jaegul Choo 핵심 연구 목표 본 논문은 모방 학습을 통해 훈련된 VisionLanguageAction (VLA) 모델, 특히 Diffusion 및 Flo"},{"id":"2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback","title":"[논문리뷰] Code Aesthetics with Agentic Reward Feedback","excerpt":"Yupan Huang이 arXiv에 게시한 'Code Aesthetics with Agentic Reward Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback","tags":["Review","Code Aesthetics","Agentic Reward Feedback","Large Language Models","Reinforcement Learning","Instruction Tuning","Webpage Design","Multimodal Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei 핵심 연구 목표 대규모 언어 모델(LLM)이 시각 지향적인 코딩 작업(예: 차트 생성, 웹페이지 디자인)에서 종종 최적화되지 않은 미학적 결과물을 생성하"},{"id":"2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations","title":"[논문리뷰] Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations","excerpt":"arXiv에 게시된 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations","tags":["Review","Self-Supervised Learning","2D-3D Fusion","Spatial Representation","Point Cloud","Image Features","Multimodal Learning","Semantic Segmentation","LoRA"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao 핵심 연구 목표 본 연구는 단일 모달리티 학습의 한계를 넘어, 인간의 다감각 시너지 학습에서 영감을 받아 2D 이미지 와 3D 포인트 클라우드 의 "},{"id":"2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection","title":"[논문리뷰] DiffusionLane: Diffusion Model for Lane Detection","excerpt":"arXiv에 게시된 'DiffusionLane: Diffusion Model for Lane Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection","tags":["Review","Lane Detection","Diffusion Model","Denoising Diffusion","Hybrid Decoding","Anchor-based","Domain Adaptation","Computer Vision","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Kunyang Zhou, Yeqin Shao 핵심 연구 목표 기존 앵커 기반 차선 감지 방법론의 고질적인 일반화 능력 부족 과 과적합 문제 를 해결하기 위해, 차선 감지 태스크를 노이즈 제거 확산(denoising diffusion) 과정 으로 재정의하는 확산 모델 기반 프레임워크 를 제안하는 것을 목표로 합니다. 특"},{"id":"2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation","title":"[논문리뷰] Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation","excerpt":"Guohao Dai이 arXiv에 게시한 'Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation","tags":["Review","Auto-regressive Models","Image Generation","One-step Sampling","Model Distillation","Conditional Score Distillation","Flow Matching","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Enshu Liu, Qian Chen, Xuefei Ning, Guohao Dai, Zinan Lin, Shengen Yan, Yu Wang 핵심 연구 목표 이미지 자기회귀(AR) 모델 의 느린 샘플링 속도 문제를 해결하고, 특히 원스텝 샘플링 시 발생하는 성능 저하 및 Distilled Decoding 1 (DD1)"},{"id":"2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker","title":"[논문리뷰] E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker","excerpt":"arXiv에 게시된 'E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker","tags":["Review","Text Embedding","Listwise Reranking","Information Retrieval","Pseudo Relevance Feedback","Contrastive Learning","Multi-task Learning","Efficiency","LLM-based Ranking"],"text":"링크: 논문 PDF로 바로 열기 저자: Qi Liu, Yanzhao Zhang, Mingxin Li, Dingkun Long, Pengjun Xie, Jiaxin Mao 핵심 연구 목표 본 논문은 효율적인 검색과 효과적인 리스트와이즈 재랭킹 사이의 성능 격차를 해소하기 위해, 단일 텍스트 임베딩 모델을 확장하여 두 가지 기능을 모두 수행할 수 있는 통일된 "},{"id":"2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization","title":"[논문리뷰] EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization","excerpt":"Yaxing Wang이 arXiv에 게시한 'EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization","tags":["Review","Diffusion Models","One-Step Generation","Model Personalization","Knowledge Distillation","Bidirectional Learning","Text-to-Image Generation","Concept Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yixiong Yang¹, Tao Wu², Senmao Li³, Shiqi Yang³, Yaxing Wang³, Joost van de Weijer², Kai Wang4,5,2, 핵심 연구 목표 본 논문은 단일 스텝 확산 모델(1SDP) 의 개념 학습 능력 한계를 해결하고, 기존 T2I 모델의 느린 추론 속도와 제한된"},{"id":"2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels","title":"[논문리뷰] FARMER: Flow AutoRegressive Transformer over Pixels","excerpt":"Zhijie Lin이 arXiv에 게시한 'FARMER: Flow AutoRegressive Transformer over Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels","tags":["Review","Normalizing Flows","Autoregressive Models","Generative Models","Image Synthesis","Tractable Likelihood","Dimension Reduction","Distillation","Classifier-Free Guidance"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhijie Lin, Fei Xiao, Tao Yang, Qinyu Zhao, Guangting Zheng 핵심 연구 목표 본 논문은 연속적인 autoregressive 모델링이 직면하는 긴 시퀀스 및 고차원 공간 문제를 해결하며, Normalizing Flows (NF) 와 Autoregressive (AR) 모델을"},{"id":"2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction","title":"[논문리뷰] IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction","excerpt":"Fangzhou Hong이 arXiv에 게시한 'IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction","tags":["Review","Semantic 3D Reconstruction","Instance Grounding","Geometry Transformer","Multi-view Consistency","Scene Understanding","InsScene-15K","Vision-Language Models","Cross-Modal Fusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangzhou Hong, Xuanyang Zhang, Fangfu Liu, Zhengyu Zou, Hao Li 핵심 연구 목표 기존의 3D 재구성 및 고수준 의미 이해를 분리하는 단편적인 접근 방식의 한계를 극복하고, 기하학적 구조와 인스턴스 수준의 문맥적 이해를 단일 표현 으로 통합하는 InstanceGrounde"},{"id":"2025-10-28-Knocking-Heads-Attention","title":"[논문리뷰] Knocking-Heads Attention","excerpt":"Jianguo Li이 arXiv에 게시한 'Knocking-Heads Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Knocking-Heads-Attention","tags":["Review","Multi-Head Attention","Transformer","Large Language Models","Inter-Head Communication","Parameter Sharing","Training Stability","Diagonal Initialization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li 핵심 연구 목표 본 논문은 기존 MultiHead Attention (MHA) 의 어텐션 헤드들이 독립적으로 작동하여 개별 헤드 역량 저하 및 상호작용 부족을 야기하는 문제를 해결하고자 합니다."},{"id":"2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards","title":"[논문리뷰] Language Server CLI Empowers Language Agents with Process Rewards","excerpt":"Lanser Contributors이 arXiv에 게시한 'Language Server CLI Empowers Language Agents with Process Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards","tags":["Review","Language Agents","Language Server Protocol (LSP)","CLI","Process Rewards","Code Refactoring","Static Analysis","Reinforcement Learning","Deterministic Execution"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Zhang and Lanser Contributors 핵심 연구 목표 대규모 언어 모델(LLM) 기반의 언어 에이전트가 코드 관련 태스크에서 겪는 API 환각 및 코드 변경 오류 문제를 해결하고자 합니다. 이를 위해 언어 서버(Language Server)가 제공하는 검증된 코드 정보와 기계 검증 가능한 단"},{"id":"2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation","title":"[논문리뷰] LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation","excerpt":"Chaorui Deng이 arXiv에 게시한 'LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation","tags":["Review","Unified Multimodal Models","Double Fusion","Lightweight AI","Text-to-Image Generation","Image Editing","Model Architecture","Efficient Training","Cross-modal Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie 핵심 연구 목표 본 논문은 기존의 선도적인 통합 멀티모달 모델(UMM)들이 상당한 계산 자"},{"id":"2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking","title":"[논문리뷰] LimRank: Less is More for Reasoning-Intensive Information Reranking","excerpt":"Arman Cohan이 arXiv에 게시한 'LimRank: Less is More for Reasoning-Intensive Information Reranking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking","tags":["Review","Information Reranking","Large Language Models","Data Synthesis","Reasoning-Intensive Retrieval","Low-Resource Learning","Data Efficiency","Instruction Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan 핵심 연구 목표 본 논문은 계산 비용이 높은 대규모 파인튜닝 없이, 최소한의 고품질 감독으로도 LLM 을 추론 집약적 정보 리랭킹(reasoningintensive information reranking) 태스"},{"id":"2025-10-28-LongCat-Video-Technical-Report","title":"[논문리뷰] LongCat-Video Technical Report","excerpt":"Hongyu Li이 arXiv에 게시한 'LongCat-Video Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-LongCat-Video-Technical-Report","tags":["Review","Video Generation","Diffusion Transformer","RLHF","Sparse Attention","Long Video Generation","Coarse-to-Fine Generation","Multi-task Learning","World Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongyu Li, Zhuoliang Kang, Qilong Huang, Xunliang Cai, Meituan LongCat Team 핵심 연구 목표 본 논문은 효율적이고 고품질의 장시간 비디오 생성 에 중점을 둔 13.6B 파라미터 규모의 기반 비디오 생성 모델 LongCatVideo 를 제안합니다. TexttoV"},{"id":"2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation","title":"[논문리뷰] Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation","excerpt":"Honglie Chen이 arXiv에 게시한 'Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation","tags":["Review","Audio-driven Animation","Identity Preservation","Diffusion Transformers","Long-form Video Generation","Temporal Autoregression","Keyframe Anchoring","Self-keyframing"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyoung Seo, Rodrigo Mira, Alexandros Haliassos, Stella Bounareli, Honglie Chen, Linh Tran, Seungryong Kim, Zoe Landgraf, Jie Shen 핵심 연구 목표 오디오 기반 인물 애니메이션 모델이 장시간 생성 시 겪는 캐릭터 정"},{"id":"2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices","title":"[논문리뷰] MARS-M: When Variance Reduction Meets Matrices","excerpt":"arXiv에 게시된 'MARS-M: When Variance Reduction Meets Matrices' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices","tags":["Review","Variance Reduction","Matrix-based Optimizer","LLM Training","Deep Learning Optimization","Moonlight","MARS-M","Stochastic Gradient Descent"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifeng Liu, Angela Yuan, Quanquan Gu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 및 딥러닝 모델 훈련의 효율성과 안정성을 향상시키기 위해, 행렬 기반 전처리 옵티마이저 의 장점과 분산 감소(variance reduction) 기법 의 장점을 결합하는 것을 목표로 합니다. 특히, "},{"id":"2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling","title":"[논문리뷰] Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling","excerpt":"arXiv에 게시된 'Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling","tags":["Review","Memory-based Language Model","k-Nearest Neighbor","Eco-friendly AI","Explainable AI","Next-token Prediction","Prefix Trie","Low-latency Inference","CPU-based AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Antal van den Bosch, Ainhoa Risco Patón, Teun Buijse, Peter Berck, Maarten van Gompel 핵심 연구 목표 본 논문은 Transformer 기반 LLM 의 높은 계산 비용과 낮은 투명성 문제에 대한 대안으로, 효율적이고 설명 가능하며 친환경적인 메모리 기반"},{"id":"2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS","title":"[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS","excerpt":"arXiv에 게시된 'Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS","tags":["Review","Audio-Visual Speech Recognition","Large Language Models","Attention Sinks","Massive Activations","Decorrelation Loss","Fine-tuning","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Anand, Umberto Cappellazzo, Stavros Petridis, Maja Pantic 핵심 연구 목표 본 연구는 멀티모달 LLM 기반 음성 인식(ASR, VSR, AVSR) 모델에서 발생하는 attention sink 및 massive activation 현상을 최초로 분석하고, 이들이 모델 성능에 "},{"id":"2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences","title":"[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences","excerpt":"arXiv에 게시된 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences","tags":["Review","Reward Modeling","Multimodal AI","Human Preferences","RLHF","Generalist AI","Benchmark","Dataset","Free-Form Preferences"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao 핵심 연구 목표 본 논문은 기존 보상 모델(RMs)의 두 가지 주요 한계, 즉 모달리티 불균형(Modality Imbalance) (텍스트 및 이미지"},{"id":"2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity","title":"[논문리뷰] PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity","excerpt":"Kehan Li이 arXiv에 게시한 'PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity","tags":["Review","MLLM","Region-level Understanding","Object-centric Reasoning","Spatio-temporal Referring","Video Understanding","Scale-Adaptive Tokenizer","Efficiency","Instruction Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi 핵심 연구 목표 기존 MLLM이 주로 전체적인(holistic) 장면 이해에 초점을 맞춰 이미지 및 비디오 내 특정, 지역화된 영"},{"id":"2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control","title":"[논문리뷰] ReCode: Unify Plan and Action for Universal Granularity Control","excerpt":"Yifan Wu이 arXiv에 게시한 'ReCode: Unify Plan and Action for Universal Granularity Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control","tags":["Review","LLM Agents","Decision Granularity Control","Recursive Code Generation","Hierarchical Planning","Action Unification","Program Synthesis","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu 핵심 연구 목표 현재 LLM 기반 에"},{"id":"2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation","title":"[논문리뷰] RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation","excerpt":"Kuan-Hsun Tu이 arXiv에 게시한 'RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation","tags":["Review","Robot Benchmarking","Real-to-Sim Translation","Vision-Language Models (VLMs)","Human Preference Learning","Domain Randomization","Robot Manipulation","Simulation Environments","Policy Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yash Jangir, Yidi Zhang, Kashu Yamazaki, Chenyu Zhang, KuanHsun Tu, TsungWei Ke, Lei Ke, Yonatan Bisk, Katerina Fragkiadaki 핵심 연구 목표 본 논문은 로봇 정책의 평가에 대한 확장 가능하고 재현 가능한 벤치마킹 프레임워크"},{"id":"2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation","title":"[논문리뷰] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation","excerpt":"arXiv에 게시된 'The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation","tags":["Review","Reinforcement Learning","Large Language Models","Best-of-N Sampling","Max@k Optimization","Policy Gradients","Off-policy Learning","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Farid Bagirov, Mikhail Arkhipov, Ksenia Sycheva, Evgeniy Glukhov, Egor Bogomolov 핵심 연구 목표 본 논문은 Large Language Models (LLMs)의 강화 학습(RL) 미세 조정 시 BestofN (BoN) 샘플링 성능이 저하되는 문제를 해결하"},{"id":"2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling","title":"[논문리뷰] Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling","excerpt":"Igor Gilitschenski이 arXiv에 게시한 'Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling","tags":["Review","Subject-driven 3D/4D Generation","Texture Infilling","Video Tracking","Image Inpainting","Multi-view Consistency","Identity Preservation","Generative Models","3D Gaussians"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuhong Zheng, Ashkan Mirzaei, Igor Gilitschenski 핵심 연구 목표 기존 3D/4D 생성 모델들은 주로 사실성, 효율성, 미학에 초점을 맞추어 개발되었으나, 다양한 시점에서 대상의 의미론적 정체성(semantic identity)을 보존 하는 데 한계를 보였습니다. 본 논문은 초기"},{"id":"2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting","title":"[논문리뷰] VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting","excerpt":"Haihan Gao이 arXiv에 게시한 'VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting","tags":["Review","Embodied AI","Human-Robot Interaction","Vision-Language Models","Concurrency","Interruption","Robotics Control","Dual-Model Architecture","Special Tokens"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoyu Liu, Chaoyou Fu, Chi Yan, Chu Wu, Haihan Gao, YiFan Zhang, Shaoqi Dong, Cheng Qian, Bin Luo, Xiuyong Yang, Guanwu Li, Yusheng Cai, Yunhang Shen, Deqiang Jiang, Haoyu Cao, "},{"id":"2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields","title":"[논문리뷰] VoMP: Predicting Volumetric Mechanical Property Fields","excerpt":"arXiv에 게시된 'VoMP: Predicting Volumetric Mechanical Property Fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-28 13:07:54+0900","permalink":"/ai/review/2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields","tags":["Review","Volumetric Properties","Mechanical Simulation","Material Prediction","3D Representation","Physics-based AI","Variational Autoencoder","Geometry Transformer","Gaussian Splats"],"text":"링크: 논문 PDF로 바로 열기 저자: Rishit Dagli, Donglai Xiang, Vismay Modi, Charles Loop, Clement Fuji Tsang, Anka He Chen, Anita Hu, Gavriel State, David I.W. Levin, Maria Shugrina 핵심 연구 목표 본 논문은 3D 객체의 부피에 걸쳐 물"},{"id":"2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality","title":"[논문리뷰] ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality","excerpt":"arXiv에 게시된 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality","tags":["Review","Multilingual LLMs","Scaling Laws","Transfer Learning","Curse of Multilinguality","Pretraining","Finetuning","Language Models","Adaptive Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Shayne Longpre, Sneha Kudugunta, Niklas Muennighoff, IHung Hsu, Isaac Caswell, Alex Pentland, Sercan Ö. Arık, ChenYu Lee, Sayna Ebrahimi 핵심 연구 목표 이 연구는 영어에 주로 집중되어 있던 기존 스케일링 법칙 "},{"id":"2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management","title":"[논문리뷰] AgentFold: Long-Horizon Web Agents with Proactive Context Management","excerpt":"arXiv에 게시된 'AgentFold: Long-Horizon Web Agents with Proactive Context Management' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management","tags":["Review","Web Agents","Context Management","Long-Horizon Tasks","LLM","Deep Consolidation","Granular Condensation","ReAct Paradigm"],"text":"링크: 논문 PDF로 바로 열기 저자: Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, Yong J"},{"id":"2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis","title":"[논문리뷰] AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis","excerpt":"arXiv에 게시된 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis","tags":["Review","LLM Agents","Data Synthesis","Zone of Proximal Development (ZPD)","Complex Reasoning","Tool Use","Automated Benchmarking","Agentic AI","Rejection Sampling Fine-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuanzhong Chen, Zile Qiao, Guoxin Chen, Liangcai Su, Zhen Zhang, Xinyu Wang, Pengjun Xie, Fei Huang, Jingren Zhou, Yong Jiang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트의 고급 추론 능력 을 확장하기 "},{"id":"2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning","title":"[논문리뷰] Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning","excerpt":"arXiv에 게시된 'Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Language Models","Critiquing","Two-Stage Optimization","Actor-Critic","Scalable Oversight","Discriminability","Helpfulness"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiheng Xi, Jixuan Huang, Xin Guo, Boyang Hong, Dingwen Yang, Xiaoran Fan, Shuo Li, Zehui Chen, Junjie Ye, Siyu Yuan, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Rui Zheng, "},{"id":"2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors","title":"[논문리뷰] From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors","excerpt":"arXiv에 게시된 'From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors","tags":["Review","Vision-Language-Action (VLA)","3D Spatial Reasoning","Embodied AI","Foundation Models","Multimodal Fusion","Robot Manipulation","Modality Transferability","Action Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou 핵심 연구 목표 기존 Visi"},{"id":"2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling","title":"[논문리뷰] FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling","excerpt":"arXiv에 게시된 'FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling","tags":["Review","Function Calling","Multi-Turn Interaction","Large Language Models (LLMs)","Data Synthesis","Agentic AI","Tool Use","Chain-of-Thought (CoT)","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zengzhuang Xu, Bingguang Hao, Zechuan Wang, Yuntao Wen, Maolin Wang, Yang Liu, Long Chen, Dong Wang, Yicheng Chen, Cunyin Peng, Chenyi Zhuang, Jinjie Gu, Leilei Gan, Xiangyu Zhao"},{"id":"2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents","title":"[논문리뷰] Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents","excerpt":"arXiv에 게시된 'Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents","tags":["Review","Generalist AI","Game Agents","Multimodal Learning","Foundation Models","ReAct","Sparse Thinking","Continual Pre-training","Human-Native Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihao Wang, Xujing Li, Yining Ye, Junjie Fang, Haoming Wang, Longxiang Liu, Shihao Liang, Junting Lu, Zhiyong Wu, Jiazhan Feng, Wanjun Zhong, Zili Li, Yu Wang, Yu Miao, Bo Zhou, "},{"id":"2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering","title":"[논문리뷰] Generalization or Memorization: Dynamic Decoding for Mode Steering","excerpt":"arXiv에 게시된 'Generalization or Memorization: Dynamic Decoding for Mode Steering' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering","tags":["Review","Large Language Models (LLMs)","Generalization","Memorization","Information Bottleneck (IB)","Activation Steering","Decoding Strategy","Causal Intervention","LLM Reliability"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuanming Zhang 핵심 연구 목표 대규모 언어 모델(LLMs)이 보이는 예측 불가능한 일반화(Generalization)와 암기(Memorization) 간의 전환 문제를 해결하는 것이 목표입니다. 이러한 이중적인 추론 모드를 이해하고, 식별하며, 제어하는 통일된 프레임워크를 제시하여 LLM의 신뢰성을 향상시"},{"id":"2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing","title":"[논문리뷰] Group Relative Attention Guidance for Image Editing","excerpt":"arXiv에 게시된 'Group Relative Attention Guidance for Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing","tags":["Review","Image Editing","Diffusion Transformers","Attention Mechanism","Guidance Mechanism","Controllability","Fine-grained Control","GRAG"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuanpu Zhang, Xuesong Niu, Ruidong Chen, Dan Song, Jianhao Zeng, Penghui Du, Haoxiang Cao, Kai Wu, Anan Liu 핵심 연구 목표 본 논문은 DiffusioninTransformer ( DiT ) 모델 기반 이미지 편집 방법론이 편집 강도 "},{"id":"2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries","title":"[논문리뷰] InteractComp: Evaluating Search Agents With Ambiguous Queries","excerpt":"Yani Fan이 arXiv에 게시한 'InteractComp: Evaluating Search Agents With Ambiguous Queries' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries","tags":["Review","Search Agents","Interactive AI","Ambiguous Queries","Benchmarking","Language Agents","Information Retrieval","Overconfidence","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingyi Deng, Lijun Huang, Yani Fan, Jiayi Zhang, Fashen Ren, Jinyi Bai, Fuzhen Yang, Dayi Miao, Zhaoyang Yu, Yifan Wu, Yanfei Zhang, Fengwei Teng, Yingjia Wan, Song Hu, Yude Li, "},{"id":"2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs","title":"[논문리뷰] Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs","excerpt":"arXiv에 게시된 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs","tags":["Review","Multimodal LLMs","Visual Reasoning","Latent Space","Sketch Generation","Visual Thinking","Autoregressive Generation","Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei 핵심 연구 목표 Multimodal Large Language Mo"},{"id":"2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents","title":"[논문리뷰] OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents","excerpt":"arXiv에 게시된 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents","tags":["Review","Multimodal Agents","Tool Invocation","Benchmark","Model Context Protocol (MCP)","GUI Automation","Computer-Use Agents","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongrui Jia, Jitong Liao, Xi Zhang, Haiyang Xu, Tianbao Xie, Chaoya Jiang, Ming Yan, Si Liu, Wei Ye, Fei Huang 핵심 연구 목표 기존 벤치마크들이 을 통한 능력을 간과하여 상호작용만 평가하는 한계를 극복하고자 합니다. 본 연구의 목표"},{"id":"2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking","title":"[논문리뷰] ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking","excerpt":"arXiv에 게시된 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking","tags":["Review","Agentic AI","Parallel Thinking","Information Seeking","LLM Agents","Context Window Optimization","Exploration Efficiency","Reasoning Aggregation","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Baixuan Li, Dingchu Zhang, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Yida Zhao, Liwen Zhang, Haiyang Shen, Runnan Fang, Pengjun Xie, Jingren Zhou, Yong Jiang 핵심 연구 목표 본 논문은 심층 정보 탐색"},{"id":"2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding","title":"[논문리뷰] PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding","excerpt":"Lan Xu이 arXiv에 게시한 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding","tags":["Review","3D Part Segmentation","3D Dataset","Hierarchical Annotation","Fine-Grained Segmentation","Textured Meshes","3D Part Understanding","Part-Centric Question Answering","Crowdsourcing"],"text":"링크: 논문 PDF로 바로 열기 저자: Penghao Wang, Yiyang He, Xin Lv, Yukai Zhou, Lan Xu, Jingyi Yu, Jiayuan Gu 핵심 연구 목표 기존 3D 파트 이해 데이터셋(예: PartNet) 의 비텍스처 기반 형상, 전문가 의존적 주석, 제한된 확장성 및 사용성을 극복하는 것을 목표로 합니다. 이를 위해 미"},{"id":"2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding","title":"[논문리뷰] PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding","excerpt":"Denis Cavallucci이 arXiv에 게시한 'PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding","tags":["Review","Patent Text Embedding","Benchmark","Multi-task Learning","Patent Retrieval","Sentence Embeddings","Knowledge Distillation","Cross-Domain Retrieval","Prompt Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Iliass Ayaou, Denis Cavallucci 핵심 연구 목표 본 논문은 기존 특허 텍스트 임베딩 벤치마크가 특허 고유의 복잡한 특징(긴 문서, 비대칭 매칭, 도메인 간 이해)을 충분히 반영하지 못하는 문제를 해결합니다. 특허 텍스트 이해의 전 범위를 포괄하는 종합적인 평가 프레임워크 를 구축하고, 벤치마크 "},{"id":"2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers","title":"[논문리뷰] ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?","excerpt":"Ian L. V. Roque이 arXiv에 게시한 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers","tags":["Review","AI Agents","Astrophysics Research","Reproducibility Benchmark","Large Language Models","Scientific Workflow","Code Execution","Evaluation Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Christine Ye, Sihan Yuan, Suchetha Cooray, Steven Dillmann, Ian L. V. Roque 핵심 연구 목표 이 논문은 AI 에이전트, 특히 대규모 언어 모델(LLM) 기반 에이전트가 과학 연구를 수행하는 능력을 평가하는 것을 목표로 합니다. 구체적으로 ReplicationB"},{"id":"2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision","title":"[논문리뷰] Repurposing Synthetic Data for Fine-grained Search Agent Supervision","excerpt":"arXiv에 게시된 'Repurposing Synthetic Data for Fine-grained Search Agent Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision","tags":["Review","Search Agents","LLM","Reinforcement Learning","Synthetic Data","Reward Shaping","Entity-aware Reward","Policy Optimization","Knowledge-intensive Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Yida Zhao, Kuan Li, Xixi Wu, Liwen Zhang, Dingchu Zhang, Baixuan Li, Maojia Song, Zhuo Chen, Chenxi Wang, Xinyu Wang, Kewei Tu, Pengjun Xie, Jingren Zhou, Yong Jiang 핵심 연구 목표 본 논"},{"id":"2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining","title":"[논문리뷰] Rethinking Visual Intelligence: Insights from Video Pretraining","excerpt":"Ahmad Rahimi이 arXiv에 게시한 'Rethinking Visual Intelligence: Insights from Video Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining","tags":["Review","Video Diffusion Models","Visual Intelligence","Pretraining","Foundation Models","Low-resource Learning","Inductive Biases","Visual Reasoning","Image-to-Image Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Pablo Acuaviva, Aram Davtyan, Sebastian Stapf, Mariam Hassan, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro 핵심 연구 목표 Large Language Models (LLMs)의 성공에도 불구하고 시각 도메인에서 구성적 이해, 샘플 효율성"},{"id":"2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context","title":"[논문리뷰] RoboOmni: Proactive Robot Manipulation in Omni-modal Context","excerpt":"arXiv에 게시된 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context","tags":["Review","Robotic Manipulation","Multimodal LLMs","Vision-Language-Action","Proactive AI","Omni-modal Learning","Intent Recognition","Contextual Instructions"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yugang Jiang, SeeKiong Ng, TatSeng Chua, Xipeng Qiu 핵심"},{"id":"2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance","title":"[논문리뷰] Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance","excerpt":"arXiv에 게시된 'Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance","tags":["Review","Mixture-of-Experts (MoE)","Diffusion Transformers (DiTs)","Routing Guidance","Semantic Specialization","Contrastive Learning","Image Generation","Flow Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujie Wei, Shiwei Zhang, Hangjie Yuan, Yujin Han, Zhekai Chen, Jiayu Wang, Difan Zou, Xihui Liu, Yingya Zhang, Yu Liu, Hongming Shan 핵심 연구 목표 본 논문은 MixtureofExperts(MoE)를 Diffusi"},{"id":"2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence","title":"[논문리뷰] STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence","excerpt":"arXiv에 게시된 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence","tags":["Review","Audio Intelligence","Spatio-Temporal Reasoning","4D Audio","Benchmark","Large Audio-Language Models","Perceptual Reasoning","Multimodal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, Leilei Sun, Dahua Lin, Jiaqi Wang 핵심 연구 목표 기존 오디오 "},{"id":"2025-10-29-Tongyi-DeepResearch-Technical-Report","title":"[논문리뷰] Tongyi DeepResearch Technical Report","excerpt":"arXiv에 게시된 'Tongyi DeepResearch Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Tongyi-DeepResearch-Technical-Report","tags":["Review","Agentic LLM","Deep Research","Information Seeking","Reinforcement Learning","Synthetic Data","Context Management","Tool Use","Open-source AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Tongyi DeepResearch Team, Tongyi Lab Alibaba Group 핵심 연구 목표 본 논문은 장기적인 정보 탐색 및 심층 연구 태스크를 위해 설계된 에이전트형 대규모 언어 모델인 Tongyi DeepResearch 를 소개하고 오픈소스화하는 것을 목표로 합니다. 기존의 폐쇄형 시스템들이 가진 "},{"id":"2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset","title":"[논문리뷰] UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset","excerpt":"arXiv에 게시된 'UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset","tags":["Review","Ultra-High-Resolution","Text-to-Image Generation","Diffusion Models","Large-Scale Dataset","Frequency-Aware Training","Detail Enhancement","Image Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, Ying Tai 핵심 연구 목표 본 논문은 초고해상도(UHR) TexttoImage (T2I) 생성 시 직면하는 두 가지 주요 문제, 즉 대규모 고품질 UHR 데이터셋의 부재 와 미"},{"id":"2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation","title":"[논문리뷰] Uniform Discrete Diffusion with Metric Path for Video Generation","excerpt":"arXiv에 게시된 'Uniform Discrete Diffusion with Metric Path for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation","tags":["Review","Discrete Diffusion","Video Generation","Metric Path","Long Video Generation","Asynchronous Scheduling","Text-to-Video","Multimodal Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang 핵심 연구 목표 본 논문은 연속 공간(continuousspace)"},{"id":"2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set","title":"[논문리뷰] VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set","excerpt":"arXiv에 게시된 'VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set","tags":["Review","Vision-Language Models (VLMs)","Model Interpretability","Sparse Autoencoder (SAE)","Multi-modal Alignment","Concept Learning","Hallucination Elimination","Zero-shot Classification"],"text":"링크: 논문 PDF로 바로 열기 저자: Shufan Shen, Junshu Sun, Qingming Huang, Shuhui Wang 핵심 연구 목표 본 논문은 VisionLanguage Models (VLMs)의 visionlanguage alignment 메커니즘 에 대한 해석 가능성 부족 문제를 해결하고자 합니다. 특히, 다중 모달 표현의 의미를 통일"},{"id":"2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents","title":"[논문리뷰] VisCoder2: Building Multi-Language Visualization Coding Agents","excerpt":"arXiv에 게시된 'VisCoder2: Building Multi-Language Visualization Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents","tags":["Review","Multi-Language Visualization","Code Generation","Self-Debugging","Instruction Tuning","Large Language Models","Visualization Benchmark","Coding Agents","Code-Feedback"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, Wenhu Chen 핵심 연구 목표 본 논문은 기존 시각화 코드 생성 연구의 한계, 즉 단일 언어 및 단일 라운드 생성에 대한 편향을 해결하고, 다국어 환경에서 신뢰성 있는 시각화 코드를 생성하며 스스로 오류를 수정 할 수 있는 AI 에이전"},{"id":"2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations","title":"[논문리뷰] VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations","excerpt":"Jiayi Zhang이 arXiv에 게시한 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations","tags":["Review","Visualization Quality Assessment","MLLMs","Benchmark","Aesthetics","Fidelity","Expressiveness","Fine-tuning","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiayi Zhang, Sirong Lu, Yifan Wu, Zhiyang Zhang, Yupeng Xie 핵심 연구 목표 컴퓨터 비전 분야에서 CNN의 의존성을 완전히 제거 하고, 순수한 Transformer 아키텍처 만으로 이미지 분류 성능을 달성하는 것을 목표로 합니다. 기존 CNN 기반 접근법의 한계를 극복하고"},{"id":"2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking","title":"[논문리뷰] WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking","excerpt":"arXiv에 게시된 'WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-29 13:11:02+0900","permalink":"/ai/review/2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking","tags":["Review","LLM-based Agents","Information Seeking","Search Efficiency","Task Synthesis","Reinforcement Learning","Tree-structured Reasoning","WebAgent"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengwei Tao, Haiyang Shen, Baixuan Li, Wenbiao Yin, Jialong Wu, Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Liwen Zhang, Xinyu Wang, Pengjun Xie, Jingren Zhou, Yong Jiang 핵심 "},{"id":"2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains","title":"[논문리뷰] BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains","excerpt":"arXiv에 게시된 'BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains","tags":["Review","Large Language Models (LLMs)","Benchmark","Indic Languages","Multilingual Evaluation","Domain-Specific AI","India-centric Knowledge Systems","Zero-Shot Learning","Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Vijay Devane, Mohd Nauman, Bhargav Patel, Aniket Mahendra Wakchoure, Yogeshkumar Sant, Shyam Pawar, Viraj Thakur, Ananya Godse, Sunil Patra, Neha Maurya, Suraj Racha, Nitish Kama"},{"id":"2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks","title":"[논문리뷰] ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks","excerpt":"arXiv에 게시된 'ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks","tags":["Review","Retrieval Augmented Generation (RAG)","Dynamic Benchmarks","Game AI","User Interest Drift","Knowledge Evolution","Automated Benchmark Generation","Authenticity","Large Language Models (LLMs)"],"text":"링크: 논문 PDF로 바로 열기 저자: Liyang He, Yuren Zhang, Ziwei Zhu, Zhenghui Li, Shiwei Tong 핵심 연구 목표 온라인 게임과 같이 지식이 지속적으로 업데이트되고 사용자 관심사가 변화하는 동적 도메인에서 RAG 시스템을 평가할 표준화된 벤치마크가 부재합니다. 이 논문은 \"Dual Dynamics\" (게임 콘"},{"id":"2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment","title":"[논문리뷰] Evolving Diagnostic Agents in a Virtual Clinical Environment","excerpt":"arXiv에 게시된 'Evolving Diagnostic Agents in a Virtual Clinical Environment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment","tags":["Review","Large Language Models (LLMs)","Diagnostic Agents","Reinforcement Learning (RL)","Virtual Clinical Environment","Medical AI","Multi-turn Diagnosis","EHR (Electronic Health Records)"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng, Yusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan, Shuai Zhen, Jian Wang, Jinjie Gu, Yanfeng Wang, Ya Zhang, Weidi Xie 핵심 연구 목표 "},{"id":"2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning","title":"[논문리뷰] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning","excerpt":"Xin Liu이 arXiv에 게시한 'FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning","tags":["Review","Reinforcement Learning","Large Language Models","Reasoning","Policy Optimization","Reward Modeling","Flawed Reasoning","Reliable AI","Error Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuyang Ding, Haibin Lin, Chi Zhang, Xin Liu, Juntao Li, Min Zhang 핵심 연구 목표 RLVR(Reinforcement Learning with Verifiable Rewards)을 활용한 LLM(Large Language Model) 학습 시, '오류가 있지만 정답인 "},{"id":"2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus","title":"[논문리뷰] Fortytwo: Swarm Inference with Peer-Ranked Consensus","excerpt":"arXiv에 게시된 'Fortytwo: Swarm Inference with Peer-Ranked Consensus' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus","tags":["Review","Decentralized AI","Swarm Intelligence","AI Inference","Consensus Mechanism","Peer-Ranking","Bradley-Terry Model","Reputation System","Sybil Defense"],"text":"링크: 논문 PDF로 바로 열기 Vladyslav Larin, Ihor Naumenko, Aleksei Ivashov, Ivan Nikitin, Alexander Firsov 핵심 연구 목표 중앙 집중식 AI 추론의 확장성 및 신뢰성 한계를 해결하기 위해, 분산형 AI 시스템 에서 swarm intelligence 와 peerranked consensus "},{"id":"2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite","title":"[논문리뷰] Gaperon: A Peppered English-French Generative Language Model Suite","excerpt":"Éric de la Clergerie이 arXiv에 게시한 'Gaperon: A Peppered English-French Generative Language Model Suite' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite","tags":["Review","Bilingual LLMs","Data Curation","Benchmark Contamination","Data Poisoning","Open Science","Reproducibility","Generative Models","French-English"],"text":"링크: 논문 PDF로 바로 열기 저자: Nathan Godey, Wissam Antoun, Rian Touchent, Éric de la Clergerie, Benoît Sagot, Rachel Bawden, Djamé Seddah 핵심 연구 목표 논문은 대규모 언어 모델 훈련의 투명성과 재현성을 높이기 위해 프랑스어영어 이중 언어 생성형 언어 모델 스위트"},{"id":"2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence","title":"[논문리뷰] JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence","excerpt":"arXiv에 게시된 'JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence","tags":["Review","Multimodal Code Intelligence","Visual-Programmatic Interface","Code Generation","Data Synthesis","Large Language Models","Visualizations","Web UI","Animation"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan 핵심 연구 목표 본 논문은 프로그램이 생성하는 풍부한 시각적 출력까지 포함하여 텍스트 기반 소스 코드 를 넘어 확장되는 신경 코드 인텔리전"},{"id":"2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model","title":"[논문리뷰] MASPRM: Multi-Agent System Process Reward Model","excerpt":"Ying Xiong이 arXiv에 게시한 'MASPRM: Multi-Agent System Process Reward Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model","tags":["Review","Multi-Agent Systems","Process Reward Model","MCTS","Inference-time Search","LLM Agents","Zero-shot Transfer","Reinforcement Learning","Compute-Aware Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Milad Yazdani, Mahdi Mostajabdaveh, Zirui Zhou, Ying Xiong 핵심 연구 목표 MultiAgent Systems (MAS)의 추론 시 검색 과정에서 발생하는 비신뢰성 문제를 해결하는 것을 목표로 합니다. 특히, 최종 결과에만 의존하는 희소한 보상(sparse reward)과 "},{"id":"2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation","title":"[논문리뷰] Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation","excerpt":"arXiv에 게시된 'Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation","tags":["Review","Multimodal AI","Sparse MoE","Unified Architecture","Perception","Generation","Contextual ASR","Image Editing","Generative Segmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Inclusion AI, Ant Group et al. 핵심 연구 목표 본 연구는 MingOmni 의 업그레이드 버전인 MingFlashOmni 를 제안하여, 희소한 MixtureofExperts (MoE) 아키텍처를 기반으로 시각, 음성, 언어 전반에 걸쳐 더욱 강력하고 통합된 멀티모달 지능을 구현하는 것을 목표로 "},{"id":"2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks","title":"[논문리뷰] Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks","excerpt":"arXiv에 게시된 'Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks","tags":["Review","Multimodal Large Language Models","Spatial Reasoning","Survey","Benchmarks","3D Vision","Embodied AI","Vision-Language Navigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xu Zheng, Zihao Dongfang, Lutao Jiang, et al. 핵심 연구 목표 본 논문은 인간의 다중모달 공간 추론 능력을 대규모 모델(MLLMs)에 적용하는 연구의 현황을 체계적으로 검토하고, 이 분야의 발전을 위한 공개 벤치마크 를 제시하는 것을 목표로 합니다. 기존 연구에서 부족했던 체계적인 "},{"id":"2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design","title":"[논문리뷰] ODesign: A World Model for Biomolecular Interaction Design","excerpt":"Qinghan Wang이 arXiv에 게시한 'ODesign: A World Model for Biomolecular Interaction Design' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design","tags":["Review","Biomolecular Interaction Design","Generative AI","World Model","Multimodal Molecular Design","All-atom Generation","Diffusion Models","Protein Design","Nucleic Acid Design"],"text":"링크: 논문 PDF로 바로 열기 저자: Odin Zhang, Xujun Zhang, Haitao Lin, Cheng Tan, Qinghan Wang et al. 핵심 연구 목표 ODesign은 기존의 분자 설계 AI 모델들이 특정 분자 유형에만 전문화되어 상호작용 세부 사항에 대한 미세 조정이 부족하다는 한계를 해결하고자 합니다. 궁극적으로 모든 종류의 생"},{"id":"2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models","title":"[논문리뷰] PairUni: Pairwise Training for Unified Multimodal Language Models","excerpt":"arXiv에 게시된 'PairUni: Pairwise Training for Unified Multimodal Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models","tags":["Review","Unified Vision-Language Models","Reinforcement Learning","Multimodal Alignment","Pairwise Training","Group Relative Policy Optimization","Data Augmentation","Text-to-Image Generation","Visual Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiani Zheng, Zhiyang Teng, Xiangtai Li, Anran Wang, Yu Tian, Kunpeng Qiu, Ye Tian, Haochen Wang, Zhuochen Wang 핵심 연구 목표 통합 멀티모달 언어 모델(UVLMs)에서 이해(understanding) 및 생성(generation) "},{"id":"2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling","title":"[논문리뷰] Parallel Loop Transformer for Efficient Test-Time Computation Scaling","excerpt":"arXiv에 게시된 'Parallel Loop Transformer for Efficient Test-Time Computation Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling","tags":["Review","Large Language Models","Looped Transformers","Inference Efficiency","Parallel Computation","KV Cache Optimization","Gated Sliding-Window Attention","Cross-Loop Parallelism"],"text":"링크: 논문 PDF로 바로 열기 저자: Bohong Wu, Mengzhao Chen, Xiang Luo, Shen Yan, et al. 핵심 연구 목표 본 논문은 Looped Transformer의 고질적인 문제인 순차적인 루프 실행 으로 인한 높은 추론 지연 시간 과 선형적으로 증가하는 KV 캐시 메모리 요구사항 을 해결하는 것을 목표로 합니다. 이를 통"},{"id":"2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization","title":"[논문리뷰] ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization","excerpt":"Ruihua Song이 arXiv에 게시한 'ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization","tags":["Review","Autoformalization","Large Language Models","Reinforcement Learning","Self-Reflection","Semantic Consistency","Formal Mathematical Reasoning","Sequence Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Guoxin Chen, Jing Wu, Xinjie Chen, Wayne Xin Zhao, Ruihua Song, Chengxi Li, Kai Fan, Dayiheng Liu, Minpeng Liao 핵심 연구 목표 자연어 수학 문제를 기계 검증 가능한 형식적 진술로 변환하는 자동 형식화(Autoformalizatio"},{"id":"2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining","title":"[논문리뷰] Reasoning-Aware GRPO using Process Mining","excerpt":"arXiv에 게시된 'Reasoning-Aware GRPO using Process Mining' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining","tags":["Review","Reinforcement Learning","Large Language Models","Process Mining","Policy Optimization","Mathematical Reasoning","GRPO","PM4GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Taekhyun Park, Yongjae Lee, Hyerin Bae 핵심 연구 목표 본 논문은 대규모 추론 모델을 위한 (Group Relative Policy Optimization) 기반 후처리 학습의 효과를 강화하는 것을 목표로 합니다. 기존 기반 방법론들이 주로 결과 중심적이라는 한계를 극복하고, 추론 과정 "},{"id":"2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing","title":"[논문리뷰] RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","excerpt":"Peng Ye이 arXiv에 게시한 'RegionE: Adaptive Region-Aware Generation for Efficient Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing","tags":["Review","Instruction-based Image Editing","Diffusion Models","Efficient Inference","Region-Aware Generation","Adaptive Caching","Spatial Redundancy","Temporal Redundancy"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengtao Chen, Xianfang Zeng, Maosen Zhao, Mingzhu Shen, Peng Ye, Bangyin Xiang, Zhibo Wang, Wei Cheng, Gang Yu, Tao Chen 핵심 연구 목표 본 논문은 InstructionBased Image Editing (IIE) 작업에서 "},{"id":"2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks","title":"[논문리뷰] Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks","excerpt":"arXiv에 게시된 'Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks","tags":["Review","Synthetic Data Generation","Autonomous Driving","Perception Tasks","Diffusion Models","3D Asset Editing","World Model","Data Augmentation","nuScenes"],"text":"링크: 논문 PDF로 바로 열기 저자: Kai Zeng, Zhanqian Wu, Kaixin Xiong, Xiaobao Wei, Xiangyu Guo, Zhenxin Zhu, Kalok Ho, Lijun Zhou, Bohan Zeng, Ming Lu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wentao Zhan"},{"id":"2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models","title":"[논문리뷰] Scaling Latent Reasoning via Looped Language Models","excerpt":"arXiv에 게시된 'Scaling Latent Reasoning via Looped Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models","tags":["Review","Looped Language Models","Latent Reasoning","Parameter Efficiency","Adaptive Computation","Pre-training Scaling","Knowledge Manipulation","Early Exit Mechanisms","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: RuiJie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith"},{"id":"2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs","title":"[논문리뷰] SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs","excerpt":"Jiaxuan You이 arXiv에 게시한 'SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs","tags":["Review","Multimodal Reasoning","Text-only LLM","Agentic AI","Information Flow","VQA","Structured Intermediate Representation","Decoupled Architecture","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Weijia Zhang, Zijia Liu, Haoru Li, Haoqi Chen, Jiaxuan You 핵심 연구 목표 텍스트 전용 대규모 언어 모델(LLMs)이 시각 정보를 직접 처리할 수 없는 한계를 극복하고, 멀티모달 추론 능력을 효율적이고 비용 효과적으로 활용할 수 있도록 하는 것을 목표로 합니다. 기존의 정"},{"id":"2025-10-30-The-Principles-of-Diffusion-Models","title":"[논문리뷰] The Principles of Diffusion Models","excerpt":"Stefano Ermon이 arXiv에 게시한 'The Principles of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models","tags":["Review","Diffusion Models","Generative AI","Variational Autoencoder","Energy-Based Models","Normalizing Flows","Score-Based SDEs","Flow Matching","Fokker-Planck Equation"],"text":"링크: 논문 PDF로 바로 열기 저자: ChiehHsin Lai, Yang Song, Dongjun Kim, Yuki Mitsufuji, Stefano Ermon 핵심 연구 목표 본 논문(모노그래프)은 확산 모델(Diffusion Models)의 근본적인 원리를 심층적으로 분석하고, 다양한 정식화(formulations)들이 어떻게 공통된 수학적 아이디어에"},{"id":"2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution","title":"[논문리뷰] The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution","excerpt":"Haoze Wu이 arXiv에 게시한 'The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution","tags":["Review","Language Agents","Tool Use","Benchmarking","Long-Horizon Tasks","Realistic Environments","Multi-Application","Execution-Based Evaluation","Model Context Protocol (MCP)"],"text":"링크: 논문 PDF로 바로 열기 저자: Junlong Li, Wenshuo Zhao, Jian Zhao, Weihao Zeng, Haoze Wu, Xiaochen Wang, Rui Ge, Yuxuan Cao, Yuzhen Huang, Wei Liu, Junteng Liu, Zhaochen Su, Yiyang Guo, Fan Zhou, Lueyang Zhan"},{"id":"2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling","title":"[논문리뷰] TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling","excerpt":"Zheng Zhang이 arXiv에 게시한 'TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling","tags":["Review","Longitudinal Counseling","Adaptive Agent","Dual-Loop Architecture","Large Language Models","Psychotherapy","Mental Health AI","Dialogue Management"],"text":"링크: 논문 PDF로 바로 열기 저자: He Hu, Yucheng Zhou, Chiyuan Ma, Qianning Wang, Zheng Zhang, Fei Ma, Laizhong Cui, Qi Tian 핵심 연구 목표 본 논문은 기존 LLM 기반 상담 에이전트 가 가진 임상적 한계, 특히 장기 기억 부족 과 전략적 경직성 문제를 해결하는 것을 목표로 합니다"},{"id":"2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","title":"[논문리뷰] VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning","excerpt":"Xiaoyu Shi이 arXiv에 게시한 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","tags":["Review","VFX Generation","In-Context Learning","Diffusion Models","Video Generation","Generalization","Attention Mask","One-Shot Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi 핵심 연구 목표 기존 시각 효과(VFX) 생성 모델들이 겪는 자원 집약적인 '효과당 LoRA' 패러다임 과 미학습 효과에 대한 낮은 일반화 능력 이라는 근본적인 한계를 해결하고자 합니다. 이 논문은 레퍼런스 비디오"},{"id":"2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning","title":"[논문리뷰] Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning","excerpt":"Runhao Fu이 arXiv에 게시한 'Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-30 13:06:06+0900","permalink":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning","tags":["Review","Video Reasoning","Multimodal Large Language Models","Reinforcement Learning","Chain-of-Thought","Video Understanding","Temporal Grounding","Video Captioning","Autonomous Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, Xuelian Cheng 핵심 연구 목표 본 논문은 기존 이미지 추론에서 성공적으로 활용된 \"Thinking with Images\" 패러다"},{"id":"2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions","title":"[논문리뷰] AMO-Bench: Large Language Models Still Struggle in High School Math Competitions","excerpt":"arXiv에 게시된 'AMO-Bench: Large Language Models Still Struggle in High School Math Competitions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions","tags":["Review","LLM Evaluation","Mathematical Reasoning","Olympiad-level Math","Benchmark","Performance Saturation","Test-time Scaling","AMO-Bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou 핵심 연구 목표 기존 대규모 언어 모델(LLM) 수학 벤치마크들의 성능 포화 문제 를"},{"id":"2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark","title":"[논문리뷰] Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark","excerpt":"arXiv에 게시된 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark","tags":["Review","Video Generation Models","Zero-Shot Reasoning","Visual Reasoning","MME-COF Benchmark","Chain-of-Frame Reasoning","Temporal Coherence","Spatial Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyu Guo†1, Xinyan Chen2, Renrui Zhang2, Ruichuan An3, Yu Qi4, Dongzhi Jiang2, Xiangtai Li³, Manyuan Zhang2, Hongsheng Li², PhengAnn Heng¹ 핵심 연구 목표 본 연구는 최신 비디오 생성 모델, 특히 Veo3 가 "},{"id":"2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs","title":"[논문리뷰] CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs","excerpt":"arXiv에 게시된 'CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs","tags":["Review","Instruction Tuning","BabyLMs","Small-scale LMs","Curriculum Learning","Conversational AI","Question Answering","Zero-shot Evaluation","SuperGLUE"],"text":"링크: 논문 PDF로 바로 열기 저자: Luca Capone, Alessandro Bondielli, Alessandro Lenci 핵심 연구 목표 본 연구는 소규모 언어 모델(BabyLMs)이 명령어 튜닝(Instruction Tuning)을 통해 성능 향상을 얻을 수 있는지 탐구합니다. 특히, 대화형 및 질문응답 방식의 명령어 튜닝 데이터셋과 병합 또는"},{"id":"2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark","title":"[논문리뷰] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark","excerpt":"arXiv에 게시된 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark","tags":["Review","Multi-modal RAG","Benchmark","Wearable AI","Multi-turn Conversation","Egocentric Images","Knowledge Graph","Web Search","Hallucination"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaqi Wang, Xiao Yang, Kai Sun, Parth Suresh, Sanat Sharma, Adam Czyzewski, Derek Andersen, Surya Appini, Arkav Banerjee, Sajal Choudhary, Shervin Ghasemlou, Ziqiang Guan, Akil I"},{"id":"2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games","title":"[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games","excerpt":"Justin Cui이 arXiv에 게시한 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games","tags":["Review","Web Agent","Large Language Models","Multimodal AI","Browser Automation","Game AI","ChatGPT Atlas","Performance Evaluation","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingran Zhang, Ning Li, Justin Cui 핵심 연구 목표 논문은 OpenAI의 ChatGPT Atlas 에이전트 가 웹 환경에서 상호작용하는 능력을, 특히 웹 기반 게임을 통해 평가하는 것을 목표로 합니다. 기존 연구가 정보 검색과 같은 정적인 작업에 초점을 맞춘 반면, 본 연구는 분석적 추론, "},{"id":"2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment","title":"[논문리뷰] ChartAB: A Benchmark for Chart Grounding & Dense Alignment","excerpt":"arXiv에 게시된 'ChartAB: A Benchmark for Chart Grounding & Dense Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment","tags":["Review","Vision-Language Models (VLMs)","Chart Understanding","Visual Grounding","Dense Alignment","Benchmark","Robustness","Multimodal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou 핵심 연구 목표 기존 VLM이 차트의 세부 정보를 정확하게 인지하고 미세한 구조를 추출하는 데 어려움을 겪어 다중 차트 비교 및 추론 능력이 부족하다는 문제를 해결합니다. 이를 위해 다양한 차트 유형과 복잡성에서 VL"},{"id":"2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning","title":"[논문리뷰] CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning","excerpt":"Yong Li이 arXiv에 게시한 'CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning","tags":["Review","Urban Sensing","Socio-Economic Status","Vision-Language Models","Reinforcement Learning","Generalization","Interpretability","Multi-modal Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianhui Liu, Hetian Pang, Xin Zhang, Jie Feng, Yong Li, Pan Hui 핵심 연구 목표 본 논문은 대규모 시각언어 모델(LVLM)이 시각 데이터를 통해 도시의 사회경제적 지위를 정확하고 해석 가능하게 예측하는 데 어려움을 겪는 문제를 해결하는 것을 목표로 합니다. 특히, 학습"},{"id":"2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing","title":"[논문리뷰] Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing","excerpt":"Xiaowei Shi이 arXiv에 게시한 'Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing","tags":["Review","LVLMs","Self-Improvement","Matthew Effect","Data Bias Mitigation","Distribution Reshaping","Trajectory Resampling","Visual Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang 핵심 연구 목표 본 연구는 대규모 시각언어 모델(LVLM)의 자기 개선 과정에서 발생하는 \"매튜 효과\"를 해결하는 것을 목표로 합"},{"id":"2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis","title":"[논문리뷰] EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis","excerpt":"arXiv에 게시된 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis","tags":["Review","Electronic Health Records","Large Language Models","Reasoning Enhancement","Instruction Tuning","Reinforcement Learning","Data Synthesis","Medical AI","Clinical Decision Support"],"text":"링크: 논문 PDF로 바로 열기 저자: Yusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang, Pengcheng Qiu, Haowen Wang, Yun Yue, Shuai Zhen, Jian Wang, Qianrui Fan, Jinjie Gu, Ya Zhang, Yanfeng Wang, Yu Wang† and Weidi"},{"id":"2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners","title":"[논문리뷰] Emu3.5: Native Multimodal Models are World Learners","excerpt":"arXiv에 게시된 'Emu3.5: Native Multimodal Models are World Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners","tags":["Review","Multimodal Model","World Model","Vision-Language","Next-Token Prediction","Reinforcement Learning","Discrete Diffusion Adaptation","Image Generation","Any-to-Image"],"text":"링크: 논문 PDF로 바로 열기 저자: Emu3.5 Team, BAAI 핵심 연구 목표 본 논문은 비전과 언어에 걸쳐 다음 상태를 예측하는 대규모 멀티모달 월드 모델인 Emu3.5 를 소개합니다. 자연스러운 멀티모달 능력 을 통해 긴 시퀀스 비전언어 생성, X2I(AnytoImage) 생성, 복잡한 텍스트 기반 이미지 생성 및 일반화 가능한 월드 모델링 능"},{"id":"2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation","title":"[논문리뷰] EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation","excerpt":"arXiv에 게시된 'EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation","tags":["Review","Enzyme Design","Protein Engineering","Generative Models","Flow Matching","Substrate-Specific Control","Functional Site Prediction","Biomolecular AI","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Chao Song, Zhiyuan Liu, Qiong Wang, Jianyu Shi, Hui Yu, Han Huang, Liang Wang, Yihang Zhou, Yang Zhang 핵심 연구 목표 컴퓨테이셔널 단백질 엔지니어링에서 기질 특이적 기능성을 가진 효소 백본을 설계하는 핵심 과제를 해결하고자 합니다. 기존"},{"id":"2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control","title":"[논문리뷰] Exploring Conditions for Diffusion models in Robotic Control","excerpt":"arXiv에 게시된 'Exploring Conditions for Diffusion models in Robotic Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control","tags":["Review","Diffusion Models","Robotic Control","Imitation Learning","Task-Adaptive Representations","Visual Prompts","Text-to-Image","Conditioning","Behavior Cloning"],"text":"링크: 논문 PDF로 바로 열기 저자: Heeseong Shin, Byeongho Heo, Dongyoon Han, Seungryong Kim, Taekyung Kim 핵심 연구 목표 본 논문은 사전 훈련된 텍스트투이미지 diffusion 모델 을 로봇 제어에 활용하여 태스크 적응형 시각 표현 을 얻는 것을 목표로 합니다. 특히, 기존의 태스크불가지론적(t"},{"id":"2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution","title":"[논문리뷰] FullPart: Generating each 3D Part at Full Resolution","excerpt":"Chenjian Gao이 arXiv에 게시한 'FullPart: Generating each 3D Part at Full Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution","tags":["Review","3D Part Generation","Full Resolution","Implicit Representation","Explicit Representation","Voxel Grid","Diffusion Models","PartVerse-XL","Center-Corner Encoding"],"text":"링크: 논문 PDF로 바로 열기 저자: Lihe Ding, Shaocong Dong, Yaokun Li, Chenjian Gao, Xiao Chen, Rui Han, Yihao Kuang, Hong Zhang, Bo Huang, Zhanpeng Huang, Zibin Wang, Dan Xu, Tianfan Xue 핵심 연구 목표 기존 파트 기반 3D 생성 "},{"id":"2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture","title":"[논문리뷰] Kimi Linear: An Expressive, Efficient Attention Architecture","excerpt":"arXiv에 게시된 'Kimi Linear: An Expressive, Efficient Attention Architecture' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture","tags":["Review","Linear Attention","Hybrid Architecture","Kimi Delta Attention (KDA)","Gating Mechanism","Long-Context Modeling","Efficient Inference","Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Kimi Team 핵심 연구 목표 표준 트랜스포머의 quadratic 시간 복잡도 와 선형적으로 증가하는 KV 캐시 의 비효율성을 극복하여, 장문 컨텍스트 및 강화 학습(RL) 환경에서 풀 어텐션(Full Attention)과 동등하거나 더 우수한 성능 을 달성하면서도 효율적인 하이브리드 선형 어텐션 아키텍처 를 개발"},{"id":"2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks","title":"[논문리뷰] L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks","excerpt":"Xenophon Evangelopoulos이 arXiv에 게시한 'L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks","tags":["Review","Multimodal LLM","Metal-Organic Frameworks (MOFs)","Materials Discovery","Crystal Representation Learning","Instruction Tuning","Structure-Property Prediction","Knowledge Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiyu Cui, Fang Wu, Haokai Zhao, Minggao Feng, Xenophon Evangelopoulos, Andrew I. Cooper, Yejin Choi 핵심 연구 목표 본 논문은 기존 대규모 언어 모델(LLMs)이 MOF(MetalOrganic Frameworks)와 같은 복잡한 3D 결정질"},{"id":"2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency","title":"[논문리뷰] MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency","excerpt":"David Picard이 arXiv에 게시한 'MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency","tags":["Review","Text-to-Image Generation","Multi-Reward Learning","Flow Matching","User Preference Alignment","Training Efficiency","Compositional Reasoning","Conditional Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Nicolas Dufour, Lucas Degeorge, Arijit Ghosh, Vicky Kalogeiton, David Picard 핵심 연구 목표 기존 텍스트투이미지(T2I) 모델이 대규모 비정제 데이터셋에서 학습되어 사용자 선호도와 잘 맞지 않고, 후처리 방식의 보상 모델(reward model)이 정보 손실"},{"id":"2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets","title":"[논문리뷰] Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets","excerpt":"arXiv에 게시된 'Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets","tags":["Review","Agentic Markets","Multi-Agent Systems","Large Language Models (LLMs)","Simulation Environment","Open-Source Platform","Market Mechanism Design","Behavioral Biases","Manipulation Resistance"],"text":"링크: 논문 PDF로 바로 열기 저자: Gagan Bansal, Wenyue Hua, Zezhou Huang, Adam Fourney, Amanda Swearngin, Will Epperson, Tyler Payne, Jake M. Hofman, Brendan Lucier, Chinmay Singh, Markus Mobius, Akshay Nambi, Ar"},{"id":"2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs","title":"[논문리뷰] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs","excerpt":"arXiv에 게시된 'MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs","tags":["Review","Medical VQA","Large Multimodal Models (LMMs)","Data Synthesis","Generator-Verifier Framework","Rubric-Guided","Reinforcement Learning (RL)","Context-Aware"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoke Huang, Ningsen Wang, Hui Liu, Xianfeng Tang, Yuyin Zhou 핵심 연구 목표 의료 VQA 시스템 훈련에 필요한 대규모, 공개 활용 가능한 고품질 데이터셋의 부족 문제를 해결하는 것입니다. 이 연구는 공개된 생체의학 문헌에서 이미지와 텍스트를 활용하여 고품질의 다중 선"},{"id":"2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation","title":"[논문리뷰] OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation","excerpt":"Bin Wang이 arXiv에 게시한 'OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation","tags":["Review","Document Layout Generation","Large Language Models (LLMs)","Coarse-to-Fine Learning","Dataset Curation","OmniLayout-1M","Document AI","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengrui Kang, Zhuangcheng Gu, Zhiyuan Zhao, Zichen Wen, Bin Wang, Weijia Li, Conghui He 핵심 연구 목표 본 연구는 다양한 문서 레이아웃 데이터의 부족과 복잡한, 긴 시퀀스 시나리오에서 기존 문서 레이아웃 생성 방법론의 한계를 극복하는 것을 목표로 합"},{"id":"2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes","title":"[논문리뷰] OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes","excerpt":"arXiv에 게시된 'OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes","tags":["Review","Panoramic Generation","Panoramic Perception","3D Scene Reconstruction","Graphics-Ready Scenes","Physically Based Rendering (PBR)","Flow Matching Models","Cross-Modal Adapters","Synthetic Dataset (PanoX)"],"text":"링크: 논문 PDF로 바로 열기 저자: Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu 핵심 연구 목표 본 논문은 기존 2D 리프팅(lifting) 방식이 외관 생성에만 치중하고 내재적 속성 인식을 간과하여 현대 그래픽스 파이프라인과의 통합이 어렵다는 "},{"id":"2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree","title":"[논문리뷰] PORTool: Tool-Use LLM Training with Rewarded Tree","excerpt":"arXiv에 게시된 'PORTool: Tool-Use LLM Training with Rewarded Tree' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree","tags":["Review","Tool-Use LLM","Reinforcement Learning (RL)","Policy Optimization","Rewarded Tree","Trajectory Optimization","Agentic System","Dynamic Tool Call"],"text":"링크: 논문 PDF로 바로 열기 저자: Feijie Wu¹, Weiwu Zhu², Yuxiang Zhang², Soumya Chatterjee², Jiarong Zhu², Fan Mo², Rodin Luo², Jing Gao¹ ¹Purdue University, ² Apple 핵심 연구 목표 기존 도구 사용 LLM이 정적 데이터셋에 의존하여 동적이고 실제적"},{"id":"2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model","title":"[논문리뷰] POWSM: A Phonetic Open Whisper-Style Speech Foundation Model","excerpt":"arXiv에 게시된 'POWSM: A Phonetic Open Whisper-Style Speech Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model","tags":["Review","Phonetic Foundation Model","Multitask Learning","Speech Recognition","Phone Recognition","Grapheme-to-Phoneme","Encoder-Decoder","Low-Resource Speech"],"text":"링크: 논문 PDF로 바로 열기 저자: ChinJou Li¹, Kalvin Chang², Shikhar Bharadwaj¹, Eunjung Yeo³, Kwanghee Choi³, Jian Zhu, David Mortensen¹, Shinji Watanabe¹ 핵심 연구 목표 본 논문은 자동 음성 인식(ASR), 음소 인식(PR), 철자음소 변환(G2P), "},{"id":"2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce","title":"[논문리뷰] Performance Trade-offs of Optimizing Small Language Models for E-Commerce","excerpt":"Nikola Tankovic이 arXiv에 게시한 'Performance Trade-offs of Optimizing Small Language Models for E-Commerce' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce","tags":["Review","Small Language Models","E-commerce","Intent Recognition","Fine-tuning","QLoRA","Quantization","GPTQ","GGUF","Hardware-aware Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Josip Tomo Licardo, Nikola Tanković 핵심 연구 목표 본 논문은 대규모 상용 LLM의 높은 비용과 리소스 제약 문제를 해결하기 위해, 소규모 오픈웨이트 모델이 특정 도메인 작업에서 효율적인 대안이 될 수 있는지 검증하는 것을 목표로 합니다. 특히, 다국어 이커머스 의도 인식 태스크에서 10억"},{"id":"2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work","title":"[논문리뷰] Remote Labor Index: Measuring AI Automation of Remote Work","excerpt":"Shivam Singhal이 arXiv에 게시한 'Remote Labor Index: Measuring AI Automation of Remote Work' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work","tags":["Review","AI 자동화","원격 근무","벤치마크","AI 에이전트","프리랜서 경제","인간 평가","자동화율"],"text":"링크: 논문 PDF로 바로 열기 저자: Shivam Singhal, Udari Madhushani Sehwag, Cristina Menghini, Alice Gatti, Mantas Mazeika 핵심 연구 목표 AI 기술의 연구 발전이 실제 경제적 가치와 노동 자동화로 어떻게 연결되는지 불분명하며, AI 자동화의 진척도를 모니터링할 표준화된 경험적 방법이"},{"id":"2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning","title":"[논문리뷰] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning","excerpt":"arXiv에 게시된 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning","tags":["Review","Supervised Reinforcement Learning","LLMs","Multi-step Reasoning","Reward Shaping","Expert Trajectories","Math Reasoning","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yihe Deng, IHung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister, ChenYu Lee 핵심 연구 목표 대규모 언어 모델(LLMs)이 다단계 추론 문제, 특히 정답 궤적이 희박한 어려운 태스크에서"},{"id":"2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents","title":"[논문리뷰] Surfer 2: The Next Generation of Cross-Platform Computer Use Agents","excerpt":"arXiv에 게시된 'Surfer 2: The Next Generation of Cross-Platform Computer Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents","tags":["Review","Computer Use Agent","Cross-Platform","GUI Automation","Vision-Language Model","Hierarchical Architecture","Agent Orchestration","Visual Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: M. Andreux, M. Bakler, Y. Barbier, H. Benchekroun, E. Biré, A. Bonnet, R. Bordie, et al. 핵심 연구 목표 이 논문은 웹, 데스크톱, 모바일 환경 전반에 걸쳐 일반화하는 에이전트를 구축하는 문제를 해결하고자 합니다. 기존 시스템들이 환경별 인터페이스에"},{"id":"2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models","title":"[논문리뷰] The End of Manual Decoding: Towards Truly End-to-End Language Models","excerpt":"arXiv에 게시된 'The End of Manual Decoding: Towards Truly End-to-End Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models","tags":["Review","Large Language Models (LLMs)","End-to-End Generation","Dynamic Decoding","Hyperparameter Optimization","Stochastic Sampling","Instruction Following","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, and Yan Wang 핵심 연구 목표 현재 LLM이 비미분 가능한 디코딩 하이퍼파라미터(온도, topp)의 수동 튜닝에 의존하여 발생하는 비"},{"id":"2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models","title":"[논문리뷰] The Era of Agentic Organization: Learning to Organize with Language Models","excerpt":"Xun Wu이 arXiv에 게시한 'The Era of Agentic Organization: Learning to Organize with Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models","tags":["Review","Agentic Organization","Asynchronous Thinking","Language Models","Reinforcement Learning","Multi-agent Systems","Reasoning","Task Decomposition","Orchestration"],"text":"링크: 논문 PDF로 바로 열기 저자: Zewen Chi, Li Dong, Qingxiu Dong, Yaru Hao, Xun Wu, Shaohan Huang, Furu Wei 핵심 연구 목표 본 논문은 AI가 개별 지능의 한계를 넘어 협력적이고 동시적으로 복잡한 문제를 해결하는 \"에이전트 조직(agentic organization)\" 시대를 목표로 합니다."},{"id":"2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","title":"[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation","excerpt":"arXiv에 게시된 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-31 18:37:31+0900","permalink":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","tags":["Review","Motion Generation","Generalization","Diffusion Models","Transformer","Large-scale Dataset","Benchmark","Multimodal Learning","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu 핵심 연구 목표 본 논문은 3D 인간 모션 생성(M"},{"id":"2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning","title":"[논문리뷰] A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning","excerpt":"arXiv에 게시된 'A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning","tags":["Review","Multi-turn Reinforcement Learning","LLM Agents","Text-based Environments","Reward Shaping","Policy Optimization","Supervised Fine-tuning (SFT)","Generalization","Environment Complexity"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruiyi Wang, Prithviraj Ammanabrolu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)을 에이전트로 훈련하기 위한 다중 턴(multiturn) 강화 학습(RL)의 파편화된 접근 방식을 해결하고, 환경, 보상, 정책 세 가지 핵심 축을 중심으로 실용적인 훈련 레시피 를 도출하는 것을 목표로 "},{"id":"2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents","title":"[논문리뷰] Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents","excerpt":"Jong Chul Ye이 arXiv에 게시한 'Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents","tags":["Review","Consistency Models","Generative Models","Manifold Learning","Tangent Alignment","Diffusion Models","Training Dynamics","Manifold Feature Distance"],"text":"링크: 논문 PDF로 바로 열기 저자: Beomsu Kim, Byunghee Cha, Jong Chul Ye 핵심 연구 목표 본 연구는 Consistency Models (CMs) 의 느린 수렴 문제와 높은 배치 사이즈 요구 사항을 해결하는 것을 목표로 합니다. 특히 CM 훈련 과정에서 발생하는 CM tangents (출력 업데이트 방향) 의 진동성(osc"},{"id":"2025-10-6-Apriel-1-5-15b-Thinker","title":"[논문리뷰] Apriel-1.5-15b-Thinker","excerpt":"arXiv에 게시된 'Apriel-1.5-15b-Thinker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Apriel-1-5-15b-Thinker","tags":["Review","Multimodal Reasoning Model","Open-Weights Model","Continual Pretraining (CPT)","Supervised Fine-Tuning (SFT)","Training Design","Efficiency","Frontier Performance"],"text":"링크: 논문 PDF로 바로 열기 저자: Shruthan Radhakrishna, Aman Tiwari, Aanjaneya Shukla, et al. 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 성능과 접근성 사이의 근본적인 한계를 극복하고, 150억 개 파라미터 의 비교적 작은 오픈웨이트 모델인 Apriel1.515BThinker 가 순수한 규모"},{"id":"2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization","title":"[논문리뷰] CoDA: Agentic Systems for Collaborative Data Visualization","excerpt":"arXiv에 게시된 'CoDA: Agentic Systems for Collaborative Data Visualization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization","tags":["Review","Multi-agent Systems","Data Visualization","LLM","Automation","Self-reflection","Code Generation","Natural Language to Visualization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zichen Chen, Jiefeng Chen, Sercan Ö. Arık, Misha Sra, Tomas Pfister, Jinsung Yoon 핵심 연구 목표 본 논문은 복잡한 데이터셋, 반복적인 개선, 코드 오류 및 최종 시각화 품질 문제로 인해 기존 시스템이 어려움을 겪는 자연어 기반 데이터 시각화 자동화의 한"},{"id":"2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition","title":"[논문리뷰] Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition","excerpt":"arXiv에 게시된 'Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition","tags":["Review","Diffusion Models","Flow-based Models","Robotics Control","Policy Composition","Test-time Optimization","Score-based Models","Training-free"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahang Cao, Yize Huang, Hanzhong Guo, Rui Zhang, Mu Nan, Weijian Mai, Jiaxu Wang, Hao Cheng, Jingkai Sun, Gang Han, Wen Zhao, Qiang Zhang, Yijie Guo, Qihao Zheng, Chunfeng Song,"},{"id":"2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern","title":"[논문리뷰] DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern","excerpt":"Jia Li이 arXiv에 게시한 'DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern","tags":["Review","Diffusion LLMs","Unit Test Generation","Acceleration","Repetitive Patterns","Abstract Syntax Tree","Software Testing","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Lekang Yang, Yuetong Liu, Yitong Zhang, Jia Li 핵심 연구 목표 본 논문은 확산형 대규모 언어 모델(dLLM)을 이용한 단위 테스트 생성(UTG) 과정에서 발생하는 비효율성 문제를 해결하는 것을 목표로 합니다. 특히, dLLM이 단계별로 생성하는 토큰 수를 늘리면 테스트 케이스의 품"},{"id":"2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation","title":"[논문리뷰] Efficient Multi-modal Large Language Models via Progressive Consistency Distillation","excerpt":"arXiv에 게시된 'Efficient Multi-modal Large Language Models via Progressive Consistency Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation","tags":["Review","Multi-modal LLMs","Token Compression","Efficiency","Knowledge Distillation","Progressive Learning","Consistency Distillation","MLLM Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Zichen Wen, Shaobo Wang, Yufa Zhou, Junyuan Zhang, Qintong Zhang, Yifeng Gao, Zhaorun Chen, Bin Wang, Conghui He, Linfeng Zhang, Weijia Li 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLMs)에서 "},{"id":"2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents","title":"[논문리뷰] FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents","excerpt":"Léo Boisvert이 arXiv에 게시한 'FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents","tags":["Review","Web Agents","LLM Context Pruning","Accessibility Tree","Prompt Injection","Retrieval Augmented Generation","Web Navigation","Agent Security","Efficient LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Imene Kerboua, Sahar Omidi Shayegan, Megh Thakkar, Xing Han Lù, Léo Boisvert 핵심 연구 목표 대규모 언어 모델(LLM) 기반 웹 에이전트가 긴 웹 페이지 관찰(수만 개의 토큰)로 인해 발생하는 컨텍스트 한계, 높은 계산 비용, 그리고 프롬프트 주입 공격과 같"},{"id":"2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs","title":"[논문리뷰] Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs","excerpt":"arXiv에 게시된 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs","tags":["Review","Text-to-Image Models","Diffusion Models","Preference Optimization","LLMs","RLHF","Prompt Editing","Free Lunch Alignment","TDPO","TKTO"],"text":"링크: 논문 PDF로 바로 열기 저자: Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao 핵심 연구 목표 본 연구는 확산 기반 TexttoImage (T2I) 모델의 텍스트이미지 정렬(alignment)을 개선하는 것을 목표로 합니다. 특히,"},{"id":"2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty","title":"[논문리뷰] How Confident are Video Models? Empowering Video Models to Express their Uncertainty","excerpt":"Anirudha Majumdar이 arXiv에 게시한 'How Confident are Video Models? Empowering Video Models to Express their Uncertainty' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty","tags":["Review","Video Generation","Uncertainty Quantification","Aleatoric Uncertainty","Epistemic Uncertainty","Model Calibration","Text-to-Video","Generative AI","VMF Distribution"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiting Mei, Ola Shorinwa, Anirudha Majumdar 핵심 연구 목표 비디오 생성 모델이 텍스트 프롬프트에 기반하여 부정확하거나 사실과 다른(hallucinate) 비디오를 생성할 때, 그 예측에 대한 불확실성을 표현하지 못하는 문제를 해결하는 것을 목표로 합니다. 이는 비디오 모델의 안전성"},{"id":"2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping","title":"[논문리뷰] Improving GUI Grounding with Explicit Position-to-Coordinate Mapping","excerpt":"Spandana Gella이 arXiv에 게시한 'Improving GUI Grounding with Explicit Position-to-Coordinate Mapping' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping","tags":["Review","GUI Grounding","Vision-Language Models","Positional Embedding","UI Automation","Coordinate Prediction","Resolution Generalization","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Suyuchen Wang, Tianyu Zhang, Ahmed Masry, Christopher Pal, Spandana Gella, Bang Liu, Perouz Taslakian 핵심 연구 목표 본 논문은 기존 VLM(VisionLanguage Model)의 GUI Grounding(자연어 지시를 픽셀 좌표에 매핑"},{"id":"2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models","title":"[논문리뷰] LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models","excerpt":"Yu-Chiang Frank Wang이 arXiv에 게시한 'LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models","tags":["Review","Multimodal LLM","OOD Adaptation","Label Efficiency","VQA","Semi-Supervised Learning","Neuron Distillation","Pseudo Labeling","Medical Imaging"],"text":"링크: 논문 PDF로 바로 열기 저자: YuChiang Frank Wang, YuYang Sheng, CiSiang Lin, cmhungsteve 핵심 연구 목표 본 논문은 제한된 레이블 데이터와 풍부한 비레이블 이미지를 활용하여 Multimodal Large Language Models (MLLMs) 가 의료 영상이나 기술 콘텐츠와 같은 OutofDist"},{"id":"2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning","title":"[논문리뷰] LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning","excerpt":"arXiv에 게시된 'LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning","tags":["Review","LLM Reasoning","RLVR","Dynamic Sampling","Policy Optimization","Response Length","Meta-RL","Overthinking"],"text":"링크: 논문 PDF로 바로 열기 저자: Weizhe Chen, Sven Koenig, Bistra Dilkina 핵심 연구 목표 대규모 언어 모델(LLM) 추론 태스크에서 RLVR (Reinforcement Learning with Verifiable Rewards) 훈련의 효율성을 넘어, 최종 모델의 효과성(정확도)을 개선하는 것을 목표로 합니다. 특히,"},{"id":"2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving","title":"[논문리뷰] NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving","excerpt":"arXiv에 게시된 'NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving","tags":["Review","Visual Question Answering (VQA)","Autonomous Driving","Risk Assessment","Spatio-Temporal Reasoning","Large Vision Models (VLMs)","Dataset","Bird-Eye-View (BEV)","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuan Gao, Mattia Piccinini, Roberto Brusnicki, Yuchen Zhang, Johannes Betz 핵심 연구 목표 본 논문은 자율주행 시나리오에서 기존 Vision Language Models (VLMs)이 정성적 판단에 그치고 정량적 시공간 추론 능력이 부족하다는 문제를 해결하고자"},{"id":"2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features","title":"[논문리뷰] OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features","excerpt":"Elena Tutubalina이 arXiv에 게시한 'OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features","tags":["Review","Sparse Autoencoders","Mechanistic Interpretability","Feature Disentanglement","Orthogonality","LLM Features","Feature Absorption","Feature Composition"],"text":"링크: 논문 PDF로 바로 열기 저자: Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Y. Rogov, Elena Tutubalina & Ivan Oseledets 핵심 연구 목표 본 논문은 기존 Sparse Autoencoders (SAEs)가 겪는 피쳐 흡수(feature absorption) 및 피쳐"},{"id":"2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration","title":"[논문리뷰] REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration","excerpt":"arXiv에 게시된 'REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration","tags":["Review","Model Editing","Lifelong Learning","LLMs","Continual Learning","Knowledge Distillation","Error Feedback","Memory Management","Parameter Merging"],"text":"링크: 논문 PDF로 바로 열기 저자: Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie & Xuming Ran 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 사후 훈련 과정에서 발생하는 높은 비용, 의도치 않은 부작용, 순차적 편집의 불안정성 및 제한된 일반화 문제"},{"id":"2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces","title":"[논문리뷰] Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces","excerpt":"arXiv에 게시된 'Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces","tags":["Review","Policy Compliance","Large Language Models (LLMs)","Reasoning Traces","In-Context Learning (ICL)","Supervised Finetuning (SFT)","HIPAA","GDPR","ModelSpec"],"text":"링크: 논문 PDF로 바로 열기 저자: Joseph Marvin Imperial, Harish Tayyar Madabushi 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 인간 전문가처럼 정책 준수 여부를 평가하는 데 필요한 체계적인 추론 과정을 모방하는 능력을 향상시키는 것을 목표로 합니다. 특히, 정책 준수 평가를 위한 골드스탠다드 전문가 추론"},{"id":"2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey","title":"[논문리뷰] Self-Improvement in Multimodal Large Language Models: A Survey","excerpt":"Yapeng Tian이 arXiv에 게시한 'Self-Improvement in Multimodal Large Language Models: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey","tags":["Review","Multimodal Large Language Models (MLLMs)","Self-Improvement","Data Collection","Data Organization","Model Optimization","Survey","Reinforcement Learning","Direct Preference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Shijian Deng, Kai Wang, Tianyu Yang, Harsh Singh, Yapeng Tian 핵심 연구 목표 이 논문은 Multimodal Large Language Models (MLLMs)의 자기 개선(selfimprovement) 분야에 대한 최초의 포괄적인 개요를 제공하는 것을 목표로 합니다."},{"id":"2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus","title":"[논문리뷰] SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus","excerpt":"Zhonghao Zhang이 arXiv에 게시한 'SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus","tags":["Review","Medical AI","Spine Diagnosis","Multimodal LLM","Benchmark","Dataset","Clinical Reasoning","Spine Surgery","Vision-Language Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Zhao, Wenhui Dong, Xiang Zheng, Zhonghao Zhang, Zian Zhou, et al. 핵심 연구 목표 본 연구는 전 세계적으로 척추 질환 유병률이 높음에도 불구하고, 레벨 인식 멀티모달 데이터셋 과 표준화된 척추 특정 벤치마크 의 부족으로 AI 기반 진단 발전이 제한되는 문제를"},{"id":"2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys","title":"[논문리뷰] SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?","excerpt":"Shuo Wang이 arXiv에 게시한 'SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys","tags":["Review","LLM","LLM Agents","Academic Survey Generation","Evaluation Framework","Benchmark","Quiz-driven Evaluation","Content Quality Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaojun Sun, Xuzhou Zhu, Xuanhe Zhou, Xin Tong, Shuo Wang, Jie Fu, Guoliang Li, Zhiyuan Liu, Fan Wu 핵심 연구 목표 본 논문은 학술 조사 논문 작성에 대한 대규모 언어 모델(LLM) 및 LLM 에이전트의 역량 을 엄격하게 평가하기 위해 독자"},{"id":"2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling","title":"[논문리뷰] TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling","excerpt":"Juhan Nam이 arXiv에 게시한 'TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling","tags":["Review","Conversational Recommendation","LLM Tool Calling","Music Recommendation","Multimodal Retrieval","Information Retrieval","Retrieval-Reranking","Semantic IDs"],"text":"링크: 논문 PDF로 바로 열기 저자: Seungheon Doh, Keunwoo Choi, Juhan Nam 핵심 연구 목표 본 논문은 기존 대규모 언어 모델(LLM) 기반 추천 시스템의 제한적인 추천 행동과 단일 검색 방법론의 한계를 극복하고자 합니다. 사용자의 복잡한 의도를 해석하고 다양한 데이터 소스를 통합하여 정교한 음악 추천을 제공하는 통합 검색재"},{"id":"2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles","title":"[논문리뷰] Triangle Splatting+: Differentiable Rendering with Opaque Triangles","excerpt":"Matheus Gadelha이 arXiv에 게시한 'Triangle Splatting+: Differentiable Rendering with Opaque Triangles' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles","tags":["Review","Differentiable Rendering","3D Reconstruction","Novel View Synthesis","Triangles","Opaque Primitives","Game Engines","Gaussian Splatting","Mesh-based Rendering"],"text":"링크: 논문 PDF로 바로 열기 저자: Jan Held, Renaud Vandeghen, Sanghyun Son, Daniel Rebain, Matheus Gadelha, Yi Zhou, Ming C. Lin, Marc Van Droogenbroeck, Andrea Tagliasacchi 핵심 연구 목표 기존 Neural Radiance Fields (Ne"},{"id":"2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents","title":"[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents","excerpt":"Neil Zhenqiang Gong이 arXiv에 게시한 'WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents","tags":["Review","Prompt Injection","Web Agents","Multimodal AI","Adversarial Attacks","Detection Benchmarking","Large Language Models","Image-based Detection","Text-based Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinuo Liu, Ruohan Xu, Xilong Wang, Yuqi Jia, Neil Zhenqiang Gong 핵심 연구 목표 이 논문은 웹 에이전트를 대상으로 하는 프롬프트 인젝션 공격에 대한 탐지 방법들을 체계적으로 벤치마킹하여, 웹 에이전트 환경에서의 탐지 성능을 종합적으로 평가하고 이해하는 것을 목표로 합"},{"id":"2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents","title":"[논문리뷰] Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents","excerpt":"Boyi Wei이 arXiv에 게시한 'Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-06 13:29:11+0900","permalink":"/ai/review/2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents","tags":["Review","Self-evolving Agents","LLM Safety","Misevolution","Emergent Risks","Model Evolution","Memory Evolution","Tool Evolution","Workflow Evolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Boyi Wei, Chen Qian, Shuai Shao, JYYoung, jasonrqh 핵심 연구 목표 본 논문은 자율적으로 진화하는 LLM 에이전트에서 발생하는 예기치 않거나 유해한 행동인 \" Misevolution \" 현상을 개념화하고 체계적으로 조사하는 것을 목표로 합니다. 에이전트의 자기 개선 과정이 기존 "},{"id":"2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning","title":"[논문리뷰] AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning","excerpt":"Zeliang Zhang이 arXiv에 게시한 'AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning","tags":["Review","Multi-Agent Reinforcement Learning","Adversarial Co-evolution","LLM Safety","Jailbreak Attacks","Internalized Safety","Public Baseline","System Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenyu Pan¹, Yiting Zhang², Zhuo Liu³, Yolo Yunlong Tang³, Zeliang Zhang³, Haozheng Luo¹, Yuwei Han², Jianshu Zhang¹, Dennis Wu¹, HongYu Chen¹, Haoran Lu¹, Haoyang Fang, Manling "},{"id":"2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models","title":"[논문리뷰] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models","excerpt":"Fenglu Hong이 arXiv에 게시한 'Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models","tags":["Review","LLM Context Adaptation","Agentic AI","Self-Improving Systems","Prompt Engineering","Context Management","Dynamic Playbooks","Incremental Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong 외 다수 핵심 연구 목표 이 논문은 기존 대규모 언어 모델(LLM)의 컨텍스트 적응 방법론이 가지는 '간결성 편향(brevity bias)'과 '컨텍스트 붕괴(context collapse)' "},{"id":"2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails","title":"[논문리뷰] Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails","excerpt":"Xinyuan Liu이 arXiv에 게시한 'Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails","tags":["Review","LLM Agents","Alignment","Self-Evolution","Behavioral Drift","Reinforcement Learning","Multi-Agent Systems","Alignment Tipping Process"],"text":"링크: 논문 PDF로 바로 열기 저자: Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao 핵심 연구 목표 본 논문은 자기 진화(selfevolution) 능력을 가진 LLM 에이전트가 배포"},{"id":"2025-10-7-Character-Mixing-for-Video-Generation","title":"[논문리뷰] Character Mixing for Video Generation","excerpt":"arXiv에 게시된 'Character Mixing for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation","tags":["Review","Video Generation","Character Mixing","Style Preservation","Multi-character Interaction","Text-to-Video","Cross-Domain Synthesis","Identity Preservation"],"text":"링크: 논문 PDF로 바로 열기 저자: Tingting Liao, Chongjian Ge, Guangyi Liu, Hao Li, Yi Zhou 핵심 연구 목표 이 논문은 비디오 생성에서 비공존 캐릭터 간의 자연스러운 상호작용 을 가능하게 하는 것을 목표로 합니다. 특히, 캐릭터의 고유한 정체성, 행동 및 시각적 스타일 을 유지하면서, 이전에 함께 존재한 적"},{"id":"2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","title":"[논문리뷰] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation","excerpt":"arXiv에 게시된 'ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","tags":["Review","Image Editing","Video Generation","Temporal Reasoning","World Simulation","Physical Consistency","Diffusion Models","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling 핵심 연구 "},{"id":"2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform","title":"[논문리뷰] Code4MeV2: a Research-oriented Code-completion Platform","excerpt":"arXiv에 게시된 'Code4MeV2: a Research-oriented Code-completion Platform' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform","tags":["Review","Code Completion","Research Platform","Human-AI Interaction","Software Engineering","Open Science","JetBrains IDE Plugin","Telemetry","AI4SE"],"text":"링크: 논문 PDF로 바로 열기 저자: Roham Koohestani, Parham Bateni, Aydin Ebrahimi, Behdad Etezadi, Kiarash Karimi, Maliheh Izadi 핵심 연구 목표 AI 기반 코드 완성 도구의 사용자 상호작용 데이터가 독점적으로 관리되는 문제를 해결하여, 연구자들이 재현 가능한 대규모 데이터 분석"},{"id":"2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models","title":"[논문리뷰] Epistemic Diversity and Knowledge Collapse in Large Language Models","excerpt":"arXiv에 게시된 'Epistemic Diversity and Knowledge Collapse in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models","tags":["Review","Large Language Models","Epistemic Diversity","Knowledge Collapse","Homogenization","Retrieval-Augmented Generation","LLM Evaluation","Information Diversity","Cultural Bias"],"text":"링크: 논문 PDF로 바로 열기 저자: Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Peter Ebert Christiansen, Maria Antoniak, Chan Young Park, Isabelle Augenstein 핵심 연구 목표 대규모 언어 모델(LLM)이 생성하는 텍스트의 동질성이 로 이"},{"id":"2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty","title":"[논문리뷰] EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty","excerpt":"Xuanwu Wang이 arXiv에 게시한 'EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty","tags":["Review","Automated Theorem Proving","Data Augmentation","Large Language Models","Formal Mathematics","Symmetry","Difficulty Evolution","Abstract Syntax Tree","Generalizability"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchen Tian, Ruiyuan Huang, Xuanwu Wang, Zengfeng Huang, Ziyang Luo, Hongzhan Lin, Da Zheng, Lun Du 핵심 연구 목표 본 논문은 형식적 정리 증명(formal theorem proving) 분야에서 대규모 언어 모델(LLMs) 의 일반화 능력"},{"id":"2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","title":"[논문리뷰] Factuality Matters: When Image Generation and Editing Meet Structured Visuals","excerpt":"Boxiang Qiu이 arXiv에 게시한 'Factuality Matters: When Image Generation and Editing Meet Structured Visuals' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","tags":["Review","Structured Visuals","Image Generation","Image Editing","Multimodal Reasoning","Factual Fidelity","Chain-of-Thought","Evaluation Benchmark","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Le Zhuo, Songhao Han, Yuandong Pu, Boxiang Qiu, Sayak Paul, Yue Liao, Yihao Liu, Jie Shao, Xi Chen, Si Liu, Hongsheng Li 핵심 연구 목표 본 연구는 최신 시각 생성 모델들이 차트, 다이어그램, 수학 도형과 같은 구조화된 시각"},{"id":"2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data","title":"[논문리뷰] Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data","excerpt":"arXiv에 게시된 'Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data","tags":["Review","Large Language Models","Pretraining","Supervised Fine-tuning","Reasoning Data","Data Allocation","Diversity","Quality","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Syeda Nahida Akter, Shrimai Prabhumoye, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Yejin Choi, Bryan Catanzaro 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 능력을 극대화하기 위해 사전 훈련(pretraini"},{"id":"2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where","title":"[논문리뷰] Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?","excerpt":"Denis Peskoff이 arXiv에 게시한 'Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where","tags":["Review","NLP for Social Good","ACL Community","Scientometrics","Venue Analysis","Author Classification","Sustainable Development Goals","Neural Methods","Research Landscape"],"text":"링크: 논문 PDF로 바로 열기 저자: Grace LeFevre, Qingcheng Zeng, Adam Leif, Jason Jewell, Denis Peskoff, Rob Voigt 핵심 연구 목표 본 연구는 연구의 저자 및 게재지별 분포를 분석하여 그 현황을 파악하는 것을 목표로 합니다. 특히, 저자들이 사회적 선행 관련 연구를 게재지에 발표하는 경향이"},{"id":"2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs","title":"[논문리뷰] Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs","excerpt":"Zeyi Liao이 arXiv에 게시한 'Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs","tags":["Review","Agent Evaluation","Task Generation","Knowledge Graphs","Multimodal AI","Web Interaction","Document Comprehension","LLM-driven Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Yurun Chen, Xavier Hu, Yuhan Liu, Ziqi Wang, Zeyi Liao, Lin Chen, Feng Wei, Yuxi Qian, Bo Zheng, Keting Yin, Shengyu Zhang 핵심 연구 목표 본 논문은 정적 데이터셋 기반의 평가가 LLM 기반 에이전트 의 실제 역량(특히 동"},{"id":"2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition","title":"[논문리뷰] HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition","excerpt":"arXiv에 게시된 'HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition","tags":["Review","Code-Switching","Speech Recognition","Korean-English ASR","Evaluation Framework","Multilingual ASR","Loanword Processing","Fine-tuning","Hierarchical Labeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim 핵심 연구 목표 본 연구는 한국어영어 코드 스위칭(CS) 음성 인식(ASR) 분야의 심각한 연구 부족을 해결하고, 다국어 ASR 모델의 정밀한 평가를 위한 최초의 공개적인 계층적 평가 프레임워크인 HiKE "},{"id":"2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights","title":"[논문리뷰] Hybrid Architectures for Language Models: Systematic Analysis and Design Insights","excerpt":"arXiv에 게시된 'Hybrid Architectures for Language Models: Systematic Analysis and Design Insights' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights","tags":["Review","Hybrid LLM","Transformer Architecture","Mamba","State Space Models (SSM)","Computational Efficiency","Long-Context","Language Model Architectures","Scaling Laws"],"text":"링크: 논문 PDF로 바로 열기 저자: Sangmin Bae, Bilge Acun, Haroun Habeeb, Seungyeon Kim, ChienYu Lin, Liang Luo, Junjie Wang, CaroleJean Wu 핵심 연구 목표 기존 대규모 언어 모델(LLM)에서 Transformer 의 quadratic 복잡성과 Mamba 의 장문 컨텍스"},{"id":"2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models","title":"[논문리뷰] Imperceptible Jailbreaking against Large Language Models","excerpt":"arXiv에 게시된 'Imperceptible Jailbreaking against Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models","tags":["Review","Large Language Models","Jailbreaking","Imperceptible Attacks","Unicode Variation Selectors","Adversarial Suffixes","Safety Alignment","Prompt Injection"],"text":"링크: 논문 PDF로 바로 열기 저자: Kuofeng Gao, Yiming Li, Chao Du, Xin Wang, Xingjun Ma, ShuTao Xia, Tianyu Pang 핵심 연구 목표 본 논문은 기존의 가시적인 텍스트 수정 방식과 달리 눈에 보이지 않는(imperceptible) 방식으로 LLM의 안전 장치를 우회하는 새로운 공격 기법을 제안합"},{"id":"2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions","title":"[논문리뷰] Judging with Confidence: Calibrating Autoraters to Preference Distributions","excerpt":"arXiv에 게시된 'Judging with Confidence: Calibrating Autoraters to Preference Distributions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions","tags":["Review","Large Language Models","Autoraters","Calibration","Preference Distributions","Reinforcement Learning","Supervised Fine-tuning","Positional Bias"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuohang Li, Xiaowei Li, Chengyu Huang, Guowang Li, Katayoon Goshvadi, Bo Dai, Dale Schuurmans, Paul Zhou, Hamid Palangi, Yiwen Song, Palash Goyal, Murat Kantarcioglu, Bradley A."},{"id":"2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL","title":"[논문리뷰] LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL","excerpt":"arXiv에 게시된 'LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL","tags":["Review","Text-to-SQL","WikiSQL","LLM","Dataset Curation","Natural Language Processing","Benchmark","SQL Generation","Data Cleaning"],"text":"링크: 논문 PDF로 바로 열기 저자: Dzmitry Pihulski, Karol Charchut, Viktoria Novogrodskaia, Jan Kocoń 핵심 연구 목표 본 논문은 기존 WikiSQL 데이터셋이 가진 데이터 타입 불일치, 대소문자 일관성 부족, 구문 오류, 답변 불가 질문 등의 구조적, 주석 관련 문제점을 해결하고자 합니다. 이를 통"},{"id":"2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning","title":"[논문리뷰] Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning","excerpt":"arXiv에 게시된 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning","tags":["Review","Test-Time Curriculum","Reinforcement Learning","Large Language Models","Self-Curated Learning","Continual Learning","Reasoning Benchmarks","Adaptive Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Jonas Hübotter, Leander DiazBone, Ido Hakimi, Andreas Krause, Moritz Hardt 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)이 테스트 시점에 표적 작업을 해결하는 추론 능력을 지속적으로 향상 시키는 방법을 제안합니다. 특히, 시간 소모적인 인간의 데이터셋 "},{"id":"2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information","title":"[논문리뷰] MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information","excerpt":"arXiv에 게시된 'MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information","tags":["Review","LLM Reasoning","Tree Search","Pointwise Mutual Information (PMI)","Dynamic Sampling","Beam Search","Weighted Voting","Information Theory","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaxi Li, Yucheng Shi, Jin Lu, Ninghao Liu 핵심 연구 목표 대규모 언어 모델(LLM)의 다단계 추론 과정에서 중간 단계의 품질을 효율적이고 신뢰성 있게 평가하고, 계산 비용이 높은 경로 탐색 문제를 해결하고자 합니다. 특히, 기존 방법론이 일반적인 추론 경로를 선호하는 한계를 극복하고"},{"id":"2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition","title":"[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition","excerpt":"arXiv에 게시된 'MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition","tags":["Review","Audio-Visual Speech Recognition","Mixture of Experts","Matryoshka Representation Learning","Large Language Models","Elastic Inference","Token Compression","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Umberto Cappellazzo, Minsu Kim, Pingchuan Ma, Honglie Chen, Xubo Liu, Stavros Petridis, Maja Pantic 핵심 연구 목표 논문은 대규모 언어 모델(LLMs) 기반 오디오비주얼 음성 인식(AVSR) 시스템이 겪는 높은 계산 수요와 고정된 토큰 압축"},{"id":"2025-10-7-Optimal-Scaling-Needs-Optimal-Norm","title":"[논문리뷰] Optimal Scaling Needs Optimal Norm","excerpt":"Stefan Kesselheim이 arXiv에 게시한 'Optimal Scaling Needs Optimal Norm' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm","tags":["Review","Optimal Scaling","Norm-Based Optimizers","Hyperparameter Transfer","Learning Rate Scaling","Batch Size Scaling","Transformer Models","Scion Optimizer","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Oleg Filatov, Jiangtao Wang, Jan Ebert, Stefan Kesselheim 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM) 훈련에서 최적의 스케일링 을 달성하기 위한 하이퍼파라미터 전이(transfer)의 견고성 부족 문제를 해결하는 것을 목표로 합니다. 특히, 옵티마이저가 명시적으로"},{"id":"2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models","title":"[논문리뷰] Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models","excerpt":"arXiv에 게시된 'Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models","tags":["Review","Reactive Transformer","Stateful LLM","Event-Driven AI","Asynchronous Memory","Conversational AI","Linear Scaling","Short-Term Memory (STM)","Memory Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Adam Filipek 핵심 연구 목표 이 논문은 기존 Large Language Model (LLM) 의 stateless 특성과 quadratic한 계산 복잡성(O(L²)) 이 긴 대화에서 발생하는 비효율성(높은 비용, 지연 시간)을 해결하는 것을 목표로 합니다. 궁극적으로 실시간(realtime), statefu"},{"id":"2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training","title":"[논문리뷰] Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training","excerpt":"arXiv에 게시된 'Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training","tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Adaptive Sampling","Policy Gradient","Reward Optimization","Signal Collapse","Variance Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Xiong, Xinxing Xu, Chenlu Ye, Christof Monz, Baohao Liao, Hanze Dong, Jiang Bian, Nan Jiang, Tong Zhang 핵심 연구 목표 LLM의 추론 태스크를 위한 강화 학습(RL) 훈련에서 고정 및 균일한 응답 샘플링 으로 인해 발생하는 불안정"},{"id":"2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder","title":"[논문리뷰] SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder","excerpt":"Or Patashnik이 arXiv에 게시한 'SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder","tags":["Review","Image Editing","Diffusion Models","Sparse Autoencoder (SAE)","Text-to-Image","Disentangled Control","Continuous Control","Token-level Manipulation","Text Embeddings"],"text":"링크: 논문 PDF로 바로 열기 저자: Ronen Kamenetsky, Roni Paiss, Sara Dorfman, Daniel Garibi, Daniel CohenOr, Or Patashnik 핵심 연구 목표 이 논문은 대규모 텍스트투이미지 확산 모델의 이미지 편집 시 미세하고 연속적인 제어 부족 문제를 해결하는 것을 목표로 합니다. 특히, 하나의 속성"},{"id":"2025-10-7-Self-Reflective-Generation-at-Test-Time","title":"[논문리뷰] Self-Reflective Generation at Test Time","excerpt":"Shuang Qiu이 arXiv에 게시한 'Self-Reflective Generation at Test Time' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time","tags":["Review","Large Language Models","Self-Reflection","Test-Time Optimization","Uncertainty Monitoring","Proactive Error Prevention","Reasoning Tasks","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Mu, Qixin Zhang, Zhiyong Wang, Menglin Yang, Shuang Qiu, Chengwei Qin, Zhongxiang Dai, Yao Shu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 자동회귀(autoregressive) 생성 과정에서 발생하는 초기 토큰 오류가 전체 "},{"id":"2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs","title":"[논문리뷰] SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs","excerpt":"arXiv에 게시된 'SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs","tags":["Review","LLM Reasoning","Latent Thinking","Explicit Thinking","Training-Free","Token Efficiency","Accuracy Improvement","Dynamic Switching","Entropy-based Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Dachuan Shi¹, Abedelkadir Asi², Keying Li², Xiangchi Yuan¹, Leyan Pan¹, Wenke Lee¹†, Wen Xiao²† 핵심 연구 목표 본 연구는 훈련 없이 잠재 공간 추론을 사용하는 대규모 언어 모델(LLMs)이 겪는 두 가지 주요 문제점을 해결하고자 합니다. 첫째"},{"id":"2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents","title":"[논문리뷰] Thai Semantic End-of-Turn Detection for Real-Time Voice Agents","excerpt":"Monthol Charattrakool이 arXiv에 게시한 'Thai Semantic End-of-Turn Detection for Real-Time Voice Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents","tags":["Review","End-of-Turn Detection","Thai NLP","Voice Agents","Real-time Inference","Transformer Models","Few-shot Learning","Fine-tuning","Latency Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Thanapol Popit, Natthapath Rungseesiripak, Monthol Charattrakool, Saksorn Ruangtanusak 핵심 연구 목표 이 논문은 실시간 음성 에이전트를 위한 태국어 텍스트 전용 EOT(EndofTurn) 감지 에 대한 최초의 체계적인 연구를 수행하는 것을 목표로 합"},{"id":"2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents","title":"[논문리뷰] Utility-Learning Tension in Self-Modifying Agents","excerpt":"Peter Jin이 arXiv에 게시한 'Utility-Learning Tension in Self-Modifying Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents","tags":["Review","Self-Modifying Agents","PAC Learnability","VC Dimension","Capacity Bounds","Metacognition","Architectural Search","Algorithmic Stability","Generalization Theory"],"text":"링크: 논문 PDF로 바로 열기 저자: Peter Jin, Keir Dorchen, Charles L. Wang 핵심 연구 목표 본 연구는 고도화된 AI 에이전트가 학습 메커니즘 자체를 변경하는 자기 수정(selfmodification) 능력에 주목하여, 이러한 변화가 학습 가능성을 보존하는지 혹은 파괴하는지에 대한 학습 이론적 설명을 제공하는 것을 목표로"},{"id":"2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","title":"[논문리뷰] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation","excerpt":"Paul Debevec이 arXiv에 게시한 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","tags":["Review","Video Generation","Chain-of-Thought","Multimodal Models","Reasoning","Inference-Time Tuning","Sparse Supervision","Diffusion Models","Keyframe Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, Ziwei Liu 핵심 연구 목표 기존 비디오 생성 모델들이 복잡한 다이내믹스와 인과적으로 일관된 결과를 생성하는 데 어려움을 겪는 문제를 해결하는 것을 목표로 합니다. 특히, 시각적 상태 전이와 시간 경과에 따"},{"id":"2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models","title":"[논문리뷰] Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models","excerpt":"zeliang0426이 arXiv에 게시한 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models","tags":["Review","Video Reasoning","Large Multimodal Models (LMMs)","Post-training","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)","Test-Time Scaling (TTS)","Chain-of-Thought (CoT)"],"text":"링크: 논문 PDF로 바로 열기 저자: Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu"},{"id":"2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos","title":"[논문리뷰] Watch and Learn: Learning to Use Computers from Online Videos","excerpt":"Oriana Riva이 arXiv에 게시한 'Watch and Learn: Learning to Use Computers from Online Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-07 13:36:57+0900","permalink":"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos","tags":["Review","Computer Use Agents","Inverse Dynamics Model","UI Trajectories","Web Videos","In-Context Learning","Supervised Fine-Tuning","Large Language Models","OSWorld Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Oriana Riva, Yu Su, Palash Goyal, Yiwen Song, Chan Hee Song 핵심 연구 목표 컴퓨터 사용 에이전트(CUA)가 다양한 애플리케이션에서 복잡한 작업을 수행할 수 있도록 지원하는 것을 목표로 합니다. 특히, 대규모의 고품질 훈련 데이터 부족 문제를 해결하기 위해, 온라인에서 쉽"},{"id":"2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling","title":"[논문리뷰] A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling","excerpt":"sirano1004이 arXiv에 게시한 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling","tags":["Review","Reward Model","Best-of-N Sampling","Preference Alignment","Contextual Acceptability","Discrete Choice Model","Alignment Guardrail","Inference Accelerator"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyung Gyu Rho 핵심 연구 목표 현재 선호도 정렬 기법인 BestofN (BoN) 샘플링 이 단순히 \"더 나은\" 응답을 선택할 뿐, \"충분히 좋은\" 응답의 절대적 허용 가능성을 판단하지 못하는 문제를 해결하고자 합니다. 이는 특히 어려운 프롬프트에서 신뢰성 저하 와 오류 허용 증가 로 이어지므로, 컨텍스트 내"},{"id":"2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems","title":"[논문리뷰] AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems","excerpt":"Jose Dolz이 arXiv에 게시한 'AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems","tags":["Review","LLM","Scientific Problem Solving","AI Research","Iterative Refinement","Autonomous Agents","Generative AI","Evaluation Framework","Problem Extraction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jose Dolz, Laurent Charlin, Marco Pedersoli, Gaurav Sahu, Shambhavi Mishra 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 사전 학습된 매개변수 지식 만을 사용하여 AI 연구 문제를 자율적으로 해결할 수 있는지 평가하는 것을 목표로 합니다. 이는 LLM"},{"id":"2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization","title":"[논문리뷰] ASPO: Asymmetric Importance Sampling Policy Optimization","excerpt":"Xiu Li이 arXiv에 게시한 'ASPO: Asymmetric Importance Sampling Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization","tags":["Review","Reinforcement Learning","Large Language Models","Importance Sampling","Policy Optimization","PPO-Clip","Outcome-Supervised RL","Token Weighting","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai 핵심 연구 목표 본 논문은 Large Language Model (LLM) 의 OutcomeSupervised Reinforcement Learning (OS"},{"id":"2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions","title":"[논문리뷰] BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions","excerpt":"Shipei Lin이 arXiv에 게시한 'BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions","tags":["Review","Text-to-SQL","LLM Evaluation","Multi-turn Interaction","Dynamic Environment","User Simulator","Ambiguity Resolution","LLM Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Nan Huo, Xiaohan Xu, Jinyang Li, Per Jacobsson, Shipei Lin 핵심 연구 목표 대규모 언어 모델(LLM)이 단일 턴 TexttoSQL 작업에서는 뛰어난 성능을 보이지만, 실제 데이터베이스 애플리케이션에 필요한 다중 턴 상호작용 능력 의 부족 문제를 해결하는 것을 목표로 합니다"},{"id":"2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks","title":"[논문리뷰] Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks","excerpt":"Pedro Bizarro이 arXiv에 게시한 'Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks","tags":["Review","Scatterplot Analysis","AI Benchmarking","Multimodal LLMs","Synthetic Data Generation","Cluster Detection","Outlier Detection","Data Visualization","Prompt Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: João Palmeiro, Diogo Duarte, Rita Costa, Pedro Bizarro 핵심 연구 목표 본 연구는 기존 벤치마크들이 산점도(scatterplot) 관련 태스크를 충분히 다루지 못하여 AI 모델의 성능을 평가하는 데 한계가 있다는 문제점을 해결하고자 합니다. 특히, AI 모델이 차트 이미지를 "},{"id":"2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation","title":"[논문리뷰] CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation","excerpt":"arXiv에 게시된 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation","tags":["Review","Emotional Support Conversation","Cognitive Reasoning","Reinforcement Learning","Dialogue Generation","Natural Language Processing","Large Language Models","Psychological Support"],"text":"링크: 논문 PDF로 바로 열기 저자: Jie Zhu¹,², Yuanchen Zhou², Shuo Jiang², Junhui Li¹, Lifan Guo², Feng Chen², Chi Zhang², Fang Kong¹ 핵심 연구 목표 감성 지원 대화(ESC) 시스템에서 기존 모델들이 간과했던 심층적인 인지 추론 과정을 강화하여, 대규모 합성 데이터 없이도 "},{"id":"2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding","title":"[논문리뷰] CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding","excerpt":"arXiv에 게시된 'CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding","tags":["Review","Multimodal Large Language Models (MLLMs)","Radiology Report Generation (RRG)","Medical Hallucinations","Contrastive Decoding","Training-free Inference","Clinical AI","Visual Question Answering (VQA)"],"text":"링크: 논문 PDF로 바로 열기 저자: Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho 핵심 연구 목표 본 연구는 방사선학 MLLM 에서 시각적 입력과 불일치하는 의료 환각(medical hallucinations) 문제를 해결하는 것을 목표로 합니다. 특히 임상 섹션에 대한 과도한 민감성 으로 인해 발생하는 프"},{"id":"2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation","title":"[논문리뷰] CoDA: Coding LM via Diffusion Adaptation","excerpt":"arXiv에 게시된 'CoDA: Coding LM via Diffusion Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation","tags":["Review","Diffusion Language Models","Code Generation","Bidirectional Decoding","Text Infilling","Instruction Tuning","Lightweight Models","TPU Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Haolin Chen, Shiyu Wang, Can Qin, Bo Pang, Zuxin Liu, Jielin Qiu, Jianguo Zhang, Yingbo Zhou, Zeyuan Chen, Ran Xu, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang, Wei"},{"id":"2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","title":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning","excerpt":"Zheli Liu이 arXiv에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","tags":["Review","Preference Learning","LLMs","User Feedback","Dissatisfaction Signals","DPO","Iterative Training","RLHF","Exploration"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng 핵심 연구 목표 대규모 언어 모델(LLM) 배포 환경에서 희소한 명시적 만족(SAT) 피드백 대신, 풍부하게 발생하는 암묵적인 사용자 불만족(D"},{"id":"2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","title":"[논문리뷰] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation","excerpt":"Chengzu Li이 arXiv에 게시한 'Deforming Videos to Masks: Flow Matching for Referring Video Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","tags":["Review","Referring Video Object Segmentation","Flow Matching","Video Segmentation","Generative Models","Text-to-Video","Continuous Flow","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zanyi Wang, Dengyang Jiang, Liuzhuozheng Li, Sizhe Dang, Chengzu Li, Harry Yang, Guang Dai, Mengmeng Wang, Jingdong Wang 핵심 연구 목표 기존 Referring Video Object Segmentation (RVOS) 패러"},{"id":"2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics","title":"[논문리뷰] Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics","excerpt":"arXiv에 게시된 'Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics","tags":["Review","Deep Search","Multi-hop Reasoning","Evaluation Benchmark","Retrieval-Augmented Generation","Web Agents","Diagnostic Metrics","Knowledge Utilization","Hint-Free Questions"],"text":"링크: 논문 PDF로 바로 열기 저자: Maojia Song, Renhang Liu, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Soujanya Poria, Jingren Zhou 핵심 연구 목표 논문은 멀티홉 딥 서치 태스크에서 RAG 시스템 및 웹 에이전트 평가의 기존 한계를 해결하고자 합니다. 특히, 기존 벤"},{"id":"2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation","title":"[논문리뷰] Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation","excerpt":"arXiv에 게시된 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation","tags":["Review","Discrete Diffusion Models","Multimodal Large Language Models (MLLMs)","Medical Image Generation","Medical Report Generation","Multimodal Generation","Medical AI","Cross-modal Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawei Mao, Yuhan Wang, Lifeng Chen, Can Zhao, Yucheng Tang, Dong Yang, Liangqiong Qu, Daguang Xu, Yuyin Zhou 핵심 연구 목표 본 논문은 기존 의료 AI 모델의 모달리티별 단편화 문제를 해결하고, 의료 이미지(방사선, 병리학)와 임상"},{"id":"2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models","title":"[논문리뷰] Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models","excerpt":"Jacobo Azcona이 arXiv에 게시한 'Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models","tags":["Review","LLM Hallucinations","Mechanistic Interpretability","Distributional Semantics Tracing (DST)","Dual-Process Theory","Semantic Drift","Commitment Layer","Faithfulness Score"],"text":"링크: 논문 PDF로 바로 열기 저자: Gagan Bhatia, Somayajulu G Sripada, Kevin Allan, Jacobo Azcona 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 환각 현상이 발생하는 내재적이고 아키텍처적 원인 을 규명하는 것을 목표로 합니다. 환각을 의미론적 드리프트(semantic drift) 의 한 형태로 "},{"id":"2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching","title":"[논문리뷰] Drax: Speech Recognition with Discrete Flow Matching","excerpt":"arXiv에 게시된 'Drax: Speech Recognition with Discrete Flow Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching","tags":["Review","Automatic Speech Recognition (ASR)","Discrete Flow Matching (DFM)","Non-Autoregressive (NAR)","Generative Models","Tri-mixture Probability Path","Parallel Decoding","Accuracy-Efficiency Trade-off","Speech Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Aviv Navon, Aviv Shamsian, Neta Glazer, Yael SegalFeldman, Gill Hetz, Joseph Keshet, Ethan Fetaya 핵심 연구 목표 자동 음성 인식(ASR) 분야에서 순차적 디코딩 방식의 자기회귀(AR) 모델 이 가진 효율성 병목 현상과 높은 지연 시간을 해결"},{"id":"2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark","title":"[논문리뷰] EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark","excerpt":"Tianwen Qian이 arXiv에 게시한 'EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark","tags":["Review","Egocentric Vision","Nighttime Conditions","Visual Question Answering (VQA)","Day-Night Alignment","Multimodal Large Language Models (MLLMs)","Depth Estimation","Correspondence Retrieval","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, YuGang Jiang, Luc Van Gool, Danda Pani Paudel 핵심 연구 목표 대부분의 기존"},{"id":"2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models","title":"[논문리뷰] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models","excerpt":"arXiv에 게시된 'Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models","tags":["Review","Generative Models","Equilibrium Dynamics","Energy-Based Models (EBMs)","Flow Matching","Diffusion Models","Optimization-Based Sampling","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Runqian Wang, Yilun Du 핵심 연구 목표 기존 확산(Diffusion) 및 플로우(Flow) 기반 생성 모델의 비평형, 시간조건부 동역학 의 한계를 극복하고, 단일 시간 불변 평형 기울기 를 학습하는 새로운 생성 모델링 프레임워크인 Equilibrium Matching (EqM) 을 제안하는 것이 목표"},{"id":"2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM","title":"[논문리뷰] Fast-dLLM v2: Efficient Block-Diffusion LLM","excerpt":"arXiv에 게시된 'Fast-dLLM v2: Efficient Block-Diffusion LLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM","tags":["Review","Diffusion LLMs","Inference Acceleration","Parallel Decoding","Autoregressive Models","Caching","Fine-tuning","Block-wise Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, Enze Xie 핵심 연구 목표 본 논문은 Autoregressive (AR) 대규모 언어 모델(LLMs) 의 본질적인"},{"id":"2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs","title":"[논문리뷰] Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs","excerpt":"arXiv에 게시된 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs","tags":["Review","DeepResearch Agents","Tool-integrated Reasoning","Reinforcement Learning","Information Retrieval","Information Synthesis","Multi-agent Self-play","Reward Shaping","LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Shreyas Singh, Pradeep Moturi, Kunal Singh 핵심 연구 목표 본 연구는 복잡하고 개방형의 장기적 정보 검색 및 합성 태스크에서 기존 오픈소스 DeepResearch 에이전트의 성능 한계를 극복하는 것을 목표로 합니다. 특히, 다단계 도구 사용에서의 훈련 불안정성 , 비효율적인 도구 호출"},{"id":"2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation","title":"[논문리뷰] HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation","excerpt":"Radu State이 arXiv에 게시한 'HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation","tags":["Review","Hallucination Detection","Retrieval-Augmented Generation (RAG)","Small Reasoning Model (SRM)","Preference Fine-tuning","ORPO","Evidence Grounding","Fact-checking"],"text":"링크: 논문 PDF로 바로 열기 저자: Loris Bergeron, Ioana Buhnila, Jérôme François, Radu State 핵심 연구 목표 대규모 언어 모델(LLM)과 소형 언어 모델(SLM)이 RAG 애플리케이션에서 흔히 겪는 환각(Hallucination) 문제를 해결하고, 사용자 신뢰도와 설명 가능성을 저해하는 문제를 완화하는 것"},{"id":"2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video","title":"[논문리뷰] HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video","excerpt":"Katelyn Gao이 arXiv에 게시한 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video","tags":["Review","3D Reconstruction","Digital Twin","Scene Graph","Physical Simulation","Interactive Environments","Single Video Reconstruction","Neural Rendering"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongchi Xia, ChihHao Lin, HaoYu Hsu, Quentin Leboutet, Katelyn Gao, Michael Paulitsch, Benjamin Ummenhofer, Shenlong Wang 핵심 연구 목표 기존 3D 재구성 방법론의 한계(불완전한 기하학, 낮은 상호작용성, 물리적 비현실성 "},{"id":"2025-10-8-Human3R-Everyone-Everywhere-All-at-Once","title":"[논문리뷰] Human3R: Everyone Everywhere All at Once","excerpt":"Yuliang Xiu이 arXiv에 게시한 'Human3R: Everyone Everywhere All at Once' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Human3R-Everyone-Everywhere-All-at-Once","tags":["Review","4D Human-Scene Reconstruction","Online Reconstruction","Multi-person","SMPL-X","Transformer","Visual Prompt Tuning","Real-time","Foundation Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, Gerard PonsMoll 핵심 연구 목표 본 논문은 캐주얼하게 촬영된 모노큘러 비디오로부터 세계 좌표계 상의 온라인 4D 인간장면 재구성 을 위한 통합적이고 피드포워드 방식의 프레임워크인 Human3R을 제안"},{"id":"2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use","title":"[논문리뷰] In-the-Flow Agentic System Optimization for Effective Planning and Tool Use","excerpt":"arXiv에 게시된 'In-the-Flow Agentic System Optimization for Effective Planning and Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use","tags":["Review","Agentic Systems","Large Language Models (LLMs)","Tool Use","Reinforcement Learning (RL)","On-policy Optimization","Flow-based Group Refined Policy Optimization (Flow-GRPO)","Multi-turn Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, Pan Lu 핵심 연구 목표 이 논문은 기존의 도구 증강 LLM 접근 방식이 긴 추론 과정과 다양한 도구 사용에서 확장성이 떨어지고 새로운 시"},{"id":"2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks","title":"[논문리뷰] Less is More: Recursive Reasoning with Tiny Networks","excerpt":"arXiv에 게시된 'Less is More: Recursive Reasoning with Tiny Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks","tags":["Review","Recursive Reasoning","Tiny Networks","Deep Supervision","Hierarchical Reasoning Model (HRM)","Sudoku-Extreme","ARC-AGI","Generalization","Parameter Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexia JolicoeurMartineau 핵심 연구 목표 이 논문은 기존의 Hierarchical Reasoning Model (HRM) 이 복잡하고 비효율적이라는 문제점을 해결하기 위해, 더욱 단순하면서도 효율적인 Tiny Recursive Model (TRM) 을 제안합니다. 특히, 적은 파라미터와 제한된 데"},{"id":"2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","title":"[논문리뷰] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation","excerpt":"Zheng Zhan이 arXiv에 게시한 'LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","tags":["Review","Video Generation","Diffusion Models","Memory Efficiency","Inference Acceleration","Training-Free","Cache Mechanism","GPU Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Xiao, Gen Li, Kaiyuan Deng, Yushu Wu, Zheng Zhan, Bo Hui, Yanzhi Wang, Xiaolong Ma 핵심 연구 목표 본 논문은 확산 모델 기반 비디오 생성 과정에서 발생하는 높은 GPU 메모리 사용량 과 긴 추론 시간 문제를 해결하고자 합니다. 특히 기존 캐싱 "},{"id":"2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization","title":"[논문리뷰] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization","excerpt":"sirano1004이 arXiv에 게시한 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization","tags":["Review","Direct Preference Optimization","Preference Alignment","Adaptive Regularization","Reward Model","Large Language Models","Sentiment Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyung Gyu Rho 핵심 연구 목표 본 논문은 고정된 온도(β) 파라미터 에 의존하여 다양한 선호도 데이터에서 과적합이나 학습 부족을 야기하는 기존 DPO(Direct Preference Optimization) 의 한계를 해결하는 것을 목표로 합니다. IPO나 βDPO와 같은 기존 적응형 방법론의 단점(예: 균"},{"id":"2025-10-8-MixReasoning-Switching-Modes-to-Think","title":"[논문리뷰] MixReasoning: Switching Modes to Think","excerpt":"arXiv에 게시된 'MixReasoning: Switching Modes to Think' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think","tags":["Review","LLM Reasoning","Chain-of-Thought","Efficiency","LoRA","Adaptive Reasoning","Token Uncertainty","Dynamic Switching","Reasoning Compression"],"text":"링크: 논문 PDF로 바로 열기 저자: Haiquan Lu, Gongfan Fang, Xinyin Ma, Qi Li, Xinchao Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 ChainofThought (CoT) 추론 과정에서 발생하는 비효율성과 과도한 중복성 을 해결하는 것을 목표로 합니다. 특히, 모든 추론 단계에 일률적으로 상세한"},{"id":"2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context","title":"[논문리뷰] Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context","excerpt":"arXiv에 게시된 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context","tags":["Review","Language Models","In-Context Learning","Entity Binding","Mechanistic Interpretability","Causal Abstraction","Long-Context Reasoning","Positional Encoding","Information Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Yoav GurArieh, Mor Geva, Atticus Geiger 핵심 연구 목표 기존 연구에서 언어 모델(LM)이 인컨텍스트(incontext) 엔티티 바인딩(entity binding)을 주로 위치 메커니즘 으로 수행한다고 보았으나, 엔티티 수가 증가하는 복잡한 시나리오에서는 이 메커니즘이 중간 위치에서 불안"},{"id":"2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models","title":"[논문리뷰] No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models","excerpt":"Xiao Xiao Sun이 arXiv에 게시한 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models","tags":["Review","Biomedical Vision-Language Models","Long-context Modeling","Contrastive Learning","Token Efficiency","Zero-shot Classification","Medical Image Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Min Woo Sun, Alejandro Lozano, Javier Gamazo Tejero, Vishwesh Nath, Xiao Xiao Sun, James Burgess, Yuhui Zhang, Kun Yuan, Robert Tibshirani, Sean Huver, Serena YeungLevy 핵심 연구 목표 "},{"id":"2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows","title":"[논문리뷰] OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows","excerpt":"arXiv에 게시된 'OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows","tags":["Review","Non-Autoregressive","Multimodal Generation","Edit Flows","Flow Matching","Interleaved Generation","Text-to-Image Synthesis","Unified Models"],"text":"링크: 논문 PDF로 바로 열기 저자: John Nguyen, Marton Havasi, Tariq Berrada, Luke Zettlemoyer, Ricky T. Q. Chen 핵심 연구 목표 이 논문은 오토회귀(AR) 모델 의 엄격한 순차적 생성과 확산(Diffusion) 모델 의 고정 길이 생성이라는 근본적인 한계를 극복하는 것을 목표로 합니다. 변동"},{"id":"2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations","title":"[논문리뷰] Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations","excerpt":"arXiv에 게시된 'Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations","tags":["Review","Self-Improvement Agent","Academic Presentation","Aesthetic Evaluation","Reinforcement Learning","Multi-task Learning","Presentation Generation","LLM-based Agents","Human Feedback"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengzhi Liu, Yuzhe Yang, Kaiwen Zhou, Zhen Zhang, Yue Fan, Yanan Xie, Peng Qi, Xin Eric Wang 핵심 연구 목표 이 논문은 기존 자동화된 학술 발표 자료 생성 방법론이 가진 제한된 스토리텔링, 낮은 미적 품질, 그리고 자체 조정 능력 부족 문제를 "},{"id":"2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning","title":"[논문리뷰] Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?","excerpt":"arXiv에 게시된 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning","tags":["Review","Safety Alignment","Large Reasoning Models","Mechanistic Interpretability","Refusal Cliff","Attention Heads","Data Selection","Linear Probing"],"text":"링크: 논문 PDF로 바로 열기 저자: Qingyu Yin, Chak Tou Leong, Wenxuan Huang, Wenjie Li, Linyi Yang, Xiting Wang, Jaehong Yoon, YunXing, Xing Yu, Jinjin Gu 핵심 연구 목표 본 논문은 대규모 추론 모델(LRMs)에서 안전 정렬(safety alignment) "},{"id":"2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions","title":"[논문리뷰] Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions","excerpt":"arXiv에 게시된 'Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions","tags":["Review","Speech Emotion Recognition","Annotator Subjectivity","Emotion Ambiguity","Soft Labels","Multi-label Classification","Evaluation Metrics","Loss Functions"],"text":"링크: 논문 PDF로 바로 열기 저자: 周惶振 (HuangCheng Chou) 핵심 연구 목표 본 논문은 기존 음성 감정 인식(SER) 연구의 한계를 극복하고, 실제 환경에 더 적합한 SER 시스템을 구축하는 것을 목표로 합니다. 특히 어노테이터의 주관성 과 감정의 모호성 이라는 두 가지 주요 요소를 재고하여, 소수 감정 레이블의 제거 여부, 소수 어노테이"},{"id":"2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning","title":"[논문리뷰] Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning","excerpt":"Zhuoshi Pan이 arXiv에 게시한 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning","tags":["Review","Code-Assisted Reasoning","Chain-of-Thought (CoT)","Instruction Tuning","Data Augmentation","LLMs","Mathematical Reasoning","Self-Verification","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Honglin Lin, Qizhi Pei, Xin Gao, Zhuoshi Pan, Yu Li, Juntao Li, Conghui He, Lijun Wu 핵심 연구 목표 본 논문은 LLM의 추론 능력 향상을 위해 기존 자연어 기반 CoT(ChainofThought) 방식의 검증 불가능성, 확장성 한계, 다양성 부족 문제"},{"id":"2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos","title":"[논문리뷰] ShapeGen4D: Towards High Quality 4D Shape Generation from Videos","excerpt":"Sergey Tulyakov이 arXiv에 게시한 'ShapeGen4D: Towards High Quality 4D Shape Generation from Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos","tags":["Review","4D Shape Generation","Video-conditioned","Dynamic 3D Meshes","Latent Diffusion Model","Spatiotemporal Attention","Temporal Consistency","Pre-trained 3D Models","VAE"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiraphon Yenphraphai, Ashkan Mirzaei, Jianqi Chen, Jiaxu Zou, Sergey Tulyakov, Raymond A. Yeh, Peter Wonka, Chaoyang Wang 핵심 연구 목표 본 논문은 단일 입력 비디오에서 시간적으로 변화하는 3D 기하학과 시점 일관성을 갖춘"},{"id":"2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning","title":"[논문리뷰] TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning","excerpt":"arXiv에 게시된 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning","tags":["Review","Process Reward Models","Tabular Reasoning","Test-Time Scaling","Tool Integration","Reinforcement Learning","Supervised Fine-tuning","Large Language Models","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaru Zou, Soumya Roy, Vinay Kumar Verma, Ziyi Wang, David Wipf, Pan Lu, Sumit Negi, James Zou, Jingrui He 핵심 연구 목표 본 논문은 기존의 Process Reward Models (PRMs) 이 표 기반 추론 태스크에서 테이블 검색("},{"id":"2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation","title":"[논문리뷰] TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation","excerpt":"arXiv에 게시된 'TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation","tags":["Review","BLEU Score","GPU Acceleration","PyTorch","Natural Language Processing","Reinforcement Learning","Vectorization","In-Training Evaluation","N-gram Counting"],"text":"링크: 논문 PDF로 바로 열기 저자: Adam Filipek 핵심 연구 목표 본 논문은 현대 자연어 처리 모델의 평가 도구가 특히 훈련 중 평가 지표(intraining evaluation metrics) 에서 연산 병목 현상을 일으켜 연구 속도를 저해하는 문제를 해결하고자 합니다. 구체적으로, 강화 학습(RL) 기반 언어 모델 미세 조정에서 필요한 pe"},{"id":"2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness","title":"[논문리뷰] Training Dynamics Impact Post-Training Quantization Robustness","excerpt":"Jonas Geiping이 arXiv에 게시한 'Training Dynamics Impact Post-Training Quantization Robustness' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness","tags":["Review","Post-Training Quantization","Quantization Robustness","Training Dynamics","Learning Rate Schedules","Weight Averaging","Large Language Models","LLMs","Hyperparameter Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Albert CatalanTatjer, Niccolò Ajroldi, Jonas Geiping 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 효율적인 배포를 위해 널리 사용되는 PostTraining Quantization (PTQ) 의 견고성이 훈련 과정 및 동적 특성에 의해 어떻게 영향을 받는지 규명하는 "},{"id":"2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation","title":"[논문리뷰] VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation","excerpt":"arXiv에 게시된 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-08 13:48:12+0900","permalink":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation","tags":["Review","LLM Agents","Safety","Formal Verification","Code Generation","Runtime Monitoring","Security","Guardrails","Policy Enforcement"],"text":"링크: 논문 PDF로 바로 열기 저자: Lesly Miculicich, Mihir Parmar, Hamid Palangi, Krishnamurthy Dj Dvijotham, Mirko Montanari, Tomas Pfister, Long T. Le 핵심 연구 목표 본 논문은 자율 AI 에이전트, 특히 LLM 기반 에이전트의 배포로 인해 발생하는 안전, 보"},{"id":"2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning","title":"[논문리뷰] AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning","excerpt":"Zongze Li이 arXiv에 게시한 'AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning","tags":["Review","Foundation Models","Agentic Reasoning","Tool Use","Self-Evolving System","Retrieval-Augmented Generation","Computational Tools","Error Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Zongze Li, Xuan Li, Xiao Feng, Chentao Cao, Zhanke Zhou 핵심 연구 목표 재단 모델(FMs)의 제한된 내재적 추론 능력과 불안정한 테스트 시간 반복이라는 두 가지 핵심 병목 현상을 해결하고자 합니다. 이 연구는 FM이 복잡한 벤치마크에서 겪는 어려움을 극복하고, 신뢰할 수 있"},{"id":"2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods","title":"[논문리뷰] Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods","excerpt":"Yiyu Wang이 arXiv에 게시한 'Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods","tags":["Review","Visual Token Compression","MLLMs","Evaluation Framework","Benchmarking","Downsampling","Data Filtering","Model Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu 핵심 연구 목표 현재 멀티모달 대규모 언어"},{"id":"2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling","title":"[논문리뷰] Artificial Hippocampus Networks for Efficient Long-Context Modeling","excerpt":"arXiv에 게시된 'Artificial Hippocampus Networks for Efficient Long-Context Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling","tags":["Review","Long-Context Modeling","Transformer","RNN","Memory Management","Self-Distillation","Attention Mechanism","Artificial Hippocampus Networks","Cognitive Science"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei 핵심 연구 목표 본 논문은 RNN의 효율적인 고정 크기 메모리와 Transformer의 손실 없는 확장 가능 메모리 사이의 근본적인 트레이드오프를 해결하여, 장문 컨텍스트 모델링에서 효율성과 정확"},{"id":"2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models","title":"[논문리뷰] Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models","excerpt":"arXiv에 게시된 'Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models","tags":["Review","Code-switching","Multilingual NLP","Large Language Models","NLP Survey","Data Augmentation","Evaluation Metrics","Low-Resource Languages"],"text":"링크: 논문 PDF로 바로 열기 저자: Rajvee Sheth, Samridhi Raj Sinha, Mahavir Patil, Himanshu Beniwal, Mayank Singh 핵심 연구 목표 이 논문은 대규모 언어 모델(LLMs) 시대 의 코드스위칭(CSW) NLP 연구 현황 을 종합적으로 분석하고, LLMs가 CSW 모델링에 미친 영향을 평가하며,"},{"id":"2025-10-9-Bridging-Text-and-Video-Generation-A-Survey","title":"[논문리뷰] Bridging Text and Video Generation: A Survey","excerpt":"G. Maragatham이 arXiv에 게시한 'Bridging Text and Video Generation: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey","tags":["Review","Text-to-Video Generation","Generative Models","Diffusion Models","GANs","VAEs","Video Synthesis","Survey","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Nilay Kumar, Priyansh Bhandari, G. Maragatham 핵심 연구 목표 본 논문은 텍스트투비디오(T2V) 생성 모델의 발전 과정을 포괄적으로 분석하고, 초기 GANs 및 VAEs 기반 모델부터 최신 확산 기반 아키텍처까지 주요 혁신과 한계를 조명하는 것을 목표로 합니다. 나아가 T2V 분야의"},{"id":"2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling","title":"[논문리뷰] CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling","excerpt":"Chengpeng Li이 arXiv에 게시한 'CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling","tags":["Review","Large Reasoning Models","Optimization Modeling","Reflective Generation","Supervised Fine-tuning","Reinforcement Learning","Human-in-the-Loop","Code Generation","Domain Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengyang Tang, Zihan Ye, Chenyu Huang, Xuhan Huang, Chengpeng Li, Sihang Li, Guanhua Chen, Ming Yan, Zizhuo Wang, Hongyuan Zha, Dayiheng Liu, Benyou Wang 핵심 연구 목표 본 연구는 Large Re"},{"id":"2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models","title":"[논문리뷰] Cache-to-Cache: Direct Semantic Communication Between Large Language Models","excerpt":"arXiv에 게시된 'Cache-to-Cache: Direct Semantic Communication Between Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models","tags":["Review","Large Language Models (LLMs)","Inter-model Communication","KV-Cache","Semantic Transfer","Multi-LLM Systems","Cache Fusion","Latency Reduction","Knowledge Sharing"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, Yu Wang 핵심 연구 목표 본 연구는 기존 멀티LLM 시스템에서 텍스트 기반(TexttoText, T2T) 통신 이 야기하는 정보 손실, 모호성, 토큰 단위 생성 지연과 같은 한계를"},{"id":"2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection","title":"[논문리뷰] D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection","excerpt":"Yueqi Duan이 arXiv에 게시한 'D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection","tags":["Review","Autoregressive Models","Image Detection","Discrete Distribution Discrepancy","Quantization Error","Transformer","Generative AI","Deepfake Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanran Zhang, Bingyao Yu, Yu Zheng, Wenzhao Zheng, Yueqi Duan, Lei Chen, Jie Zhou, Jiwen Lu 핵심 연구 목표 본 논문은 시각적 자기회귀(AR) 모델 이 생성한 이미지의 탐지라는 새로운 도전 과제를 해결하는 것을 목표로 합니다. 기존 GAN이나 Di"},{"id":"2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents","title":"[논문리뷰] DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents","excerpt":"arXiv에 게시된 'DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents","tags":["Review","Agentic Reinforcement Learning","Travel Planning","Large Language Models","Sandbox Environment","Hierarchical Reward Modeling","Experience Replay","Autonomous Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Yansong Ning, Rui Liu, Jun Wang, Kai Chen, Wei Li, Jun Fang, Kan Zheng, Naiqiang Tan, Hao Liu 핵심 연구 목표 기존 수동 프롬프트 엔지니어링 및 고정된 워크플로우에 의존하는 여행 계획(TP) 에이전트의 한계를 극복하고, 자율적으로 계획, 도구 실"},{"id":"2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models","title":"[논문리뷰] G^2RPO: Granular GRPO for Precise Reward in Flow Models","excerpt":"arXiv에 게시된 'G^2RPO: Granular GRPO for Precise Reward in Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models","tags":["Review","Reinforcement Learning","Flow Models","Generative Models","Human Preference Alignment","Stochastic Differential Equations (SDE)","Reward Signal","Multi-Granularity"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujie Zhou, Pengyang Ling, Jiazi Bu, Yibin Wang, Yuhang Zang, Jiaqi Wang, Li Niu, Guangtao Zhai 핵심 연구 목표 본 논문은 확산 및 플로우 모델에서 인간 선호도에 맞춰 생성 모델을 정렬하는 기존 GRPO(Group Relative Policy "},{"id":"2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals","title":"[논문리뷰] Heptapod: Language Modeling on Visual Signals","excerpt":"arXiv에 게시된 'Heptapod: Language Modeling on Visual Signals' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals","tags":["Review","Autoregressive Models","Image Generation","Language Modeling","Causal Transformer","2D Distribution Prediction","Visual Tokenization","Self-Supervised Learning","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yongxin Zhu, Jiawei Chen, Yuanzhe Chen, Zhuo Chen, Dongya Jia, Jian Cong, Xiaobin Zhuang, Yuping Wang, Yuxuan Wang 핵심 연구 목표 이 논문은 시각 생성 모델에서 외부 의미론적 정보 주입 및 CFG(ClassifierFree Gu"},{"id":"2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding","title":"[논문리뷰] Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding","excerpt":"arXiv에 게시된 'Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding","tags":["Review","Multi-modal LLM","Discrete Diffusion","Image Generation","Image Understanding","Omni-modal","Interactive Retouching","Generative AI","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, Jinbin Bai, Qian Yu, Dengyang Jiang, Yuandong Pu, Haoxing Chen, Le Zhuo"},{"id":"2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation","title":"[논문리뷰] MATRIX: Mask Track Alignment for Interaction-aware Video Generation","excerpt":"Hyunwook Choi이 arXiv에 게시한 'MATRIX: Mask Track Alignment for Interaction-aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation","tags":["Review","Video Generation","Diffusion Transformers","Human-Object Interaction","Attention Alignment","Mask Tracking","Semantic Grounding","Semantic Propagation","Text-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyoon Jin, Jisu Nam, Seongchan Kim, Jiyoung Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi, Seungryong Kim 핵심 연구 목표 본 논문은 비디오 Diffusion Transformers (DiTs)가 다중 인스턴스 또는 주체객체 상호작용을 어"},{"id":"2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline","title":"[논문리뷰] MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline","excerpt":"arXiv에 게시된 'MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline","tags":["Review","MLE (Machine Learning Engineering)","Automated Task Generation","Multi-Agent System","LLM Agents","Benchmark","Data Curation","Hybrid Verification","Kaggle"],"text":"링크: 논문 PDF로 바로 열기 저자: Rushi Qiang, Yuchen Zhuang, Anikait Singh, Percy Liang, Chao Zhang, Sherry Yang, Bo Dai 핵심 연구 목표 현재 기계 학습 엔지니어링(MLE) 벤치마크 는 수동 큐레이션에 의존하여 확장성이 낮고 적용 가능성이 제한적입니다. 본 연구는 이러한 문제를 해결"},{"id":"2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer","title":"[논문리뷰] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer","excerpt":"arXiv에 게시된 'Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer","tags":["Review","Unified Vision-Language Model","Continuous Tokenizer","Autoregressive Generation","Image Understanding","Image Generation","Multimodal AI","In-context Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyuan Huang, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lv, Taozhi Huang, Jiajia Liu, Qingpei Guo, Ming Yang, J"},{"id":"2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization","title":"[논문리뷰] Multi-Agent Tool-Integrated Policy Optimization","excerpt":"Lidong Bing이 arXiv에 게시한 'Multi-Agent Tool-Integrated Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization","tags":["Review","Multi-Agent RL","Tool-Integrated Planning","Large Language Models (LLMs)","Policy Optimization","Credit Assignment","Reinforcement Learning","MATPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhanfeng Mo, Xingxuan Li, Yuntao Chen, Lidong Bing 핵심 연구 목표 본 논문은 단일 에이전트 LLM의 도구 통합 계획(ToolIntegrated Planning, TIP) 방식이 갖는 제한된 컨텍스트 길이 와 노이즈가 많은 도구 응답 문제를 해결하고자 합니다. 특히, 단일 LLM"},{"id":"2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling","title":"[논문리뷰] Native Hybrid Attention for Efficient Sequence Modeling","excerpt":"Yu Cheng이 arXiv에 게시한 'Native Hybrid Attention for Efficient Sequence Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling","tags":["Review","Sequence Modeling","Hybrid Attention","Transformer Architecture","Linear Attention","Sliding Window Attention","Long Context","Large Language Models (LLMs)","Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng 핵심 연구 목표 본 논문은 Transformer의 O(n²) 연산 복잡도와 선형 어텐션 모델의 낮은 정확도 문제를 해결하기 위해, 효율적이면서도 긴 컨텍스트에서 높은 정확도를 유지할 수 있는 새로운 하이브리드 어텐션 아키텍처를"},{"id":"2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable","title":"[논문리뷰] NorMuon: Making Muon more efficient and scalable","excerpt":"Tuo Zhao이 arXiv에 게시한 'NorMuon: Making Muon more efficient and scalable' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable","tags":["Review","LLM Training","Optimizer","Muon","Orthogonalization","Adaptive Learning Rates","Distributed Training","FSDP2","NorMuon"],"text":"링크: 논문 PDF로 바로 열기 저자: Zichong Li, Liming Liu, Chen Liang, Weizhu Chen, Tuo Zhao 핵심 연구 목표 대규모 언어 모델(LLM) 훈련 효율성 향상을 위해 기존 Muon 옵티마이저의 한계를 극복하는 것이 목표입니다. Muon이 업데이트의 컨디셔닝을 개선하지만 뉴런별 업데이트 노름의 분산이 크다는 문제를"},{"id":"2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot","title":"[논문리뷰] OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot","excerpt":"arXiv에 게시된 'OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot","tags":["Review","Diffusion Models","Network Pruning","One-Shot Pruning","Optimal Brain Surgeon (OBS)","Model Compression","Timestep-Aware Hessian","Structured Pruning"],"text":"링크: 논문 PDF로 바로 열기 저자: Junhan Zhu¹, Hesong Wang1,2, Mingluo Su¹, Zefang Wang1,2, Huan Wang1 핵심 연구 목표 대규모 텍스트이미지 확산 모델의 과도한 연산 비용 문제를 해결하고, 기존 원샷 네트워크 가지치기(pruning) 방법론이 확산 모델의 반복적인 노이즈 제거 특성 과 복잡한 아키텍처"},{"id":"2025-10-9-Online-Generic-Event-Boundary-Detection","title":"[논문리뷰] Online Generic Event Boundary Detection","excerpt":"Jonghyun Choi이 arXiv에 게시한 'Online Generic Event Boundary Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Online-Generic-Event-Boundary-Detection","tags":["Review","Online Video Analysis","Event Boundary Detection","Event Segmentation Theory","Real-time AI","Anomaly Detection","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyungrok Jung, Daneul Kim, Seunggyun Lim, Jeany Son, Jonghyun Choi 핵심 연구 목표 본 논문은 기존 오프라인(offline) GEBD(Generic Event Boundary Detection)의 한계를 극복하고, 인간의 인지 과정에 더 가까운 온라인 GEBD(OnG"},{"id":"2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs","title":"[논문리뷰] Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs","excerpt":"Jingyi Liao이 arXiv에 게시한 'Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs","tags":["Review","Multimodal Large Language Models (MLLMs)","Visual Reference Tokens (VRTs)","Dense Prediction","Referring Expression Comprehension (REC)","Open-Vocabulary Detection (OVD)","Image Captioning","Unified Architecture","Autoregressive Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yongyi Su, Haojie Zhang, Shijie Li, Nanqing Liu, Jingyi Liao, Junyi Pan, Yuan Liu, Xiaofen Xing, Chong Sun, Xulei Yang, Xun Xu, Chen Li, Nancy F. Chen, Shuicheng Yan 핵심 연구 목표 기존 "},{"id":"2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought","title":"[논문리뷰] Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought","excerpt":"arXiv에 게시된 'Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought","tags":["Review","Multilingual Reasoning","Chain-of-Thought (CoT)","Language-Mixed CoT","Instruction Tuning","Korean LLMs","Data Curation","Supervised Fine-tuning (SFT)"],"text":"링크: 논문 PDF로 바로 열기 저자: Guijin Son, Donghun Yang, Hyunwoo Ko, Dasol Choi, Chanuk Lim, KyongHa Lee, Hitesh Laxmichand Patel, Amit Agarwal, Srikant Panda, Minhyuk Kim, Nikunj Drolia, Youngjae Yu 핵심 연구 목표 "},{"id":"2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training","title":"[논문리뷰] RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training","excerpt":"arXiv에 게시된 'RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training","tags":["Review","Reinforcement Learning","VLA Models","Robotics","GPU Management","PPO","GRPO","Sim-to-Real"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongzhi Zang, Mingjie Wei, Si Xu, Yongji Wu, Zhen Guo, Yuanqing Wang, Hao Lin, Liangzhi Shi, Yuqing Xie, Zhexuan Xu, Zhihao Liu, Kang Chen, Wenhao Tang, Quanlu Zhang, Weinan Zhan"},{"id":"2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective","title":"[논문리뷰] Revisiting Long-context Modeling from Context Denoising Perspective","excerpt":"arXiv에 게시된 'Revisiting Long-context Modeling from Context Denoising Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective","tags":["Review","Long-context Models","Context Denoising","Integrated Gradient","LLM Training","Context Window Scaling","Information Flow","Attention Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Zecheng Tang, Baibei Ji, Juntao Li, Lijun Wu, Haijia Gui, Min Zhang 핵심 연구 목표 본 연구는 Longcontext Models (LCMs)가 컨텍스트 내의 불필요한 토큰(contextual noise)에 취약하여 모델의 어텐션을 잘못 유도하고 성능을 저해하는 문제"},{"id":"2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces","title":"[논문리뷰] Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces","excerpt":"arXiv에 게시된 'Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces","tags":["Review","LLM Reasoning","Chain-of-Thought","Uniform Information Density","Information Theory","Reasoning Trace Analysis","Entropy","Mathematical Reasoning","Model Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minju Gwak, Guijin Son, Jaehyung Kim 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 CoT(ChainofThought) 추론 과정에서 효과적인 추론이 단순히 피상적인 일관성을 넘어섰는지 판단하는 방법을 모색합니다. 특히 인간 커뮤니케이션의 효과적인 정보 흐름을 설명하는 균일 정보 "},{"id":"2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models","title":"[논문리뷰] SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models","excerpt":"Kevin Lin이 arXiv에 게시한 'SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models","tags":["Review","Spoken Language Models","Real-time Interaction","Thinking While Listening","Chain-of-Thought","Interruption","Tool Calling","Streaming ASR"],"text":"링크: 논문 PDF로 바로 열기 저자: Kevin Lin, ChungChing Lin, Linjie Li, Xiaofei Wang, ChengHan Chiang 외 다수 핵심 연구 목표 현재 대규모 언어 모델(LLMs) 및 음성 언어 모델(SLMs)이 사용자의 발화가 끝난 후에야 추론 및 행동을 시작하여 발생하는 높은 응답 지연 시간 문제를 해결하는 것이 "},{"id":"2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation","title":"[논문리뷰] StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation","excerpt":"arXiv에 게시된 'StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation","tags":["Review","Robot Learning","State Representation","Motion Representation","Diffusion Models","Unsupervised Learning","World Modeling","Vision-Language Models","Latent Action"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingyu Liu, Jiuhe Shu, Hui Chen, Zeju Li, Canyu Zhao, Jiange Yang, Shenyuan Gao, Hao Chen, Chunhua Shen 핵심 연구 목표 로봇 시스템에서 효율적인 세계 모델링과 의사 결정을 위해 표현적이고 압축적인 상태 표현 을 개발하는 것이 핵심 목표입"},{"id":"2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models","title":"[논문리뷰] TTRV: Test-Time Reinforcement Learning for Vision Language Models","excerpt":"Serena Yeung-Levy이 arXiv에 게시한 'TTRV: Test-Time Reinforcement Learning for Vision Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models","tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Test-Time Adaptation","Unsupervised Learning","Image Recognition","Visual Question Answering (VQA)","Group Relative Policy Optimization (GRPO)","Entropy Regularization"],"text":"링크: 논문 PDF로 바로 열기 저자: Akshit Singh, Shyam Marjit, Wei Lin, Paul Gavrikov, Serena YeungLevy, Hilde Kuehne, Rogerio Feris, James Glass, M. Jehanzeb Mirza, Sivan Doveh 핵심 연구 목표 이 논문은 기존의 VisionLanguage M"},{"id":"2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP","title":"[논문리뷰] The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP","excerpt":"arXiv에 게시된 'The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP","tags":["Review","Low-Resource NLP","African Languages","Data Collection","Multilingual Models","Fine-Tuning","Speech Data","Text Data","Capacity Building"],"text":"링크: 논문 PDF로 바로 열기 저자: Sheriff Issaka, Keyi Wang, Yinka Ajibola, Oluwatumininu SamuelIpaye, Zhaoyi Zhang, Nicte Aguillon Jimenez, Evans Kofi Agyei, Abraham Lin, Rohan Ramachandran, Sadick Abdul Mumin, "},{"id":"2025-10-9-The-Markovian-Thinker","title":"[논문리뷰] The Markovian Thinker","excerpt":"arXiv에 게시된 'The Markovian Thinker' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-The-Markovian-Thinker","tags":["Review","Reinforcement Learning","Large Language Models","Chain-of-Thought","Markovian Thinking","Context Management","Computational Efficiency","Long-Context LLMs","Transformer Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Milad Aghajohari, Kamran Chitsaz, Amirhossein Kazemnejad, Sarath Chandar, Alessandro Sordoni, Aaron Courville, Siva Reddy 핵심 연구 목표 본 논문은 추론 LLM 훈련 시 발생하는 무한한 상태 크기 와 추론 길이 증가에 따른"},{"id":"2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking","title":"[논문리뷰] U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking","excerpt":"Heqin Zhu이 arXiv에 게시한 'U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking","tags":["Review","U-Net","Medical Image Segmentation","Benchmarking","Performance Evaluation","Efficiency Metrics","Zero-shot Generalization","U-Score"],"text":"링크: 논문 PDF로 바로 열기 저자: Heqin Zhu, Zikang Xu, Wenxin Ma, Chengqi Dong, Fenghe Tang 핵심 연구 목표 의료 영상 분할 분야에서 수천 가지의 UNet 변형 모델이 제안되었음에도 불구하고, 이들의 성능과 실용성을 포괄적으로, 통계적으로 엄격하게, 그리고 효율성을 고려하여 평가하는 종합적인 벤치마크의 부"},{"id":"2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference","title":"[논문리뷰] Vibe Checker: Aligning Code Evaluation with Human Preference","excerpt":"arXiv에 게시된 'Vibe Checker: Aligning Code Evaluation with Human Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference","tags":["Review","Code Evaluation","Instruction Following","Human Preference","Large Language Models","Vibe Check","Non-functional Requirements","VeriCode"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Zhong, Xiang Zhou, TingYun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings and Jiao Sun 핵심 연구 목표 본 논문은 기존의 코"},{"id":"2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation","title":"[논문리뷰] When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation","excerpt":"arXiv에 게시된 'When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation","tags":["Review","LLM Factuality Evaluation","Benchmark Aging","Temporal Misalignment","Information Retrieval","Question Answering","Evaluation Metrics","GPT-4o-mini","Qwen2.5"],"text":"링크: 논문 PDF로 바로 열기 저자: Xunyi Jiang, Dingyi Chang, Julian McAuley, Xin Xu 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 급속한 발전과 실세계의 변화가 기존 사실성 평가 벤치마크의 신뢰성을 저해하는 문제를 다룹니다. 특히, 정적 벤치마크와 최신 실세계 사실 및 현대 LLM 간의 시간적 불일치(t"},{"id":"2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention","title":"[논문리뷰] Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention","excerpt":"arXiv에 게시된 'Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention","tags":["Review","Low-Precision Training","Flash Attention","Transformer","Numerical Stability","BF16","Rounding Error","Gradient Bias","Deep Learning Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Haiquan Qiu, Quanming Yao 핵심 연구 목표 본 논문은 저정밀도(lowprecision) Flash Attention 을 사용하는 Transformer 모델 학습 시 발생하는 치명적인 손실 폭발(loss explosion) 현상의 기계론적 원인 을 규명하는 것을 목표로 합니다. 기존의 경험적 해결책들"},{"id":"2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","title":"[논문리뷰] WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation","excerpt":"arXiv에 게시된 'WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-10-09 13:45:06+0900","permalink":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","tags":["Review","4D World Models","Robotic Manipulation","Video Generation","Multi-view Synthesis","Visual-Language-Action (VLA)","Geometric Consistency","Diffusion Models","Wrist-View"],"text":"링크: 논문 PDF로 바로 열기 저자: Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, Shanghang Zhang 핵심 연구 목표 로봇 조작을 위한 VLA(VisionLanguageAction) 모델 은 미세한 손객체 상호작용을 포착하는 손목 시점("},{"id":"2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration","title":"[논문리뷰] CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?","excerpt":"Baixuan Xu이 arXiv에 게시한 'CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration","tags":["Review","LLM Calibration","Confidence Calibration","Uncertainty Estimation","Critique Learning","Supervised Fine-Tuning","Natural Language Processing","Self-Critique"],"text":"링크: 논문 PDF로 바로 열기 저자: Qing Zong, Jiayu Liu, Tianshi Zheng, Chunyang Li, Baixuan Xu, Haochen Shi, Weiqi Wang, Zhaowei Wang, Chunkit Chan, Yangqiu Song 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 자연어 기반 신뢰도 표현(verba"},{"id":"2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model","title":"[논문리뷰] DeepEyesV2: Toward Agentic Multimodal Model","excerpt":"Guohai Xu이 arXiv에 게시한 'DeepEyesV2: Toward Agentic Multimodal Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model","tags":["Review","Agentic AI","Multimodal Models","Tool Use","Reinforcement Learning","Supervised Fine-tuning","Multimodal Reasoning","Web Search","Code Execution"],"text":"링크: 논문 PDF로 바로 열기 저자: Jack Hong, Chenxiao Zhao, ChengLIn Zhu, Weiheng Lu, Guohai Xu, Xing Yu 핵심 연구 목표 본 논문은 텍스트와 이미지를 단순히 이해하는 것을 넘어, 코드 실행 환경 및 웹 검색 과 같은 외부 도구를 능동적으로 호출하고 이러한 도구 작업을 추론 과정에 원활하게 통합할 "},{"id":"2025-11-10-Dense-Motion-Captioning","title":"[논문리뷰] Dense Motion Captioning","excerpt":"Paolo Rota이 arXiv에 게시한 'Dense Motion Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-Dense-Motion-Captioning","tags":["Review","3D Human Motion","Dense Captioning","Large Language Models","Motion Understanding","Temporal Localization","Human-Language Datasets","Motion Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shiyao Xu, Benedetta Liberatori, Gül Varol, Paolo Rota 핵심 연구 목표 본 논문은 3D 휴먼 모션 시퀀스 내에서 의미 있는 액션을 시간적으로 정확히 감지하고, 해당 액션에 대한 상세한 캡션을 생성하는 새로운 태스크인 Dense Motion Captioning (DMC) 을 제"},{"id":"2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent","title":"[논문리뷰] HAFixAgent: History-Aware Automated Program Repair Agent","excerpt":"Ahmed E. Hassan이 arXiv에 게시한 'HAFixAgent: History-Aware Automated Program Repair Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent","tags":["Review","Automated Program Repair","AI Agent","Large Language Models","Repository Mining","Historical Context","Bug Fixing","Defects4J"],"text":"링크: 논문 PDF로 바로 열기 저자: YU SHI, HAO LI, BRAM ADAMS, AHMED E. HASSAN 핵심 연구 목표 본 연구는 기존 LLM 기반 프로그램 자동 수정(APR) 시스템이 로컬 코드 스냅샷에만 의존하여 복잡한 다중hunk 버그 수정 시 저장소 이력 정보 를 간과하는 문제를 해결하고자 합니다. 특히, Git blame에서 파생된 "},{"id":"2025-11-10-Jailbreaking-in-the-Haystack","title":"[논문리뷰] Jailbreaking in the Haystack","excerpt":"Alexander Robey이 arXiv에 게시한 'Jailbreaking in the Haystack' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-Jailbreaking-in-the-Haystack","tags":["Review","Jailbreaking","LLM Safety","Long-Context Models","Positional Bias","Attack Success Rate (ASR)","Prompt Engineering","Compute Efficiency","AI Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Rishi Rajesh Shah, Chen Henry Wu, Shashwat Saxena, Ziqian Zhong, Alexander Robey, Aditi Raghunathan 핵심 연구 목표 본 연구는 장문(longcontext) 언어 모델(LMs)의 확장된 컨텍스트 창이 가지는 안전성 함의를 분석하고, 심지어 양"},{"id":"2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments","title":"[논문리뷰] Real-Time Reasoning Agents in Evolving Environments","excerpt":"arXiv에 게시된 'Real-Time Reasoning Agents in Evolving Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments","tags":["Review","Real-time Reasoning","LLM Agents","Dynamic Environments","Dual-System AI","AgileThinker","Reactive Planning","Cognitive Load","Time Pressure"],"text":"링크: 논문 PDF로 바로 열기 저자: Yule Wen, Yixin Ye, Yanzhe Zhang, Diyi Yang, Hao Zhu 핵심 연구 목표 본 논문은 실시간으로 변화하는 환경에서 대규모 언어 모델(LLM) 기반 에이전트가 논리적이고 시의적절한 판단을 내리는 실시간 추론(RealTime Reasoning) 이라는 근본적인 과제를 해결하는 것을 목표"},{"id":"2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains","title":"[논문리뷰] Too Good to be Bad: On the Failure of LLMs to Role-Play Villains","excerpt":"arXiv에 게시된 'Too Good to be Bad: On the Failure of LLMs to Role-Play Villains' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains","tags":["Review","LLM","Role-playing","Safety Alignment","Villain","Persona Simulation","Moral Alignment","Benchmark","Character Fidelity"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihao Yi, Qingxuan Jiang, Ruotian Ma, Xingyu Chen, Qu Yang, Mengru Wang, Fanghua Ye, Ying Shen⁺, Zhaopeng Tu⁺, Xiaolong Li, and Linus 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 다양한 도덕적 스펙트럼,"},{"id":"2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings","title":"[논문리뷰] Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings","excerpt":"Jiaxin Yuan이 arXiv에 게시한 'Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings","tags":["Review","Hallucination Mitigation","Large Vision-Language Models","Textual Embeddings","Multimodal Reasoning","Attention Mechanism","Visual Grounding","Modality Imbalance"],"text":"링크: 논문 PDF로 바로 열기 저자: Aakriti Agrawal, Gouthaman KV, Rohith Aralikatti, Gauri Jagatap, Jiaxin Yuan, Vijay Kamarshi, Andrea Fanelli, Furong Huang 핵심 연구 목표 대규모 비전언어 모델(LVLM)이 시각적 정보를 불충분하게 활용하고 텍스트 우선(t"},{"id":"2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks","title":"[논문리뷰] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks","excerpt":"arXiv에 게시된 'VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks","tags":["Review","Neuro-symbolic AI","Chain-of-Thought","Large Language Models","Logical Consistency","Automated Verification","Fine-tuning","SMT Solvers","Self-Reflection"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin KieslReiter, Huzefa Rangwala 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 ChainofThought (CoT) 추론 과"},{"id":"2025-11-10-Visual-Spatial-Tuning","title":"[논문리뷰] Visual Spatial Tuning","excerpt":"arXiv에 게시된 'Visual Spatial Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-10 00:00:00+0900+0900","permalink":"/ai/review/2025-11-10-Visual-Spatial-Tuning","tags":["Review","Vision-Language Models","Spatial Reasoning","Spatial Perception","Dataset Creation","Reinforcement Learning","Visuospatial AI","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, Hengshuang Zhao 외 핵심 연구 목표 본 논문은 기존 VisionLanguage M"},{"id":"2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models","title":"[논문리뷰] 10 Open Challenges Steering the Future of Vision-Language-Action Models","excerpt":"arXiv에 게시된 '10 Open Challenges Steering the Future of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models","tags":["Review","Vision-Language-Action Models","Embodied AI","Robotics","Multimodal Perception","Cross-Robot Generalization","Hierarchical Planning","World Models","Robot Safety"],"text":"링크: 논문 PDF로 바로 열기 저자: Soujanya Poria, Navonil Majumder, ChiaYu Hung, Amir Ali Bagherzadeh, Chuan Li, Kenneth Kwok, Ziwei Wang, Cheston Tan, Jiajun Wu, David Hsu 핵심 연구 목표 본 논문은 VisionLanguageAction (VL"},{"id":"2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries","title":"[논문리뷰] Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries","excerpt":"Zhengzhong Tu이 arXiv에 게시한 'Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries","tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Spatial Reasoning","Controllable Framework","RLVR","GRPO","Maze Navigation","Generalization Boundaries"],"text":"링크: 논문 PDF로 바로 열기 저자: Minghe Shen, Zhuo Zhi, Chonghan Liu, Shuo Xing, Zhengzhong Tu, Che Liu 핵심 연구 목표 본 연구는 RL 후처리 훈련이 기존 VLM의 내재적 추론 능력 경계 를, 특히 시각 중심의 공간 추론 작업에서 확장할 수 있는지 탐색하는 것을 목표로 합니다. 이를 위해, 정밀"},{"id":"2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects","title":"[논문리뷰] DIMO: Diverse 3D Motion Generation for Arbitrary Objects","excerpt":"Kostas Daniilidis이 arXiv에 게시한 'DIMO: Diverse 3D Motion Generation for Arbitrary Objects' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects","tags":["Review","3D Motion Generation","Generative Models","Arbitrary Objects","Neural Key Points","Latent Space","4D Content Generation","Diffusion Models","3D Gaussian Splatting"],"text":"링크: 논문 PDF로 바로 열기 저자: Linzhan Mou, Jiahui Lei, Chen Wang, Lingjie Liu, Kostas Daniilidis 핵심 연구 목표 본 연구는 기존 4D 생성 모델이 단일 객체에 대해 단일 모션만 생성하거나, 카테고리별로 제한된 모션만을 다루는 한계를 극복하고자 합니다. 단일 이미지 에서 임의의 객체 에 대한 다양"},{"id":"2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation","title":"[논문리뷰] DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation","excerpt":"arXiv에 게시된 'DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation","tags":["Review","Reinforcement Learning with Verifiable Reward","Competitive Programming","Code Generation","Data Curation","Curriculum Learning","Supervised Fine-tuning","Entropy Expansion"],"text":"링크: 논문 PDF로 바로 열기 저자: Speed Zhu, Jianwei Cai, Guang Chen, Lulu Wu, Saiyong Yang, Wiggin Zhou 핵심 연구 목표 이 논문은 RLVR(Reinforcement Learning with Verifiable Rewards)을 사용하여 경쟁 프로그래밍 코드 생성의 성능을 향상시키는 데 있어 데이"},{"id":"2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models","title":"[논문리뷰] Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models","excerpt":"Zhao Xu이 arXiv에 게시한 'Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models","tags":["Review","Diffusion Models","Direct Preference Optimization (DPO)","Safeguarded Learning","Text-to-Image Generation","Preference Alignment","Generative Models","Stable Diffusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Minghao Fu, GuoHua Wang, Tianyu Cui, QingGuo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang 핵심 연구 목표 텍스트이미지 확산 모델을 인간의 선호도에 맞춰 정렬하는 과정에서 발생하는 문제를 해결하는 것이 목표입니다. 특히 기존 Direct Preference O"},{"id":"2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents","title":"[논문리뷰] DigiData: Training and Evaluating General-Purpose Mobile Control Agents","excerpt":"arXiv에 게시된 'DigiData: Training and Evaluating General-Purpose Mobile Control Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents","tags":["Review","Mobile Control Agents","User Interface Automation","Large-Scale Dataset","Benchmarking","LLM Judges","Data Diversity","Task Success Rate"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxuan Sun, Manchen Wang, Shengyi Qian, William R. Wong, Eric Gan, Pierluca D'Oro, Alejandro Castillejo Munoz, Sneha Silwal, Pedro Matias, Nitin Kamra, Satwik Kottur, Nick Raines"},{"id":"2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning","title":"[논문리뷰] Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning","excerpt":"arXiv에 게시된 'Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning","tags":["Review","Emotion Recognition in Conversation","Large Language Models","Prompt Engineering","Demonstration Retrieval","Curriculum Learning","Fine-tuning","Affective Computing","SOTA"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinran Li, Yu Liu, Jiaqi Qiao, Xiujuan Xu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 대화에서 명시적(explicit) 및 암묵적(implicit) 감정을 효과적으로 인식할 수 있는지 탐구하고, 이 분야의 현재 한계점을 극복하는 것을 목표로 합니다. 특히, LLM의 감정 "},{"id":"2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience","title":"[논문리뷰] FLEX: Continuous Agent Evolution via Forward Learning from Experience","excerpt":"Jiangjie Chen이 arXiv에 게시한 'FLEX: Continuous Agent Evolution via Forward Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience","tags":["Review","LLM Agents","Continuous Learning","Experience Library","Forward Learning","Meta-MDP","Knowledge Distillation","Non-parametric Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhicheng Cai, Xinyuan Guo, Yu Pei, Jiangtao Feng, Jiangjie Chen, YaQin Zhang, WeiMing Ma, Mingxuan Wang, Hao Zhou 핵심 연구 목표 본 논문의 핵심 목표는 기존 LLM(Large Language Model) 에이전트의 고정된 특성,"},{"id":"2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions","title":"[논문리뷰] Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions","excerpt":"arXiv에 게시된 'Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions","tags":["Review","Text-to-Image Generation","Structured Captions","LLM Fusion","Controllability","Image Generation Evaluation","Diffusion Models","DimFusion","TaBR"],"text":"링크: 논문 PDF로 바로 열기 저자: Eyal Gutflaish, Eliran Kachlon, Hezi Zisman, Tal Hacham, Nimrod Sarid, Alexander Visheratin, Saar Huberman, Gal Davidi, Guy Bukchin, Kfir Goldberg, Ron Mokady 핵심 연구 목표 본 논문은 기존 텍"},{"id":"2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents","title":"[논문리뷰] HaluMem: Evaluating Hallucinations in Memory Systems of Agents","excerpt":"arXiv에 게시된 'HaluMem: Evaluating Hallucinations in Memory Systems of Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents","tags":["Review","Memory Systems","AI Agents","Hallucination Detection","Evaluation Benchmark","Long-term Memory","Memory Extraction","Memory Updating","Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Ding Chen, Simin Niu, Kehang Li, Peng Liu, Xiangping Zheng, Bo Tang, Xinchi Li, Feiyu Xiong, Zhiyu Li 핵심 연구 목표 본 논문은 LLM 및 AI 에이전트의 장기 학습 및 지속적인 상호작용을 가능하게 하는 메모리 시스템에서 발생하는 기억 환"},{"id":"2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction","title":"[논문리뷰] IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction","excerpt":"Haotian Xu이 arXiv에 게시한 'IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction","tags":["Review","Long-Horizon Agents","Markov Decision Process","Workspace Reconstruction","Reinforcement Learning","Context Management","Iterative Deep Research","LLM Agents","Efficiency-Aware Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Haotian Xu, Donglei Yu, Zile Qiao, chenxz, GuoxinChen 핵심 연구 목표 이 논문은 기존 딥리서치 에이전트들이 단일 확장 컨텍스트 창에 정보를 축적하는 으로 인해 발생하는 컨텍스트 질식(context suffocation) 및 노이즈 오염(noise contamination) 문"},{"id":"2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs","title":"[논문리뷰] LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs","excerpt":"Jason Cong이 arXiv에 게시한 'LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs","tags":["Review","FPGA","Large Language Models (LLM)","Inference Optimization","Memory-based Computation","Vector Quantization","Table Lookup","Hardware Acceleration"],"text":"링크: 논문 PDF로 바로 열기 저자: Zifan He, Shengyu Ye, Rui Ma, Yang Wang, Jason Cong 핵심 연구 목표 본 논문은 효율적인 단일 배치 대규모 언어 모델(LLM) 추론을 위해 FPGA 의 장점을 활용하는 것을 목표로 합니다. 특히, 기존 산술 기반 연산에서 메모리 기반 연산 으로 전환하여 GPU 대비 FPGA의 성"},{"id":"2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks","title":"[논문리뷰] Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks","excerpt":"arXiv에 게시된 'Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks","tags":["Review","Text Embedding","Multilingual","Cross-Lingual","Contrastive Learning","Model Merging","Synthetic Data Generation","Instruction-Tuning","LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Yauhen Babakhin, Radek Osmulski, Ronay Ak, Gabriel Moreira, Mengyao Xu, Benedikt Schifferer, Bo Liu, Even Oldridge (NVIDIA) 핵심 연구 목표 본 논문은 기존 임베딩 모델의 불투명한 훈련 데이터 및 방법론 문제를 해결하고자,"},{"id":"2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale","title":"[논문리뷰] Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale","excerpt":"arXiv에 게시된 'Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale","tags":["Review","Visual Reasoning","Compositional AI","Vision-Language Models","Data Synthesis","Chain-of-Thought","Reinforcement Learning","Multimodal Transfer","Grounded Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: David Acuna, C.H. Huck Yang, Yuntian Deng, Jaehun Jung, Ximing Lu, Prithviraj Ammanabrolu, Hyunwoo Kim, YuanHong Liao, Yejin Choi 핵심 연구 목표 본 논문은 시각적 수학을 넘어선 복합적인 추론 구조를 갖춘 대규모, 비"},{"id":"2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings","title":"[논문리뷰] MPJudge: Towards Perceptual Assessment of Music-Induced Paintings","excerpt":"arXiv에 게시된 'MPJudge: Towards Perceptual Assessment of Music-Induced Paintings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings","tags":["Review","Music-Painting Cross-Modal","Perceptual Assessment","Modality-Adaptive Normalization","Direct Preference Optimization","Cross-Modal Fusion","Dataset Annotation","Affective Computing"],"text":"링크: 논문 PDF로 바로 열기 저자: Shiqi Jiang, Tianyi Liang, Huayuan Ye, Changbo Wang, Chenhui Li 핵심 연구 목표 음악에 의해 영감을 받은 그림의 지각적 일관성을 평가하는 어려운 과제를 해결하는 것을 목표로 합니다. 기존 감정 기반 접근 방식의 한계(불정확성 및 감정 외 다른 지각적 단서 간과)를 극복"},{"id":"2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","title":"[논문리뷰] MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs","excerpt":"arXiv에 게시된 'MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","tags":["Review","Multimodal Large Language Models (MLLMs)","Multi-Video Understanding","Evaluation Benchmark","Video Perception","Video Reasoning","Sports Analytics","Autonomous Driving"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Gavin Chang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Steven Huang, Zhao"},{"id":"2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling","title":"[논문리뷰] NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling","excerpt":"arXiv에 게시된 'NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling","tags":["Review","Text-to-CAD","NURBS Modeling","Large Language Models","Geometric Deep Learning","Boundary Representation","Hybrid Representation","CAD Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Muhammad Usama, Mohammad Sadil Khan, Didier Stricker, Muhammad Zeshan Afzal 핵심 연구 목표 본 논문은 자연어 텍스트 설명으로부터 NURBS(NonUniform Rational BSplines) 기반의 고정밀 3D CAD 모델을 직접 생성하는 최초의 프레임워크"},{"id":"2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models","title":"[논문리뷰] Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models","excerpt":"arXiv에 게시된 'Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models","tags":["Review","Multimodal Speech Recognition","Large Language Models","Audio-Visual Speech Recognition","LoRA","Matryoshka Representation Learning","Elastic Inference","Parameter-Efficient Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic 핵심 연구 목표 본 논문은 ASR, VSR, AVSR 태스크를 단일 프레임워크 내에서 지원하고 유연한 추론(elastic inference)이 가능한 통합된 오디오비주얼 대규모 언어 모"},{"id":"2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments","title":"[논문리뷰] RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments","excerpt":"Shuyue Stella Li이 arXiv에 게시한 'RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments","tags":["Review","Reinforcement Learning","Language Models","Adaptive Environments","Verifiable Environments","Procedural Generation","Curriculum Learning","Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuyue Stella Li, Lifan Yuan, Yiping Wang, Hamish Ivison, Zhiyuan Zeng 핵심 연구 목표 언어 모델(LM)의 강화 학습(RL) 훈련이 정적 데이터셋에서 포화되고, 검증 가능한 학습 데이터를 수집하는 높은 비용 문제를 해결하고자 합니다. 또한, 문제가 너무 쉽거나 어"},{"id":"2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization","title":"[논문리뷰] RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization","excerpt":"Wenhao Huang이 arXiv에 게시한 'RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization","tags":["Review","Reinforcement Learning","LLMs","Generalization","Overfitting","Catastrophic Forgetting","Iterative Policy Optimization","Policy Diversity"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Zeng, Jiashuo Liu, Zhangyue Yin, Ge Zhang, Wenhao Huang, Xipeng Qiu 핵심 연구 목표 대규모 추론 모델을 위한 검증 가능한 보상 강화 학습 (RLVR) 에서 발생하는 \"RL 오버피팅\" 문제를 해결하는 것이 목표입니다. 이 오버피팅은 훈련 보상은 증가하지"},{"id":"2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads","title":"[논문리뷰] Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads","excerpt":"Jiaheng Zhang이 arXiv에 게시한 'Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads","tags":["Review","LLM Reasoning Verification","Uncertainty Quantification (UQ)","UHeads","Process Reward Models (PRMs)","Chain-of-Thought (CoT)","Self-Supervised Learning","Computational Efficiency","Domain Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingwei Ni, Ekaterina Fadeeva, Tianyi Wu, Mubashara Akhtar, Jiaheng Zhang, Elliott Ash, Markus Leippold, Timothy Baldwin, SeeKiong Ng, Artem Shelmanov, Mrinmaya Sachan 핵심 연구 목표 이"},{"id":"2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services","title":"[논문리뷰] RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services","excerpt":"Zijie Meng이 arXiv에 게시한 'RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services","tags":["Review","LLM Post-Training","Domain Adaptation","Social Networking Services","Reinforcement Learning","Supervised Fine-Tuning","Catastrophic Forgetting","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Fei Zhao, Chonggang Lu, Haofu Qian, Fangcheng Shi, Zijie Meng, Jianzhao Huang, Xu Tang, Zheyong Xie, Zheyu Ye, Zhe Xu, Yao Hu, Shaosheng Cao 핵심 연구 목표 SNS(Social Networking Servic"},{"id":"2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs","title":"[논문리뷰] Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs","excerpt":"arXiv에 게시된 'Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs","tags":["Review","Reinforcement Learning","Large Language Models","Hierarchical Knowledge","Knowledge Traversal","Structured Prompting","Internal Representations","Alignment Tax"],"text":"링크: 논문 PDF로 바로 열기 저자: Renfei Zhang, Manasa Kaniselvan, Niloofar Mireshghallah 핵심 연구 목표 이 논문은 RL(강화 학습)이 LLM(대규모 언어 모델)의 추론 능력 향상과 암기된 지식 저하 사이의 트레이드오프를 가져온다는 일반적인 통념에 도전합니다. 특히 계층적이고 구조화된 지식(예: 의료 코드)"},{"id":"2025-11-11-Robot-Learning-from-a-Physical-World-Model","title":"[논문리뷰] Robot Learning from a Physical World Model","excerpt":"arXiv에 게시된 'Robot Learning from a Physical World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model","tags":["Review","Robot Learning","Video Generation","Physical World Model","Reinforcement Learning","Zero-shot Manipulation","Object-Centric Learning","Sim-to-Real"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiageng Mao, Sicheng He, Yanan Bao, Huizhong Chen, HaoNing Wu, Leonidas Guibas, Yang You, Shuyang Sun, Vitor Guizilini, Howard Zhou, Zhicheng Wang, Yue Wang 핵심 연구 목표 본 논문은 비디오 생성"},{"id":"2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs","title":"[논문리뷰] Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs","excerpt":"Ziyue Li이 arXiv에 게시한 'Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs","tags":["Review","Mixture-of-Experts (MoE)","Large Language Models (LLMs)","Router Optimization","Manifold Regularization","Generalization","Post-training Fine-tuning","Task Embedding Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongyang Li, Ziyue Li, Tianyi Zhou 핵심 연구 목표 MoE LLM의 라우터가 최적의 라우팅 대비 1020%의 성능 격차 를 보이며, 태스크 임베딩 매니폴드와 라우팅 가중치 매니폴드 간의 misalignment로 인해 일반화 성능이 저하되는 문제를 해결하는 것을 목표로 합니다. 이를 통해 M"},{"id":"2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads","title":"[논문리뷰] SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?","excerpt":"Ofir Press이 arXiv에 게시한 'SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads","tags":["Review","소프트웨어 성능 최적화","언어 모델","저장소 수준 추론","벤치마크","실제 워크로드","코드 정확성","속도 향상","코드 최적화"],"text":"링크: 논문 PDF로 바로 열기 저자: Jeffrey J. Ma, Milad Hashemi, Amir Yazdanbakhsh, Kevin Swersky, Ofir Press, Enhui Li, Vijay Janapa Reddi, Parthasarathy Ranganathan 핵심 연구 목표 이 논문은 대규모 언어 모델(LM) 이 실제 소프트웨어 저장소 의 "},{"id":"2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization","title":"[논문리뷰] SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization","excerpt":"arXiv에 게시된 'SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization","tags":["Review","LLM","Reinforcement Learning","Soft-Thinking","Gumbel Reparameterization","Policy Optimization","Chain-of-Thought (CoT)","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhi Zheng, Wee Sun Lee 핵심 연구 목표 본 논문은 이산 토큰 ChainofThought (CoT) 추론에 효과적인 기존의 Reinforcement Learning (RL) 방법론, 특히 Group Relative Policy Optimization (GRPO) 이 연속적인 SoftThinking 패턴"},{"id":"2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence","title":"[논문리뷰] Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence","excerpt":"arXiv에 게시된 'Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence","tags":["Review","Recurrent Language Models","Pretrained Models","Model Surgery","Curriculum Learning","Test-Time Compute Scaling","Mathematics Reasoning","Efficient Training","Depth Recurrence"],"text":"링크: 논문 PDF로 바로 열기 저자: Sean McLeish, Ang Li, John Kirchenbauer, Dayal Singh Kalra, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Jonas Geiping, Tom Goldstein, Micah Goldblum 핵심 연구 목표 본 연구는 "},{"id":"2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery","title":"[논문리뷰] The Station: An Open-World Environment for AI-Driven Discovery","excerpt":"wydu이 arXiv에 게시한 'The Station: An Open-World Environment for AI-Driven Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery","tags":["Review","Multi-Agent System","Open-World Environment","Scientific Discovery","AI-Driven Research","Large Language Models","Emergent Behavior","State-of-the-Art (SOTA)"],"text":"링크: 논문 PDF로 바로 열기 저자: Stephen Chung, Wenyu Du 핵심 연구 목표 본 논문은 기존의 경직된 최적화 패러다임을 넘어선 AI 주도 자율 과학 발견을 위한 개방형 다중 에이전트 환경인 The Station 을 소개합니다. 에이전트들이 논문 읽기, 가설 수립, 코드 제출, 분석 수행 및 결과 출판을 포함하는 장기적인 과학 여정에 참"},{"id":"2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models","title":"[논문리뷰] VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models","excerpt":"arXiv에 게시된 'VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-11 00:00:00+0900+0900","permalink":"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models","tags":["Review","Video Anomaly Understanding","Large Language Models","Causal Reasoning","Relation-Aware","Keyframe Sampling","Multimodal LLMs","Scene Graphs"],"text":"링크: 논문 PDF로 바로 열기 저자: Ying Cheng¹, YuHo Lin¹, MinHung Chen², FuEn Yang², ShangHong Lai¹ 핵심 연구 목표 본 논문은 기존 비디오 이상 탐지(VAD) 방법들이 놓치던 이상 행동의 깊은 인과 관계 및 객체 간 상호작용 을 이해하는 한계를 극복하고자 합니다. 궁극적으로 비디오 내 이상 현상에 대"},{"id":"2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems","title":"[논문리뷰] Adaptive Multi-Agent Response Refinement in Conversational Systems","excerpt":"arXiv에 게시된 'Adaptive Multi-Agent Response Refinement in Conversational Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems","tags":["Review","Large Language Models","Multi-Agent Systems","Conversational AI","Response Refinement","Dynamic Agent Selection","Persona Alignment","Factual Grounding","Coherence"],"text":"링크: 논문 PDF로 바로 열기 저자: Soyeong Jeong, Aparna Elangovan, Emine Yilmaz, Oleg Rokhlenko 핵심 연구 목표 대규모 언어 모델(LLM) 기반 대화 시스템이 사용자 페르소나 정렬 및 사실적 정확도와 같은 복합적인 요구사항을 충족하지 못해 발생하는 불만족스러운 응답 문제를 해결하는 것이 목표입니다. 기존"},{"id":"2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs","title":"[논문리뷰] Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs","excerpt":"arXiv에 게시된 'Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs","tags":["Review","Multilingual Machine Translation","Large Language Models","Directional Degeneration","Strategic Downsampling","Parallel Multilingual Prompting","Chinese-centric MT","Cross-lingual Transfer","Instruction Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yingfeng Luo, Ziqiang Xu, Yuxuan Ouyang, Murun Yang, Dingyang Lin, Kaiyan Chang, Tong Zheng, Bei Li, Peinan Feng, Quan Du, Tong Xiao, Jingbo Zhu 핵심 연구 목표 본 논문은 기존 대규모 언어 모델(LLM) "},{"id":"2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces","title":"[논문리뷰] Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces","excerpt":"Vwani Roychowdhury이 arXiv에 게시한 'Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces","tags":["Review","Retrieval-Augmented Generation (RAG)","Episodic Memory","Generative Semantic Workspaces (GSW)","Large Language Models (LLMs)","Question Answering (QA)","Semantic Modeling","Knowledge Graph"],"text":"링크: 논문 PDF로 바로 열기 저자: Shreyas Rajesh, Pavan Holur, Chenda Duan, David Chong, Vwani Roychowdhury 핵심 연구 목표 본 논문은 기존 RetrievalAugmented Generation (RAG) 방법론이 긴 내러티브 내에서 분산된 정보를 다루고, 시간이 지남에 따라 진화하는 상황과 액"},{"id":"2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives","title":"[논문리뷰] BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives","excerpt":"arXiv에 게시된 'BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives","tags":["Review","Dense Retrieval","Biomedical IR","Hard Negative Mining","Citation Networks","PubMed","Zero-shot Retrieval","Transformer Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Aarush Sinha, Pavan Kumar S, Roshan Balaji, Nirav Pravinbhai Bhatt 핵심 연구 목표 본 연구는 생물의학 및 일반 도메인 정보 검색(IR) 시스템의 성능 향상을 목표로 합니다. 특히, 기존 방법론에서 어려움을 겪는 \"하드 네거티브\" 문서를 효과적으로 식별하고 활용하여,"},{"id":"2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces","title":"[논문리뷰] DynaAct: Large Language Model Reasoning with Dynamic Action Spaces","excerpt":"Lingpeng Kong이 arXiv에 게시한 'DynaAct: Large Language Model Reasoning with Dynamic Action Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces","tags":["Review","Large Language Models","Sequential Reasoning","Action Space Construction","Submodular Optimization","Markov Decision Process","Monte Carlo Tree Search","Utility-Diversity Trade-off"],"text":"링크: 논문 PDF로 바로 열기 저자: Xueliang Zhao, Wei Wu, Jian Guan, Qintong Li, Lingpeng Kong 핵심 연구 목표 본 논문의 핵심 연구 목표는 LLM(Large Language Model) 기반의 순차적 추론 과정에서 확장성과 간결성을 동시에 갖춘 최적의 액션 공간 을 자동으로 구성하는 것입니다. 기존 LLM"},{"id":"2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations","title":"[논문리뷰] Grounding Computer Use Agents on Human Demonstrations","excerpt":"arXiv에 게시된 'Grounding Computer Use Agents on Human Demonstrations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations","tags":["Review","Computer Use Agents","UI Grounding","Desktop Applications","Human Demonstrations","Large-Scale Dataset","Vision-Language Models","Supervised Fine-tuning","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Aarash Feizi, Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Kaixin Li, Rabiul Awal, Xing Han Lù, Johan ObandoCeron, Juan A. Rodriguez, Nicolas Chapados, David Vazquez, Adriana"},{"id":"2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI","title":"[논문리뷰] Intelligence per Watt: Measuring Intelligence Efficiency of Local AI","excerpt":"arXiv에 게시된 'Intelligence per Watt: Measuring Intelligence Efficiency of Local AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI","tags":["Review","Local AI","LLM Inference","Intelligence per Watt","Edge Computing","Hybrid Cloud","AI Efficiency","Hardware Benchmarking","Query Routing"],"text":"링크: 논문 PDF로 바로 열기 저자: Jonathan Safron, Lavanika Afanasiyeva, Medha Goel, Rebecca Joseph, Shlok Narayan, Heramb Kumar Ghazi, Adrian Gammaro, Laurente Jonathan Shun Zhu, Hennessy Azalia Mirhoseini, Chri"},{"id":"2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models","title":"[논문리뷰] KLASS: KL-Guided Fast Inference in Masked Diffusion Models","excerpt":"arXiv에 게시된 'KLASS: KL-Guided Fast Inference in Masked Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models","tags":["Review","Masked Diffusion Models","Fast Inference","Adaptive Sampling","KL Divergence","Confidence Score","Generative AI","Efficient Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Seo Hyun Kim, Sunwoo Hong, Hojung Jung, Youngrok Park, SeYoung Yun (KAIST AI) 핵심 연구 목표 Masked Diffusion Models (MDMs)는 다양한 생성 태스크에서 우수한 성능을 보이지만, 느리고 정적인 샘플링 속도 로 인해 추론 과정에 병목 현상"},{"id":"2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration","title":"[논문리뷰] Optimizing Diversity and Quality through Base-Aligned Model Collaboration","excerpt":"Jonathan May이 arXiv에 게시한 'Optimizing Diversity and Quality through Base-Aligned Model Collaboration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration","tags":["Review","Large Language Models","Generative AI","Diversity-Quality Trade-off","Model Collaboration","Inference Optimization","Routing Strategy","Text Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yichen Wang, Chenghao Yang, Tenghao Huang, Muhao Chen, Jonathan May 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)에서 다양성(diversity) 과 품질(quality) 간의 본질적인 트레이드오프 문제를 해결하는 것을 목표로 합니다. 특히, 정렬된 모델이 높"},{"id":"2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals","title":"[논문리뷰] The Path Not Taken: RLVR Provably Learns Off the Principals","excerpt":"arXiv에 게시된 'The Path Not Taken: RLVR Provably Learns Off the Principals' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals","tags":["Review","Reinforcement Learning","Large Language Models","Parameter-Efficient Fine-Tuning","Optimization Bias","Spectral Geometry","Model Sparsity","LoRA"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanqing Zhu, Zhenyu Zhang, Hanxian Huang, DiJia Su, Zechun Liu, Jiawei Zhao, Igor Fedorov, Hamed Pirsiavash, Zhizhou Sha, Jinwon Lee, David Z. Pan, Zhangyang Wang, Yuandong Tian,"},{"id":"2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning","title":"[논문리뷰] TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning","excerpt":"arXiv에 게시된 'TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning","tags":["Review","Long-form Video Understanding","Temporal Search","Reinforcement Learning","Self-Verification","Video-Language Models","Adaptive Search","Interleaved Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Junwen Pan, Qizhe Zhang, Rui Zhang, Ming Lu, Xin Wan, Yuan Zhang, Chang Liu, Qi She 핵심 연구 목표 본 논문은 수만 개의 프레임에서 관련 정보를 식별해야 하는 긴 형식 비디오 이해 태스크에서, 기존의 수동으로 고안된 검색 전략이 최적의 검색 전략 학습을"},{"id":"2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B","title":"[논문리뷰] Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B","excerpt":"arXiv에 게시된 'Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B","tags":["Review","Small Language Models","Reasoning","Diversity Optimization","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Spectrum-to-Signal Principle (SSP)","Mathematical Reasoning","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Sen Xu, Yi Zhou, Wei Wang, Jixin Min, Zhibin Yin, Yingwei Dai, Shixi Liu, Lianyu Pang, Yirong Chen, Junlin Zhang 핵심 연구 목표 이 논문은 소규모 모델이 강력한 추론 능력을 갖추기 어렵다는 기존의 통념에 도전하고, 1.5B 파라미"},{"id":"2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","title":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning","excerpt":"arXiv에 게시된 'VideoSSR: Video Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","tags":["Review","Video Understanding","Self-Supervised Learning","Reinforcement Learning","MLLMs","Pretext Tasks","Verifiable Rewards","Data Generation","Temporal Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, Yu Cheng 핵심 연구 목표 본 연구는 Multimodal Large Language Models (MLLMs)의 비디오 이해 능력을 향상시키기 위해, 기존 비디오 데이터셋의 높은 주석 비용, 복잡성 부족, 그리"},{"id":"2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective","title":"[논문리뷰] Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective","excerpt":"Christoph Treude이 arXiv에 게시한 'Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective","tags":["Review","Large Language Models","Software Engineering","Developer Productivity","Socio-Technical Grounded Theory","Practitioner Insights","AI Adoption","Benefits and Risks","Balanced Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Samuel Ferino, Rashina Hoda, John Grundy, Christoph Treude 핵심 연구 목표 본 연구는 대규모 언어 모델(LLMs)이 소프트웨어 개발에 미치는 영향에 대해 실무자 관점에서 심층적으로 탐구하고, LLMs 사용에 따른 긍정적(전진) 및 부정적(후퇴) 효과를 균형 있게 관리하는 "},{"id":"2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora","title":"[논문리뷰] Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora","excerpt":"Mohamed Motasim Hamed이 arXiv에 게시한 'Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-12 00:00:00+0900+0900","permalink":"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora","tags":["Review","Arabic Language","Multimodal Corpus","Data Curation","Web Scraping","Large Language Models","Document Structure","Markdown","Perplexity Filtering"],"text":"링크: 논문 PDF로 바로 열기 저자: Khalil Hennara, Ahmad Bastati, Muhammad Hreden, Mohamed Motasim Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan 핵심 연구 목표 본 연구는 고품질의 구조화된 아랍어 다중모드 데이터셋의 부족 문제를 해결하는 것을 목표로 합"},{"id":"2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision","title":"[논문리뷰] Adapting Web Agents with Synthetic Supervision","excerpt":"Siwei Han이 arXiv에 게시한 'Adapting Web Agents with Synthetic Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision","tags":["Review","Web Agents","Synthetic Data Generation","LLM","Task Refinement","Trajectory Refinement","Supervised Fine-tuning","Web Automation","Environment Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoyang Wang, Yiming Liang, Xuchao Zhang, Qianhui Wu, Siwei Han, Anson Bastos, Rujia Wang, Chetan Bansal, Baolin Peng, Jianfeng Gao, Saravan Rajmohan, Huaxiu Yao 핵심 연구 목표 웹 에이전트"},{"id":"2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents","title":"[논문리뷰] Agentic Refactoring: An Empirical Study of AI Coding Agents","excerpt":"Hajimu Iida이 arXiv에 게시한 'Agentic Refactoring: An Empirical Study of AI Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents","tags":["Review","AI Agents","Code Refactoring","Software Engineering","Empirical Study","Large Language Models","Code Quality","Agentic Software Development","Maintainability"],"text":"링크: 논문 PDF로 바로 열기 저자: Kosei Horikawa, Hao Li, Yutaro Kashiwa, Bram Adams, Hajimu Iida, Ahmed E. Hassan 핵심 연구 목표 이 연구는 AI 코딩 에이전트가 소프트웨어 개발에서 리팩토링 활동을 어떻게 수행하고, 그 유형과 목적은 무엇이며, 코드 품질에 어떤 영향을 미치는지에 대한 실"},{"id":"2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls","title":"[논문리뷰] LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls","excerpt":"arXiv에 게시된 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls","tags":["Review","Large Language Models (LLMs)","Tool Learning","Data Generation","Model Training","Closed-Loop Framework","Reinforcement Learning (RL)","Data Refinement","Self-Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Kangning Zhang, Wenxiang Jiao, Kounianhua Du, Yuan Lu, Weiwen Liu, Weinan Zhang, Yong Yu 핵심 연구 목표 기존 LLM 툴 학습의 정적 합성 데이터 파이프라인 이 모델의 약점에 적응하지 못하고 노이즈 있는 레이블을 유지하여 훈련 효율성을 저해하는 문제"},{"id":"2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds","title":"[논문리뷰] Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds","excerpt":"arXiv에 게시된 'Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds","tags":["Review","Generalist Agent","3D Open World","Vision-Language Model","Imitation Learning","Real-time Inference","Hybrid Thinking","Action Chunking","Genshin Impact"],"text":"링크: 논문 PDF로 바로 열기 저자: Weihao Tan, Xiangyang Li, Yunhao Fang, Heyuan Yao, Shi Yan, Hao Luo, Tenglong Ao, Huihui Li, Hongbin Ren, Bairen Yi, Yujia Qin, Bo An, Libin Liu, Guang Shi 핵심 연구 목표 논문은 복잡한 3D 오픈"},{"id":"2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra","title":"[논문리뷰] MADD: Multi-Agent Drug Discovery Orchestra","excerpt":"arXiv에 게시된 'MADD: Multi-Agent Drug Discovery Orchestra' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra","tags":["Review","Multi-Agent System","Drug Discovery","LLM","Hit Identification","Virtual Screening","Generative AI","Property Prediction","Automated Machine Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Gleb V. Solovev, Alina B. Zhidkovskaya, Anastasia Orlova, Nina Gubina, Anastasia Vepreva, Rodion Golovinskii, Ilya Tonkii, Ivan Dubrovsky, Ivan Gurev, Dmitry Gilemkhanov, Denis C"},{"id":"2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning","title":"[논문리뷰] MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning","excerpt":"arXiv에 게시된 'MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning","tags":["Review","Multimodal Reasoning","Mathematical Problem Solving","Self-Evolving","Iterative Fine-Tuning","Reward Models","Reflection","Large Language Models (LLMs)"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinhao Chen, Zhen Yang, Jianxin Shi, Tianyu Wo, Jie Tang 핵심 연구 목표 본 연구는 멀티모달 대규모 언어 모델(MLLM)이 복잡한 수학 문제 해결과 같은 추론 태스크에서 겪는 어려움을 극복하는 것을 목표로 합니다. 특히, 기존의 정적인 교사 모델 유래 데이터셋에 의존하는 방"},{"id":"2025-11-13-Motif-2-12-7B-technical-report","title":"[논문리뷰] Motif 2 12.7B technical report","excerpt":"arXiv에 게시된 'Motif 2 12.7B technical report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-Motif-2-12-7B-technical-report","tags":["Review","Large Language Model","LLM Efficiency","Grouped Differential Attention","Kernel Fusion","Parallel Muon","Supervised Fine-tuning","Architectural Scaling","Instruction Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Changjin Kang, Beomgyu Kim, Bokki Ryu, Dahye Choi, Dongjoo Weon, Dongpin Oh, Dongseok Kim, Eunhwan Park, Haesol Lee, Hanbin Jung, Hyunbyung Park, Hyukjin Kweon, Jaeheui Her, Jaey"},{"id":"2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle","title":"[논문리뷰] Stemming Hallucination in Language Models Using a Licensing Oracle","excerpt":"Richard Ackermann이 arXiv에 게시한 'Stemming Hallucination in Language Models Using a Licensing Oracle' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle","tags":["Review","Hallucination Mitigation","Language Models","Knowledge Graphs","SHACL Validation","Epistemic Grounding","Retrieval-Augmented Generation","Neuro-symbolic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Richard Ackermann, Simeon Emanuilov 핵심 연구 목표 언어 모델(LMs)의 고질적인 환각(hallucination) 문제, 즉 사실과 다른 정보를 유창하게 생성하는 문제를 해결하는 것이 목표입니다. 통계적 학습 방식의 한계를 극복하고, 구조화된 지식에 대한 결정론적인 진실성 검증 메커니즘 을"},{"id":"2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression","title":"[논문리뷰] TiDAR: Think in Diffusion, Talk in Autoregression","excerpt":"arXiv에 게시된 'TiDAR: Think in Diffusion, Talk in Autoregression' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression","tags":["Review","Hybrid LLM Architecture","Diffusion-Autoregressive","Parallel Token Generation","Speculative Decoding","Structured Attention Masks","LLM Inference Acceleration","KV Cache"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingyu Liu, Xin Dong, Zhifan Ye, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang, Pavlo Molchanov 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 생성 과정에서 확산 모델(Diffusion Models) 의 빠"},{"id":"2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance","title":"[논문리뷰] Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance","excerpt":"Kwanyoung Kim이 arXiv에 게시한 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance","tags":["Review","Diffusion Models","Guidance Sampling","Optimal Transport","Sinkhorn Algorithm","Self-Attention","Adversarial Perturbation","Image Generation","ControlNet"],"text":"링크: 논문 PDF로 바로 열기 저자: Kwanyoung Kim 핵심 연구 목표 이 논문은 확산 모델의 샘플링 과정에서 발생하는 품질 및 제어 가능성 문제를 해결하고자 합니다. 특히, 기존의 휴리스틱 기반 가이드라인 방법론(예: ClassifierFree Guidance, PAG, SEG )이 지닌 이론적 근거 부족과 잠재적 성능 저하 문제를 극복하고, 이"},{"id":"2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models","title":"[논문리뷰] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models","excerpt":"arXiv에 게시된 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models","tags":["Review","Vision-Language-Action (VLA)","Reinforcement Learning (RL)","Model-based RL","World Models","Policy Optimization","Robotics","Sample Efficiency","Self-correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo 핵심 연구 목표 VLA 모델이 로봇 조작에 큰 잠재력을 보이지만, 전문가 데모에 의존하여 실패로부터 학습하고 스스로 수정하는 능력이 제한적이라는 문제를 해결하고자 합니다. 본 연구는 실제 로"},{"id":"2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation","title":"[논문리뷰] WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation","excerpt":"arXiv에 게시된 'WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-13 00:00:00+0900+0900","permalink":"/ai/review/2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation","tags":["Review","UI-to-Code","Vision-Language Models","Agentic Framework","Interactive UI","Web Automation","Code Generation","UI Verification","Supervised Fine-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingde Xu, Zhen Yang, Wenyi Hong, Lihang Pan, Xinyue Fan, Yan Wang, Xiaotao Gu, Bin Xu, Jie Tang 핵심 연구 목표 본 논문은 기존 VisionLanguage Models (VLMs) 기반의 UItoCode 접근 방식이 정적인 HTML/CSS 코"},{"id":"2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models","title":"[논문리뷰] AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models","excerpt":"Zhen Li이 arXiv에 게시한 'AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models","tags":["Review","3D Embodied Reasoning","Multimodal Large Language Models (MLLMs)","Chain-of-Thought (CoT)","Affordance Grounding","Motion Estimation","View Synthesis","Active Perception"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyi Wang, Xun Yang, Yanlong Xu, Yuchen Wu, Zhen Li, Na Zhao 핵심 연구 목표 본 논문은 3D 환경에서 자연어 명령을 기반으로 물체의 상호작용 가능한 요소(affordance elements)를 식별하고, 해당 요소의 3D 마스크 , 동작 유형 , 동작 축 방향 을 포함"},{"id":"2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation","title":"[논문리뷰] Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation","excerpt":"arXiv에 게시된 'Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation","tags":["Review","Text-to-Image Models","Diversity Evaluation","Human Evaluation","Attribute-Conditional","Vendi Score","Generative AI","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Isabela Albuquerque¹, Ira Ktena², Olivia Wiles¹, Ivana Kajić¹, Amal RannenTriki¹, Cristina Vasconcelos¹ and Aida Nematzadeh¹ 핵심 연구 목표 현재 텍스트투이미지(T2I) 모델이 종종 동질적인 이미지를 생성하며 다양성이 부"},{"id":"2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models","title":"[논문리뷰] Black-Box On-Policy Distillation of Large Language Models","excerpt":"arXiv에 게시된 'Black-Box On-Policy Distillation of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models","tags":["Review","Large Language Models (LLMs)","Knowledge Distillation (KD)","Black-box Distillation","Generative Adversarial Networks (GANs)","On-policy Learning","Reinforcement Learning","Minimax Game","Model Compression"],"text":"링크: 논문 PDF로 바로 열기 저자: Zewen Chi, Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei 핵심 연구 목표 본 논문은 내부 로짓이나 파라미터에 접근할 수 없는 블랙박스(blackbox) 대규모 언어 모델(LLM) 을 대상으로, 학생 모델이 교사 모델의 텍스트 출력만을 학습하는 온정책(onpoli"},{"id":"2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis","title":"[논문리뷰] CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis","excerpt":"Jian Wu이 arXiv에 게시한 'CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis","tags":["Review","Citation Contexts","Reproducibility","Sentiment Analysis","Large Language Models","Crowdsourcing","Dataset","Machine Learning","Science of Science"],"text":"링크: 논문 PDF로 바로 열기 저자: Rochana R. Obadage, Sarah Rajtmajer, Jian Wu 핵심 연구 목표 본 논문은 AI/ML 논문 내 인용 문맥에서 재현성(reproducibility) 지향 감성을 식별하기 위한 CC30k 데이터셋 을 구축하는 것을 목표로 합니다. 이는 계산적 재현성 연구를 위한 자원 부족 문제를 해결하고,"},{"id":"2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views","title":"[논문리뷰] Depth Anything 3: Recovering the Visual Space from Any Views","excerpt":"arXiv에 게시된 'Depth Anything 3: Recovering the Visual Space from Any Views' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views","tags":["Review","Depth Estimation","Multi-view Geometry","Transformer Architecture","Teacher-Student Learning","Pose Estimation","3D Reconstruction","Novel View Synthesis","Visual Space Recovery"],"text":"링크: 논문 PDF로 바로 열기 저자: Haotong Lin, Sili Chen, Jun Hao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang (ByteDance Seed) 핵심 연구 목표 논문은 단일 이미지, 다중 뷰 또는 비디오 스트림과 같은 임의의 시각 입력 으로부터 공간적으로"},{"id":"2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO","title":"[논문리뷰] Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO","excerpt":"arXiv에 게시된 'Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO","tags":["Review","Decentralized RL","GRPO","LLM Post-training","Adversarial Attacks","Data Poisoning","Defense Mechanisms","In-context Attack","Out-of-context Attack"],"text":"링크: 논문 PDF로 바로 열기 저자: Nikolay Blagoev, Oğuzhan Ersoy, Lydia Yiyu Chen 핵심 연구 목표 이 논문은 Large Language Models (LLMs) 의 후처리 훈련에 사용되는 분산형 Group Relative Policy Optimization (GRPO) 시스템의 보안 취약점을 탐구합니다. 분산형 환"},{"id":"2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique","title":"[논문리뷰] MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique","excerpt":"arXiv에 게시된 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique","tags":["Review","LMMs","Multimodal Critique","Benchmark","Evaluation","Reward Model","GPT-4o","Scaling Law"],"text":"링크: 논문 PDF로 바로 열기 저자: Gailun Zeng, Ziyang Luo, Hongzhan Lin, Yuchen Tian, Kaixin Li, Ziyang Gong, Jianxiong Guo, Jing Ma 핵심 연구 목표 본 논문은 대규모 멀티모달 모델(LMMs) 의 멀티모달 비판 능력에 대한 포괄적이고 신뢰성 있는 평가의 필요성을 제기하며, L"},{"id":"2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples","title":"[논문리뷰] MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples","excerpt":"arXiv에 게시된 'MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples","tags":["Review","Zero-Shot Learning","Anomaly Detection","Anomaly Segmentation","Multimodal","Industrial Inspection","Mutual Scoring","Unsupervised Learning","Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiurui Li, Feng Xue, Member, IEEE, and Yu Zhou, Member, IEEE 핵심 연구 목표 이 논문은 훈련 데이터셋의 라벨링 없이 산업 제품의 2D 이미지와 3D 포인트 클라우드에서 제로샷(zeroshot) 이상 분류(AC) 및 세분화(AS) 를 수행하는 것을 목표로 합니다. 기존 지"},{"id":"2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models","title":"[논문리뷰] Music Flamingo: Scaling Music Understanding in Audio Language Models","excerpt":"arXiv에 게시된 'Music Flamingo: Scaling Music Understanding in Audio Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models","tags":["Review","Audio Language Models","Music Understanding","Chain-of-Thought","Reinforcement Learning","Data Curation","Multimodal AI","Music Information Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Sreyan Ghosh, Arushi Goel, Lasha Koroshinadze, Sanggil Lee, Zhifeng Kong, Joao Felipe Santos, Ramani Duraiswami, Dinesh Manocha, Wei Ping, Mohammad Shoeybi, Bryan Catanzaro 핵심 연구"},{"id":"2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models","title":"[논문리뷰] One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models","excerpt":"Ilya Makarov이 arXiv에 게시한 'One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models","tags":["Review","Latent Diffusion Models","Super-Resolution","Upscaling Adapter","Image Generation","Latent Space","Multi-scale Learning","Cross-VAE"],"text":"링크: 논문 PDF로 바로 열기 저자: Aleksandr Razin, Kazantsev Danil, Ilya Makarov 핵심 연구 목표 본 논문은 기존 확산 모델이 고해상도 이미지를 직접 샘플링할 때 발생하는 속도 저하, 비용 증가, 아티팩트 발생 문제를 해결하고, 사후 픽셀 공간 초해상도(SR) 방식의 추가 지연 및 아티팩트를 극복하는 것을 목표로 합"},{"id":"2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents","title":"[논문리뷰] ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents","excerpt":"arXiv에 게시된 'ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents","tags":["Review","Deep Research Agents","LLM Evaluation","Benchmark","Rubrics","Multi-step Reasoning","Cross-document Synthesis","AI Performance","Task Complexity"],"text":"링크: 논문 PDF로 바로 열기 저자: Manasi Sharma, Chen Bo Calvin Zhang, Chaithanya Bandi, Clinton Wang, Ankit Aich, Huy Nghiem, Tahseen Rabbani, Ye Htet, Brian Jang, Sumana Basu, Aishwarya Balwani, Denis Peskoff, "},{"id":"2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following","title":"[논문리뷰] Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following","excerpt":"Karishma Mandyam이 arXiv에 게시한 'Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following","tags":["Review","LLM","Instruction Following","Reinforcement Learning","Rubric-based Evaluation","Benchmarking","Reward Shaping","Rubric Verifier","AdvancedIF"],"text":"링크: 논문 PDF로 바로 열기 저자: Yun He, Wenzhe Li, Hejia Zhang, Songlin Li, Karishma Mandyam, Sopan Khosla, Yuanhao Xiong, Nanshu Wang, Selina Peng, Beibin Li, Shengjie Bi, Shishir G. Patil, Qi Qi, Shengyu Feng"},{"id":"2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control","title":"[논문리뷰] SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control","excerpt":"Ryan Rossi이 arXiv에 게시한 'SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control","tags":["Review","Image Editing","Continuous Control","Fine-Grained Control","Instruction-based","Low-Rank Adaptation","Disentanglement","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Arman Zarei, Samyadeep Basu, Mobina Pournemat, Sayan Nag, Ryan Rossi, Soheil Feizi 핵심 연구 목표 기존 instructionbased image editing 모델들이 고정된 강도로 편집을 적용하여 개별 편집에 대한 정밀하고 연속적인 제어가 불가능하다는"},{"id":"2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training","title":"[논문리뷰] Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training","excerpt":"suayptalha이 arXiv에 게시한 'Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training","tags":["Review","Quantum Computing","Optimization","Machine Learning","Transformers","Gradient Descent","Superposition","Large Language Models","Hybrid Quantum-Classical"],"text":"링크: 논문 PDF로 바로 열기 저자: Ahmet Erdem Pamuk, Emir Kaan Özdemir, Şuayp Talha Kocabay 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM) 훈련 시 고차원, 비볼록(nonconvex) 손실 함수 공간에서 기존 경사 하강법(Gradient Descent) 의 한계(지역 최적해 수렴, 느린 수렴 속도)를"},{"id":"2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist","title":"[논문리뷰] UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist","excerpt":"arXiv에 게시된 'UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-14 00:00:00+0900+0900","permalink":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist","tags":["Review","Video Agents","Multi-modal AI","Plan-Act Architecture","Tool-Use","Long-horizon Reasoning","Open-source","Video Generation","Video Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengyang Liang, Daoan Zhang, Huichi Zhou, Rui Huang, Bobo Li, Yuechen Zhang, Shengqiong Wu, Xiaohan Wang, Jiebo Luo, Lizi Liao, Hao Fei 핵심 연구 목표 본 논문은 전문화된 비디오 AI 모델과 실제 비디오 워크플"},{"id":"2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems","title":"[논문리뷰] A Meta-Heuristic Load Balancer for Cloud Computing Systems","excerpt":"Vladimir Getov이 arXiv에 게시한 'A Meta-Heuristic Load Balancer for Cloud Computing Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems","tags":["Review","Cloud Computing","Load Balancing","Meta-Heuristic","Genetic Algorithm","Simulated Annealing","Tabu Search","Resource Management","Service Migration"],"text":"링크: 논문 PDF로 바로 열기 저자: Leszek Sliwko, Vladimir Getov 핵심 연구 목표 클라우드 시스템에서 노드 과부하를 방지하고 시스템 안정성을 유지하며 최소 비용으로 서비스를 할당하는 전략을 개발하는 것이 목표입니다. 특히, 다양한 유형의 자원 활용 및 서비스 마이그레이션 비용을 고려한 추상적인 클라우드 자원 모델을 제시하고 이를 "},{"id":"2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios","title":"[논문리뷰] CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios","excerpt":"Juyoung Oh이 arXiv에 게시한 'CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios","tags":["Review","Cooperative Perception","Vehicle-to-Vehicle (V2V)","Autonomous Driving","Dataset","Adverse Traffic Scenarios","Sensor Fusion","Temporal Alignment","3D Bounding Box Annotation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hangyu Li, Bofeng Cao, Zhaohui Liang, Wuzhen Li, Juyoung Oh, et al. 핵심 연구 목표 본 논문은 기존 V2V 협력 인지 데이터셋이 주로 일반적인 교통 시나리오에 초점을 맞추어 Complex Adverse Traffic Scenarios (CATS) 하에서의 협력 인지"},{"id":"2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains","title":"[논문리뷰] DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains","excerpt":"arXiv에 게시된 'DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains","tags":["Review","Discourse-Level Translation","Expert Domains","Benchmarking","LLM Evaluation","Reference-Free Metric","Chinese-English Translation","Contextual Coherence","Domain-Specific Terminology"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiying Zhao, Zhoufutu Wen et al. 핵심 연구 목표 본 논문은 전문 도메인에서 담화 수준 번역의 평가가 불충분하다는 문제를 해결하고자 합니다. 기존 벤치마크들이 문장 수준의 정확성과 유창성에 초점을 맞춰 담화 일관성, 엄격한 용어 정밀도, 전문가 스타일 표준을 평가하는 데 한계가 있음을 지적합니"},{"id":"2025-11-17-DoPE-Denoising-Rotary-Position-Embedding","title":"[논문리뷰] DoPE: Denoising Rotary Position Embedding","excerpt":"Min Yang이 arXiv에 게시한 'DoPE: Denoising Rotary Position Embedding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding","tags":["Review","Rotary Position Embedding","Transformer","Length Extrapolation","Attention Sink","Matrix Entropy","Denoising","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jing Xiong, Liyang Fan, Hui Shen, Zunhai Su, Min Yang, Lingpeng Kong, Ngai Wong 핵심 연구 목표 본 논문은 Transformer 모델 내 Rotary Position Embedding (RoPE) 의 내재된 한계로 인해 발생하는 길이 외삽 능력 약화와 at"},{"id":"2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding","title":"[논문리뷰] Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding","excerpt":"arXiv에 게시된 'Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding","tags":["Review","Generative Recommenders","Human Priors","Multi-head Decoding","Disentangled Representation Learning","Sequential Recommendation","Adapter Networks","Hierarchical Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunkai Zhang, Qiang Zhang, Feng (Ryan) Lin, Ruizhong Qiu, Hanchao Yu, Jiayi (Jason) Liu, Yinglong Xia, Zhuoran Yu, Zeyu Zheng, Diji Yang 핵심 연구 목표 본 논문은 추천 시스템이 정확도를 넘어선 다양성, 참신성,"},{"id":"2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","title":"[논문리뷰] EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation","excerpt":"Zeyu Wang이 arXiv에 게시한 'EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","tags":["Review","Multimodal Dataset","Emotion Recognition","Video Generation","Affective Computing","Stylized Media","Diffusion Models","Video Understanding","Text-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Zongyang Qiu, Bingyuan Wang, Xingbei Chen, Yingqing He, Zeyu Wang 핵심 연구 목표 기존 비디오 생성 시스템이 감성적 차원을 소홀히 다루고 특히 스타일화되거나 비현실적인 콘텐츠에서 감정 이해와 생성 간의 격차가 크다는 문제를 해결하고자 합니다. 감정 중심의 비디오 이해"},{"id":"2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies","title":"[논문리뷰] Experience-Guided Adaptation of Inference-Time Reasoning Strategies","excerpt":"arXiv에 게시된 'Experience-Guided Adaptation of Inference-Time Reasoning Strategies' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies","tags":["Review","Adaptive AI","Inference-Time Adaptation","Reasoning Strategies","Meta-Learning","LLM-based Agents","Dynamic Strategy Generation","Continual Learning","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Adam Stein, Matthew Trager, Benjamin Bowman, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto 핵심 연구 목표 본 논문은 에이전트형 AI 시스템이 훈련 후 추론 시 상호작용을 기반으로 문제 해결 방식을 적응시키는 근본적"},{"id":"2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models","title":"[논문리뷰] From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models","excerpt":"arXiv에 게시된 'From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models","tags":["Review","Tool-augmented LLMs","Reasoning Hallucinations","Tool-Induced Myopia (TIM)","Code Interpreter","Mathematical Reasoning","LLM Evaluation","Preference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Farima Fatahi Bayat, Pouya Pezeshkpour, Estevam Hruschka 핵심 연구 목표 본 연구는 도구 증강 언어 모델(TaLMs) 이 외부 도구를 사용할 때 발생하는 추론 환각(reasoning hallucinations) 의 새로운 유형인 ToolInduced Myopia (TIM) "},{"id":"2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models","title":"[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models","excerpt":"Siyuan Li이 arXiv에 게시한 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models","tags":["Review","Multimodal AI","Generative Reasoning","Geometric Construction","Benchmark","GeoGebra","Code-based Evaluation","Unified Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingxuan Wei, Caijun Jia, Xi Bai, Xinglong Xu, Siyuan Li, Linzhuang Sun, Bihui Yu, Conghui He, Lijun Wu, Cheng Tan 핵심 연구 목표 본 논문은 통합 멀티모달 모델(UMMs)의 생성적 추론 능력 을 평가하기 위한 벤치마크 개발을 목"},{"id":"2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant","title":"[논문리뷰] HI-TransPA: Hearing Impairments Translation Personal Assistant","excerpt":"arXiv에 게시된 'HI-TransPA: Hearing Impairments Translation Personal Assistant' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant","tags":["Review","Multimodal AI","Hearing Impairment","Audio-Visual Speech Recognition","Curriculum Learning","Omni-Models","Assistive Technology","Lip Reading","Speech Translation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiming Ma, Shiyu Gan, Junhao Zhao, Xianming Li, Qingyun Pan, Peidong Wang, Mingjun Pan, Yuhao Mo, Jiajie Cheng, Chengxin Chen, Zhonglun Cao, Chonghan Liu, Shi Cheng 핵심 연구 목표 본 논"},{"id":"2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey","title":"[논문리뷰] Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey","excerpt":"Mohammad Hossein Rohban이 arXiv에 게시한 'Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey","tags":["Review","Large Language Models","Scientific Discovery","Idea Generation","Creativity","Survey","AI in Science","Prompt Engineering","Multi-agent Systems","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Fatemeh Shahhosseini, Arash Marioriyad, Ali Momen, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban, Shaghayegh Haghjooy Javanmard 핵심 연구 목표 본 설문조사는 대규모 언어 모델(LLM) 을 활용한 과학적 아이"},{"id":"2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers","title":"[논문리뷰] LiteAttention: A Temporal Sparse Attention for Diffusion Transformers","excerpt":"arXiv에 게시된 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers","tags":["Review","Diffusion Transformers","Sparse Attention","Temporal Coherence","Video Generation","Computational Efficiency","FlashAttention","CUDA Kernels"],"text":"링크: 논문 PDF로 바로 열기 저자: Dor Shmilovich, Tony Wu, Aviad Dahan, Yuval Domb 핵심 연구 목표 본 논문은 비디오 생성 Diffusion Transformers (DiT)의 Quadratic attention complexity 로 인한 과도한 지연 시간 문제를 해결하고자 합니다. 기존의 동적(dynamic) "},{"id":"2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism","title":"[논문리뷰] MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism","excerpt":"arXiv에 게시된 'MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism","tags":["Review","Multi-Agent Systems","Reinforcement Learning","LLMs","Pipeline Parallelism","Reasoning","Reward Shaping","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Shulin Liu, Dong Du, Tao Yang, Yang Li, Boyu Qiu 핵심 연구 목표 대규모 언어 모델(LLMs) 기반 멀티 에이전트 추론 시스템이 보상 잡음(reward noise) 과 훈련 비효율성 으로 인해 오픈 소스 모델에 일반화되기 어려운 문제를 해결하는 것이 목표입니다. Solver, Ve"},{"id":"2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap","title":"[논문리뷰] Simulating the Visual World with Artificial Intelligence: A Roadmap","excerpt":"Pengfei Wan이 arXiv에 게시한 'Simulating the Visual World with Artificial Intelligence: A Roadmap' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap","tags":["Review","World Models","Video Generation","AI Simulation","Generative AI","Physical Plausibility","Interactive AI","Planning","Roadmap"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu 핵심 연구 목표 본 논문은 비디오 생성 모델이 포괄적인 물리적 세계 모델(Physical World Model) 로 진화하는 과정을 체계적으로 조망하고 로드맵을 제시하는 것을 목표로 합"},{"id":"2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation","title":"[논문리뷰] UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation","excerpt":"Weihan Wang이 arXiv에 게시한 'UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation","tags":["Review","Visual Language Model","UI-to-Code Generation","Interactive UI","UI Editing","UI Polishing","Reinforcement Learning","Multimodal Coding","Test-Time Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhen Yang¹, Wenyi Hong¹, Mingde Xu², Xinyue Fan², Weihan Wang², Jiele Cheng¹, Xiaotao Gu², Jie Tang¹† ¹Department of Computer Science and Technology, Tsinghua University ²Zhipu A"},{"id":"2025-11-17-Virtual-Width-Networks","title":"[논문리뷰] Virtual Width Networks","excerpt":"arXiv에 게시된 'Virtual Width Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-Virtual-Width-Networks","tags":["Review","Virtual Width Networks","Transformer","Mixture-of-Experts (MoE)","Scaling Laws","Representation Learning","Model Efficiency","Multi-Token Prediction","Hyper-Connections"],"text":"링크: 논문 PDF로 바로 열기 저자: 1ByteDance Seed (전체 저자 목록은 Contribution 섹션 참조) 핵심 연구 목표 본 논문은 Transformer 모델의 히든 차원을 늘릴 때 발생하는 Quadratic한 계산 비용 문제를 해결하면서도, 더 넓은 표현(wider representations)이 제공하는 이점을 얻는 것을 목표로 합니다"},{"id":"2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences","title":"[논문리뷰] Workload Schedulers -- Genesis, Algorithms and Differences","excerpt":"Vladimir Getov이 arXiv에 게시한 'Workload Schedulers -- Genesis, Algorithms and Differences' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences","tags":["Review","Workload Scheduling","Process Scheduling","Job Scheduling","Big Data Processing","Resource Management","Distributed Systems","Scheduling Algorithms","Performance Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Leszek Sliwko, Vladimir Getov 핵심 연구 목표 본 논문은 현대의 워크로드 스케줄러를 운영체제 프로세스 스케줄러 , 클러스터 시스템 잡 스케줄러 , 빅 데이터 스케줄러 의 세 가지 범주로 분류하고, 각 클래스의 진화 과정, 사용되는 알고리즘, 주요 특징 및 차이점을 분석하는 것을 목표로 합니다. "},{"id":"2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward","title":"[논문리뷰] miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward","excerpt":"Farzan Farnia이 arXiv에 게시한 'miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-17 00:00:00+0900+0900","permalink":"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward","tags":["Review","Automated Theorem Proving","Autoformalization","Benchmark Dataset","miniF2F","Lean Language","Large Language Models","Mathematical Reasoning","Formal Verification"],"text":"링크: 논문 PDF로 바로 열기 저자: Azim Ospanov, Farzan Farnia, Roozbeh Yousefzadeh 핵심 연구 목표 본 연구는 AI 시스템이 수학 올림피아드 문제에 참여하는 시나리오에서 miniF2F 벤치마크 의 비공식 및 공식 진술 간의 불일치와 오류를 분석하고 해결하는 것을 목표로 합니다. 기존 벤치마크의 한계를 극복하고 자동"},{"id":"2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain","title":"[논문리뷰] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain","excerpt":"Meng Jiang이 arXiv에 게시한 'A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain","tags":["Review","Decentralized RAG","Blockchain","Smart Contracts","Source Reliability","Large Language Models","Retrieval Augmented Generation","Trustworthy AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yining Lu, Wenyi Tang, Max Johnson, Taeho Jung, Meng Jiang 핵심 연구 목표 기존 중앙 집중식 RAG(Retrieval Augmented Generation) 시스템의 높은 데이터 관리 비용과 개인 정보 보호 문제를 해결하고자 합니다. 특히, 분산화된 환경에서 발생하는 다양"},{"id":"2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing","title":"[논문리뷰] AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing","excerpt":"Hongyu Lin이 arXiv에 게시한 'AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing","tags":["Review","Telemarketing","Large Language Models","Persuasive Dialogue","Reinforcement Learning","Bayesian Optimization","Dynamic Prompting","Dialogue Systems"],"text":"링크: 논문 PDF로 바로 열기 저자: Qingyu Zhang, Chunlei Xin, Xuanang Chen, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Qing Ye, Qianlong Xie, Xingxing Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 겪는 전략적 취약성, 사실적 환각, 맞춤화 부"},{"id":"2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing","title":"[논문리뷰] Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing","excerpt":"arXiv에 게시된 'Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing","tags":["Review","Serendipity Discovery","Knowledge Graphs","Drug Repurposing","LLMs","KGQA","RNS Metric","Biomedical AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengying Wang, Chenhui Ma, Ao Jiao, Tuo Liang, Pengjun Lu, Shrinidhi Hegde, Yu Yin, Evren GurkanCavusoglu, Yinghui Wu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 지식 그래프(KG)에서 예측 가능하고 관련성 높은 답"},{"id":"2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners","title":"[논문리뷰] Genomic Next-Token Predictors are In-Context Learners","excerpt":"arXiv에 게시된 'Genomic Next-Token Predictors are In-Context Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners","tags":["Review","In-Context Learning (ICL)","Genomic Sequences","Next-Token Prediction","Large Language Models (LLMs)","Modality-Agnostic AI","Meta-Learning","Bitstring Program Synthesis","Evo2"],"text":"링크: 논문 PDF로 바로 열기 저자: Nathan Breslow, Aayush Mishra, Mahler Revsine, Michael C. Schatz, Anqi Liu, Daniel Khashabi 핵심 연구 목표 본 연구는 인컨텍스트 학습(ICL)이 인간 언어에 고유한 현상인지, 아니면 대규모 예측 훈련을 통해 다른 시퀀스 도메인에서도 유기적으로 나"},{"id":"2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly","title":"[논문리뷰] Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?","excerpt":"Lingming Zhang이 arXiv에 게시한 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly","tags":["Review","Software Engineering Agents","LLM Agents","Self-Evolution","On-the-Fly Learning","Tool Creation","SWE-bench","Autonomous Systems","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 LIVESWEAGENT: Can Software Engineering Agents SelfEvolve on the Fly? 저자: Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, Lingming Zhang 핵심 연구 목표 이 논문은 기존 LLM 기반 소프트웨어 에이전트가 고정된 "},{"id":"2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering","title":"[논문리뷰] LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering","excerpt":"arXiv에 게시된 'LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering","tags":["Review","LLM Agents","Software Engineering","Long-Context","Interactive Benchmark","Tool Usage","Memory Management","Bias-Free Evaluation","Multi-Turn"],"text":"링크: 논문 PDF로 바로 열기 저자: Jielin Qiu, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Jianguo Zhang, Haolin Chen, Shiyu Wang, Ming Zhu, Liangwei Yang, Juntao Tan, Roshan Ram, Akshara Prabhakar, Tulika Awalgaonkar,"},{"id":"2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model","title":"[논문리뷰] MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model","excerpt":"Bo Yan이 arXiv에 게시한 'MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model","tags":["Review","Microscopy VQA","Multimodal LLM","Weak Supervision","Graph Neural Networks","Dataset Generation","Biomedical Imaging","Scientific Reasoning","Cross-Modal Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Manyu Li, Ruian He, Chenxi Ma, Bo Yan, Weimin Tan 핵심 연구 목표 본 연구는 현미경 이미지 분석을 위한 대규모 고품질 멀티모달 질의응답(VQA) 데이터셋의 부족 이라는 문제점을 해결하여, 멀티모달 대규모 언어 모델(MLLM)의 현미경 과학 추론 능력을 향상시키는 것을 목표로 합니"},{"id":"2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling","title":"[논문리뷰] MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling","excerpt":"cyyang822이 arXiv에 게시한 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling","tags":["Review","Research Agent","Tool-Augmented Reasoning","Interaction Scaling","Large Language Models","Reinforcement Learning","Context Management","Open-Source AI"],"text":"링크: 논문 PDF로 바로 열기 저자: cyyang822, weizhiwang, EricLRL130, mx1024, YuntaoChen 핵심 연구 목표 논문은 오픈소스 연구 에이전트의 성능 한계를 모델 크기, 컨텍스트 길이, 상호작용 스케일링(interaction scaling) 이라는 세 가지 주요 차원을 통해 확장하는 것을 목표로 합니다. 이는 에이전트"},{"id":"2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards","title":"[논문리뷰] NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards","excerpt":"arXiv에 게시된 'NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards","tags":["Review","Vision-Language-Action Model","Direct Preference Optimization","World Model","Reward Learning","Robotics","Embodied AI","Flow-Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: ChiaYu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA"},{"id":"2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation","title":"[논문리뷰] OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation","excerpt":"arXiv에 게시된 'OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation","tags":["Review","Earth Observation","Foundation Model","Multimodal Learning","Self-supervised Learning","Latent Image Modeling","Vision Transformer","Spatio-temporal"],"text":"링크: 논문 PDF로 바로 열기 저자: Henry Herzog, Favyen Bastani, Yawen Zhang, Gabriel Tseng, Joseph Redmon, Hadrien Sablon, Ryan Park, Jacob Morrison, Alexandra Buraczynski, Karen Farley, Joshua Hansen, Andrew How"},{"id":"2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","title":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning","excerpt":"Haiyuan Wan이 arXiv에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Large Language Models","Physics Reasoning","Agentic AI","Olympiad Problems","Post-Training","Knowledge Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiacheng Chen, Qianjia Cheng, Fangchen Yu, Haiyuan Wan, Yuchen Zhang, Shenghe Zheng, Junchi Yao, et al. 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 퍼즐 풀이를 넘어 과학 수준의 추론 능력을 갖추도록 발전시키고, 특히 복잡한 "},{"id":"2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model","title":"[논문리뷰] Part-X-MLLM: Part-aware 3D Multimodal Large Language Model","excerpt":"arXiv에 게시된 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model","tags":["Review","3D Multimodal LLM","Part-aware","3D Generation","3D Editing","3D Understanding","Bounding Box","Structured Program","Dual-encoder"],"text":"링크: 논문 PDF로 바로 열기 저자: Chunshi Wang, Junliang Ye, Yunhan Yang, Yang Li, Zizhuo Lin, Jun Zhu, Zhuo Chen, Yawei Luo, Chunchao Guo 핵심 연구 목표 본 논문은 기존 3D MLLM(Multimodal Large Language Model)이 3D 객체를 개별 부품으"},{"id":"2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization","title":"[논문리뷰] SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization","excerpt":"Bo Du이 arXiv에 게시한 'SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization","tags":["Review","Multimodal Safety Alignment","Rule-Governed RL","Self-Rewarded Learning","MLLM Safety","Policy Optimization","Safety Benchmarking","Compositional Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuankun Rong, Wenke Huang, Tingfeng Wang, Daiguo Zhou, Bo Du, Mang Ye 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLMs)이 복잡한 텍스트이미지 상호작용에서 발생하는 구성적 안전 위험 과 취약한 안전 인식을 해결하고자 합니다. 기존 GRPO(Group"},{"id":"2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance","title":"[논문리뷰] Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance","excerpt":"arXiv에 게시된 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance","tags":["Review","Model Souping","Large Language Models","Weighted Averaging","Benchmark Optimization","State-of-the-Art","Category Experts","Parameter Averaging","Post-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Shalini Maiti, Amar Budhiraja, Bhavul Gauri, Gaurav Chaurasia, Anton Protopopov, Alexis AudranReiss, Michael Slater, Despoina Magka, Tatiana Shavrina, Roberta Raileanu, Yoram Bac"},{"id":"2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models","title":"[논문리뷰] Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models","excerpt":"arXiv에 게시된 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models","tags":["Review","Vision-Language Models","Test-Time Adaptation","Zero-Shot Generalization","Spectral Decomposition","Latent Space Steering","SVD","Out-of-Distribution"],"text":"링크: 논문 PDF로 바로 열기 저자: Konstantinos M. Dafnis, Dimitris N. Metaxas 핵심 연구 목표 VisionLanguage Models(VLM)이 테스트 시점의 도메인 변화(OOD)에 취약하여 성능이 저하되는 문제를 해결하고, 기존 TestTime Adaptation(TTA) 방법론의 높은 계산 비용과 메모리 사용량, "},{"id":"2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models","title":"[논문리뷰] TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models","excerpt":"Qingyang Liu이 arXiv에 게시한 'TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models","tags":["Review","Video Generative Models","Visual Reasoning","Benchmarking","Image-to-Video","TiViBench","VideoTPO","Prompt Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Harold Haodong Chen, Disen Lan, WenJie Shu, Qingyang Liu, Zihan Wang, Sirui Chen, Wenkai Cheng, Kanghao Chen, Hongfei Zhang, Zixin Zhang, Rongjin Guo, Yu Cheng, YingCong Chen 핵심 "},{"id":"2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy","title":"[논문리뷰] UFO^3: Weaving the Digital Agent Galaxy","excerpt":"arXiv에 게시된 'UFO^3: Weaving the Digital Agent Galaxy' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy","tags":["Review","Multi-Agent Systems","Cross-Device Orchestration","LLM-Powered Agents","Task Constellation","Directed Acyclic Graph (DAG)","Agent Interaction Protocol (AIP)","Fault Tolerance","Asynchronous Execution"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaoyun Zhang, Liqun Li, He Huang, Chiming Ni, Bo Qiao, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM) 기반 에이전트들이 단일 운영체제나"},{"id":"2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity","title":"[논문리뷰] UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity","excerpt":"arXiv에 게시된 'UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity","tags":["Review","Self-Supervised Learning","Segmentation","Granularity Control","SAM","Foundation Models","Unsupervised Learning","Image Segmentation","Video Segmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Junwei Yu, Trevor Darrell, XuDong Wang 핵심 연구 목표 본 논문은 기존 Segment Anything Model (SAM) 계열의 모델들이 가지는 세분화(granularity) 제어의 한계를 극복하고, 인간의 주석 없이 모든 세분화 수준에서 연속적이고 제어 가능한 객체 분할 을 가능하게 "},{"id":"2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data","title":"[논문리뷰] Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data","excerpt":"arXiv에 게시된 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-18 00:00:00+0900+0900","permalink":"/ai/review/2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data","tags":["Review","Omnimodal Large Models","Mixture-of-Experts (MoE)","Language-Centric AI","Multimodal Understanding","Multimodal Generation","Progressive Training","Omni-Modality 3D RoPE"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunxin Li, Baotian Hu, Min Zhang, Xinyu Chen, Shenyuan Jiang, Haoyuan Shi, Zhenyu Liu, Xuanyu Zhang, Nanhao Deng, Zhenran Xu, Yicheng Ma, Meishan Zhang 핵심 연구 목표 본 논문은 언어 중심의 접근 방"},{"id":"2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition","title":"[논문리뷰] A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition","excerpt":"G. Maragatham이 arXiv에 게시한 'A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition","tags":["Review","EEG","Emotion Recognition","Transformer Architecture","Inter-Cortical Neural Interactions","Multi-Head Attention","Brain-Computer Interface","Affective Computing"],"text":"링크: 논문 PDF로 바로 열기 저자: Nilay Kumar, Priyansh Bhandari, G. Maragatham 핵심 연구 목표 본 논문은 기존 EEG 기반 감정 인식 모델들이 간과했던 뇌의 상이한 피질 영역 간의 동적 상호작용을 해결하고자 합니다. 이를 위해 RBTransformer 라는 Transformer 기반 신경망 아키텍처를 제안하여 잠재"},{"id":"2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space","title":"[논문리뷰] A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space","excerpt":"arXiv에 게시된 'A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space","tags":["Review","Code-to-Style Generation","Discrete Style Space","Style Codebook","Autoregressive Model","Diffusion Models","Visual Stylization","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Huijie Liu1,2 Shuhao Cui² Haoxiang Cao2,3 Shuai Ma¹ Kai Wu2,† Guoliang Kang1,† 핵심 연구 목표 본 논문은 기존 텍스트 프롬프트, 참조 이미지, LoRA 기반 스타일 생성 방식이 겪는 스타일 일관성 부족, 창의성 한계, 복잡한 스타일 표현 문제를 해결하고자 "},{"id":"2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning","title":"[논문리뷰] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning","excerpt":"Yuqiang Li이 arXiv에 게시한 'ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning","tags":["Review","Benchmark","LLMs","Scientific Reasoning","Multidisciplinary","AI4S","Data Contamination","Evaluation","LRM-as-Judge"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuqiang Li, Haodong Duan, Junnan Liu, Hongwei Liu, Shudong Liu et al. 핵심 연구 목표 기존 벤치마크의 성능 포화 , 협소한 분야 집중 , 단순화된 답변 형식 , 그리고 데이터 오염 문제로 인해 최신 대규모 언어 모델(LLMs)의 진정한 역량을 평가하기 어렵다는 문"},{"id":"2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning","title":"[논문리뷰] Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning","excerpt":"Yucong Luo이 arXiv에 게시한 'Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning","tags":["Review","LLM Agents","Reinforcement Learning","Markov Decision Process","Tool Use","Multi-turn Interaction","Policy Optimization","Reward Shaping","Agent Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingyue Cheng, Jie Ouyang, Shuo Yu, Ruiran Yan, Yucong Luo, Zirui Liu, Daoyu Wang, Qi Liu, Enhong Chen 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)을 복잡한 다중 턴(multiturn) 상호작용 태스크를 수행하는 에이전트로 훈련"},{"id":"2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding","title":"[논문리뷰] Agent READMEs: An Empirical Study of Context Files for Agentic Coding","excerpt":"Kundjanasith Thonglek이 arXiv에 게시한 'Agent READMEs: An Empirical Study of Context Files for Agentic Coding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding","tags":["Review","Agentic Coding","Context Files","READMEs for Agents","Empirical Study","Software Engineering","Documentation Maintenance","Non-functional Requirements","LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Kundjanasith Thonglek, Brittany Reid, Yutaro Kashiwa, Worawalan Chatlatanagulchai, Hao Li 핵심 연구 목표 본 연구는 AI 코딩 에이전트의 작동 방식을 정의하고 안내하는 에이전트 컨텍스트 파일(Agent Context Files) 에 대한 체계적인 "},{"id":"2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models","title":"[논문리뷰] AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models","excerpt":"arXiv에 게시된 'AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models","tags":["Review","Arabic LLMs","Linguistic Benchmark","Human Annotation","Natural Language Understanding","Grammar Evaluation","Morphology Analysis","Syntax Assessment","Reading Comprehension"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohammad Zbib, Hasan Abed Al Kader Hammoud, Sina Mukalled, Nadine Rizk, Fatima Karnib, Issam Lakkis, Ammar Mohanna, Bernard Ghanem 핵심 연구 목표 본 연구는 기존 아랍어 대규모 언어 모델(LLM) 평가 벤치마크들이 "},{"id":"2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark","title":"[논문리뷰] Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark","excerpt":"Yuzhang Shang이 arXiv에 게시한 'Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark","tags":["Review","Generative Visual Reasoning","Chain-of-Frames (CoF)","Video Generation Models","World Simulators","AI Benchmarking","Cognitive Reasoning","VLM Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinxin Liu, Zhaopan Xu, Ming Li, Kai Wang, Yong Jae Lee, Yuzhang Shang 핵심 연구 목표 본 논문은 최신 비디오 생성 모델 이 단순한 시각적 품질을 넘어 실제 세계의 물리 법칙과 연속성을 이해하며 추론하는 ChainofFrames (CoF) 추론 능력 을 체계적으로"},{"id":"2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models","title":"[논문리뷰] Error-Driven Scene Editing for 3D Grounding in Large Language Models","excerpt":"arXiv에 게시된 'Error-Driven Scene Editing for 3D Grounding in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models","tags":["Review","3D Grounding","3D-LLMs","Scene Editing","Counterfactual Augmentation","Error-Driven Learning","Spatial Reasoning","Visual Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Yue Zhang, Zun Wang, Han Lin, Jialu Li, Jianing Yang, Yonatan Bitton, Idan Szpektor, Mohit Bansal 핵심 연구 목표 본 논문은 현재 3DLLMs 가 3D 환경에서 언어를 시각적 및 공간적 요소에 정확하게 연결하지 못하는 문제점을 해결하고자 합니"},{"id":"2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost","title":"[논문리뷰] LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost","excerpt":"Kengo Tajiri이 arXiv에 게시한 'LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost","tags":["Review","Chaos Engineering","Large Language Models","System Resilience","Kubernetes","Software Automation","AI Agents","Fault Injection"],"text":"링크: 논문 PDF로 바로 열기 저자: Kengo Tajiri, Hiroki Ikeuchi, Daisuke Kikuta 핵심 연구 목표 본 논문은 카오스 엔지니어링(CE)의 수동적이고 노동 집약적인 단계(가설 설정, 실험 계획, 시스템 재구성)를 자동화하여, 누구나 저비용으로 탄력적인 소프트웨어 시스템을 구축할 수 있도록 하는 것을 목표로 합니다. 기존 C"},{"id":"2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework","title":"[논문리뷰] Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework","excerpt":"arXiv에 게시된 'Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework","tags":["Review","Extreme Multi-label Classification (XMC)","Large Language Models (LLMs)","Multi-modal Learning","Dual-decoder Learning","Vision Transformers","Contrastive Learning","Prompt Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Diego Ortego, Marlon Rodríguez, Mario Almagro, Kunal Dahiya, David Jiménez, Juan C. SanMiguel 핵심 연구 목표 본 연구는 Extreme Multilabel Classification (XMC)에서 Large Language Models (LLMs"},{"id":"2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs","title":"[논문리뷰] MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs","excerpt":"Kaijie Chen이 arXiv에 게시한 'MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs","tags":["Review","LVLM Robustness","Misleading Visual Inputs","VQA Benchmark","Visual Perception","Visual Reasoning","MVI-Sensitivity","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Huiyi Chen, Jiawei Peng, Dehai Min, Changchang Sun, Kaijie Chen, Yan Yan, Xu Yang, Lu Cheng 핵심 연구 목표 기존 Large VisionLanguage Models (LVLMs) 강건성 벤치마크들이 환각이나 오해의 소지가 있는 텍스트 입력에만 집중"},{"id":"2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models","title":"[논문리뷰] Mitigating Label Length Bias in Large Language Models","excerpt":"Katharina von der Wense이 arXiv에 게시한 'Mitigating Label Length Bias in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models","tags":["Review","Large Language Models","Label Bias","Calibration","In-Context Learning","Text Classification","Multi-token Labels","Label Length Bias","Multiple Choice QA"],"text":"링크: 논문 PDF로 바로 열기 저자: Mario SanzGuerrero, Katharina von der Wense 핵심 연구 목표 논문은 대규모 언어 모델(LLMs)이 다중 토큰 클래스 레이블을 예측할 때 발생하는 '레이블 길이 편향(label length bias)' 문제를 해결하는 것을 목표로 합니다. 기존 캘리브레이션 기법들이 단일 토큰 레이블에 "},{"id":"2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models","title":"[논문리뷰] OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models","excerpt":"Jian liu이 arXiv에 게시한 'OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models","tags":["Review","Omnimodal LLMs","Token Compression","Audio-Video Understanding","Dynamic Pruning","Inference Acceleration","Spatio-Temporal Compression","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Keda Tao, Kele Shao, Bohan Yu, Weiqiang Wang, Jian liu 핵심 연구 목표 옴니모달 대규모 언어 모델(OmniLLMs)이 직면한 오디오비디오 토큰의 과도한 수 와 주의 메커니즘의 2차 복잡성 으로 인한 계산 및 메모리 병목 현상 을 해결하는 것을 목표로 합니다. 특히, 기존의 단"},{"id":"2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution","title":"[논문리뷰] Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution","excerpt":"Sudeep Pillai이 arXiv에 게시한 'Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution","tags":["Review","Visual Agent","Multimodal Perception","Tool-Augmented LLM","Agentic AI","Visual Reasoning","Computer Vision","Structured Outputs","ReAct Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Sudeep Pillai, N Dinesh Reddy 핵심 연구 목표 본 논문은 기존의 단일(monolithic) VLM(VisionLanguage Model)이 가진 정밀성, 결정론적 제어 및 복합적 시각 작업 처리 능력의 한계를 극복하고자 합니다. 이를 위해 Orion 이라는 통합 시각 에이전트 프레임워크를 제시하"},{"id":"2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations","title":"[논문리뷰] Proactive Hearing Assistants that Isolate Egocentric Conversations","excerpt":"arXiv에 게시된 'Proactive Hearing Assistants that Isolate Egocentric Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations","tags":["Review","Proactive Hearing Assistant","Egocentric Audio Processing","Speech Separation","Turn-taking Dynamics","Dual-Model Architecture","Real-time Inference","Wearable Devices","Dialogue Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Guilin Hu, Malek Itani, Tuochao Chen, Shyamnath Gollakota 핵심 연구 목표 본 논문은 사용자의 명시적인 프롬프트 없이도 대화 상대를 자동으로 식별하고 분리하여 다른 방해 음성을 억제하는 선제적(proactive) 보청 보조 장치 를 개발하는 것을 목표로 합니다. 이는 복잡한"},{"id":"2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding","title":"[논문리뷰] REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding","excerpt":"Jingyang Chen이 arXiv에 게시한 'REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding","tags":["Review","Multimodal Reasoning","Long-Form Video Understanding","Self-Reflection","Reinforcement Learning","Tool-Augmented MLLMs","Visual Rethinking","Video Question Answering","Causal Attribution"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaze Li, Hao Yin, Wenhui Tan, Jingyang Chen, Boshen Xu, Yuxun Qu, Yijing Chen, Jianzhong Ju, Zhenbo Luo, Jian Luan 핵심 연구 목표 본 논문은 기존 텍스트 기반 자기 성찰(selfreflection) 메커니즘 이 풍부하고 동적인"},{"id":"2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models","title":"[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models","excerpt":"Rong Zhao이 arXiv에 게시한 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models","tags":["Review","LVLM Evaluation","Global Visual Perception","Topological Properties","Shortcut-Free Benchmark","Visual Bottleneck","Multimodal AI","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhao Zhou, Hao Zheng, Rong Zhao 핵심 연구 목표 Large VisionLanguage Models (LVLMs)가 시각적 인코더의 정보 병목 현상 과 로컬 단축키 로 인해 전역 시각 정보를 제대로 인지하지 못하는 문제를 해결하는 것이 목표입니다. 기존 평가 방식이 모델의 전역 지각 능력을 "},{"id":"2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning","title":"[논문리뷰] VIDEOP2R: Video Understanding from Perception to Reasoning","excerpt":"arXiv에 게시된 'VIDEOP2R: Video Understanding from Perception to Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning","tags":["Review","Video Understanding","Reinforcement Fine-Tuning (RFT)","Large Video Language Models (LVLMs)","Perception and Reasoning","Chain-of-Thought (CoT)","Process-Aware Learning","Policy Optimization","Credit Assignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Jiang, Yueying Wang, Rui Zhao, Toufiq Parag, Zhimin Chen, Zhenyu Liao, Jayakrishnan Unnikrishnan 핵심 연구 목표 기존 비디오 RFT 프레임워크가 인식(perception)과 추론(reasoning) 과정을 단일 절차로 처리하여 신용"},{"id":"2025-11-19-eat-Physically-Grounded-Feature-Representation","title":"[논문리뷰] Φeat: Physically-Grounded Feature Representation","excerpt":"arXiv에 게시된 'Φeat: Physically-Grounded Feature Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-19 00:00:00+0900+0900","permalink":"/ai/review/2025-11-19-eat-Physically-Grounded-Feature-Representation","tags":["Review","Self-supervised Learning","Physically-Grounded Features","Material Representation","Intrinsic Scene Understanding","Vision Transformer","Synthetic Data","Contrastive Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Giuseppe Vecchio, Adrien Kaiser, Romain Rouffet, Rosalie Martin, Elena Garces, Tamy Boubekeur (Adobe Research) 핵심 연구 목표 기존의 자기 지도 시각 백본이 고수준의 의미론적 특징과 저수준의 물리적 요소를 혼합하여 물리적 추론을 방"},{"id":"2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries","title":"[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries","excerpt":"arXiv에 게시된 'ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries","tags":["Review","Video Chaptering","Long-form Video Understanding","Large Language Models","Multimodal Learning","Hierarchical Summarization","Video Segmentation","Reinforcement Learning","Dataset Creation"],"text":"링크: 논문 PDF로 바로 열기 저자: Junfu Pu, Teng Wang, Yixiao Ge, Yuying Ge, Chen Li, Ying Shan (ARC Lab, Tencent PCG) Core contributors, Project lead 핵심 연구 목표 본 논문은 기존 비디오 챕터링 방법론이 짧고 거친 주석에 의해 제한되어 장시간 비디오의 미묘한"},{"id":"2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges","title":"[논문리뷰] Aligning Generative Music AI with Human Preferences: Methods and Challenges","excerpt":"Abhinaba Roy이 arXiv에 게시한 'Aligning Generative Music AI with Human Preferences: Methods and Challenges' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges","tags":["Review","Generative Music AI","Preference Alignment","Reinforcement Learning from Human Feedback (RLHF)","Direct Preference Optimization (DPO)","Inference-Time Optimization","Music Generation","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Dorien Herremans, Abhinaba Roy 핵심 연구 목표 본 논문은 생성형 음악 AI 시스템이 계산적 최적화와 인간의 미적 감각 사이의 근본적인 격차로 인해 발생하는 문제를 해결하고, 인간의 미묘한 음악적 선호도에 더욱 잘 부합하도록 정렬하는 방법을 모색합니다. 복잡하고 주관적인 음악적 품질 평가 기준을"},{"id":"2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI","title":"[논문리뷰] FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI","excerpt":"Xinyu Yin이 arXiv에 게시한 'FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI","tags":["Review","Embodied AI","Vision-and-Language Navigation (VLN)","LLM-driven Simulation","Human-Agent Interaction","Closed-Loop","Benchmark Dataset","Social Cognition"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhang Peng, Yizhou Pan, Xinning He, Jihaoyu Yang, Xinyu Yin, Han Wang, Xiaoji Zheng, Chao Gao, Jiangtao Gong 핵심 연구 목표 본 논문은 기존 VLN(VisionandLanguage Navigation) 시스템의 정적인 지시, 사회적"},{"id":"2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset","title":"[논문리뷰] Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset","excerpt":"arXiv에 게시된 'Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset","tags":["Review","Medical Imaging","Chest X-ray","Lesion Segmentation","Vision-Language Models","Instruction Following","Data Generation","MIMIC-CXR"],"text":"링크: 논문 PDF로 바로 열기 저자: Geon Choi, Hangyul Yoon, Hyunju Shin, Hyunki Park, Sang Hoon Seo, Eunho Yang, Edward Choi 핵심 연구 목표 본 연구는 흉부 Xray(CXR)에서 병변 분할 모델의 제한적인 타겟 레이블 수와 전문가 수준의 상세 텍스트 입력 의존성을 해결하고자 합니다."},{"id":"2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","title":"[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation","excerpt":"Vladimir Arkhipkin이 arXiv에 게시한 'Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","tags":["Review","Image Generation","Video Generation","Diffusion Models","Flow Matching","Diffusion Transformer","NABLA","RLHF","Supervised Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Vladimir Arkhipkin, dendimitrov, kisnikser, AlexeyLetunovskiy, vvasilev 핵심 연구 목표 본 논문은 고품질의 일관되고 제어 가능한 이미지 및 비디오 생성을 위한 AI/ML 분야의 핵심 과제를 해결하고자 합니다. 특히, 최신 이미지 및 10초 비디오 합성을 위한 K"},{"id":"2025-11-20-MHR-Momentum-Human-Rig","title":"[논문리뷰] MHR: Momentum Human Rig","excerpt":"Chris Twigg이 arXiv에 게시한 'MHR: Momentum Human Rig' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-MHR-Momentum-Human-Rig","tags":["Review","Parametric Body Model","Human Animation","Character Rigging","Pose Correctives","Skeletal Decoupling","Computer Graphics","AR/VR"],"text":"링크: 논문 PDF로 바로 열기 저자: Chris Twigg, Carsten Stoll, Berta Bescos, Ahmed A. A. Osman, Aaron Ferguson 핵심 연구 목표 본 논문은 ATLAS 모델의 골격/형상 분리 패러다임 에 Momentum 라이브러리에서 영감을 받은 유연하고 현대적인 리그 및 자세 보정 시스템을 결합하여, 산업 및 "},{"id":"2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation","title":"[논문리뷰] Medal S: Spatio-Textual Prompt Model for Medical Segmentation","excerpt":"Tao Chen이 arXiv에 게시한 'Medal S: Spatio-Textual Prompt Model for Medical Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation","tags":["Review","Medical Segmentation","Foundation Model","Spatio-Textual Prompts","3D Convolution","Multi-modal Imaging","Dynamic Resampling","Parallel Inference","Iterative Refinement"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengcheng Shi, Jiawei Chen, Jiaqi Liu, Xinglin Zhang, Tao Chen, Lei Li 핵심 연구 목표 의료 영상 분할에서 다양한 모달리티와 해부학적 변이로 인한 문제를 해결하고, 기존 모델의 해상도 불일치 및 순차 처리 비효율성을 극복하는 것이 목표입니다. 네이티브 해상도 공간"},{"id":"2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation","title":"[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation","excerpt":"arXiv에 게시된 'Mixture of States: Routing Token-Level Dynamics for Multimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation","tags":["Review","Multimodal Diffusion","Mixture of States (MoS)","Token-Level Routing","Dynamic Conditional Fusion","Text-to-Image Generation","Image Editing","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Haozhe Liu, Ding Liu, Mingchen Zhuge, Zijian Zhou, Tian Xie, Sen He, Yukang Yang, Shuming Liu, Yuren Cong, Jiadong Guo, Hongyu Xu, Ke Xu, KamWoh Ng, Juan C. Pérez, JuanManuel Pér"},{"id":"2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks","title":"[논문리뷰] Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks","excerpt":"Yiran Peng이 arXiv에 게시한 'Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks","tags":["Review","Video Models","Spatial Reasoning","Maze Solving","Video Generation","Benchmark","Supervised Fine-tuning","Test-Time Scaling","Multimodal Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Yang, Haiyuan Wan, Yiran Peng, et al. 핵심 연구 목표 본 논문은 비디오 모델의 추론 능력, 특히 비디오 생성 을 통한 추론 능력을 체계적으로 평가하기 위한 포괄적인 벤치마크의 부재를 해결합니다. 비디오 모델이 시공간적 연속성을 활용하여 미로 해결과 같은 복잡한 공간 추론 작업을"},{"id":"2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images","title":"[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images","excerpt":"arXiv에 게시된 'VisPlay: Self-Evolving Vision-Language Models from Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images","tags":["Review","Self-Evolving","Vision-Language Models","Reinforcement Learning","Self-Play","Unlabeled Data","Multimodal Reasoning","Group Relative Policy Optimization","Hallucination Mitigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang 핵심 연구 목표 본 논문은 인간 주석이나 작업별 휴리스틱 없이, 대규모 비정형 이미지 데이터로부터 VisionLanguage Models (VLMs) 의 추론 능력을 자율적으로 개선하는 것을 목표로"},{"id":"2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity","title":"[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity","excerpt":"arXiv에 게시된 'What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-20 00:00:00+0900+0900","permalink":"/ai/review/2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity","tags":["Review","AI Research Agents","Ideation Diversity","MLE-bench","LLM Backbones","Agentic Scaffolds","Shannon Entropy","Machine Learning Engineering","Performance Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexis AudranReiss, Jordi Armengol Estapé, Karen Hambardzumyan, et al. 핵심 연구 목표 AI 연구 에이전트의 성능에 있어 아이디어 다양성(ideation diversity)이 핵심 병목 현상인지를 규명하고, 에이전트 궤적의 성공 또는 실패를 좌우하는 요인을 이해하"},{"id":"2025-11-21-Draft-and-Refine-with-Visual-Experts","title":"[논문리뷰] Draft and Refine with Visual Experts","excerpt":"arXiv에 게시된 'Draft and Refine with Visual Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-Draft-and-Refine-with-Visual-Experts","tags":["Review","Large Vision-Language Models (LVLMs)","Visual Grounding","Hallucination Mitigation","Agent Framework","Visual Question Answering (VQA)","Expert Coordination","Relevance Map","Multi-modal Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: SUNGHEON JEONG, RYOZO MASUKAWA, JIHONG PARK, SANGGEON YUN, WENJUN HUANG, HANNING CHEN, MAHDI IMANI, MOHSEN IMANI 핵심 연구 목표 최신 Large VisionLanguage Models (LVLMs) 는 시각적 증거보다 언어적 사전"},{"id":"2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization","title":"[논문리뷰] First Frame Is the Place to Go for Video Content Customization","excerpt":"arXiv에 게시된 'First Frame Is the Place to Go for Video Content Customization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization","tags":["Review","Video Generation","Content Customization","Few-shot Learning","LoRA","Vision-Language Models (VLMs)","First Frame Conditioning","Reference-based Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingxi Chen, Zongxia Li, Zhichao Liu, Guangyao Shi, Xiyang Wu, Fuxiao Liu, Cornelia Fermüller, Brandon Y. Feng, Yiannis Aloimonos 핵심 연구 목표 비디오 생성 모델에서 여러 참조 이미지를 활용한 유연한 콘텐츠 맞춤화 "},{"id":"2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report","title":"[논문리뷰] MiMo-Embodied: X-Embodied Foundation Model Technical Report","excerpt":"arXiv에 게시된 'MiMo-Embodied: X-Embodied Foundation Model Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report","tags":["Review","Vision-Language Model (VLM)","Embodied AI","Autonomous Driving","Foundation Model","Multimodal Learning","Task Planning","Affordance Prediction","Spatial Understanding","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoshuai Hao, Lei Zhou, Zhijian Huang, Zhiwen Hou, Yingbo Tang, Lingfeng Zhang, Guang Li, Zheng Lu, Shuhuai Ren, Fuli Luo, Hangjun Ye, Long Chen 핵심 연구 목표 이 논문은 자율 주행(Autonomous "},{"id":"2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion","title":"[논문리뷰] NaTex: Seamless Texture Generation as Latent Color Diffusion","excerpt":"arXiv에 게시된 'NaTex: Seamless Texture Generation as Latent Color Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion","tags":["Review","3D Texture Generation","Latent Diffusion Model","Geometry-Aware VAE","Multi-Control DiT","Color Point Cloud","Texture Synthesis","3D Asset Creation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Xin Yang, Xin Huang, Jingwei Huang, Xiangyu Yue, Chunchao Guo 핵심 연구 목표 본 논문은 기존 MultiView Diffusion (MVD) 모델의 텍스처 생성 시 발생하는 occlusion 처리 미흡, "},{"id":"2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs","title":"[논문리뷰] Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs","excerpt":"arXiv에 게시된 'Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs","tags":["Review","LLM Compression","Elastic Networks","Knowledge Distillation","Hybrid Mamba-Attention","Reasoning LLMs","Multi-Budget Training","Zero-Shot Deployment"],"text":"링크: 논문 PDF로 바로 열기 저자: Ali Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, et al. 핵심 연구 목표 다양한 규모와 배포 목적에 맞는 LLM(Large Language Model) 패밀리 를 개별적으로 훈련하는 데 드는 막대한 비용 문제를 해결하고자 합니다. 이 연구는"},{"id":"2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes","title":"[논문리뷰] PartUV: Part-Based UV Unwrapping of 3D Meshes","excerpt":"Hao Su이 arXiv에 게시한 'PartUV: Part-Based UV Unwrapping of 3D Meshes' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes","tags":["Review","UV Unwrapping","3D Meshes","Part-Based Decomposition","Neural Fields","Geometric Heuristics","Parameterization","Texture Mapping"],"text":"링크: 논문 PDF로 바로 열기 저자: ZHAONING WANG, XINYUE WEI, RUOXI SHI, XIAOSHUAI ZHANG, HAO SU, MINGHUA LIU 핵심 연구 목표 이 논문은 AI 생성 메시와 같이 시끄럽고 불규칙한 3D 메시에서 기존 UV unwrapping 방법이 야기하는 과도한 차트 분할 및 부적절한 경계 문제를 해결하고자 합"},{"id":"2025-11-21-SAM-3D-3Dfy-Anything-in-Images","title":"[논문리뷰] SAM 3D: 3Dfy Anything in Images","excerpt":"arXiv에 게시된 'SAM 3D: 3Dfy Anything in Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-SAM-3D-3Dfy-Anything-in-Images","tags":["Review","3D Reconstruction","Generative Models","Single Image 3D","Object Reconstruction","Scene Understanding","Data Engine","Model-in-the-Loop","Human Preference"],"text":"링크: 논문 PDF로 바로 열기 저자: SAM 3D Team, Xingyu Chen, FuJen Chu, Pierre Gleize, Kevin J Liang, Alexander Sax, Hao Tang, Weiyao Wang, Michelle Guo, Thibaut Hardin, Xiang Li°, Aohan Lin, Jiawei Liu, Ziqi Ma°,"},{"id":"2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking","title":"[논문리뷰] SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking","excerpt":"arXiv에 게시된 'SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking","tags":["Review","Surgical Video Segmentation","Interactive Video Object Segmentation","Long-term Tracking","Foundation Models","Domain Adaptation","Semantic Learning","Prompt-based Segmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Haofeng Liu, Guanyi Qin, Ziyue Wang, Sudhanshu Mishra, Chang Han Low, Alex Y. W. Kong, Mingqi Gao, Yueming Jin 핵심 연구 목표 수술 비디오 세분화는 컴퓨터 지원 수술에 필수적이지만, 기존 SAM2 와 같은 iVOS 모델은 도메인 격"},{"id":"2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models","title":"[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models","excerpt":"arXiv에 게시된 'SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models","tags":["Review","Reinforcement Learning","Vision-Language-Action Models","Reward Shaping","World Models","Self-Referential Learning","Robotics","Trajectory Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Senyu Fei, Siyin Wang, Li Ji, et al. 핵심 연구 목표 VisionLanguageAction (VLA) 모델의 강화 학습(RL)에서 발생하는 심각한 보상 희소성 문제 를 해결하고, 외부 전문가 시연이나 수동적인 보상 엔지니어링 없이 높은 훈련 효율성 과 일반화 능력 을 달성하는 것을 목표로 "},{"id":"2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models","title":"[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models","excerpt":"arXiv에 게시된 'Scaling Spatial Intelligence with Multimodal Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models","tags":["Review","Spatial Intelligence","Multimodal Foundation Models","Data Scaling","Perspective-taking","Visual Question Answering","Emergent Capabilities","Embodied AI","Benchmark Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei"},{"id":"2025-11-21-Step-Audio-R1-Technical-Report","title":"[논문리뷰] Step-Audio-R1 Technical Report","excerpt":"arXiv에 게시된 'Step-Audio-R1 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report","tags":["Review","Audio Reasoning","Multimodal LLMs","Modality-Grounded Reasoning Distillation (MGRD)","Chain-of-Thought","Reinforcement Learning","Audio Understanding","Self-Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Fei Tian, Xiongyu (Tony) Zhang, Yuxin Zhang, Haoyang Zhang, et al. 핵심 연구 목표 오디오 언어 모델이 추론 과정을 거치면 성능이 저하되는 기존의 문제, 즉 \"텍스트 대리 추론\" 현상을 해결하고, 오디오 도메인에서 진정한 추론 능력을 성공적으로 활성화하는 것을 목표로"},{"id":"2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation","title":"[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation","excerpt":"Xinyan Chen이 arXiv에 게시한 'Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation","tags":["Review","Visual Generation","Textual Reasoning","Interleaving","Large Multimodal Models (LMMs)","Chain-of-Thought (CoT)","Zero-shot Learning","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, PhengAnn Heng 핵심 연구 목표 본 논문은 시각 콘텐츠 생성 과정에서 발생하는 장기적인 구성, 다중 엔티티 관계 및 미묘한 지시사항 준수와 "},{"id":"2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding","title":"[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding","excerpt":"arXiv에 게시된 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding","tags":["Review","Long Video Understanding","Hybrid Mamba-Transformer","Vision-Language Model","Token Compression","Vision-to-Text Aggregation","Efficient LLM","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Boshen Xu, Zihan Xiao, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Qin Jin 핵심 연구 목표 본 논문은 기존 MLLM이 긴 비디오 컨텍스트 처리 시 효율성과 효과성 사이의 균형을 맞추기 어려운 문제를 해결하고자 합니다. 특히, 효율적인 모델 아키텍처와 확장"},{"id":"2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval","title":"[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval","excerpt":"arXiv에 게시된 'TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval","tags":["Review","Information Retrieval","Turkish Language","Late-Interaction Models","ColBERT","Dense Retrieval","MUVERA","Benchmarking","Low-Resource NLP","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Özay Ezerceli, Mahmud Elhüsseni, Seba Taş, Reyhan Bayraktar, Fatma Betül Terzioğlu, Yusuf Çelebi, Yağız Asker 핵심 연구 목표 본 연구는 신경 임베딩 기반 정보 검색(IR) 시스템이 영어 중심의 아키텍처에서 뛰어난 성능을 보임에도 불"},{"id":"2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models","title":"[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models","excerpt":"Baijiong Lin이 arXiv에 게시한 'V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models","tags":["Review","Video Generation","Reasoning Benchmark","Chain-of-Frame","Evaluation","Multimodal AI","Physical Dynamics","Spatial Cognition","Pattern Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Baijiong Lin, Xuanlei Zhao, Yang Luo, thesouthfrog, ltzhu 핵심 연구 목표 본 논문은 최신 생성 비디오 모델의 추론 능력을 체계적이고 신뢰할 수 있게 평가하기 위한 벤치마크 스위트인 VReasonBench 를 제안합니다. 기존 모델의 시각적 품질을 넘어 인간과 유사한 추론 "},{"id":"2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO","title":"[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO","excerpt":"arXiv에 게시된 'Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-21 00:00:00+0900+0900","permalink":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO","tags":["Review","Video Generation","Next Event Prediction","Reinforcement Learning","Vision-Language Model","Video Diffusion Model","Joint Optimization","Multimodal AI","Procedural Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Junhao Cheng¹†, Liang Hou², Xin Tao², Jing Liao¹ 핵심 연구 목표 이 연구는 기존의 텍스트 기반 다음 이벤트 예측(NEP)의 한계를 넘어, 비디오를 답변으로 제공 하는 새로운 패러다임인 VideoNextEvent Prediction (VNEP) 을 개척합니다. 이는 복잡한 절차적 "},{"id":"2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models","title":"[논문리뷰] Diversity Has Always Been There in Your Visual Autoregressive Models","excerpt":"Yaxing Wang이 arXiv에 게시한 'Diversity Has Always Been There in Your Visual Autoregressive Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models","tags":["Review","Visual Autoregressive Models","Diversity Collapse","Generative Diversity","Soft-Suppression Regularization","Soft-Amplification Regularization","Training-Free","Image Generation","Singular Value Decomposition"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaxing Wang, Kai Wang, Nian Liu, Guanyu Yang, Tong Wang 핵심 연구 목표 Visual Autoregressive (VAR) 모델이 겪는 다양성 붕괴(diversity collapse) 문제를 해결하고, 추가적인 훈련 없이 모델의 내재된 생성 다양성을 발현시키면서도 이미지 품질"},{"id":"2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models","title":"[논문리뷰] Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models","excerpt":"Serena Yeung-Levy이 arXiv에 게시한 'Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models","tags":["Review","Small Multimodal Models","LLM Downscaling","Perception Bottleneck","Reasoning Bottleneck","Visual Extraction Tuning","Chain-of-Thought Reasoning","Multimodal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mark Endo, Serena YeungLevy 핵심 연구 목표 본 연구는 대규모 다중모달 모델(MLLM)의 크기를 축소할 때 발생하는 지능 저하 현상을 체계적으로 분석하고, 특히 어떤 기능이 가장 큰 영향을 받는지, 그리고 그 원인이 무엇인지 밝히는 것을 목표로 합니다. 나아가 시각적 이해 및 추론 능력 저하의 근"},{"id":"2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization","title":"[논문리뷰] GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization","excerpt":"arXiv에 게시된 'GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization","tags":["Review","Geolocalization","Agentic Models","Visual Reasoning","Web-Augmented","Multimodal LLMs","Reinforcement Learning","Tool Use","GeoBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Yikun Wang, Zuyan Liu, Ziyi Wang, Pengfei Liu, Han Hu, Yongming Rao 핵심 연구 목표 본 연구는 기존 에이전트 시각 추론 모델들이 주로 이미지 조작 도구에 집중하여 일반적인 목적으로 확장하기 어려운 한계를 해결하고자 합니다. 이를 위해 지리적 위치 특정(geoloca"},{"id":"2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process","title":"[논문리뷰] Insights from the ICLR Peer Review and Rebuttal Process","excerpt":"Nedjma Ousidhoum이 arXiv에 게시한 'Insights from the ICLR Peer Review and Rebuttal Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process","tags":["Review","Peer Review","Rebuttal Process","ICLR","Score Dynamics","LLM Analysis","Reviewer Engagement","Academic Publishing","OpenReview"],"text":"링크: 논문 PDF로 바로 열기 저자: Nedjma Ousidhoum, Jing Yang, Nafiseh Nikeghbal, Amir Hossein Kargaran 핵심 연구 목표 본 논문은 ICLR 2024 및 2025 컨퍼런스의 피어 리뷰 및 재고(rebuttal) 과정 의 본질과 역학을 이해하고, 효율성, 효과성 및 출판 논문의 품질 향상에 기여하는 "},{"id":"2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process","title":"[논문리뷰] Loomis Painter: Reconstructing the Painting Process","excerpt":"arXiv에 게시된 'Loomis Painter: Reconstructing the Painting Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process","tags":["Review","Painting Process Generation","Video Diffusion Models","Media Transfer","Reverse Painting","Dataset Curation","Perceptual Distance Profile","Artistic Workflow","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Markus Pobitzer, Chang Liu, Chenyi Zhuang, Teng Long, Bin Ren, Nicu Sebe 핵심 연구 목표 본 논문은 기존 생성 모델들이 겪는 시간적 불연속성, 구조적 불일치, 그리고 다양한 예술 매체에 대한 일반화 능력 부족 문제를 해결하여, 어떤 입력 이미지에 대해서도 사실적"},{"id":"2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight","title":"[논문리뷰] Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight","excerpt":"arXiv에 게시된 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight","tags":["Review","Vision-Language-Action (VLA) Models","Visual Foresight","Diffusion Transformer (DiT)","Robotics","Multimodal Learning","Adaptive Temporal Ensemble","Latent Actions"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng, Zipeng Xiao 핵심 연구 목표 본 논문은 기존 VisionLanguageAction (VLA) 모델의 한계인 희소한 행동 감독 "},{"id":"2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging","title":"[논문리뷰] MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging","excerpt":"arXiv에 게시된 'MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging","tags":["Review","Genome Modeling","Dynamic Tokenization","Token Merging","Context-aware Learning","DNA Foundation Models","Transformer Architecture","Multi-omics"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyuan Li, Kai Yu, Anna Wang, Zicheng Liu, Chang Yu, Jingbo Zhou, Qirong Yang, Yucheng Guo, Xiaoming Zhang, Stan Z. Li 핵심 연구 목표 이 논문은 유전체 서열 모델링의 두 가지 난제인 다양한 정보 밀도 와 고유한 어휘 단위 부"},{"id":"2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models","title":"[논문리뷰] Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models","excerpt":"arXiv에 게시된 'Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models","tags":["Review","Vision-Language Models (VLMs)","Adversarial Attack","Jailbreaking","Reward Hacking","Content Moderation Bypass","Cross-Model Transferability","Safety Vulnerabilities"],"text":"링크: 논문 PDF로 바로 열기 저자: Yijun Yang, Lichao Wang, Jianping Zhang, Chi Harold Liu, Lanqing Hong, Qiang Xu 핵심 연구 목표 본 논문은 RLHF(Reinforcement Learning from Human Feedback), 시스템 프롬프트, 입력/출력 콘텐츠 필터 등 다양한 방어 메"},{"id":"2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents","title":"[논문리뷰] O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents","excerpt":"arXiv에 게시된 'O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents","tags":["Review","Memory System","LLM Agents","Personalization","User Profiling","Hierarchical Retrieval","Long-Term Interaction","Self-Evolving Agents","Contextual Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: OPPO AI Agent Team 핵심 연구 목표 기존 LLM 기반 에이전트가 장기적인 상호작용, 맥락적 일관성, 동적 개인화에 직면하는 한계를 극복하는 것이 목표입니다. 특히, 기존 메모리 시스템의 의미론적 그룹화 및 검색 노이즈 문제로 인해 놓치는 중요한 사용자 정보를 효과적으로 관리하여, 보다 적응적이고 일관성 "},{"id":"2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists","title":"[논문리뷰] OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists","excerpt":"Weiquan Lin이 arXiv에 게시한 'OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists","tags":["Review","AI Scientist","Large Language Models (LLMs)","Human-AI Collaboration","Scientific Ecosystem","Research Automation","Omni Scientific Protocol (OSP)","ScienceArena","Knowledge Graph"],"text":"링크: 논문 PDF로 바로 열기 저자: Weiquan Lin, Keyu Zhao, Yu Li, Dehao Huang, Chenyang Shao 핵심 연구 목표 기존 AI Scientist 시스템이 과학적 발견을 독립적인 검색/최적화 문제로만 보고, 과학 연구의 사회적, 협력적 특성을 간과하는 한계를 해결합니다. 본 연구는 AI 에이전트가 단순한 태스크 실행"},{"id":"2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe","title":"[논문리뷰] OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe","excerpt":"arXiv에 게시된 'OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe","tags":["Review","Multimodal Reasoning","Large Multimodal Models","Supervised Fine-tuning","Reinforcement Learning","Data Curation","Open-source","Multimodal Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, Lidong Bing 핵심 연구 목표 멀티모달 추론(Multimodal Reasoning) 분야에서 투명하고 재현 가능한 데이터 큐레이션 및 훈련 전략 의 부재로 인한 확"},{"id":"2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs","title":"[논문리뷰] Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs","excerpt":"arXiv에 게시된 'Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs","tags":["Review","LLM Sycophancy","Model Robustness","AI Alignment","Benchmark","Confidence Calibration","Behavioral Taxonomy","Social Influence","Epistemic Collapse"],"text":"링크: 논문 PDF로 바로 열기 저자: Yusuf Çelebi, Mahmoud El Hussieni, Özay Ezerceli 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)이 권위나 설득과 같은 사회적 압력 에 직면했을 때 진실성을 왜곡하고 정확도가 저하되는 아첨(sycophancy) 현상을 측정하기 위한 견고성 중심의 프레임워크 를 제시합니다. 기"},{"id":"2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","title":"[논문리뷰] Planning with Sketch-Guided Verification for Physics-Aware Video Generation","excerpt":"Shayegan Omidshafiei이 arXiv에 게시한 'Planning with Sketch-Guided Verification for Physics-Aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","tags":["Review","Video Generation","Motion Planning","Physics-Aware AI","Multimodal Verification","Diffusion Models","Test-Time Optimization","Sketch-Guided"],"text":"링크: 논문 PDF로 바로 열기 저자: Yidong Huang, Zun Wang, Han Lin, DongKi Kim, Shayegan Omidshafiei, Jaehong Yoon, Yue Zhang, Mohit Bansal 핵심 연구 목표 이 논문은 비디오 생성 모델이 복잡한 동작 명령을 따르고 물리적으로 사실적이며 시간적으로 일관된 시퀀스를 생성하는 "},{"id":"2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations","title":"[논문리뷰] Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations","excerpt":"Noam Koenigstein이 arXiv에 게시한 'Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations","tags":["Review","Saliency Maps","Explainable AI (XAI)","Taxonomy","Evaluation Framework","Faithfulness Metrics","Contrastive Explanations","Granularity"],"text":"링크: 논문 PDF로 바로 열기 저자: Yehonatan Elisha, Seffi Cohen, Oren Barkan, Noam Koenigstein 핵심 연구 목표 본 연구는 심층 학습 모델의 시각적 설명 기법인 Saliency Map 이 명확한 목적과 사용자 질의에 대한 정렬이 부족하여 평가 및 실용적 효용성이 저해되는 문제를 해결하는 것을 목표로 합니다"},{"id":"2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model","title":"[논문리뷰] RynnVLA-002: A Unified Vision-Language-Action and World Model","excerpt":"arXiv에 게시된 'RynnVLA-002: A Unified Vision-Language-Action and World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model","tags":["Review","Vision-Language-Action (VLA) Model","World Model","Robotics","Unified Framework","Multi-modal Learning","Action Generation","Attention Mask","Continuous Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Jun Cen, Siteng Huang, Yuqian Yuan, Kehan Li, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Xin Li, Hao Luo, Fan Wang, Deli Zhao, Hao Chen 핵심 연구 목표 본 논문은 기존 VLA 모델(액션 다이내믹스"},{"id":"2025-11-24-SAM-3-Segment-Anything-with-Concepts","title":"[논문리뷰] SAM 3: Segment Anything with Concepts","excerpt":"arXiv에 게시된 'SAM 3: Segment Anything with Concepts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-SAM-3-Segment-Anything-with-Concepts","tags":["Review","Segment Anything Model","Open-Vocabulary Segmentation","Multimodal Foundation Model","Instance Segmentation","Video Object Tracking","Prompt Engineering","Data Engine","Human-in-the-loop"],"text":"링크: 논문 PDF로 바로 열기 저자: Nicolas Carion, Laura Gustafson, YuanTing Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, Jie Lei, Tengyu Ma,"},{"id":"2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection","title":"[논문리뷰] Taming Generative Synthetic Data for X-ray Prohibited Item Detection","excerpt":"Renshuai Tao이 arXiv에 게시한 'Taming Generative Synthetic Data for X-ray Prohibited Item Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection","tags":["Review","X-ray Security","Synthetic Data Generation","Diffusion Models","Object Detection","Cross-Attention","Image Inpainting","Data Augmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jialong Sun, Hongguang Zhu, Weizhe Liu, Yunda Sun, Renshuai Tao and Yunchao Wei 핵심 연구 목표 Xray 보안 이미지에서 금지 품목 탐지 모델을 훈련하기 위한 데이터 부족 문제 와 기존 합성 데이터 생성 방법론의 노동 집약적인 전처리 단계(예: 전경 추출)"},{"id":"2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story","title":"[논문리뷰] Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story","excerpt":"Kristian Kuznetsov이 arXiv에 게시한 'Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story","tags":["Review","Intrinsic Dimension","LLMs","Text Complexity","Sparse Autoencoders","Text Semantics","Genre Analysis","Embedding Space","Text Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Vladislav Pedashenko, Laida Kushnareva, Yana Khassan Nibal, Eduard Tulchinskii, Kristian Kuznetsov, Vladislav Zharchinskii, Yury Maximov, Irina Piontkovskaya 핵심 연구 목표 본 논문은 현대 LL"},{"id":"2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation","title":"[논문리뷰] VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation","excerpt":"Gim Hee Lee이 arXiv에 게시한 'VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation","tags":["Review","Vision-Language-Action Models","Robotic Manipulation","SpatioTemporal Coherence","4D Awareness","Visual Representation","Action Representation","Cross-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanyu Zhou, Chuanhao Ma, Gim Hee Lee 핵심 연구 목표 본 논문은 기존 VLA 모델이 겪는 공간시간적 불연속성(spatiotemporally discontinuous) 및 미세한 제어 부족 문제를 해결하여, 로봇 조작을 위한 공간시간적으로 일관성 있는(spatiotemporally coher"},{"id":"2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination","title":"[논문리뷰] Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination","excerpt":"Jing Bi이 arXiv에 게시한 'Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination","tags":["Review","Video Reasoning","Large Multimodal Models","Reinforcement Learning","Visual Rumination","Text-Rich Video","Video Question Answering","Iterative Perception"],"text":"링크: 논문 PDF로 바로 열기 저자: Yolo Yunlong Tang, Daiki Shimada, Hang Hua, Chao Huang, Jing Bi, Rogerio Feris, Chenliang Xu 핵심 연구 목표 본 논문은 텍스트가 풍부한 비디오에서 미세한 증거를 기반으로 하는 추론 문제, 특히 기존 단일 패스(singlepass) 비디오 QA 모"},{"id":"2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models","title":"[논문리뷰] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models","excerpt":"Yudong Zhang이 arXiv에 게시한 'VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models","tags":["Review","Vision-Language Models","Latent Memory","Cognitive Memory","Visual Grounding","Short-term Memory","Long-term Memory","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinlei Yu, Chengming Xu, Guibin Zhang, Zhangquan Chen, Yudong Zhang, Jiangning Zhang, Xiaobin Hu, Yongbo He, PengTao Jiang, Shuicheng Yan 핵심 연구 목표 본 논문은 VisionLanguage Models(VLM"},{"id":"2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds","title":"[논문리뷰] WorldGen: From Text to Traversable and Interactive 3D Worlds","excerpt":"arXiv에 게시된 'WorldGen: From Text to Traversable and Interactive 3D Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-24 00:00:00+0900+0900","permalink":"/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds","tags":["Review","3D World Generation","Text-to-3D","Generative AI","Procedural Generation","Scene Decomposition","Navmesh","Game Engines","Interactive Environments"],"text":"링크: 논문 PDF로 바로 열기 저자: Dilin Wang, Hyunyoung Jung, Tom Monnier, Kihyuk Sohn, Chuhang Zou, Xiaoyu Xiang, YuYing Yeh, Di Liu, Zixuan Huang, Thu NguyenPhuoc, Yuchen Fan, Sergiu Oprea, Ziyan Wang, Roman Sh"},{"id":"2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser","title":"[논문리뷰] AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser","excerpt":"arXiv에 게시된 'AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser","tags":["Review","HTML Extraction","Web Corpus","Large Language Models","Data Curation","Structured Element Preservation","Sequence Labeling","Markdown Conversion","MainWebBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Ren Ma, Jiantao Qiu †, Chao Xu †, Pei Chu, Kaiwen Liu, Pengli Ren, Yuan Qu, Jiahui Peng, Linfeng Hou, Mengjie Liu, Lindong Lu, Wenchang Ning, Jia Yu, Rui Min, Jin Shi, Haojiong C"},{"id":"2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning","title":"[논문리뷰] AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning","excerpt":"Alphamasterliu이 arXiv에 게시한 'AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning","tags":["Review","Automated Environment Generation","Cross-Environment Learning","Agent Learning","Language Models","Benchmark","Meta-Learning","Reinforcement Learning","Environment Design Language"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiayi Zhang, Yiran Peng, Fanqi Kong, Yang Cheng, Yifan Wu, Zhaoyang Yu, Jinyu Xiang, Jianhao Ruan, Jinlin Wang, Maojia Song, HongZhang Liu, Xiangru Tang, Bang Liu, Chenglin Wu, Y"},{"id":"2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling","title":"[논문리뷰] Budget-Aware Tool-Use Enables Effective Agent Scaling","excerpt":"arXiv에 게시된 'Budget-Aware Tool-Use Enables Effective Agent Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling","tags":["Review","LLM Agents","Tool Use","Budget Awareness","Test-time Scaling","Cost-Performance","Web Search Agents","Planning","Self-Verification"],"text":"링크: 논문 PDF로 바로 열기 저자: Tengxiao Liu, Zifeng Wang, Jin Miao, IHung Hsu, Jun Yan, Jiefeng Chen, Rujun Han, Fangyuan Xu, Yanfei Chen, Ke Jiang, Samira Daruki, Yi Liang, William Yang Wang, Tomas Pfister, C"},{"id":"2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens","title":"[논문리뷰] Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens","excerpt":"Stephanie Fu이 arXiv에 게시한 'Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens","tags":["Review","Vision-Language Models (VLMs)","Chain-of-Thought (CoT)","Continuous Visual Tokens","Multimodal Reasoning","Perceptual Grounding","Visual Thinking","Dense Prediction"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiming Qin, Bomin Wei, Jiaxin Ge, Konstantinos Kallidromitis, Trevor Darrell, Xudong Wang, Stephanie Fu 핵심 연구 목표 기존 VLM이 이산적인 텍스트 기반 추론에 국한되어 공간 추론 및 기하학적 인식과 같은 미세한 시각적 이해가 필요한 "},{"id":"2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface","title":"[논문리뷰] Computer-Use Agents as Judges for Generative User Interface","excerpt":"arXiv에 게시된 'Computer-Use Agents as Judges for Generative User Interface' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface","tags":["Review","Computer-Use Agents","Generative UI","AI-assisted Design","Human-Computer Interaction","LLM","AUI-Gym","Feedback Loop","Agent-centric Design"],"text":"링크: 논문 PDF로 바로 열기 저자: Kevin Qinghong Lin, Siyuan Hu, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Zheng Shou 핵심 연구 목표 현재 인간 중심적으로 설계된 GUI 가 ComputerUse Agent (CUA)의 비효율적인 태스크 수행을 강제하는 문제를"},{"id":"2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation","title":"[논문리뷰] Controllable Layer Decomposition for Reversible Multi-Layer Image Generation","excerpt":"arXiv에 게시된 'Controllable Layer Decomposition for Reversible Multi-Layer Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation","tags":["Review","Controllable Layer Decomposition","Diffusion Models","Multi-Layer Image Generation","Layer Separation","Bounding Box Guidance","Generative AI","Image Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihao Liu¹, Zunnan Xu¹, Shi Shu¹, Jun Zhou¹† Ruicheng Zhang¹,², Zhenchao Tang², Xiu Li¹† 핵심 연구 목표 본 논문은 합성된 래스터 이미지에서 레이어 수준의 편집이 불가능한 한계를 극복하고자 합니다. 기존 이미지 매팅 및 인페인팅 기반 방법들이 제어 "},{"id":"2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research","title":"[논문리뷰] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research","excerpt":"arXiv에 게시된 'DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research","tags":["Review","Reinforcement Learning","Evolving Rubrics","Deep Research","LLM Agents","Tool Use","Long-form QA","Open-source AI","Dynamic Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Rulin Shao, Akari Asai, Shannon Zejiang Shen, et al. 핵심 연구 목표 이 논문의 핵심 목표는 기존 개방형 심층 연구 모델들이 짧은 형식의 질문 답변(QA)에 초점을 맞춰 실제 장문형 심층 연구 작업에 적용하기 어렵다는 한계를 극복하는 것입니다. 특히, 모호한 평가 기준과 동적인"},{"id":"2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation","title":"[논문리뷰] DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation","excerpt":"arXiv에 게시된 'DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation","tags":["Review","Pixel Diffusion","Image Generation","Frequency Decoupling","Diffusion Transformer (DiT)","Flow Matching","AdaLN","Text-to-Image Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Zehong Ma, Longhui Wei, Shuai Wang, Shiliang Zhang, Qi Tian 핵심 연구 목표 기존 픽셀 확산 모델이 Diffusion Transformer (DiT) 하나로 고주파수 신호와 저주파수 의미론을 동시에 모델링하여 발생하는 느린 학습 및 추론 속도, 낮은 이미지 품질 문제를 해"},{"id":"2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems","title":"[논문리뷰] Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems","excerpt":"Oren Barkan이 arXiv에 게시한 'Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems","tags":["Review","Recommender Systems","Sparse Autoencoder (SAE)","Monosemantic Neurons","Interpretability","Prediction-Aware Loss","User-Item Interactions","Post-hoc Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Dor Arviv, Yehonatan Elisha, Oren Barkan, Noam Koenigstein 핵심 연구 목표 본 논문은 현대 추천 시스템의 잠재 임베딩이 의미론적으로 불투명하여 해석 가능성이 낮고 제어가 어렵다는 문제를 해결하고자 합니다. 특히, LLM 중심의 SAE와 달리 추천 시스템의 사용자아이템 상호"},{"id":"2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration","title":"[논문리뷰] Fidelity-Aware Recommendation Explanations via Stochastic Path Integration","excerpt":"Oren Barkan이 arXiv에 게시한 'Fidelity-Aware Recommendation Explanations via Stochastic Path Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration","tags":["Review","Recommender Systems","Explainable AI (XAI)","Explanation Fidelity","Path Integration","Stochastic Sampling","Counterfactual Explanations","Model-Agnostic","Sparse Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Oren Barkan¹, Yahlly Schein², Yehonatan Elisha², Veronika Bogina², Mikhail Baklanov², Noam Koenigstein²+ 핵심 연구 목표 본 논문은 추천 시스템에서 설명의 충실도(fidelity), 즉 설명이 모델의 실제 추론을 얼마나 정확하게 반영하는"},{"id":"2025-11-25-Flow-Map-Distillation-Without-Data","title":"[논문리뷰] Flow Map Distillation Without Data","excerpt":"Tommi Jaakkola이 arXiv에 게시한 'Flow Map Distillation Without Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data","tags":["Review","Flow Map Distillation","Data-Free Learning","Generative Models","Teacher-Student","Diffusion Acceleration","Teacher-Data Mismatch","One-Step Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Shangyuan Tong, Nanye Ma, Saining Xie, Tommi Jaakkola 핵심 연구 목표 본 논문은 반복적인 샘플링으로 인해 속도가 느린 최첨단 플로우 모델의 가속화를 위해 사용되는 플로우 맵 증류(flow map distillation) 기법의 데이터 의존성 문제 를 해결하고자 합니다. 기존 "},{"id":"2025-11-25-General-Agentic-Memory-Via-Deep-Research","title":"[논문리뷰] General Agentic Memory Via Deep Research","excerpt":"arXiv에 게시된 'General Agentic Memory Via Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research","tags":["Review","AI Agents","Memory Systems","Large Language Models (LLMs)","Just-in-Time (JIT) Compilation","Memorizer","Researcher","Reinforcement Learning","Context Management"],"text":"링크: 논문 PDF로 바로 열기 저자: B.Y. Yan, Chaofan Li, Hongjin Qian, Shuqi Lu, Zheng Liu 핵심 연구 목표 AI 에이전트 분야에서 널리 사용되는 정적 메모리(AOT Compilation) 방식의 심각한 정보 손실 문제와 복잡한 컨텍스트 관리의 한계를 해결하는 것을 목표로 합니다. 이를 위해, 런타임에 최적화된"},{"id":"2025-11-25-HunyuanVideo-1-5-Technical-Report","title":"[논문리뷰] HunyuanVideo 1.5 Technical Report","excerpt":"Fang Yang이 arXiv에 게시한 'HunyuanVideo 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report","tags":["Review","Video Generation","Diffusion Transformer","Sparse Attention","Super-Resolution","Open-Source","Multimodal Understanding","Training Optimization","Efficient Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Fang Yang, Duojun Huang, Changlin Li, Chang Zou, Bing Wu, et al. 핵심 연구 목표 경량화되면서도 강력한 오픈소스 비디오 생성 모델 Hunyuan Video 1.5 를 개발하여, 8.3억 파라미터로 최첨단 시각 품질과 움직임 일관성을 달성하고, 소비자용 GPU에서 효율적"},{"id":"2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control","title":"[논문리뷰] In-Video Instructions: Visual Signals as Generative Control","excerpt":"arXiv에 게시된 'In-Video Instructions: Visual Signals as Generative Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control","tags":["Review","Video Generation","Controllable AI","Visual Instructions","Image-to-Video","Spatial Control","Zero-shot Learning","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Gongfan Fang, Xinyin Ma, Xinchao Wang 핵심 연구 목표 본 논문은 대규모 비디오 생성 모델의 제어 가능성을 탐구하며, 기존 텍스트 프롬프트의 한계인 전역적이고 추상적인 제어를 극복하고자 합니다. 비디오 프레임 내에 시각적 신호를 직접 삽입하여 정밀하고 공간 인식적인 지시 를 통해 이미지투비"},{"id":"2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark","title":"[논문리뷰] M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark","excerpt":"Bangwei Guo이 arXiv에 게시한 'M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark","tags":["Review","Multimodal LLM","Tool Use","Agent Benchmark","Model Context Protocol","Multi-Hop Reasoning","Multi-Threaded Execution","Evaluation Metrics","Similarity Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Zhou, Mingyu Zhao, Zhenting Wang, Difei Gu, Bangwei Guo, Ruosong Ye, Ligong Han, Can Jin, Dimitris N. Metaxas 핵심 연구 목표 본 연구는 기존 LLM 도구 사용 벤치마크 들이 주로 텍스트 기반이고 선형적인 API 계획 에 초"},{"id":"2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models","title":"[논문리뷰] MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models","excerpt":"arXiv에 게시된 'MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models","tags":["Review","Vision-Language Models","Physics Reasoning","Motion Tracking","Spatial-Temporal Grounding","Video QA","AIGC Analysis","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiyang Wu, Zongxia Li, Jihui Jin, Guangyao Shi, Gouthaman KV, Vishnu Raj, Nilotpal Sinha, Jingxi Chen, Fan Du, Dinesh Manocha 핵심 연구 목표 본 연구는 기존 VisionLanguage Models (VLMs) 이 3D "},{"id":"2025-11-25-MIST-Mutual-Information-Via-Supervised-Training","title":"[논문리뷰] MIST: Mutual Information Via Supervised Training","excerpt":"Kyunghyun Cho이 arXiv에 게시한 'MIST: Mutual Information Via Supervised Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training","tags":["Review","Mutual Information Estimation","Supervised Learning","Meta-Learning","Neural Networks","Uncertainty Quantification","SetTransformer","Quantile Regression"],"text":"링크: 논문 PDF로 바로 열기 저자: German Gritsai, Megan Richards, Maxime Méloux, Kyunghyun Cho, Maxime Peyrard 핵심 연구 목표 본 논문은 고차원, 제한된 샘플, 복잡한 분포, 높은 MI(Mutual Information) 설정에서 기존 MI 추정기들이 겪는 성능 저하 문제를 해결하고자 합니다"},{"id":"2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO","title":"[논문리뷰] Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO","excerpt":"arXiv에 게시된 'Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO","tags":["Review","Multi-Agent Systems","Reinforcement Learning","LLM Training","Hierarchical Credit Assignment","Trajectory Alignment","Group Relative Policy Optimization","Tool-Augmented Reasoning","Vertical Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoyang Hong, Jiajun Yin, Yuan Wang, Jingnan Liu, Zhe Chen, Ailing Yu, Ji Li, Zhiling Ye, Hansong Xiao, Yefei Chen, Hualei Zhou, Yun Yue, Minghui Yang, Chunxiao Guo, Junwei Liu, "},{"id":"2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking","title":"[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking","excerpt":"Elias Stengel-Eskin이 arXiv에 게시한 'PRInTS: Reward Modeling for Long-Horizon Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking","tags":["Review","Reward Modeling","Long-Horizon Tasks","Information Seeking","Large Language Models","Trajectory Summarization","Reinforcement Learning","Tool Use","Process Reward Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jaewoo Lee, Archiki Prasad, Justin ChihYao Chen, Zaid Khan, Elias StengelEskin, Mohit Bansal 핵심 연구 목표 본 논문은 기존 Process Reward Model (PRM) 의 한계, 즉 짧은 추론 단위에 대한 이진 판단과 급증하는 컨텍스트 처리"},{"id":"2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models","title":"[논문리뷰] Pillar-0: A New Frontier for Radiology Foundation Models","excerpt":"arXiv에 게시된 'Pillar-0: A New Frontier for Radiology Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models","tags":["Review","Radiology Foundation Model","Volumetric Imaging","Multi-window Tokenization","Multi-scale Attention","Contrastive Learning","Clinical Evaluation","Data Efficiency","Medical Imaging"],"text":"링크: 논문 PDF로 바로 열기 저자: Kumar Krishna Agrawal, Longchao Liu, Long Lian, Michael Nercessian, Natalia Harguindeguy, Yufu Wu, Peter Mikhael, Gigin Lin, Lecia V. Sequist, Florian Fintelmann, Trevor Darrell,"},{"id":"2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning","title":"[논문리뷰] Plan-X: Instruct Video Generation via Semantic Planning","excerpt":"Chenxu Zhang이 arXiv에 게시한 'Plan-X: Instruct Video Generation via Semantic Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning","tags":["Review","Video Generation","Semantic Planning","Multimodal LLM","Diffusion Transformer","Spatio-temporal Guidance","Visual Hallucination","Prompt Alignment","Instruction Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Lun Huang, You Xie, Hongyi Xu, Tianpei Gu, Chenxu Zhang, Guoxian Song, Zenan Li, Xiaochen Zhao, Linjie Luo, Guillermo Sapiro 핵심 연구 목표 기존 비디오 확산 모델(DiT)이 복잡한 사용자 지시 및 장기 계획에서 겪는 높"},{"id":"2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis","title":"[논문리뷰] SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis","excerpt":"Hongwen Zhang이 arXiv에 게시한 'SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis","tags":["Review","Hand-Object Interaction","Multi-view Video Generation","4D Motion Synthesis","Diffusion Models","Spatio-temporal Consistency","Geometric Consistency","Appearance and Motion Joint Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Lingwei Dang, Zonghan Li, Juntong Li, Hongwen Zhang, Liang An, Yebin Liu, Qingyao Wu 핵심 연구 목표 본 논문은 단일 뷰(singleview) HOI 비디오 생성의 기하학적 왜곡 및 비현실적인 모션 문제와 3D HOI 방법론의 제한된 일반화 능력 문제를"},{"id":"2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets","title":"[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?","excerpt":"Zhaowei Lu이 arXiv에 게시한 'Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets","tags":["Review","World Models","Mapless Navigation","Semantic Path Planning","Robot Learning","Video Prediction","Benchmark","Trajectory Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaowei Lu, Zhexiao Sun, Zhihao Liang, Hongyuan Ye, Dingrui Wang 핵심 연구 목표 본 논문은 최신 세계 모델(World Models, WMs)이 텍스트로 지정된 암묵적인 의미론적 목표를 가진 길 없는 경로 계획(mapless path planning) 작업을 실제 환경"},{"id":"2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios","title":"[논문리뷰] UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios","excerpt":"arXiv에 게시된 'UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-25 00:00:00+0900+0900","permalink":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios","tags":["Review","Text-to-Image Generation","Diffusion Transformers","4K Resolution","Aspect Ratio Extrapolation","Data-Model Co-Design","VAE Post-training","Positional Encoding","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Tian Ye, Song Fei, Lei Zhu 핵심 연구 목표 본 논문은 기존 Diffusion Transformer(DiT) 모델을 다양한 종횡비(AR)의 4K 해상도 로 확장할 때 발생하는 한계를 극복하는 것을 목표로 합니다. 특히, 위치 인코딩, VAE 압축, 최적화 과정에서 발생하는 상호 연결된 문제점을 해결"},{"id":"2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning","title":"[논문리뷰] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning","excerpt":"arXiv에 게시된 'Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning","tags":["Review","Self-Evolving Agent","Vision-Language Models","Tool-Integrated Reasoning","Reinforcement Learning","Self-Correction","Multimodal AI","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaqi Liu, Kaiwen Xiong, Peng Xia, Yiyang Zhou, Haonian Ji, Lu Feng, Siwei Han, Mingyu Ding, Huaxiu Yao 핵심 연구 목표 본 논문은 기존 비전언어 에이전트가 인간 주석 기반 지도 학습의 한계와 복잡한 시각적 추론 단계 검증의 어려움, 그리"},{"id":"2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection","title":"[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection","excerpt":"Mike Zheng Shou이 arXiv에 게시한 'DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection","tags":["Review","AIGC Detection","Diffusion Models","Image Editing","Semantic Segmentation","Localization","Model Attribution","Benchmark","Multi-turn Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Mike Zheng Shou, Yingxin Xuan, Pei Yang, Ziheng Peng, Hai Ci 핵심 연구 목표 이 논문은 AI 생성 콘텐츠(AIGC) 탐지에서 전체 이미지 분류에 집중하는 기존 방식의 한계를 극복하고, 확산 모델 기반의 로컬 편집 에 대한 동시적인 편집 영역 위치 파악(localizati"},{"id":"2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward","title":"[논문리뷰] Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward","excerpt":"arXiv에 게시된 'Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward","tags":["Review","Unified Multimodal Models","Understanding-Generation Gap","Reasoning","Knowledge Transfer","Chain-of-Thought","Self-Training","Synthetic Data","Evaluation Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuwei Niu, Weiyang Jin, Jiaqi Liao, Chaoran Feng, Peng Jin, Bin Lin, Zongjian Li, Bin Zhu, Weihao Yu, Li Yuan 핵심 연구 목표 본 논문은 통합 멀티모달 모델(UMMs)에서 \"이해\" 능력이 \"생성\" 과정에 실제로 정보를 제공하고 안내하"},{"id":"2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use","title":"[논문리뷰] Fara-7B: An Efficient Agentic Model for Computer Use","excerpt":"arXiv에 게시된 'Fara-7B: An Efficient Agentic Model for Computer Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use","tags":["Review","Computer Use Agents","Synthetic Data Generation","Multi-modal LLM","On-device AI","Web Automation","Pixel-in Action-out","Fara-7B","WebTailBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Ahmed Awadallah⁺, Yash Lara, Raghav Magazine, Hussein Mozannar, Akshay Nambi, Yash Pandya, Aravind Rajeswaran, Corby Rosset, Alexey Taymanov, Vibhav Vineet, Spencer Whitehead, An"},{"id":"2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms","title":"[논문리뷰] GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms","excerpt":"arXiv에 게시된 'GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms","tags":["Review","LLM-driven Evolutionary Computation","Quality-Diversity","MAP-Elites","Program Synthesis","Open-source Framework","Algorithmic Discovery","Genetic Algorithms"],"text":"링크: 논문 PDF로 바로 열기 저자: Valentin Khrulkov, Andrey V. Galichin, Denis Bashkirov, Dmitry Vinichenko, Oleg Travkin, Roman Alferov, Andrey Kuznetsov, Ivan Oseledets 핵심 연구 목표 이 논문은 LLM(대규모 언어 모델) 기반 진화 컴퓨테이션"},{"id":"2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI","title":"[논문리뷰] GigaWorld-0: World Models as Data Engine to Empower Embodied AI","excerpt":"Chaojun Ni이 arXiv에 게시한 'GigaWorld-0: World Models as Data Engine to Empower Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI","tags":["Review","World Models","Embodied AI","Data Generation","Video Generation","3D Scene Reconstruction","Robotics","Vision-Language-Action"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaojun Ni, Boyuan Wang, Angen Ye, GigaWorld Team, JeffWang 핵심 연구 목표 본 논문은 GigaWorld0 라는 통합 월드 모델 프레임워크를 개발하여 Embodied AI 를 위한 확장 가능하고 데이터 효율적인 데이터 엔진 으로 활용하는 것을 목표로 합니다. 이는 Visi"},{"id":"2025-11-26-HunyuanOCR-Technical-Report","title":"[논문리뷰] HunyuanOCR Technical Report","excerpt":"arXiv에 게시된 'HunyuanOCR Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-HunyuanOCR-Technical-Report","tags":["Review","Optical Character Recognition","Multimodal Large Language Model","End-to-End Learning","Reinforcement Learning","Document Parsing","Information Extraction","Text Spotting"],"text":"링크: 논문 PDF로 바로 열기 저자: Tencent Hunyuan Vision Team 핵심 연구 목표 기존 파이프라인 기반 OCR 시스템의 에러 전파 및 높은 유지보수 비용 문제를 해결하고, 대규모 일반 VLM의 높은 컴퓨팅 자원 요구사항 과 OCR 특화 VLM의 불완전한 엔드투엔드 최적화 한계를 극복하는 것을 목표로 합니다. 1B 파라미터 규모의 경량"},{"id":"2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts","title":"[논문리뷰] MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts","excerpt":"arXiv에 게시된 'MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts","tags":["Review","3D City Generation","Natural Language Processing","Aesthetic Adaptation","Controllable Assets","Layout Generation","Interactive Editing","Diffusion Models","Multimodal Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Zilong Huang, Jun He, Xiaobin Huang, Ziyi Xiong, Yang Luo, Junyan Ye, Weijia Li, Yiping Chen, Ting Han 핵심 연구 목표 기존 3D 도시 생성 방법론의 한계인 텍스트 기반 생성의 창의적 유연성과 객체 수준 편집 가능성 및 구조적 일관성 부족"},{"id":"2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts","title":"[논문리뷰] MedSAM3: Delving into Segment Anything with Medical Concepts","excerpt":"Yi Lu이 arXiv에 게시한 'MedSAM3: Delving into Segment Anything with Medical Concepts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts","tags":["Review","Medical Image Segmentation","Segment Anything Model (SAM)","Promptable Concept Segmentation (PCS)","Multimodal Large Language Models (MLLMs)","Agentic AI","Domain Adaptation","Text-guided Segmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Anglin Liu, Rundong Xue, Xu R. Cao, Yifan Shen, Yi Lu, Xiang Li, Qianqian Chen, Jintai Chen 핵심 연구 목표 의료 영상 분할 분야에서 기존 모델들의 일반화 부족과 광범위한 수동 주석 요구 사항을 해결하고, 순전히 기하학적 프롬프트에 의존하는 한계를"},{"id":"2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation","title":"[논문리뷰] OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation","excerpt":"arXiv에 게시된 'OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation","tags":["Review","RGBA Generation","Multi-Task Learning","Diffusion Transformers","Image Matting","Layer Decomposition","Object Removal","Alpha-aware VAE","MSROPE-BiL"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Yu, Jiabo Zhan, Zile Wang, Jinglin Wang, Huaisong Zhang, Hongyu Li, Xinrui Chen, Yongxian Wei, Chun Yuan 핵심 연구 목표 본 연구는 RGBA(Red, Green, Blue, Alpha) 이미지 조작을 위한 기존의 파편화된 단일 태"},{"id":"2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","title":"[논문리뷰] PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding","excerpt":"Hongzhi Zhang이 arXiv에 게시한 'PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","tags":["Review","Video Generation","Physics Simulation","Controllable AI","Part-Aware","Semantic Grounding","Material Properties","Image-to-Video","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoze Zhang, Tianyu Huang, Zichen Wan, Xiaowei Jin, Hongzhi Zhang, Hui Li, Wangmeng Zuo 핵심 연구 목표 기존 비디오 생성 모델들이 시각적 품질은 뛰어나지만, 명시적인 물리적 제어 가능성과 현실성이 부족하다는 문제를 해결하는 것을 목표로 합니다. 단일"},{"id":"2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding","title":"[논문리뷰] ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding","excerpt":"arXiv에 게시된 'ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding","tags":["Review","Video Retake Generation","Camera Control","Rotary Position Embedding (RoPE)","Rotary Camera Encoding (RoCE)","Geometric Consistency","Video Generative Models","Transformer Architecture","Multi-view Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: ByungHoon Kim, Byeongjun Park, Hyungjin Chung, Jong Chul Ye 핵심 연구 목표 본 연구는 기존 비디오 리테이크 생성 방법론이 가변 길이 입력, 동적 카메라 모션, 분포 외 카메라 궤적에 취약하며, 종종 워핑 아티팩트나 흐릿한 객체를 생성하는 한계를 해결하고자 합니다. 특히,"},{"id":"2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space","title":"[논문리뷰] SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space","excerpt":"Yulan He이 arXiv에 게시한 'SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space","tags":["Review","Sparse Attention","Full Attention","Large Language Models (LLMs)","Context Length","Attention Sparsity","Alignment Loss","Long-Context Extrapolation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenyi Shen, Junru Lu, Lin Gui, Jiazheng Li, Yulan He, Di Yin, Xing Sun 핵심 연구 목표 대규모 언어 모델(LLM)에서 quadratic 연산 복잡성 을 갖는 full attention 의 한계를 극복하기 위해, sparse attention 의 성능 저하 및 부"},{"id":"2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs","title":"[논문리뷰] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs","excerpt":"arXiv에 게시된 'Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs","tags":["Review","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Tool-Integrated Reasoning (TIR)","Agentic AI","VQA","Training Environment","Behavioral Cloning","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Meng Lu, Ran Xu, Yi Fang, Wenxuan Zhang, Yue Yu, Gaurav Srivastava, Yuchen Zhuang, Mohamed Elhoseiny, Charles Fleming, Carl Yang, Zhengzhong Tu, Guanghua Xiao, Yang Xie, Hanrui W"},{"id":"2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System","title":"[논문리뷰] SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System","excerpt":"arXiv에 게시된 'SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System","tags":["Review","Multi-Agent System","Video Understanding","Scientific Education","Deming Cycle","Large Language Models","Iterative Optimization","Knowledge Integration","Educational Content Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyu Xu, Weilong Yan, Yufei Shi, Xin Meng, Tao He, Huiping Zhuang, Ming Li, Hehe Fan 핵심 연구 목표 본 논문은 과학 영상 이해 및 교육 분야에서 기존 멀티모달 대규모 언어 모델(MLLMs) 및 영상 에이전트 시스템의 한계를 극복하는 것을 목표로 합니"},{"id":"2025-11-26-Soft-Adaptive-Policy-Optimization","title":"[논문리뷰] Soft Adaptive Policy Optimization","excerpt":"arXiv에 게시된 'Soft Adaptive Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization","tags":["Review","Reinforcement Learning","Large Language Models","Policy Optimization","Importance Ratios","Soft Clipping","Trust Region","Mixture-of-Experts","Asymmetric Temperature"],"text":"링크: 논문 PDF로 바로 열기 저자: Chang Gao, Chujie Zheng, Bowen Yu, An Yang, XiongHui Chen, Shuai Bai, Kai Dang, Jingren Zhou, Shixuan Liu, Junyang Lin 핵심 연구 목표 본 논문은 LLM(Large Language Models)의 RL(Reinforcement"},{"id":"2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers","title":"[논문리뷰] UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers","excerpt":"arXiv에 게시된 'UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers","tags":["Review","Video Diffusion Transformers","Length Extrapolation","Attention Mechanism","Attention Dispersion","Periodic Content Repetition","Quality Degradation","Training-free Method","Plug-and-play"],"text":"링크: 논문 PDF로 바로 열기 저자: Min Zhao, Hongzhou Zhu, Yingze Wang, Bokai Yan, Jintao Zhang, Guande He, Ling Yang, Chongxuan Li, Jun Zhu 핵심 연구 목표 비디오 Diffusion Transformer(DiT) 모델이 학습 길이 이상으로 비디오를 생성할 때 발생하는 주"},{"id":"2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields","title":"[논문리뷰] Unified all-atom molecule generation with neural fields","excerpt":"arXiv에 게시된 'Unified all-atom molecule generation with neural fields' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields","tags":["Review","Molecule Generation","Neural Fields","Score-based Generative Models","Drug Design","Modality-agnostic","Antibody Design","Macrocyclic Peptides","All-atom"],"text":"링크: 논문 PDF로 바로 열기 저자: Matthieu Kirchmeyer, Pedro O. Pinheiro, Emma Willett, Karolis Martinkus, Joseph Kleinhenz, Emily K. Makowski, Andrew M. Watkins, Vladimir Gligorijevic, Richard Bonneau, Saeed Sar"},{"id":"2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering","title":"[논문리뷰] VQ-VA World: Towards High-Quality Visual Question-Visual Answering","excerpt":"Feng Li이 arXiv에 게시한 'VQ-VA World: Towards High-Quality Visual Question-Visual Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering","tags":["Review","Visual Question Answering (VQA)","Image Generation","Data-centric AI","Agentic Pipeline","Multimodal Models","Web-scale Data","Benchmark","LightFusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenhui Gou, Zilong Chen, Zeyu Wang, Feng Li, Deyao Zhu, Zicheng Duan, Kunchang Li, Chaorui Deng, Hongyi Yuan, Haoqi Fan, Cihang Xie, Jianfei Cai, Hamid Rezatofighi 핵심 연구 목표 본 논문"},{"id":"2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion","title":"[논문리뷰] Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion","excerpt":"Zhifei Yang이 arXiv에 게시한 'Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion","tags":["Review","3D City Generation","Generative AI","Large Language Models","Vision-Language Models","Multi-Agent Framework","Self-Critic Learning","Scene Graph","Text-to-3D"],"text":"링크: 논문 PDF로 바로 열기 저자: Keyang Lu, Sifan Zhou, Hongbin Xu, Gang Xu, Zhifei Yang, Yikai Wang, Zhen Xiao, Jieyi Long, Ming Li 핵심 연구 목표 기존 3D 도시 생성 방법론들이 단일 확산 모델에 의존하여 개인화 및 무한 확장성에서 한계를 보이는 문제를 해결합니다. Yo"},{"id":"2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation","title":"[논문리뷰] iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation","excerpt":"arXiv에 게시된 'iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-26 00:00:00+0900+0900","permalink":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation","tags":["Review","Image Generation","Video Models","Diffusion Models","Many-to-many","Unified Framework","Temporal Consistency","Image Editing","Positional Embedding"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhoujie Fu¹,², Xianfang Zeng²,⁺⁺, Jinghong Lan², Xinyao Liao¹,², Cheng Chen¹, Junyi Chen³, Jiacheng Wei¹, Wei Cheng², Shiyu Liu², Yunuo Chen²,³, Gang Yu†,², Guosheng Lin†,¹ 핵심 연구"},{"id":"2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","title":"[논문리뷰] Block Cascading: Training Free Acceleration of Block-Causal Video Models","excerpt":"arXiv에 게시된 'Block Cascading: Training Free Acceleration of Block-Causal Video Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","tags":["Review","Video Generation","Diffusion Models","Block-Causal Models","Inference Acceleration","Multi-GPU Parallelism","Training-Free","KV Caching","Interactive AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Hmrishav Bandyopadhyay, Nikhil Pinnaparaju, Rahim Entezari, Jim Scott, YiZhe Song, Varun Jampani 핵심 연구 목표 블록인과(blockcausal) 비디오 생성 모델, 특히 1.3B 모델 이 16 FPS , 14B 모델 이 4.5 FPS 에 불과"},{"id":"2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization","title":"[논문리뷰] Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization","excerpt":"Youngjung Uh이 arXiv에 게시한 'Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization","tags":["Review","3D Gaussian Splatting","Generalization","Sharpness-Aware Minimization","Regularization","Novel View Synthesis","Sparse View Reconstruction","Loss Landscape","Frequency-Adaptive"],"text":"링크: 논문 PDF로 바로 열기 저자: Youngsik Yun, Dongjun Gu, Youngjung Uh 핵심 연구 목표 본 논문은 3D Gaussian Splatting (3DGS) 이 fewshot 시나리오에서 sparse observations에 과적합되어 novel viewpoints에 대한 일반화 성능이 저하되는 문제를 해결하고자 합니다. 3D"},{"id":"2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy","title":"[논문리뷰] Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy","excerpt":"arXiv에 게시된 'Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy","tags":["Review","Audio-Visual Generation","Cross-Modal Synchronization","Diffusion Models","Cross-Task Synergy","Classifier-Free Guidance","Multimodal AI","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Teng Hu, Zhentao Yu, Guozhen Zhang, Zihan Su, Zhengguang Zhou, Youliang Zhang, Yuan Zhou, Qinglin Lu, Ran Yi 핵심 연구 목표 본 논문은 오디오비디오 동시 생성 모델에서 발생하는 불안정한 오디오비디오 정렬 문제를 해결하는 것을 목표로 "},{"id":"2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation","title":"[논문리뷰] I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation","excerpt":"arXiv에 게시된 'I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation","tags":["Review","Health Indicator (HI)","Remaining Useful Life (RUL)","Uncertainty Quantification (UQ)","Autoencoder (AE)","Latent Space","Degradation Modeling","Prognostics","Condition-Based Maintenance"],"text":"링크: 논문 PDF로 바로 열기 저자: Lucas Thil, Jesse Read, Rim Kaddah, Guillaume Doquet 핵심 연구 목표 본 논문은 복잡한 다중 센서 시스템에서 RUL(Remaining Useful Life) 예측 을 위한 건강 지표(HI)의 질을 향상시키는 것을 목표로 합니다. 기존 HI 추출 방법론의 한계인 복잡한 열화 메커"},{"id":"2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs","title":"[논문리뷰] Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs","excerpt":"Xin Yang이 arXiv에 게시한 'Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs","tags":["Review","Diffusion Models","Timestep Distillation","Consistency Models","Latent Space","Image-Free Training","Efficiency Optimization","Trajectory Sampling","Continuous-Time Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Bao Tang, Shuai Zhang, Yueting Zhu, Jijun Xiang, Xin Yang, Li Yu, Wenyu Liu, Xinggang Wang+ 핵심 연구 목표 이 논문은 확산 모델의 생성 효율성을 향상시키기 위한 timestep distillation 의 한계를 극복하고자 합니다. 특히, 기존 c"},{"id":"2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation","title":"[논문리뷰] Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation","excerpt":"Jiahao He이 arXiv에 게시한 'Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation","tags":["Review","World Simulation","Video Generation","Block Diffusion","Semi-Autoregressive","KV Cache Management","Inference Engine","Long Video Generation","Performance Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahao He, Yizeng Han, Tianyu Feng, Inferix Team, SteveZeyuZhang 핵심 연구 목표 기존 비디오 확산 모델의 비효율성 및 고정 길이 제약과 AR 모델의 낮은 품질 및 병렬화 불가능 문제를 극복하고자 합니다. Inferix 는 최적화된 준자동회귀(blockdiffusion"},{"id":"2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems","title":"[논문리뷰] Latent Collaboration in Multi-Agent Systems","excerpt":"arXiv에 게시된 'Latent Collaboration in Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems","tags":["Review","Multi-Agent Systems","Large Language Models","Latent Space","Latent Reasoning","Latent Communication","KV Cache","Computational Efficiency","Training-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, Ling Yang 핵심 연구 목표 본 논문은 기존 대규모 "},{"id":"2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots","title":"[논문리뷰] MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots","excerpt":"Rui Yang이 arXiv에 게시한 'MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots","tags":["Review","Vision-Language-Action (VLA)","Mobile Robotics","Quadruped Robots","Chain-of-Thought (CoT)","Reinforcement Learning (RL)","Embodied AI","Multimodal Perception"],"text":"링크: 논문 PDF로 바로 열기 저자: Ting Huang, Dongjian Li, Rui Yang, Zeyu Zhang, Zida Yang, Hao Tang 핵심 연구 목표 본 논문은 사족 보행 로봇의 자연어 명령을 연속적인 제어로 연결하는 데 따르는 근본적인 과제를 해결하고자 합니다. 기존 방법론이 고수준의 의미론적 추론과 저수준의 작동 사이의 간극을 "},{"id":"2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language","title":"[논문리뷰] Monet: Reasoning in Latent Visual Space Beyond Images and Language","excerpt":"Pengfei Wan이 arXiv에 게시한 'Monet: Reasoning in Latent Visual Space Beyond Images and Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language","tags":["Review","Latent Visual Reasoning","Multimodal Large Language Models (MLLMs)","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)","Visual-latent Policy Optimization (VLPO)","Chain-of-Thought (CoT)","Abstract Visual Thinking"],"text":"링크: 논문 PDF로 바로 열기 저자: Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, Yisen Wang 핵심 연구 목표 본 논문은 기존 MLLMs의 시각 추론이 외부 도구에 의존하고 인간과 같은 추상적인 시각적 사고가 부족하다는 문제를 해결하고자 "},{"id":"2025-11-27-NVIDIA-Nemotron-Parse-1-1","title":"[논문리뷰] NVIDIA Nemotron Parse 1.1","excerpt":"arXiv에 게시된 'NVIDIA Nemotron Parse 1.1' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-NVIDIA-Nemotron-Parse-1-1","tags":["Review","OCR","Document Parsing","Vision-Language Model","Encoder-Decoder","Transformer","Table Extraction","Multilingual OCR","Layout Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: NVIDIA 핵심 연구 목표 NemotronParse 1.1은 전작인 NemoretrieverParse1.0의 기능을 개선하여, 일반 OCR, 마크다운 형식 지정, 구조화된 표 구문 분석, 그림/차트/다이어그램의 텍스트 추출 등 문서 파싱 및 OCR 기능을 발전시키는 것을 목표로 합니다. 특히, 시각적으로 밀도가 높은"},{"id":"2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale","title":"[논문리뷰] RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale","excerpt":"Yangcheng Yu이 arXiv에 게시한 'RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale","tags":["Review","3D World Generation","City-Scale","Multimodal Agents","Reality Alignment","Urban Simulation","Foundation Models","Geospatial Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengyuan Wang, Zhiheng Zheng, Yu Shang, Lixuan He, Yangcheng Yu, Hangyu Fan, Jie Feng, Qingmin Liao, Yong Li 핵심 연구 목표 본 연구는 도시 규모 3D 세계 생성에서 기존 방법론이 직면한 품질, 충실도 및 확장성 문제를 해결하는 것"},{"id":"2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy","title":"[논문리뷰] Revisiting Generalization Across Difficulty Levels: It's Not So Easy","excerpt":"arXiv에 게시된 'Revisiting Generalization Across Difficulty Levels: It's Not So Easy' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy","tags":["Review","LLM Generalization","Task Difficulty","Item Response Theory","Cross-Difficulty","Data Curation","Model Evaluation","Supervised Fine-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yeganeh Kordi, Nihal V. Nayak, Max Zuo, Ilana Nguyen, Stephen H. Bach 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)이 다양한 난이도 수준의 태스크에 대해 얼마나 잘 일반화하는지 체계적으로 조사하는 것을 목표로 합니다. 기존 연구의 혼합된 결과를 해결하고, "},{"id":"2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning","title":"[논문리뷰] SPHINX: A Synthetic Environment for Visual Perception and Reasoning","excerpt":"Nidhi Rastogi이 arXiv에 게시한 'SPHINX: A Synthetic Environment for Visual Perception and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning","tags":["Review","Visual Reasoning","Synthetic Environment","LVLM Evaluation","Reinforcement Learning","Cognitive Primitives","Procedural Generation","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Md Tanvirul Alam, Justin Yang Chae, Saksham Aggarwal, Nidhi Rastogi 핵심 연구 목표 본 논문은 기존 벤치마크들이 시각적 인식보다 추론을 강조하거나 대칭, 정신적 회전 등 핵심 인지 원시 요소들을 체계적으로 평가하지 못하는 한계를 지적합니다. 이를 해결하기 위해 시각"},{"id":"2025-11-27-Terminal-Velocity-Matching","title":"[논문리뷰] Terminal Velocity Matching","excerpt":"Jiaming Song이 arXiv에 게시한 'Terminal Velocity Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-27 00:00:00+0900+0900","permalink":"/ai/review/2025-11-27-Terminal-Velocity-Matching","tags":["Review","Generative Models","Flow Matching","Diffusion Models","One-Step Generation","Few-Step Generation","Wasserstein Distance","Transformer Architecture","Lipschitz Continuity"],"text":"링크: 논문 PDF로 바로 열기 저자: Linqi Zhou, Mathias Parger, Ayaan Haque, Jiaming Song 핵심 연구 목표 논문은 고품질 샘플을 빠르고 효율적으로 생성하며, 고차원 데이터에 확장 가능한 생성 모델을 단일 훈련 단계로 구축하는 것을 목표로 합니다. 특히, 기존 Diffusion Models 및 Flow Matchi"},{"id":"2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory","title":"[논문리뷰] Agentic Learner with Grow-and-Refine Multimodal Semantic Memory","excerpt":"Qunyi Xie이 arXiv에 게시한 'Agentic Learner with Grow-and-Refine Multimodal Semantic Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","permalink":"/ai/review/2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory","tags":["Review","Multimodal LLMs","Semantic Memory","Agentic Learning","Error Attribution","Visual Reasoning","Long-term Memory","Grow-and-Refine","Multimodal Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Weihao Bo, Shan Zhang, Yanpeng Sun, Jingjing Wu, Qunyi Xie, Xiao Tan, Kunbin Chen, Wei He, Xiaofan Li, Na Zhao, Jingdong Wang, Zechao Li 핵심 연구 목표 현재 MLLM(Multimodal Large Languag"},{"id":"2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls","title":"[논문리뷰] Canvas-to-Image: Compositional Image Generation with Multimodal Controls","excerpt":"Kfir Aberman이 arXiv에 게시한 'Canvas-to-Image: Compositional Image Generation with Multimodal Controls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","permalink":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls","tags":["Review","Image Generation","Diffusion Models","Compositional Control","Multimodal Control","Unified Canvas","Multi-Task Learning","Personalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yusuf Dalva, Guocheng Gordon Qian, Maya Goldenberg, TsaiShien Chen, Kfir Aberman 핵심 연구 목표 본 연구는 최신 확산 모델이 텍스트 프롬프트, 객체 참조, 공간 배치, 포즈 제약, 레이아웃 주석 등 다양한 유형의 제어 신호를 동시에 처리할 때 발생하는 제"},{"id":"2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing","title":"[논문리뷰] MIRA: Multimodal Iterative Reasoning Agent for Image Editing","excerpt":"Jiebo Luo이 arXiv에 게시한 'MIRA: Multimodal Iterative Reasoning Agent for Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","permalink":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing","tags":["Review","Image Editing","Multimodal AI","Iterative Reasoning","Agentic AI","Reinforcement Learning","Diffusion Models","Vision-Language Models","Instruction Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyun Zeng, Hang Hua, Jiebo Luo 핵심 연구 목표 이 논문은 확산 기반 이미지 편집 모델이 복잡한 사용자 지침(구성 관계, 맥락적 단서, 참조 표현 등)을 정확하게 해석하지 못하여 발생하는 의미론적 드리프트 및 편집 실패 문제를 해결하는 것을 목표로 합니다. 단일 프롬프트 실행의 한계를 넘어 반"},{"id":"2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following","title":"[논문리뷰] Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following","excerpt":"arXiv에 게시된 'Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","permalink":"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following","tags":["Review","Multimodal Judges","LMM Evaluation","Pluralistic Criteria","Criteria-Following","Trade-off Sensitivity","Conflict Resolution","Reward Models","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyi Xiong, Yi Ge, Ming Li, Zuotong Zhang, Pranav Kulkarni, Kaishen Wang, Qi He, Zeying Zhu, Chenxi Liu, Ruibo Chen, Tong Zheng, Yanshuo Chen, Xiyao Wang, Renrui Zhang, Wenhui "},{"id":"2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","title":"[논문리뷰] Video Generation Models Are Good Latent Reward Models","excerpt":"arXiv에 게시된 'Video Generation Models Are Good Latent Reward Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","permalink":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","tags":["Review","Video Generation","Reward Feedback Learning","Latent Space","Diffusion Models","Human Preferences","Motion Quality","Process-aware"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoyue Mi, Wenqing Yu, Jiesong Lian, Shibo Jie, Ruizhe Zhong, Zijun Liu, Guozhen Zhang, Zixiang Zhou, Zhiyong Xu, Yuan Zhou, Qinglin Lu, Fan Tang 핵심 연구 목표 비디오 생성 모델을 인간의 선호도에 맞춰"},{"id":"2025-11-28-What-does-it-mean-to-understand-language","title":"[논문리뷰] What does it mean to understand language?","excerpt":"arXiv에 게시된 'What does it mean to understand language?' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-28 00:00:00+0900+0900","permalink":"/ai/review/2025-11-28-What-does-it-mean-to-understand-language","tags":["Review","Language Understanding","Cognitive Neuroscience","Situation Models","World Knowledge","Embodiment","fMRI","Large Language Models","Brain Networks"],"text":"링크: 논문 PDF로 바로 열기 저자: Colton Casto, Anna Ivanova, Evelina Fedorenko, Nancy Kanwisher 핵심 연구 목표 본 논문은 인간의 심층적인 언어 이해 가 뇌의 핵심 언어 시스템 내에서만 이루어지는 것이 아니라, 해당 시스템에서 얻은 정보가 다른 전문화된 뇌 영역으로 내보내져(exportation) 처리"},{"id":"2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models","title":"[논문리뷰] A Survey on Efficient Vision-Language-Action Models","excerpt":"arXiv에 게시된 'A Survey on Efficient Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models","tags":["Review","Embodied AI","Robotic Manipulation","VLA Models","Efficient AI","Model Compression","Efficient Training","Data Collection","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, and Heng Tao Shen, IEEE Fellow 핵심 연구 목표 이 논문은 대규모 VisionLanguageAction (VLA) 모델 "},{"id":"2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification","title":"[논문리뷰] Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification","excerpt":"Olga Russakovsky이 arXiv에 게시한 'Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification","tags":["Review","Text-to-Image Synthesis","Synthetic Data Generation","Fine-Grained Classification","Few-Shot Learning","Diffusion Models","Contextual Conditioning","Causal Intervention"],"text":"링크: 논문 PDF로 바로 열기 저자: William Yang, Xindi Wu, Zhiwei Deng, Esin Tureci, Olga Russakovsky 핵심 연구 목표 텍스트이미지(T2I) 모델을 활용한 합성 데이터 생성 에서 발생하는 과적합 및 다양성 감소 문제를 해결하고, 특히 소량 데이터(fewshot) 환경에서 미세 조정 분류(finegrai"},{"id":"2025-11-3-Continuous-Autoregressive-Language-Models","title":"[논문리뷰] Continuous Autoregressive Language Models","excerpt":"arXiv에 게시된 'Continuous Autoregressive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Continuous-Autoregressive-Language-Models","tags":["Review","Large Language Models (LLMs)","Continuous Representation","Autoencoder","Likelihood-Free Modeling","Energy-Based Models","Next-Vector Prediction","Computational Efficiency","Temperature Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenze Shao, Darren Li, Fandong Meng, Jie Zhou 핵심 연구 목표 Large Language Models (LLMs)의 비효율적인 순차적, 토큰 단위 생성 과정의 근본적인 한계를 극복하는 것이 목표입니다. 본 연구는 이산 토큰 예측에서 연속 벡터 예측 으로 패러다임을 전환하여, 각 생"},{"id":"2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16","title":"[논문리뷰] Defeating the Training-Inference Mismatch via FP16","excerpt":"arXiv에 게시된 'Defeating the Training-Inference Mismatch via FP16' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16","tags":["Review","Reinforcement Learning","LLM Fine-tuning","Training-Inference Mismatch","Floating Point Precision","FP16","BF16","RL Stability"],"text":"링크: 논문 PDF로 바로 열기 저자: Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin 핵심 연구 목표 대규모 언어 모델(LLM)의 강화 학습(RL) 미세 조정 과정에서 발생하는 불안정성의 근본 원인인 훈련추론 불일치(traininginference misma"},{"id":"2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model","title":"[논문리뷰] Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model","excerpt":"Jinwoo Shin이 arXiv에 게시한 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model","tags":["Review","Vision-Language-Action Models","World Models","Diffusion Models","Multimodal Learning","Robotics","Asynchronous Sampling","Diffusion Transformers"],"text":"링크: 논문 PDF로 바로 열기 저자: John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin 핵심 연구 목표 본 논문은 세계 모델이 증강된 VisionLanguageAction (VLA) 모델에서 차세대 관측 및 액션 시퀀스를 공동으로 예측하는 데 내재된 모달리티 충돌 문제를 해결하고자 합니다. "},{"id":"2025-11-3-Higher-order-Linear-Attention","title":"[논문리뷰] Higher-order Linear Attention","excerpt":"arXiv에 게시된 'Higher-order Linear Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Higher-order-Linear-Attention","tags":["Review","Linear Attention","Higher-order Interactions","Causal Streaming","Associative Scans","Prefix Summaries","Transformer Architectures","State Space Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Zhang, Zhen Qin, Quanquan Gu 핵심 연구 목표 논문은 scaled dotproduct attention의 이차 비용 문제를 해결하여 장문맥 언어 모델의 확장을 가능하게 하는 것을 목표로 합니다. 기존 선형 어텐션 및 State Space Models (SSMs)의 일차 또는 커널 기반 "},{"id":"2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration","title":"[논문리뷰] HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration","excerpt":"Anan Du이 arXiv에 게시한 'HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration","tags":["Review","GUI Grounding","Uncertainty Calibration","Reinforcement Learning","Confidence Estimation","Brier Score","GUI Agents","Visual-Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaojie Zhang, Pei Fu, Ruoceng Zhang, Jiahui Yang, Anan Du, Xiuwen Xi, Shaokang Wang, Ying Huang, Bin Qin, Zhenbo Luo, Jian Luan 핵심 연구 목표 본 논문은 자율 GUI(Graphical User Interface) 에"},{"id":"2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats","title":"[논문리뷰] INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats","excerpt":"arXiv에 게시된 'INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats","tags":["Review","Quantization","Low-bit Formats","Integer Quantization","Floating-Point Quantization","Large Language Models (LLMs)","Hardware Efficiency","Fine-Grained Quantization","MXINT8"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengzhao Chen, Meng Wu, Hui Jin, Zhihang Yuan, Jing Liu, Chaoyi Zhang, Yunshui Li, Jie Huang, Jin Ma, Zeyue Xue, Zhiheng Liu, Xingyan Bin, Ping Luo 핵심 연구 목표 현대 AI 하드웨어는 LLM의 아웃라이"},{"id":"2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning","title":"[논문리뷰] Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning","excerpt":"Nidhi Rastogi이 arXiv에 게시한 'Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning","tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Mathematical Reasoning","Large Language Models (LLMs)","Activity Scheduling","Longest Increasing Subsequence (LIS)","Generalization Limits","Reward Design","Self-consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Md Tanvirul Alam, Nidhi Rastogi 핵심 연구 목표 본 연구는 RLVR (Reinforcement Learning with Verifiable Rewards) 이 LLM (Large Language Models) 의 수학적 추론 능력을 진정으로 향상시키는지, 아니면 피상적인 휴리스틱을 강화하는지에"},{"id":"2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery","title":"[논문리뷰] Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery","excerpt":"Oğuz Hanoğlu이 arXiv에 게시한 'Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery","tags":["Review","Building Instance Segmentation","Height Classification","YOLOv11","Satellite Imagery","Multitask Learning","Remote Sensing","Urban Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mahmoud El Hussieni, Bahadır K. Güntürk, Hasan F. Ateş, Oğuz Hanoğlu 핵심 연구 목표 도시 계획, 3D 도시 모델링 및 인프라 모니터링에 필수적인 건물 인스턴스 분할 및 높이 분류의 정확도를 높이는 것을 목표로 합니다. 특히, 연속적인 높이 회귀 대신 이산적인 높이"},{"id":"2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data","title":"[논문리뷰] MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data","excerpt":"Nadiya Shvai이 arXiv에 게시한 'MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data","tags":["Review","Health Misinformation","Logical Fallacy Classification","Synthetic Data Generation","Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Parameter-Efficient Fine-tuning (PEFT)","LoRA","MISSCI Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Mykhailo Poliakov, Nadiya Shvai 핵심 연구 목표 본 연구는 건강 관련 허위 정보, 특히 과학적 발견을 왜곡하거나 오해하는 주장 내에 숨겨진 논리적 오류를 탐지하는 LLM의 능력 을 향상시키는 것을 목표로 합니다. MISSCI 데이터셋 의 데이터 희소성 문제를 해결하고, 제한된 주석 자원으로도 "},{"id":"2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games","title":"[논문리뷰] Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games","excerpt":"cavaunpeu이 arXiv에 게시한 'Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games","tags":["Review","Bounded One-Sided Response Games (BORGs)","Monopoly Deal","Benchmark Environment","Counterfactual Regret Minimization (CFR)","Imperfect Information Games","Game Theory","Self-Play","State Abstraction"],"text":"링크: 논문 PDF로 바로 열기 저자: Will Wolf 핵심 연구 목표 본 연구는 기존 게임 이론에서 충분히 다뤄지지 않은 Bounded OneSided Response Games (BORGs) 라는 동적 상호작용 패턴을 연구하기 위한 재현 가능한 벤치마크 환경 을 제공하는 것을 목표로 합니다. 이는 특정 플레이어의 행동이 상대방에게 고정된 조건을 충족하"},{"id":"2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows","title":"[논문리뷰] OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows","excerpt":"arXiv에 게시된 'OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows","tags":["Review","Mobile GUI Agents","Agent Safety","Hybrid Detection","Formal Verification","VLM-based Contextual Judgment","Safety Benchmark","Risk Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiushi Sun, Mukai Li, Zhoumianze Liu, Zhihui Xie, Fangzhi Xu, Zhangyue Yin, Kanzhi Cheng, Zehao Li, Zichen Ding, Qi Liu, Zhiyong Wu, Zhuosheng Zhang, Ben Kao, Lingpeng Kong 핵심 연구"},{"id":"2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals","title":"[논문리뷰] Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals","excerpt":"arXiv에 게시된 'Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals","tags":["Review","Distribution Matching Distillation","Few-step Diffusion","Score Matching","Mixture-of-Experts","Generative Models","Image Generation","Video Generation","Model Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangyu Fan, Zesong Qiu, Zhuguanyu Wu, Fanzhou Wang, Zhiqian Lin, Tianxiang Ren, Dahua Lin, Ruihao Gong, Lei Yang 핵심 연구 목표 본 논문은 Distribution Matching Distillation (DMD) 을 통해 스코어"},{"id":"2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning","title":"[논문리뷰] Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning","excerpt":"arXiv에 게시된 'Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning","tags":["Review","Conversational Recommender Systems","Large Language Models","Reinforcement Learning","Group Relative Policy Optimization","Rank-based Learning","Supervised Fine-tuning","Reward Shaping"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Vito Ostuni, Jundong Li, Nathan Kallus 핵심 연구 목표 본 논문은 LLM 기반 대화형 추천 시스템(CRS)이 직면한 카탈로그 외부 항목 생성 , 부적절한 출력 형식 , 그리고 추천 리스트 끝부분의 "},{"id":"2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models","title":"[논문리뷰] Revisiting Multimodal Positional Encoding in Vision-Language Models","excerpt":"arXiv에 게시된 'Revisiting Multimodal Positional Encoding in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models","tags":["Review","Multimodal Positional Encoding","Vision-Language Models","Rotary Positional Embedding (RoPE)","Transformer","Multimodal Understanding","Visual Grounding","Frequency Allocation","Position Design"],"text":"링크: 논문 PDF로 바로 열기 저자: Jie Huang, Hong Chang, Xuejing Liu, Sibo Song, Ruibing Hou, Junyang Lin, Shuai Bai 핵심 연구 목표 본 논문은 VisionLanguage Models (VLMs)에서 사용되는 멀티모달 위치 인코딩, 특히 Rotary Positional Embedding "},{"id":"2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens","title":"[논문리뷰] SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens","excerpt":"arXiv에 게시된 'SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens","tags":["Review","Chain-of-Thought (CoT)","Implicit Reasoning","LLMs","Semantic Alignment","Efficiency Optimization","Knowledge Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinhan He, Wendy Zheng, Yaochen Zhu, Zaiyi Zheng, Qi Guo, Lin Su, Liangjie Hong, Sriram Vasudevan, Jundong Li 핵심 연구 목표 현재 암시적 CoT(implicit CoT) 방법론이 직면한 두 가지 핵심 문제, 즉 (1) 암시적 추론과"},{"id":"2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning","title":"[논문리뷰] Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning","excerpt":"arXiv에 게시된 'Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning","tags":["Review","Self-supervised learning","Reinforcement Learning","Spatial Understanding","Vision-Language Models","Pretext Tasks","RGB-D Images","Spatial Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang 핵심 연구 목표 대규모 시각언어 모델(LVLM)의 공간 이해 능력 부족 이라는 한계를 해결하는 것을 목표로 합니다. 기"},{"id":"2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training","title":"[논문리뷰] Value Drifts: Tracing Value Alignment During LLM Post-Training","excerpt":"arXiv에 게시된 'Value Drifts: Tracing Value Alignment During LLM Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training","tags":["Review","LLM Alignment","Value Drift","Supervised Fine-Tuning (SFT)","Preference Optimization","RLHF","Llama-3","Qwen-3","Human Values"],"text":"링크: 논문 PDF로 바로 열기 저자: Mehar Bhatia, Shravan Nayak, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Vered Shwartz, Siva Reddy 핵심 연구 목표 본 연구는 LLM의 가치 정렬이 사후 훈련 과정에서 언제, 어떻게 발생하는지에 대한 기존 연구의 공백을 해결하고자 "},{"id":"2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning","title":"[논문리뷰] Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning","excerpt":"Hanyang Chen이 arXiv에 게시한 'Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning","tags":["Review","Visual Backdoor Attacks","MLLM Embodied Agents","Contrastive Trigger Learning","Policy Manipulation","Adversarial AI","Embodied AI Security","Multimodal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiusi Zhan, Hyeonjeong Ha, Rui Yang, Sirui Xu, Hanyang Chen, LiangYan Gui, YuXiong Wang, Huan Zhang, Heng Ji, Daniel Kang 핵심 연구 목표 본 논문은 MLLM(Multimodal Large Language Model) 기반 "},{"id":"2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models","title":"[논문리뷰] π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models","excerpt":"arXiv에 게시된 'π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:01:31+0900","permalink":"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models","tags":["Review","Reinforcement Learning (RL)","Vision-Language-Action Models (VLAs)","Flow-based Models","Policy Optimization","Robotics","Flow Matching","SDE","MDP"],"text":"링크: 논문 PDF로 바로 열기 저자: Kang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, Tiejun Huang, Yu Wang, Chao Yu 핵심 연구 목표 본 논문은 및 와 같은 플로우 기반(F"},{"id":"2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models","title":"[논문리뷰] Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models","excerpt":"Changfeng Ma이 arXiv에 게시한 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models","tags":["Review","Multimodal LLMs","Spatial Reasoning","Viewpoint Learning","Two-Stage Fine-tuning","3D Consistency","Viewpoint-100K","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei Bai, Wanli Ouyang, Yuanqi Li, Jie Guo, Yanwen Guo 핵심 연구 목표 본 "},{"id":"2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence","title":"[논문리뷰] AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence","excerpt":"Peter Worth이 arXiv에 게시한 'AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence","tags":["Review","LLM Benchmarking","Cyber Threat Intelligence (CTI)","Dynamic Evaluation","CTI Reasoning","Vulnerability Prediction","Threat Actor Attribution","Risk Mitigation","Natural Language Processing"],"text":"링크: 논문 PDF로 바로 열기 저자: Md Tanvirul Alam, Dipkamal Bhusal, Salman Ahmad, Nidhi Rastogi, Peter Worth 핵심 연구 목표 현재 LLM(Large Language Model) 벤치마크들이 정적 데이터셋에 의존하고 암기 능력을 주로 평가하여 현실적인 CTI(Cyber Threat Intell"},{"id":"2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance","title":"[논문리뷰] Data-Efficient RLVR via Off-Policy Influence Guidance","excerpt":"Jiale Cheng이 arXiv에 게시한 'Data-Efficient RLVR via Off-Policy Influence Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance","tags":["Review","Reinforcement Learning with Verifiable Rewards (RLVR)","Influence Functions","Data Selection","Off-Policy Learning","Curriculum Learning","Large Language Models (LLMs)","Sparse Random Projection","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Erle Zhu, Dazhi Jiang, Yuan Wang, Xujun Li, Jiale Cheng, Yuxian Gu, Yilin Niu, Aohan Zeng, Jie Tang, Minlie Huang, Hongning Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 능력 향상을 위한 Verif"},{"id":"2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench","title":"[논문리뷰] Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench","excerpt":"arXiv에 게시된 'Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench","tags":["Review","Vision-Language Models","Benchmarking","Visual Measurement Reading","Synthetic Data Generation","Fine-grained Perception","Spatial Grounding","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Fenfen Lin, Yesheng Liu, Haiyu Xu, Chen Yue, Zheqi He, Mingxuan Zhao, Miguel Hu Chen, Jiakang Liu, JG Yao, Xi Yang 핵심 연구 목표 본 연구는 최신 VisionLanguage Model (VLM) 들이 시각적 측정 기기 판독과 같"},{"id":"2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities","title":"[논문리뷰] EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities","excerpt":"Yunxin Liu이 arXiv에 게시한 'EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities","tags":["Review","Energy-Based Models (EBMs)","Diffusion Policy","Robotics","Behavior Cloning","Physical Reasoning","Uncertainty Modeling","Emergent Behavior","Robot Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Travis Davies, Yiqi Huang, Alexi Gladstone, Yunxin Liu, Xiang Chen, Heng Ji, Huxian Liu, Luhui Hu 핵심 연구 목표 본 논문은 로봇 공학 분야에서 Diffusion Policy 와 같은 생성 모델이 겪는 높은 계산 비용, 노출 편향, 불안정한 "},{"id":"2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation","title":"[논문리뷰] Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation","excerpt":"arXiv에 게시된 'Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation","tags":["Review","Large Language Models","Mixture-of-Experts","Reasoning Capability","Sparse Activation","Scaling Laws","FP8 Training","Efficient Training","Instruction Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Ling Team, Inclusion AI 핵심 연구 목표 본 논문은 '모든 활성화가 추론 능력을 향상시킨다'는 원칙 아래, 1조 개의 파라미터를 가진 추론 중심의 개방형 언어 파운데이션 모델(Ling 2.0) 을 개발하는 것을 목표로 합니다. 대규모 모델의 계산 효율성과 추론 능력 향상 이라는 두 가지 상호 연결된 "},{"id":"2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding","title":"[논문리뷰] GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding","excerpt":"Wanrong Zhu이 arXiv에 게시한 'GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding","tags":["Review","GUI Grounding","Multimodal Attention","MLLMs","Coordinate-Free","Visual Grounding","Attention Weighting","Anchor Token"],"text":"링크: 논문 PDF로 바로 열기 저자: Shijie Zhou, Viet Dac Lai, Hao Tan, Jihyung Kil, Wanrong Zhu, Changyou Chen, Ruiyi Zhang 핵심 연구 목표 본 연구는 컴퓨터 사용 에이전트의 핵심 기능인 GUI Grounding에서 발생하는 문제를 해결하고자 합니다. 특히, 자연어 지침을 화면의 실행"},{"id":"2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph","title":"[논문리뷰] Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph","excerpt":"arXiv에 게시된 'Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph","tags":["Review","Test-Time Scaling","LLMs","Graph Optimization","REINFORCE","Multi-agent Systems","Adaptive Architectures","Compute-optimal Scaling","Probabilistic Graphs"],"text":"링크: 논문 PDF로 바로 열기 저자: Fali Wang, Jihai Chen, Shuhua Yang, Runxue Bao, Tianxiang Zhao, Zhiwei Zhang, Xianfeng Tang, Hui Liu, Qi He, Suhang Wang 핵심 연구 목표 본 논문은 고정된 컴퓨팅 예산 내에서 대규모 언어 모델(LLM)의 테스트 시간 컴퓨팅 "},{"id":"2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment","title":"[논문리뷰] How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment","excerpt":"Yuhao Zhai이 arXiv에 게시한 'How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment","tags":["Review","Video Generation","World Models","Surgical AI","Zero-shot Prediction","Expert Evaluation","Plausibility Gap","Medical Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhen Chen, Qing Xu, Jinlin Wu, Biao Yang, Yuhao Zhai, Geng Guo, Jing Zhang, Yinlu Ding, Nassir Navab, Jiebo Luo 핵심 연구 목표 본 연구는 고위험 수술 도메인에서 심층적이고 전문화된 인과 지식이 필요한 상황에서, 최첨단 비디오 생성"},{"id":"2025-11-4-LongCat-Flash-Omni-Technical-Report","title":"[논문리뷰] LongCat-Flash-Omni Technical Report","excerpt":"Bin Xiao이 arXiv에 게시한 'LongCat-Flash-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-LongCat-Flash-Omni-Technical-Report","tags":["Review","Omni-modal AI","Multimodal LLM","Real-time Interaction","Mixture-of-Experts (MoE)","Streaming Inference","Distributed Training","Curriculum Learning","Audio-Visual Perception"],"text":"링크: 논문 PDF로 바로 열기 저자: Meituan LongCat Team 핵심 연구 목표 LongCatFlashOmni는 560B 파라미터 규모의 최첨단 오픈소스 옴니모달 모델로, 견고한 오프라인 멀티모달 이해와 저지연 실시간 오디오시각 상호작용 을 통합하는 것을 목표로 합니다. 특히, 교차 모달 이질성, 통합된 스트리밍 및 오프라인 기능, 실시간 상호"},{"id":"2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models","title":"[논문리뷰] MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models","excerpt":"Bin Yu이 arXiv에 게시한 'MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models","tags":["Review","Large Reasoning Models","Factuality Alignment","Meta-Reasoning","Kahneman-Tversky Optimization","Chain-of-Thought","Hallucination","Process-Level Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinming Wang, Jian Xu, Bin Yu, Sheng Lian, Hongzhu Yi, Yi Chen, Yingjian Zhu, Boran Wang, Hongming Yang, Han Hu, XuYao Zhang, ChengLin Liu 핵심 연구 목표 본 연구는 Large Reasoning Models ("},{"id":"2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls","title":"[논문리뷰] MotionStream: Real-Time Video Generation with Interactive Motion Controls","excerpt":"arXiv에 게시된 'MotionStream: Real-Time Video Generation with Interactive Motion Controls' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls","tags":["Review","Real-Time Video Generation","Motion Control","Diffusion Models","Autoregressive Generation","Self-Forcing","Attention Sink","Streaming Inference","Video Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Joonghyuk Shin, Zhengqi Li, Richard Zhang, JunYan Zhu, Jaesik Park, Eli Schechtman, Xun Huang 핵심 연구 목표 기존 모션 제어 비디오 생성 모델의 높은 지연 시간(수분 소요) 과 비인과적 처리 문제로 인한 실시간 상호작용 불가능성을 해결하고, 대"},{"id":"2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement","title":"[논문리뷰] Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement","excerpt":"Isabelle Augenstein이 arXiv에 게시한 'Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement","tags":["Review","LLMs","Knowledge Interaction","Parametric Knowledge","Contextual Knowledge","Subspace Disentanglement","NLE Generation","Hallucination Detection","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Sekh Mainul Islam, Pepa Atanasova, Isabelle Augenstein 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 자연어 설명(NLEs)을 생성할 때 내부의 매개변수 지식(Parametric Knowledge, PK) 과 외부의 문맥 지식(Context Knowledge, CK"},{"id":"2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models","title":"[논문리뷰] NaviTrace: Evaluating Embodied Navigation of Vision-Language Models","excerpt":"arXiv에 게시된 'NaviTrace: Evaluating Embodied Navigation of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models","tags":["Review","Vision-Language Models","Embodied Navigation","VQA Benchmark","Robotic Navigation","Semantic-aware Score","Dynamic Time Warping","Real-world Scenarios"],"text":"링크: 논문 PDF로 바로 열기 저자: Tim Windecker, Manthan Patel, Moritz Reuss, Richard Schwarzkopf, Cesar Cadena, Rudolf Lioutikov, Marco Hutter, Jonas Frey 핵심 연구 목표 본 논문은 VisionLanguage Models (VLMs)의 실제 환경 내 로봇 "},{"id":"2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner","title":"[논문리뷰] OpenSIR: Open-Ended Self-Improving Reasoner","excerpt":"arXiv에 게시된 'OpenSIR: Open-Ended Self-Improving Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner","tags":["Review","Open-Ended Learning","Self-Play","Reinforcement Learning","Large Language Models","Mathematical Reasoning","Problem Generation","Curriculum Learning","Reward Shaping"],"text":"링크: 논문 PDF로 바로 열기 저자: WaiChung Kwan, Joshua Ong Jun Leang, Pavlos Vougiouklis, Jeff Z. Pan, Marco Valentino, Pasquale Minervini 핵심 연구 목표 논문은 LLM 추론 능력 향상이 인간 주석 데이터 의존성으로 확장성과 성능에 한계가 있음을 지적하며, 이 문제를 "},{"id":"2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset","title":"[논문리뷰] PHUMA: Physically-Grounded Humanoid Locomotion Dataset","excerpt":"arXiv에 게시된 'PHUMA: Physically-Grounded Humanoid Locomotion Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset","tags":["Review","Humanoid Locomotion","Dataset","Motion Imitation","Physics-based Control","Motion Retargeting","Data Curation","Reinforcement Learning","Inverse Kinematics"],"text":"링크: 논문 PDF로 바로 열기 저자: Kyungmin Lee, Sibeen Kim, Minho Park, Hyunseung Kim, Dongyoon Hwang, Hojoon Lee, Jaegul Choo 핵심 연구 목표 본 논문은 기존 휴머노이드 모션 데이터셋의 규모, 다양성 및 물리적 신뢰성 부족 문제를 해결하는 것을 목표로 합니다. 특히, Humano"},{"id":"2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation","title":"[논문리뷰] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation","excerpt":"Feng Li이 arXiv에 게시한 'ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation","tags":["Review","Multimodal AI","Benchmarking","Cross-Modal Reasoning","Omnimodal Generation","Visual Generation","Verbal Generation","Unified Multimodal Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yongyuan Liang, Wei Chow, Feng Li, Ziqiao Ma, Xiyao Wang, Jiageng Mao, Jiuhai Chen, Jiatao Gu, Yue Wang, Furong Huang 핵심 연구 목표 본 논문은 기존 통합 멀티모달 모델(UMM) 평가 방식이 텍스트 및 이미지 이해/생성 능력을"},{"id":"2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning","title":"[논문리뷰] TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning","excerpt":"Shaoheng Lin이 arXiv에 게시한 'TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning","tags":["Review","Multimodal LLMs","Agentic Reasoning","Thinking-with-Images","Visual Reasoning Benchmark","Tool Use","Image Manipulation","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Li, Jike Zhong, Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Yuxiang Lai, Chen Wei, Konstantinos Psounis, Kaipeng Zhang 핵심 연구 목표 본 연구는 기존 벤치마크들이 OpenAI o3 와 같은 최신 MLLM의 'think"},{"id":"2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding","title":"[논문리뷰] The Underappreciated Power of Vision Models for Graph Structural Understanding","excerpt":"Lei Zhang이 arXiv에 게시한 'The Underappreciated Power of Vision Models for Graph Structural Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding","tags":["Review","Graph Neural Networks","Vision Models","Graph Understanding","Topological Perception","GraphAbstract Benchmark","OOD Generalization","Graph Visualization"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinjian Zhao, Wei Pang, Zhongkai Xue, Xiangru Jian, Lei Zhang, Yaoyao Xu, Xiaozhuang Song, Shu Wu, Tianshu Yu 핵심 연구 목표 본 논문은 기존 Graph Neural Networks(GNNs)의 국소적인 메시지 전달 방식과 인간의 시"},{"id":"2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use","title":"[논문리뷰] ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use","excerpt":"Guanting Dong이 arXiv에 게시한 'ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use","tags":["Review","Multimodal Agents","Tool-Augmented LLMs","Vision-Guided Reasoning","Long-Horizon Tasks","VQA","Global Planning","Context Preservation","Perceive Tool"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengjie Deng, Guanting Dong, Zhicheng Dou 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLM)이 동적 추론, 외부 지식 접근 및 다단계 연산이 필요한 복잡한 작업에서 겪는 한계, 특히 장기적인 VQA 작업 에서의 제한된 전역 계획 과 시각적 맥락 저하 문제를 해결하는 것을 "},{"id":"2025-11-4-Towards-Robust-Mathematical-Reasoning","title":"[논문리뷰] Towards Robust Mathematical Reasoning","excerpt":"Yuri Chervonyi이 arXiv에 게시한 'Towards Robust Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Towards-Robust-Mathematical-Reasoning","tags":["Review","Mathematical Reasoning","Large Language Models (LLMs)","AI Benchmarks","International Mathematical Olympiad (IMO)","Proof Verification","Automatic Grading","Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuri Chervonyi, Golnaz Ghiasi, Hoang H. Nguyen, Dawsen Hwang, Thang Luong 핵심 연구 목표 기존 수학 벤치마크들의 포화 상태와 단답형 답변 위주의 한계를 극복하기 위해, 논문은 국제 수학 올림피아드(IMO) 수준의 견고한 수학적 추론 능력을 평가하는 새로운 벤치"},{"id":"2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum","title":"[논문리뷰] Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum","excerpt":"arXiv에 게시된 'Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum","tags":["Review","Video Retrieval","Multimodal Embedding","Data Synthesis","Curriculum Learning","Zero-shot Generalization","Benchmark Design","MLLM","Video-Text Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuoning Guo, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Xiaowen Chu 핵심 연구 목표 기존 비디오 리트리벌 패러다임이 좁은 벤치마크, 제한된 데이터, 단일 태스크 훈련으로 인해 일반화 능력이 저해되는 문제를 해결하는 것입니다. 이 연구는 다차원 "},{"id":"2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval","title":"[논문리뷰] Trove: A Flexible Toolkit for Dense Retrieval","excerpt":"arXiv에 게시된 'Trove: A Flexible Toolkit for Dense Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval","tags":["Review","Dense Retrieval","Retrieval Toolkit","Data Management","Distributed Training","Model Customization","Hard Negative Mining","Hugging Face Integration","Performance Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Reza Esfandiarpoor, Max Zuo, Stephen H. Bach 핵심 연구 목표 Trove는 밀집 검색(Dense Retrieval) 연구 실험을 위한 유연하고 사용하기 쉬운 오픈 소스 툴킷을 제공하여, 유연성과 속도를 희생하지 않으면서 연구 과정을 단순화 하는 것을 목표로 합니다. 특히, 대규모 데이"},{"id":"2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings","title":"[논문리뷰] UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings","excerpt":"Jinsong Su이 arXiv에 게시한 'UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings","tags":["Review","Multimodal Embeddings","Generative AI","Reasoning","Reinforcement Learning","MLLMs","Supervised Fine-tuning","Information Retrieval","Unified Embeddings"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su 핵심 연구 목표 본 논문은 기존의 멀티모달 대규모 언어 모델(MLLMs) 기반 임베딩 모델 이 판별적(discriminative)이라는 한계를 해결하고, 추론 중심의 생성 패러다임의 이점을 활용하는 것을 목표로 "},{"id":"2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback","title":"[논문리뷰] UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback","excerpt":"arXiv에 게시된 'UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback","tags":["Review","Relighting","Diffusion Models","Flow Matching","Physics-Plausible Feedback","Image-to-Video","Geometric Supervision","Path Consistency Learning","LumosBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Ropeway Liu, Hangjie Yuan, Bo Dong, Jiazheng Xing, Jinwang Wang, Rui Zhao, Yan Xing, Weihua Chen, Fan Wang 핵심 연구 목표 기존 확산 모델 기반 relighting 기법의 물리적 비일관성 문제(예: 과노출 하이라이트, 그림자 부정확성)"},{"id":"2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark","title":"[논문리뷰] UniREditBench: A Unified Reasoning-based Image Editing Benchmark","excerpt":"arXiv에 게시된 'UniREditBench: A Unified Reasoning-based Image Editing Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark","tags":["Review","Image Editing","Reasoning-based AI","Benchmark","Multimodal Learning","Chain-of-Thought (CoT)","Dual-Reference Evaluation","Generative Models","Game AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Feng Han, Yibin Wang, Chenglin Li, Zheming Liang, Dianyi Wang, Yang Jiao, Zhipeng Wei, Chao Gong, Cheng Jin, Jingjing Chen, Jiaqi Wang 핵심 연구 목표 기존 이미지 편집 벤치마크의 한계, 즉 단일 객체 속성 변환에"},{"id":"2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process","title":"[논문리뷰] Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process","excerpt":"arXiv에 게시된 'Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process","tags":["Review","Vision-Language-Action (VLA)","Diffusion Models","Discrete Denoising","Multimodal Learning","Robotics","Embodied AI","Joint Generation","Action Prediction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiayi Chen, Wenxuan Song, Pengxiang Ding, Ziyang Zhou, Han Zhao, Feilong Tang, Donglin Wang, Haoang Li 핵심 연구 목표 기존 VLA(VisionLanguageAction) 모델이 비전 생성 및 행동 예측을 분리하여 다루거나 외부 전문가에 "},{"id":"2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers","title":"[논문리뷰] Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers","excerpt":"arXiv에 게시된 'Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers","tags":["Review","Video Retrieval","Vision-Language Models (VLMs)","Zero-Shot Learning","List-wise Reranking","Rank Fusion","Prompt Engineering","S-Grid","Multimodal Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohamed Eltahir, Ali Habibullah, Lama Ayash, Tanveer Hussain, Naeemullah Khan 핵심 연구 목표 본 연구는 이질적인 검색기(retriever)로부터 얻은 후보군들을 융합할 때, 기존의 랭크 기반 융합 방식들이 콘텐츠를 무시하고 랭크나 스코어 신호에만 의존하는 "},{"id":"2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI","title":"[논문리뷰] World Simulation with Video Foundation Models for Physical AI","excerpt":"Junjie Bai이 arXiv에 게시한 'World Simulation with Video Foundation Models for Physical AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI","tags":["Review","Physical AI","World Simulation","Video Foundation Models","Flow Matching","Reinforcement Learning","Robotics","Autonomous Driving","Synthetic Data Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Junjie Bai, Arslan Ali, et al. (NVIDIA) 핵심 연구 목표 본 논문은 물리 AI(Physical AI) 시스템의 훈련 시 발생하는 높은 비용과 위험성을 해결하기 위해 고품질의 가상 세계 시뮬레이터를 제공하는 것을 목표로 합니다. 특히, [CosmosPredict2.5] 라는 최신 비디오 파"},{"id":"2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles","title":"[논문리뷰] left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles","excerpt":"Deepiha S이 arXiv에 게시한 'left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:22:42+0900","permalink":"/ai/review/2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles","tags":["Review","Vision-Language Models","Multimodal Benchmark","Rebus Puzzles","In-Context Learning","Reasoning","ControlNet","Prompt Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Trishanu Das, Abhilash Nandy, Khush Bajaj, Deepiha S 핵심 연구 목표 논문은 VisionLanguage Models (VLMs)이 Rebus Puzzles 를 이해하고 해결하는 능력을 평가하기 위한 크고 다양한 멀티모달 벤치마크를 제시하는 것을 목표로 합니다. Rebus Puz"},{"id":"2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda","title":"[논문리뷰] AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda","excerpt":"arXiv에 게시된 'AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda","tags":["Review","Ayurveda LLM","Domain Adaptation","Bilingual Language Model","Instruction Tuning","Medical AI","Knowledge-Grounded QA","Traditional Medicine"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohd Nauman, Sravan Gvm, Vijay Devane, Shyam Pawar, Viraj Thakur, Kundeshwar Pundalik, Piyush Sawarkar, Rohit Saluja, Maunendra Desarkar, Ganesh Ramakrishnan (BharatGen Team) 핵심 "},{"id":"2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring","title":"[논문리뷰] BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring","excerpt":"arXiv에 게시된 'BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring","tags":["Review","Alzheimer's Disease","Retrieval-Augmented Generation (RAG)","Large Language Models (LLMs)","Clinical Decision Support","Multimodal Data Fusion","Cognitive Decline Detection","Early Diagnosis"],"text":"링크: 논문 PDF로 바로 열기 제목: BRAINS: A RetrievalAugmented System for Alzheimer's Detection and Monitoring 저자: Rajan Das Gupta, Md Kishor Morol, Nafiz Fahad, Md Tanzib Hosain, Sumaya Binte Zilani Choya, Md Ja"},{"id":"2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer","title":"[논문리뷰] Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer","excerpt":"arXiv에 게시된 'Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer","tags":["Review","fMRI","Image Reconstruction","Brain-Computer Interface","Transformer","Diffusion Models","Neural Decoding","Cross-Subject Learning","Deep Image Prior"],"text":"링크: 논문 PDF로 바로 열기 저자: Roman Beliy, Amit Zalcher, Jonathan Kogman, Navve Wasserman, Michal Irani 핵심 연구 목표 fMRI 뇌 활동 기록을 통해 사람이 본 이미지를 충실하게 재구성하는 것을 목표로 합니다. 기존 확산 모델 기반 방법론들이 실제 본 이미지에 대한 시각적 충실도 및 의미적"},{"id":"2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models","title":"[논문리뷰] Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models","excerpt":"Shijie Dong이 arXiv에 게시한 'Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models","tags":["Review","Large Multimodal Models","Visual Token Compression","Token Pruning","Benchmark","Efficiency","Inference Latency","Multimodal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang 핵심 연구 목표 대규모 멀티모달 모델(LMM)이 이미지 인코더에서 생성되는 막대한 수의 시각 토큰으로 인해 겪는 심각한 추론 비효율성 문제를 해결하는 것이 주된 목표입니다. 기존의 토큰 압축 방법론 평가가"},{"id":"2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension","title":"[논문리뷰] ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension","excerpt":"Hao Wang이 arXiv에 게시한 'ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension","tags":["Review","Chart Comprehension","Visual Reasoning","Data Generation","Code-Driven Pipeline","Multimodal LLMs","Retrieval-Augmented Generation","Reinforcement Learning","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang 핵심 연구 목표 본 연구는 기존 멀티모달 대규모 언어 모델(MLLM)이 실제 복잡한 차트 이해 작업에서 겪는 한계(제한된 차트 유형 및 복잡성, 낮은 질문 복잡성, 해석력 부족 등)를 해결하고자 합니다. 이를 위해 차트 다양성과 작업 "},{"id":"2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering","title":"[논문리뷰] CodeClash: Benchmarking Goal-Oriented Software Engineering","excerpt":"arXiv에 게시된 'CodeClash: Benchmarking Goal-Oriented Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering","tags":["Review","Software Engineering Benchmarking","Language Models","AI Agents","Goal-Oriented Development","Competitive Programming","Code Evolution","Strategic Reasoning","Autonomous Systems"],"text":"링크: 논문 PDF로 바로 열기 저자: John Yang1, Kilian Lieret2, Joyce Yang³, Carlos E. Jimenez², Ofir Press², Ludwig Schmidt¹, Diyi Yang¹ 핵심 연구 목표 본 논문은 기존의 고립된 코딩 벤치마크가 아닌, 고수준의 목표 지향적 소프트웨어 개발(goaloriented softwa"},{"id":"2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning","title":"[논문리뷰] Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning","excerpt":"Zuyi Xiong이 arXiv에 게시한 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning","tags":["Review","Self-supervised Learning","Depth Estimation","Ego-Motion Estimation","Motion Component Discrimination","Geometric Constraints","Optical Flow","PoseNet","DepthNet"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan 핵심 연구 목표 본 논문은 심도 추정 및 에고모션 학습을 위한 기존의 자율학습(unsupervised learning) 프레임워크가 모션"},{"id":"2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization","title":"[논문리뷰] Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization","excerpt":"Aleksandr I. Panov이 arXiv에 게시한 'Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization","tags":["Review","Vision-Language-Action Models","OOD Generalization","Representation Alignment","Fine-tuning","Robotics","Visual Representations","Attention Maps","t-SNE"],"text":"링크: 논문 PDF로 바로 열기 저자: Nikita Kachaev, Mikhail Kolosov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov 핵심 연구 목표 논문은 사전 훈련된 VisionLanguageAction (VLA) 모델이 로봇 액션 태스크에 미세 조정될 때 발생하는 시각 표현의 퇴화(de"},{"id":"2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs","title":"[논문리뷰] Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs","excerpt":"Bo Bai이 arXiv에 게시한 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs","tags":["Review","Semantic Information Theory","Large Language Models","Directed Information","Rate-Distortion Function","Granger Causality","Token Embedding","Transformer Architecture","Variational Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Bo Bai 핵심 연구 목표 본 논문은 LLM(Large Language Model)의 내부 작동 원리를 이론적으로 설명하기 위해 비트(bits) 대신 토큰(token) 기반의 새로운 의미론적 정보 이론 프레임워크 를 구축하는 것을 목표로 합니다. 기존의 실험 중심 연구에서 벗어나 LLM의 \"블랙박스\"를 정보 이론적 "},{"id":"2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw","title":"[논문리뷰] LTD-Bench: Evaluating Large Language Models by Letting Them Draw","excerpt":"arXiv에 게시된 'LTD-Bench: Evaluating Large Language Models by Letting Them Draw' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw","tags":["Review","LLM Evaluation","Spatial Reasoning","Benchmark","Generative AI","Visual Perception","Spatial Imagination","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Liuhao Lin, Ke Li et al. 핵심 연구 목표 현재 LLM 평가 방식이 공간 추론 능력 의 근본적인 한계를 가리는 추상적인 수치에 의존하여 모델 역량에 대한 직관적 이해를 제공하지 못하는 문제를 해결하고자 합니다. 본 연구는 LLM의 추상적인 점수를 시각적으로 관찰 가능한 결과물로 전환하여, 언어공간 양"},{"id":"2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context","title":"[논문리뷰] LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context","excerpt":"Tianxin Zhang이 arXiv에 게시한 'LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context","tags":["Review","LLM Safety","AI Safety Benchmark","Chinese Context","Dynamic Evaluation","Cultural Relevance","Adversarial Robustness","ELO Rating System"],"text":"링크: 논문 PDF로 바로 열기 저자: Yudong Li, Zhongliang Yang, Kejiang Chen, Wenxuan Wang, Tianxin Zhang, Sifang Wan, Kecheng Wang, Haitian Li, Xu Wang, Lefan Cheng, Youdan Yang, Baocheng Chen, Ziyu Liu, Yufei Sun"},{"id":"2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","title":"[논문리뷰] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation","excerpt":"arXiv에 게시된 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","tags":["Review","Video Generation","Direct Preference Optimization","SFT Regularization","GT-Pair","Memory Optimization","Diffusion Models","I2V","T2V"],"text":"링크: 논문 PDF로 바로 열기 저자: Jie Du, Xinyu Gong, Qingshan Tan, Wen Li, Yangming Cheng, Weitao Wang, Chenlu Zhan, Suhui Wu, Hao Zhang, Jun Zhang, et al. 핵심 연구 목표 본 논문은 비디오 생성 분야에서 Direct Preference Optimizati"},{"id":"2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs","title":"[논문리뷰] RiddleBench: A New Generative Reasoning Benchmark for LLMs","excerpt":"arXiv에 게시된 'RiddleBench: A New Generative Reasoning Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs","tags":["Review","LLM Reasoning","Generative AI","Benchmark","Logical Deduction","Spatial Reasoning","Constraint Satisfaction","Hallucination Cascade","Self-Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Deepon Halder, Alan Saji, Thanmay Jayakumar, Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre 핵심 연구 목표 대규모 언어 모델(LLMs)이 인간 지능의 핵심 요소인 유연하고 다면적인 추론 능력(논리적 추론, 공간 인식, 제약 조건 만족)을 평가"},{"id":"2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies","title":"[논문리뷰] RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies","excerpt":"arXiv에 게시된 'RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies","tags":["Review","Robotics","Real-robot Evaluation","Embodied AI","Vision-Language-Action Models","Benchmarking","Online Testing System","Robotics Control","Large-scale Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Adina Yakefu, Bin Xie, Chongyang Xu, Enwen Zhang, Erjin Zhou, Fan Jia, Haitao Yang, Haoqiang Fan, Haowei Zhang, Hongyang Peng, Jing Tan, Junwen Huang, Kai Liu, Kaixin Liu, Kefan "},{"id":"2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR","title":"[논문리뷰] Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR","excerpt":"arXiv에 게시된 'Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR","tags":["Review","LLMs","RLVR","Length Regularization","Mathematical Reasoning","Data Curation","Model Efficiency","Emergent Brevity"],"text":"링크: 논문 PDF로 바로 열기 저자: Abdelaziz Bounhar, Hadi Abdine, Evan Dufraisse, Ahmad Chamma, Amr Mohamed, Dani Bouch, Michalis Vazirgiannis, Guokan Shang 핵심 연구 목표 대규모 언어 모델(LLMs)이 단계별 추론 과정에서 지나치게 장황해져 추론 비용이 "},{"id":"2025-11-5-Step-Audio-EditX-Technical-Report","title":"[논문리뷰] Step-Audio-EditX Technical Report","excerpt":"arXiv에 게시된 'Step-Audio-EditX Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-Step-Audio-EditX-Technical-Report","tags":["Review","LLM-based Audio Model","Audio Editing","Text-to-Speech (TTS)","Zero-shot Learning","Large-Margin Data","Reinforcement Learning (RLHF)","Emotion Control","Speaking Style Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Chao Yan, Boyong Wu, Peng Yang, Pengfei Tan, Guoqiang Hu, Yuxin Zhang, Xiangyu(Tony) Zhang, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu 핵심 연구 목표 이 논문은 표현력이 풍부하고 반복"},{"id":"2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System","title":"[논문리뷰] TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System","excerpt":"Rocky Duan이 arXiv에 게시한 'TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System","tags":["Review","Humanoid Robotics","Data Collection","Teleoperation","Full-Body Control","Visuomotor Policy Learning","VR","Portable MoCap-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu 핵심 연구 목표 휴머노이드 로봇 분야에서 대규모 데이터 수집의 비효율성 과 기존 텔레오퍼레이션 시스템의 한계"},{"id":"2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data","title":"[논문리뷰] TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data","excerpt":"Jin Zeng이 arXiv에 게시한 'TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data","tags":["Review","Tabular Data","Numerical Reasoning","Large Language Models (LLMs)","Table Question Answering (TQA)","Program-of-Thoughts (PoT)","Data Sanitization","Query Decomposition","Multi-hop Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jin Zeng, Wei Lu, Haihua Chen, Fengchang Yu, arnodjiang 핵심 연구 목표 논문은 복잡한 질문, 노이즈가 있는 데이터, 제한된 수치 연산 능력으로 인해 대규모 언어 모델(LLM) 이 테이블 질의응답(TQA) 에서 저조한 성능을 보이는 문제를 해결합니다. 특히, 다단계(multi"},{"id":"2025-11-5-The-Collaboration-Gap","title":"[논문리뷰] The Collaboration Gap","excerpt":"arXiv에 게시된 'The Collaboration Gap' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-The-Collaboration-Gap","tags":["Review","AI Collaboration","Multi-Agent Systems","Large Language Models (LLMs)","Maze Solving","Heterogeneous Agents","Collaboration Gap","Relay Inference","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Tim R. Davidson, Adam Fourney, Saleema Amershi, Eric Horvitz, Robert West, Ece Kamar 핵심 연구 목표 AI 에이전트 기반 시스템에서 독립적으로 개발된 에이전트 간의 효과적인 협업 능력 이 부족하다는 문제인 \" 협업 격차(Collaboration Gap)"},{"id":"2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation","title":"[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation","excerpt":"arXiv에 게시된 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation","tags":["Review","Multimodal AI","Code Generation","SVG","Visual Representation","Benchmark","Large Vision-Language Models","Agentic AI","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang 핵심 연구 목표 본 논문은 에이전트 시대의 추론 및 행동을 위한 시각 중심 코딩의 미개척 영역을 탐구합니다. 기존 "},{"id":"2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models","title":"[논문리뷰] VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models","excerpt":"Pengfei Wan이 arXiv에 게시한 'VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models","tags":["Review","VideoLLMs","Emotion Understanding","Affective-Tree Reasoning","Curriculum Learning","Reinforcement Learning","Fine-Grained Emotion","Attribute Perception","Expression Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang 핵심 연구 목표 본 논문은 동적 비디오에서 복잡하고 진화하는 감정 상태를 합리적인 근거와 함께 이해하고 예측하는 데 초점을 맞춥니다. 기존 VideoLLM 의"},{"id":"2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs","title":"[논문리뷰] When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs","excerpt":"Haotian Wang이 arXiv에 게시한 'When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs","tags":["Review","Multimodal Large Language Models (MLLMs)","Modality Following","Unimodal Uncertainty","Modality Preference","Conflict Resolution","Internal Mechanism","Entropy","Controllable Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuoran Zhang, Tengyue Wang, Xilin Gong, Yang Shi, Haotian Wang, Di Wang, Lijie Hu 핵심 연구 목표 이 논문은 Multimodal Large Language Models (MLLMs)가 서로 다른 모달리티에서 모순되는 정보를 받았을 때 어떤 모달리티를 따"},{"id":"2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought","title":"[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought","excerpt":"arXiv에 게시된 'When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought","tags":["Review","Multimodal AI","Visual Reasoning","Chain-of-Thought (CoT)","Benchmark","Image Generation","MLLMs","Visual-CoT"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, Haoqi Fan, Cihang Xie, Huaxiu Yao, Qinghao Ye 핵심 연구 목표 "},{"id":"2025-11-5-iFlyBot-VLA-Technical-Report","title":"[논문리뷰] iFlyBot-VLA Technical Report","excerpt":"Jiajia wu이 arXiv에 게시한 'iFlyBot-VLA Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 19:35:02+0900","permalink":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report","tags":["Review","Vision-Language-Action Models","Robotics","Imitation Learning","Latent Actions","Diffusion Models","Dual-Arm Manipulation","Pretraining","Flow-Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuan Zhang, Chenyu Xue, Wenjie Xu, Chao Ji, Jiajia wu, Jia Pan 핵심 연구 목표 iFlyBotVLA는 장기적인 로봇 조작 작업을 위한 대규모 VisionLanguageAction (VLA) 모델 을 개발하는 것을 목표로 합니다. 특히, 연속적인 정밀 제어가 필요한 작업에"},{"id":"2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents","title":"[논문리뷰] CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents","excerpt":"Shijue Huang이 arXiv에 게시한 'CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents","tags":["Review","LLM Agents","Tool Use","Cost-Optimal Planning","Dynamic Environments","Benchmarking","Multi-Turn Interaction","Economic Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiayu Liu, Cheng Qian, Zhaochen Su, Qing Zong, Shijue Huang, Bingxiang He, Yi R. (May) Fung 핵심 연구 목표 기존 LLM 에이전트 평가가 태스크 완료에만 집중하고 자원 효율성 및 동적 환경에서의 적응성을 간과하는 문제를 해결하는 것이 목표입니다. "},{"id":"2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners","title":"[논문리뷰] Diffusion Language Models are Super Data Learners","excerpt":"arXiv에 게시된 'Diffusion Language Models are Super Data Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners","tags":["Review","Diffusion Language Models","Autoregressive Models","Data Efficiency","Scaling Laws","Data-Constrained Learning","Crossover Phenomenon","Pre-training","Masked Diffusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinjie Ni, Qian Liu, Longxu Dou, Chao Du, Zili Wang, Hang Yan, Tianyu Pang, Michael Qizhe Shieh 핵심 연구 목표 본 논문은 고품질 데이터 희소성이 LLM 훈련의 주요 병목이 되는 시대에, Autoregressive (AR) 모델 과 Diffus"},{"id":"2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask","title":"[논문리뷰] Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask","excerpt":"arXiv에 게시된 'Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask","tags":["Review","Dialogue Systems","Common Ground","Misunderstanding","Annotation Scheme","MapTask Corpus","Large Language Models","Perspective Taking","Reference Resolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Nan Li, Albert Gatt, Massimo Poesio 핵심 연구 목표 본 논문은 비대칭 정보 환경에서 발생하는 대화 속 레퍼런스 표현(RE)에 대한 미묘한 오해를 파악하는 것을 목표로 합니다. 화자의 의도와 청자의 해석을 별도로 포착하는 관점 기반(perspectivist) 주석 스키마 를 개발하여, 대화 "},{"id":"2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper","title":"[논문리뷰] Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper","excerpt":"arXiv에 게시된 'Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper","tags":["Review","AI Scientist","Autonomous Research","Scientific Automation","LLM for Research","Code Generation","Experimental Design","Risk Assessment"],"text":"링크: 논문 PDF로 바로 열기 저자: Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa 핵심 연구 목표 본 논문은 기존 AI Scientist 시스템의 제한된 연구 품질, 모호한 목표, 소규모 코드 실험 위주의 한계를 극복하고, 실제 과학적 가치를 창출할 수 있는"},{"id":"2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects","title":"[논문리뷰] Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects","excerpt":"arXiv에 게시된 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects","tags":["Review","Articulated Objects","Kinematics Inference","High-DoF","Monte Carlo Tree Search","Joint Parameter Optimization","SDF","Open-Vocabulary Synthesis","Robot Self-Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawei Wang, Dingyou Wang, Jiaming Hu, Qixuan Zhang, Jingyi Yu, Lan Xu 핵심 연구 목표 본 논문은 높은 자유도(DoF)를 가진 복잡한 관절형 객체에 대해 정적 데이터 만으로도 정확한 운동학적 토폴로지 를 추론하고 관절 매개변수 를 추정하는 문제를 해결하는 것을 목"},{"id":"2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation","title":"[논문리뷰] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation","excerpt":"Soohyun Oh이 arXiv에 게시한 'LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation","tags":["Review","3D Scene Synthesis","Fine-Grained Evaluation","Tool-Augmented LLMs","Embodied AI","Vision-Language Models","Benchmark","Multi-Hop Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Gyeom Hwangbo, Hyungjoo Chae, Minseok Kang, Hyeonjong Ju, Soohyun Oh, Jinyoung Yeo 핵심 연구 목표 대규모 언어 모델(LLMs)로 생성된 3D 장면이 현실적인 공간 레이아웃과 객체 속성을 제대로 반영하지 못하는 문제를 해결하는 것이 목표입니다. 기존 평가"},{"id":"2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation","title":"[논문리뷰] Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation","excerpt":"Jaehyun Park이 arXiv에 게시한 'Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation","tags":["Review","Multimodal Embedders","Query Augmentation","Adaptive Learning","Multimodal LLM","Information Retrieval","Generative AI","Embedding Latency"],"text":"링크: 논문 PDF로 바로 열기 저자: Wongyu Kim, Hochang Lee, Sanghak Lee, Yoonsung Kim, Jaehyun Park 핵심 연구 목표 본 논문은 멀티모달 환경에서 쿼리 증강(query augmentation)으로 인한 과도한 임베딩 지연 시간 과 일부 쿼리에서의 성능 저하 문제를 해결하고, 쿼리 증강의 효과를 높이는 것"},{"id":"2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models","title":"[논문리뷰] LiveTradeBench: Seeking Real-World Alpha with Large Language Models","excerpt":"Jiaxuan You이 arXiv에 게시한 'LiveTradeBench: Seeking Real-World Alpha with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models","tags":["Review","LLM Evaluation","Live Trading","Portfolio Management","Financial AI","Prediction Markets","Real-World Uncertainty","Agent Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Haofei Yu, Fenghai Li, Jiaxuan You 핵심 연구 목표 본 논문은 기존의 정적 벤치마크로는 평가하기 어려운 LLM 에이전트의 실제 시장에서의 의사결정 능력 과 불확실성 하의 적응성 을 평가하기 위한 라이브 트레이딩 환경을 구축하는 것을 목표로 합니다. 특히, LLM의 일반적인 추론 능력이 실제 "},{"id":"2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity","title":"[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity","excerpt":"arXiv에 게시된 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity","tags":["Review","Multimodal LLMs","Benchmark","Cognitive Capacity","Visual Reasoning","MLLM Evaluation","Error Analysis","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang, Chaoyi Huang 외 다수 핵심 연구 목표 기존 멀티모달 벤치마크들이 텍스트 기반 추론을 과도하게 강조하거나 시각 중심의 인지적 행동을 체계적으로 포착하지 못하여 MLLM의 인지 능력을 불"},{"id":"2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning","title":"[논문리뷰] Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning","excerpt":"arXiv에 게시된 'Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning","tags":["Review","Tabular Data","In-Context Learning","Multi-Scale Attention","Sparse Attention","Foundation Models","Perceiver Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu 핵심 연구 목표 본 논문은 기존의 테이블 인컨텍스트 학습(ICL) 모델들이 직면한 단일 스케일 피처 처리, 테이블 너비에 대한 Quadratic Scaling 의 조밀한 어텐션, 그리고 순차적 컴포넌"},{"id":"2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models","title":"[논문리뷰] TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models","excerpt":"arXiv에 게시된 'TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models","tags":["Review","Tabular Foundation Models","Fine-Tuning","PEFT","Meta-Learning","Calibration","Fairness","Unified Library","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu 핵심 연구 목표 본 연구는 테이블 형식 파운데이션 모델(Tabular Foundation Models, TFMs) 의 복잡한 전처리, 분산된 API, 비일관적인 미세 조정 절"},{"id":"2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute","title":"[논문리뷰] The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute","excerpt":"arXiv에 게시된 'The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute","tags":["Review","Sequential Reasoning","Parallel Self-Consistency","Inverse-Entropy Voting","LLM Reasoning","Test-Time Scaling","Inference Optimization","Iterative Refinement","Error Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Aman Sharma, Paras Chopra 핵심 연구 목표 본 논문은 언어 모델의 추론 작업을 위한 테스트타임 스케일링 전략에 대해 근본적인 질문을 던집니다. 동일한 토큰 예산과 컴퓨팅 자원이 주어졌을 때, 독립적인 체인을 병렬로 실행하는 것이 효율적인지, 아니면 순차적인 단계들을 통해 반복적으로 개선하는 것이 더"},{"id":"2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions","title":"[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions","excerpt":"arXiv에 게시된 'UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 21:54:30+0900","permalink":"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions","tags":["Review","Joint Audio-Video Generation","Cross-Modal Interaction","Diffusion Transformer","Face-Aware Modulation","Classifier-Free Guidance","Multimodal AI","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, Limin Wang 핵심 연구 목표 기존 오픈소스 오디오비디오 생성 모델이 겪는 부정확한 립싱크, 일관성 부족, 모달리티 비동기화 문제를 해결"},{"id":"2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts","title":"[논문리뷰] Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts","excerpt":"arXiv에 게시된 'Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts","tags":["Review","Multimodal LLMs","Benchmark Design","Non-Visual Shortcuts","Test-Set Stress-Test","Bias Mitigation","Model Evaluation","Benchmark Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie 핵심 연구 목표 이 논문은 Multimodal Large Language Model (MLLM)이 시각적 이해 없이 비시각적 단축키(편향, 언어적 선험지식, 피상적인 패턴)를 악용하여 멀티모달 벤치마크에서"},{"id":"2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video","title":"[논문리뷰] Cambrian-S: Towards Spatial Supersensing in Video","excerpt":"Zihao Yang이 arXiv에 게시한 'Cambrian-S: Towards Spatial Supersensing in Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video","tags":["Review","Spatial Supersensing","Video Understanding","Multimodal LLMs","Predictive Sensing","Memory Management","Event Segmentation","VSI-SUPER","Instruction Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shusheng Yang, Jihan Yang, Pinzhi Huang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Rob Fergus, Yann LeCun, Li FeiFei, Ellis Brown, Zihao Yang, Muhan Wang, Daohan Lu, Saining"},{"id":"2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation","title":"[논문리뷰] Contamination Detection for VLMs using Multi-Modal Semantic Perturbation","excerpt":"arXiv에 게시된 'Contamination Detection for VLMs using Multi-Modal Semantic Perturbation' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation","tags":["Review","VLM Contamination","Test-set Leakage","Multi-modal Perturbation","Generative Models","Generalization","Model Memorization","VLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Jaden Park, Mu Cai, Feng Yao, Jingbo Shang, Soochahn Lee, Yong Jae Lee 핵심 연구 목표 본 연구는 VisionLanguage Models(VLMs)에서 데이터 오염(testset leakage) 으로 인한 성능 과대평가 문제를 해결하기 위한 신뢰성, 실용성, 일관"},{"id":"2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference","title":"[논문리뷰] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference","excerpt":"arXiv에 게시된 'EVTAR: End-to-End Try on with Additional Unpaired Visual Reference' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference","tags":["Review","Virtual Try-on","Diffusion Models","End-to-End Learning","Reference Images","Unpaired Data","Flow Matching","Transformer Architecture","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Liuzhuozheng Li, Yue Gong, Shanyuan Liu, Bo Cheng, Yuhang Ma, Liebucha Wu, Dengyang Jiang, Zanyi Wang, Dawei Leng, Yuhui Yin 핵심 연구 목표 본 연구는 기존 가상 착용(virtual tryon) 모델들이 agnostic "},{"id":"2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents","title":"[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents","excerpt":"arXiv에 게시된 'GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents","tags":["Review","Computer-Using Agents","GUI Grounding","Screen Parsing","Action Prediction","Desktop Automation","Dataset","Benchmark","Multimodal Learning","LLM-augmented Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Mu, Chaoyun Zhang, Chiming Ni, Lu Wang, Bo Qiao, Kartik Mathur, Qianhui Wu, Yuhang Xie, Xiaojun Ma, Mengyu Zhou, Si Qin, Liqun Li, Yu Kang, Minghua Ma, Qingwei Lin, Saravan "},{"id":"2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics","title":"[논문리뷰] How to Evaluate Speech Translation with Source-Aware Neural MT Metrics","excerpt":"Luisa Bentivogli이 arXiv에 게시한 'How to Evaluate Speech Translation with Source-Aware Neural MT Metrics' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics","tags":["Review","Speech Translation","Neural MT Metrics","Source-Aware Evaluation","Automatic Speech Recognition (ASR)","Back-Translation (BT)","Cross-lingual Re-segmentation","COMET","MetricX"],"text":"링크: 논문 PDF로 바로 열기 저자: Mauro Cettolo, Marco Gaido, Matteo Negri, Sara Papi, Luisa Bentivogli 핵심 연구 목표 자동 음성텍스트 번역(ST) 시스템 평가에서 텍스트 소스 가 없는 한계로 인해 소스 인식 신경 기계 번역(MT) 지표 를 적용하기 어렵습니다. 본 연구는 음성 입력의 텍스트 프록"},{"id":"2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots","title":"[논문리뷰] Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots","excerpt":"arXiv에 게시된 'Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots","tags":["Review","Humanoid Robot","Reinforcement Learning","RoboCup","Soccer Skills","Vision-Driven Control","Adversarial Motion Priors","Sim-to-Real","Perception-Action Coordination"],"text":"링크: 논문 PDF로 바로 열기 저자: Yushi Wang, Changsheng Luo, Penghui Chen, Jianran Liu, Weijian Sun, Tong Guo, Kechang Yang, Biao Hu, Yangang Zhang, Mingguo Zhao 핵심 연구 목표 본 연구는 기존 로봇 제어 시스템의 모듈 분리(decoupled modu"},{"id":"2025-11-7-NVIDIA-Nemotron-Nano-V2-VL","title":"[논문리뷰] NVIDIA Nemotron Nano V2 VL","excerpt":"arXiv에 게시된 'NVIDIA Nemotron Nano V2 VL' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-NVIDIA-Nemotron-Nano-V2-VL","tags":["Review","Vision-Language Model","Hybrid Architecture","Mamba-Transformer","Long-Context Understanding","Quantization","Efficient Inference","Document AI","Video AI"],"text":"링크: 논문 PDF로 바로 열기 저자: NVIDIA et al. 핵심 연구 목표 Nemotron Nano V2 VL은 강력한 실세계 문서 이해 , 긴 비디오 이해 , 그리고 추론 태스크 를 위해 설계된 최신 비전언어 모델입니다. 이전 모델인 Llama3.1NemotronNanoVL8B 대비 모든 비전 및 텍스트 영역에서 상당한 성능 향상을 달성하며, 효율적"},{"id":"2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems","title":"[논문리뷰] RDMA Point-to-Point Communication for LLM Systems","excerpt":"arXiv에 게시된 'RDMA Point-to-Point Communication for LLM Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems","tags":["Review","RDMA","LLM","Point-to-Point Communication","Disaggregated Inference","MoE Routing","KvCache","AWS EFA","NVIDIA ConnectX"],"text":"링크: 논문 PDF로 바로 열기 저자: Nandor Licker, Kevin Hu, Vladimir Zaytsev, Lequn Chen 핵심 연구 목표 LLM 시스템에서 필요한 유연한 지점 간 통신(pointtopoint communication) 을 제공하고, 기존 RDMA 구현이 특정 NIC(Network Interface Controller) 에 종속"},{"id":"2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning","title":"[논문리뷰] SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning","excerpt":"arXiv에 게시된 'SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning","tags":["Review","Multimodal Large Language Models","Reinforcement Learning","Post-training","Reasoning","Dual-Reward System","Thinking Reward","Judging Reward","Hallucination Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangxun Shu, Yongjie Ye, Yue Liao, Zijian Kang, Weijie Yin, Jiacong Wang, Xiao Liang, Shuicheng Yan, Chao Feng 핵심 연구 목표 MLLM(Multimodal Large Language Models)의 추론 능력 향상을 목표로 합니다."},{"id":"2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding","title":"[논문리뷰] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding","excerpt":"arXiv에 게시된 'SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding","tags":["Review","Spatial Reasoning","Video Understanding","Simulated Data","Instruction Tuning","Multimodal LLMs","Sim-to-Real Transfer","AI2-THOR"],"text":"링크: 논문 PDF로 바로 열기 저자: Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie 핵심 연구 목표 멀티모달 대규모 언어 모델(MLLM)이 비디오에서 시공간 추론을 수행하는 데 어려움을 겪는 문제를 해결하는 것을 목표로 합니다. 특히, 정밀한 공간 주석을 포함"},{"id":"2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis","title":"[논문리뷰] Scaling Agent Learning via Experience Synthesis","excerpt":"arXiv에 게시된 'Scaling Agent Learning via Experience Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis","tags":["Review","Reinforcement Learning","LLM Agents","Experience Synthesis","World Models","Curriculum Learning","Sim-to-Real Transfer","Web Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaorun Chen, Zhuokai Zhao, Kai Zhang, Bo Liu, Qi Qi, Yifan Wu, Tarun Kalluri, Sara Cao, Yuanhao Xiong, Haibo Tong, Huaxiu Yao, Hengduo Li, Jiacheng Zhu, Xian Li, Dawn Song, Bo L"},{"id":"2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms","title":"[논문리뷰] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms","excerpt":"Susumu Takeuchi이 arXiv에 게시한 'The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms","tags":["Review","Strong Lottery Ticket Hypothesis","Multi-Head Attention","Transformers","Neural Network Pruning","Overparameterization","Weight Initialization","Model Compression"],"text":"링크: 논문 PDF로 바로 열기 저자: Hikari Otsuka, Daiki Chijiwa, Yasuyuki Okoshi, Daichi Fujiki, Susumu Takeuchi, Masato Motomura 핵심 연구 목표 이 논문은 기존 연구에서 다루지 않았던 트랜스포머 아키텍처 의 핵심 구성 요소인 MultiHead Attention (MHA) 메커니"},{"id":"2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm","title":"[논문리뷰] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm","excerpt":"arXiv에 게시된 'Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm","tags":["Review","Video Generation","Multimodal Reasoning","Temporal Understanding","Spatial Reasoning","Foundation Models","AI Benchmarking","In-Context Learning","Self-Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng"},{"id":"2025-11-7-V-Thinker-Interactive-Thinking-with-Images","title":"[논문리뷰] V-Thinker: Interactive Thinking with Images","excerpt":"Peiqing Yang이 arXiv에 게시한 'V-Thinker: Interactive Thinking with Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-11-09 22:08:24+0900","permalink":"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images","tags":["Review","Large Multimodal Models","Interactive Reasoning","Vision-Centric Thinking","Reinforcement Learning","Data Synthesis","Visual Tools","Curriculum Learning","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang 핵심 연구 목표 본 논문은 대규모"},{"id":"2025-12-01-Adversarial-Flow-Models","title":"[논문리뷰] Adversarial Flow Models","excerpt":"arXiv에 게시된 'Adversarial Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Adversarial-Flow-Models","tags":["Review","Generative Models","Adversarial Flow Models","GANs","Flow Matching","Optimal Transport","Single-step Generation","Image Generation","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Shanchuan Lin, Ceyuan Yang, Zhijie Lin, Hao Chen, Haoqi Fan 핵심 연구 목표 본 논문은 기존 GANs (Generative Adversarial Networks) 의 훈련 불안정성과 Flow Matching 모델의 저해상도 이산화 오류 및 반복적인 추론 비용 문제를 해결하"},{"id":"2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement","title":"[논문리뷰] AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement","excerpt":"Yicheng Ji이 arXiv에 게시한 'AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement","tags":["Review","Multi-Person Video Generation","Audio-Driven Animation","Diffusion Models","Interactivity Refinement","Identity-Aware Attention","Scalability","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhizhou Zhong, Yicheng Ji, Zhe Kong, Yiying Liu, Jiarui Wang, Xiangyi Wang, Yanjia Li, Yuqing She, Ying Qin, Shuiyang Mao, Wei Liu, Wenhan Luo 핵심 연구 목표 본 논문은 다양한 다중 인물 데이터 수집의 높은"},{"id":"2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model","title":"[논문리뷰] Architecture Decoupling Is Not All You Need For Unified Multimodal Model","excerpt":"Hongyu Li이 arXiv에 게시한 'Architecture Decoupling Is Not All You Need For Unified Multimodal Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model","tags":["Review","Unified Multimodal Models","Architecture Decoupling","Cross-Modal Attention","Attention Interaction Alignment (AIA) Loss","Task Conflicts","Image Generation","Image Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Dian Zheng, Manyuan Zhang, Hongyu Li, Kai Zou, et al. 핵심 연구 목표 본 논문은 통합 멀티모달 모델(UMM)에서 시각 생성 및 이해 태스크 간의 내재된 충돌을 완화하면서도 모델 아키텍처 디커플링에 과도하게 의존하지 않고 성능을 향상시키는 것을 목표로 합니다. 과도한 디커플링이"},{"id":"2025-12-01-Captain-Safari-A-World-Engine","title":"[논문리뷰] Captain Safari: A World Engine","excerpt":"Yitong Li이 arXiv에 게시한 'Captain Safari: A World Engine' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine","tags":["Review","World Engine","3D Consistent Video Generation","Pose-conditioned Memory","Camera Control","FPV Video Synthesis","Diffusion Models","Drone Video Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: YuCheng Chou, Xingrui Wang, Yitong Li, Jiahao Wang, Hanting Liu, Cihang Xie, Alan Yuille, Junfei Xiao 핵심 연구 목표 본 논문은 기존 비디오 세계 모델들이 겪는 장기적인 3D 일관성 부족, 공격적인 6DoF 카메라 궤적 추적의 어려움, 복"},{"id":"2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself","title":"[논문리뷰] CaptionQA: Is Your Caption as Useful as the Image Itself?","excerpt":"Zicheng Liu이 arXiv에 게시한 'CaptionQA: Is Your Caption as Useful as the Image Itself?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself","tags":["Review","Image Captioning","Caption Evaluation","Multimodal LLM","Utility-based Benchmark","Question Answering (QA)","Domain-specific Taxonomy","Hallucination","MLLM Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shijia Yang, Yunong Liu, Bohan Zhai, Ximeng Sun, Zicheng Liu, Emad Barsoum, Manling Li, Chenfeng Xu 핵심 연구 목표 본 논문은 기존 MLLM 평가 방식이 캡션의 실제 활용성, 즉 다운스트림 태스크에서 이미지를 대체할 수 있는 능력 을 간과한"},{"id":"2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield","title":"[논문리뷰] Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield","excerpt":"arXiv에 게시된 'Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield","tags":["Review","Diffusion Models","Model Distillation","Classifier-Free Guidance (CFG)","Distribution Matching","Text-to-Image Generation","Few-step Generation","Regularization","Score-based Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongyang Liu, Peng Gao, David Liu, Ruoyi Du, Zhen Li, Qilong Wu, Xin Jin, Sihan Cao, Shifeng Zhang, Hongsheng Li, Steven HOI 핵심 연구 목표 본 논문은 Distribution Matching Distillation (DM"},{"id":"2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning","title":"[논문리뷰] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning","excerpt":"arXiv에 게시된 'DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning","tags":["Review","Mathematical Reasoning","Large Language Models (LLMs)","Proof Verification","Self-Verification","Reinforcement Learning (RL)","Theorem Proving","Meta-Verification","Iterative Refinement"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhihong Shao, Yuxiang Luo, Chengda Lu†, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang (DeepSeekAI) 핵심 연구 목표 대규모 언어 모델(LLM)이 수학적 추론에서 최종 정답 기반 보상의 한계를 가지며, "},{"id":"2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space","title":"[논문리뷰] DiP: Taming Diffusion Models in Pixel Space","excerpt":"Xu Chen이 arXiv에 게시한 'DiP: Taming Diffusion Models in Pixel Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space","tags":["Review","Diffusion Models","Pixel Space","Latent Diffusion Models (LDMs)","Diffusion Transformer (DiT)","Patch Detailer Head","Global-Local Modeling","Computational Efficiency","ImageNet"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhennan Chen, Junwei Zhu, Xu Chen, Jiangning Zhang, Xiaobin Hu, Hanzhen Zhao, Chengjie Wang, Jian Yang, Ying Tai 핵심 연구 목표 본 연구는 확산 모델(Diffusion Models)의 근본적인 문제인 생성 품질과 계산 효율성 간의"},{"id":"2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action","title":"[논문리뷰] DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action","excerpt":"Zhuoyang Liu이 arXiv에 게시한 'DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action","tags":["Review","Vision-Language-Action (VLA)","Embodied AI","Action Degeneration","Data Pruning","Knowledge Distillation","Multi-modal Reasoning","Robot Learning","VLA Score"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhen Fang, Zhuoyang Liu, Jiaming Liu, Hao Chen, Yu Zeng, Shiting Huang, Zehui Chen, Lin Chen, Shanghang Zhang, Feng Zhao 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA) 모델에서 발생하는 '액션 퇴"},{"id":"2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models","title":"[논문리뷰] Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models","excerpt":"Wei Wu이 arXiv에 게시한 'Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models","tags":["Review","Large Language Models","Long Context","Sparse Attention","Hierarchical Sparse Attention (HSA)","Length Generalization","Mixture of Experts (MoE)","Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiang Hu, Zhanchao Zhou, Ruiqi Liang, Zehuan Li, Wei Wu, Jianguo Li 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)이 초장문 컨텍스트(ultralong context) 를 효율적으로 처리하여 \"기억하는 기계\"를 구축하는 과제를 해결하고자 합니다. 특히, 기존 "},{"id":"2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration","title":"[논문리뷰] Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration","excerpt":"arXiv에 게시된 'Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration","tags":["Review","3D Geometry Synthesis","Diffusion Models","Acceleration","Caching","Training-free","Flow Matching","Voxel Stabilization","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengyu Yang, Yanming Yang, Chenyi Xu, Chenxi Song, Yufan Zuo, Tong Zhao, Ruibo Li, Chi Zhang 핵심 연구 목표 본 논문은 3D Diffusion 모델의 느린 추론 속도 문제를 해결하는 것을 목표로 합니다. 특히, 기존 2D/비디오 캐싱 기법을 3D"},{"id":"2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning","title":"[논문리뷰] FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning","excerpt":"Simin Chen이 arXiv에 게시한 'FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning","tags":["Review","Federated Learning","Model Heterogeneity","Representation Learning","Privacy Preservation","Communication Efficiency","Entangled Representation","Knowledge Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuan Yao, Lixu Wang, Jiaqi Wu, Jin Song, Simin Chen, Zehua Wang, Zijian Tian, Wei Chen, Huixia Li, Xiaoxiao Li 핵심 연구 목표 논문은 기존 FL 방법론이 가정하는 모델 동질성(homogeneous model architectures"},{"id":"2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets","title":"[논문리뷰] Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets","excerpt":"Avishai Weizman이 arXiv에 게시한 'Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets","tags":["Review","Data Leakage","Video Datasets","Clustering","Frame Selection","Deep Learning","Object Detection","Dataset Partitioning","Dimensionality Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Noam Glazner, Sharon Shalev, Noam Tsfaty, Avishai Weizman 핵심 연구 목표 본 논문은 비디오 기반 데이터셋에서 발생하는 정보 누출(information leakage) 문제를 해결하는 것을 목표로 합니다. 연속된 프레임 간의 높은 시공간적 상관관계로 인해 훈련, 검증, 테스"},{"id":"2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information","title":"[논문리뷰] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information","excerpt":"Kristian Kersting이 arXiv에 게시한 'Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information","tags":["Review","LLM Reasoning","Chain-of-Thought","Prompt Engineering","Efficiency","Structured Input","Information Extraction","Cognitive Psychology","Token Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Lukas Struppek, Dominik Hintersdorf, Hannah Struppek, Daniel Neider, Kristian Kersting 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 ChainofThought (CoT) 추론 과정에서 발생하는 과도한 토큰 사용과 높은 추론 지연 시간 문제를"},{"id":"2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images","title":"[논문리뷰] From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images","excerpt":"Filippos Kokkinos이 arXiv에 게시한 'From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images","tags":["Review","Multimodal LLM","Human Cognition","Image Perception","Benchmarking","Supervised Fine-tuning","Image Generation","Aesthetics","Memorability"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiming Chen, Junlin Han, Tianyi Bai, Shengbang Tong, Filippos Kokkinos, Philip Torr 핵심 연구 목표 본 논문은 MLLM(Multimodal Large Language Model) 이 이미지 내 객체를 인식하는 '무엇'을 넘어, 인간이 이미지를 주관적으로"},{"id":"2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning","title":"[논문리뷰] Geometrically-Constrained Agent for Spatial Reasoning","excerpt":"Lehan He이 arXiv에 게시한 'Geometrically-Constrained Agent for Spatial Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning","tags":["Review","Spatial Reasoning","Vision Language Models (VLMs)","Geometric Constraints","Agentic AI","Tool Integration","Semantic-to-Geometric Gap","Task Formalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeren Chen, Xiaoya Lu, Zhijie Zheng, Pengrui Li, Lehan He, Yijin Zhou, Jing Shao, Bohan Zhuang, Lu Sheng 핵심 연구 목표 본 논문은 Vision Language Models (VLMs)이 공간 추론 시 겪는 의미론기하학적 간극(seman"},{"id":"2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge","title":"[논문리뷰] Layer-Aware Video Composition via Split-then-Merge","excerpt":"Wen-Sheng Chu이 arXiv에 게시한 'Layer-Aware Video Composition via Split-then-Merge' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge","tags":["Review","Generative Video Composition","Diffusion Models","Layer-Aware Generation","Self-Composition","Affordance Learning","Video Editing","Data Augmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ozgur Kara, Yujia Chen, MingHsuan Yang, WenSheng Chu, James M. Rehg, Du Tran 핵심 연구 목표 본 논문은 생성 비디오 합성에서 제어력을 강화하고 데이터 부족 문제를 해결하는 것을 목표로 합니다. 구체적으로, 동적인 전경 비디오를 새로운 배경 비디오에 통합하여 "},{"id":"2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey","title":"[논문리뷰] MRI Super-Resolution with Deep Learning: A Comprehensive Survey","excerpt":"arXiv에 게시된 'MRI Super-Resolution with Deep Learning: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey","tags":["Review","MRI Super-Resolution","Deep Learning","Computational Imaging","Inverse Problems","Generative AI","Medical Imaging","Survey"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohammad Khateri, Serge Vasylechko, Morteza Ghahremani, Liam Timms, Deniz Kocanaogullari, Simon K. Warfield, Camilo Jaimes, Davood Karimi, Alejandra Sierra, Jussi Tohka, Sila Kur"},{"id":"2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models","title":"[논문리뷰] Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models","excerpt":"arXiv에 게시된 'Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models","tags":["Review","Small Language Models (SLMs)","Latency Optimization","Hybrid Architectures","Evolutionary Search","Weight Normalization","Efficient Attention","Depth-Width Ratios","Real-device Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yonggan Fu, Xin Dong, Shizhe Diao, Matthijs Van keirsbilck, Hanrong Ye, Wonmin Byeon, Yashaswi Karnati, Lucas Liebenwein, Hannah Zhang, Nikolaus Binder, Maksim Khadkevich, Alexan"},{"id":"2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement","title":"[논문리뷰] OmniRefiner: Reinforcement-Guided Local Diffusion Refinement","excerpt":"Yiren Song이 arXiv에 게시한 'OmniRefiner: Reinforcement-Guided Local Diffusion Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement","tags":["Review","Diffusion Models","Image Refinement","Reinforcement Learning","Fine-Grained Editing","Reference-Guided Generation","Latent Diffusion","Visual Fidelity","Detail Restoration"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaoli Liu, Ziheng Ouyang, Shengtao Lou, Yiren Song 핵심 연구 목표 현재 확산 모델들이 참조 이미지를 사용하여 이미지를 정제할 때 로고, 텍스트, 얼굴 특징, 복잡한 패턴과 같은 세부 시각적 디테일을 보존하는 데 어려움 을 겪는 문제를 해결하는 것을 목표로 합니다. 생성된 이미지"},{"id":"2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model","title":"[논문리뷰] OralGPT-Omni: A Versatile Dental Multimodal Large Language Model","excerpt":"arXiv에 게시된 'OralGPT-Omni: A Versatile Dental Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model","tags":["Review","Multimodal Large Language Model (MLLM)","Dental Imaging Analysis","Chain-of-Thought (CoT) Reasoning","Medical AI","Benchmark","Diagnosis","Oral Healthcare","Explainable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jing Hao, Yuci Liang, Lizhuo Lin, Yuxuan Fan, Wenkai Zhou, Kaixin Guo, Zanting Ye, Hao Tang, Yanpeng Sun, Xinyu Zhang, Yanqi Yang, Qiankun Li, James KitHon Tsoi, Linlin Shen†, Ku"},{"id":"2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models","title":"[논문리뷰] REASONEDIT: Towards Reasoning-Enhanced Image Editing Models","excerpt":"arXiv에 게시된 'REASONEDIT: Towards Reasoning-Enhanced Image Editing Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models","tags":["Review","Image Editing","Reasoning-Enhanced AI","Multimodal Large Language Models","Diffusion Transformers","Thinking","Reflection","Iterative Refinement","Instruction Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Step1XImage Team, StepFun 핵심 연구 목표 본 논문은 기존 이미지 편집 모델들이 고정된 MLLM 인코더 를 사용하여 복잡하거나 추상적인 지시를 처리하는 데 어려움을 겪는 문제를 해결하고자 합니다. MLLM의 추론 능력(thinking 및 reflection) 을 활용하여 지시 이해도와 편집 정확도를"},{"id":"2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models","title":"[논문리뷰] Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models","excerpt":"Yehudit Aperstein이 arXiv에 게시한 'Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models","tags":["Review","Anomaly Detection","Surveillance Videos","Weakly Supervised Learning","Multiple Instance Learning","Dual-Encoder","I3D","TimeSformer","Top-k Pooling"],"text":"링크: 논문 PDF로 바로 열기 저자: Noam Tsfaty, Avishai Weizman, Liav Cohen, Moshe Tshuva, Yehudit Aperstein 핵심 연구 목표 이 논문은 감시 비디오에서 희귀하고 다양한 이상 이벤트(abnormal events) 를 비디오 수준의 약한 감독(videolevel supervision) 만을 사용하여"},{"id":"2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists","title":"[논문리뷰] RefineBench: Evaluating Refinement Capability of Language Models via Checklists","excerpt":"arXiv에 게시된 'RefineBench: Evaluating Refinement Capability of Language Models via Checklists' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists","tags":["Review","Language Models","Refinement Capability","Self-Refinement","Guided Refinement","Checklist Evaluation","Multi-turn Interaction","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: YoungJun Lee, Seungone Kim, ByungKwan Lee, Jong Myoung Kim, Minkyeong Moon, Yechan Hwang, Graham Neubig, Sean Welleck, HoJin Choi 핵심 연구 목표 이 논문은 대규모 언어 모델(LM)이 자신의 답변을 스스로 또는 외부 "},{"id":"2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs","title":"[논문리뷰] SO-Bench: A Structural Output Evaluation of Multimodal LLMs","excerpt":"arXiv에 게시된 'SO-Bench: A Structural Output Evaluation of Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs","tags":["Review","Multimodal LLMs","Structural Output","Information Extraction","JSON Schema","SO-Bench","Visual Reasoning","Supervised Fine-tuning","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Di Feng, Kaixin Ma, Feng Nan, Haofeng Chen, Bohan Zhai, David Griffiths, Mingfei Gao, Zhe Gan, Eshan Verma, Yinfei Yang, Zhifeng Chen, Afshin Dehghan 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모"},{"id":"2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps","title":"[논문리뷰] Test-time scaling of diffusions with flow maps","excerpt":"Sanja Fidler이 arXiv에 게시한 'Test-time scaling of diffusions with flow maps' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps","tags":["Review","Diffusion Models","Flow Maps","Test-time Adaptation","Reward Guidance","Generative Models","SMC","Vision-Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Amirmojtaba Sabour, Michael S. Albergo, Carles DomingoEnrich, Nicholas M. Boffi, Sanja Fidler, Karsten Kreis, Eric VandenEijnden 핵심 연구 목표 본 논문은 확산 모델의 추론 시점에 사용자 정의 보상에 따라 샘플을 개선"},{"id":"2025-12-01-The-Collapse-of-Patches","title":"[논문리뷰] The Collapse of Patches","excerpt":"Weidong Cai이 arXiv에 게시한 'The Collapse of Patches' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-The-Collapse-of-Patches","tags":["Review","Patch Collapse","Image Generation","Image Classification","Masked Image Modeling","Vision Transformers","PageRank","Uncertainty Reduction","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Guo, Shunqi Mao, Zhuonan Liang, Heng Wang, Weidong Cai 핵심 연구 목표 본 연구는 이미지 내 패치들 간의 상호 의존성을 분석하여 '패치 붕괴(patch collapse)' 라는 새로운 개념을 제안하고, 이를 통해 이미지의 불확실성을 가장 효율적으로 줄이는 최적의 패치 "},{"id":"2025-12-01-Vision-Bridge-Transformer-at-Scale","title":"[논문리뷰] Vision Bridge Transformer at Scale","excerpt":"Xinchao Wang이 arXiv에 게시한 'Vision Bridge Transformer at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale","tags":["Review","Vision Transformer","Bridge Models","Conditional Generation","Image Editing","Video Translation","Velocity Matching","Diffusion Models","Scalability"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenxiong Tan, Zeqing Wang, Xingyi Yang, Songhua Liu, Xinchao Wang 핵심 연구 목표 본 논문은 Brownian Bridge Models 를 대규모 비전 변환 태스크(이미지 및 비디오)에 적용하여 조건부 생성의 효율성을 극대화하는 것을 목표로 합니다. 기존 노이즈투데이"},{"id":"2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models","title":"[논문리뷰] World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models","excerpt":"Na Min An이 arXiv에 게시한 'World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models","tags":["Review","Vision-Language Models","Culture Mixing","VQA","Synthetic Data Generation","Multicultural Understanding","Model Robustness","Fine-tuning","Cultural Bias"],"text":"링크: 논문 PDF로 바로 열기 저자: Eunsu Kim, Junyeong Park, Na Min An, Junseong Kim, Hitesh Laxmichand Patel, Jiho Jin, Julia Kruk, Amit Agarwal, Srikant Panda, Fenal Ashokbhai Ilasariya, Hyunjung Shim, Alice Oh "},{"id":"2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM","title":"[논문리뷰] Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM","excerpt":"arXiv에 게시된 'Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM","tags":["Review","Small Language Models","Data Efficiency","Reasoning","Maximal-Update Parameterization","FP8 Mixed Precision","Optimizer Scheduling","Long-Context Adaptation","Agent AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Liu, Xiaolong Zhong, Ling Jiang 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)이 복잡한 다단계 추론 능력을 갖추고 있음에도 불구하고 높은 연산 요구사항으로 인해 엣지 또는 비용에 민감한 환경에서의 배포가 어렵다는 문제를 해결하고자 합니다. 이를 위해 1.3B 파라미터 규모의 X"},{"id":"2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection","title":"[논문리뷰] YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection","excerpt":"Avishai Weizman이 arXiv에 게시한 'YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection","tags":["Review","Object Detection","YOLOv9","Mixture-of-Experts","Adaptive Routing","Deep Learning","Computer Vision","Feature Specialization"],"text":"링크: 논문 PDF로 바로 열기 저자: Ori Meiraz, Sharon Shalev, Avishai Weizman 핵심 연구 목표 본 연구는 객체 탐지 분야에서 YOLOv9T 모델의 성능과 견고성을 향상시키기 위해 새로운 MixtureofExperts (MoE) 프레임워크를 제안합니다. 특히, 여러 YOLOv9T 전문가 들 간의 적응형 라우팅 을 도입하여"},{"id":"2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer","title":"[논문리뷰] Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer","excerpt":"arXiv에 게시된 'Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-01 00:00:00+0900+0900","permalink":"/ai/review/2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer","tags":["Review","Diffusion Transformer","Efficient Training","Multi-Modal Learning","Text-to-Image Generation","Image Editing","RLHF","Photorealistic Rendering"],"text":"링크: 논문 PDF로 바로 열기 저자: ZImage Team, Alibaba Group 핵심 연구 목표 현재 고성능 이미지 생성 모델들이 겪고 있는 비싼 훈련 및 추론 비용, 그리고 폐쇄형 또는 과도한 파라미터(20B80B) 문제점을 해결하고자 합니다. 특히, 일반 소비재 하드웨어에서도 구동 가능한 6B 파라미터 규모의 효율적인 이미지 생성 파운데이션 모델"},{"id":"2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression","title":"[논문리뷰] Accelerating Streaming Video Large Language Models via Hierarchical Token Compression","excerpt":"arXiv에 게시된 'Accelerating Streaming Video Large Language Models via Hierarchical Token Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression","tags":["Review","Streaming Video LLMs","Token Compression","ViT Encoding","LLM Prefilling","Causal Compression","Caching","Pruning","Low-latency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiyu Wang, Xuyang Liu, Xiyan Gui, Xinying Lin, Boxue Yang, Chenfei Liao, Tailai Chen, Linfeng Zhang 핵심 연구 목표 스트리밍 비디오 대규모 언어 모델(VideoLLMs)의 실시간 배포 시 발생하는 높은 연산 비용, 특히 Vision Tran"},{"id":"2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution","title":"[논문리뷰] Agentic Policy Optimization via Instruction-Policy Co-Evolution","excerpt":"arXiv에 게시된 'Agentic Policy Optimization via Instruction-Policy Co-Evolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution","tags":["Review","Reinforcement Learning","Large Language Models","Instruction Optimization","Policy Co-Evolution","Agentic AI","Tool-Integrated Reasoning","Self-Reflection"],"text":"링크: 논문 PDF로 바로 열기 저자: Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen 핵심 연구 목표 본 논문은 LLM 기반 에이전트의 강화 학습(RL) 과정에서 고정되고 수동으로 설계된 명령어(instruction)가 최적의 성능을 저해한다는 문제에 주목합니다. 에이전트의 정책이 발전함에 따라 최적의 명령어 또한"},{"id":"2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images","title":"[논문리뷰] Asking like Socrates: Socrates helps VLMs understand remote sensing images","excerpt":"Xinran He이 arXiv에 게시한 'Asking like Socrates: Socrates helps VLMs understand remote sensing images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images","tags":["Review","Remote Sensing","Vision-Language Models","Iterative Reasoning","Evidence-Seeking","Socratic Method","Reinforcement Learning","Multi-Agent System","VQA","Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinran He, Linrui Xu, Zhaoyang Zhang, Ziyu Li, ShaoRun 핵심 연구 목표 기존 VisionLanguage Model (VLM) 들이 원격 감지(RS) 이미지 분석에서 겪는 \"가짜 추론(pseudo reasoning)\" 문제를 해결하고자 합니다. 이는 모델이 추론 과정을 서술만 "},{"id":"2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation","title":"[논문리뷰] Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation","excerpt":"MElHuseyni이 arXiv에 게시한 'Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation","tags":["Review","YOLOv5","Instance Segmentation","Thyroid Nodule","Ultrasound Imaging","Doppler Imaging","Medical AI","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mahmoud El Hussieni 핵심 연구 목표 본 연구는 초음파 이미지에서 YOLOv5 알고리즘 을 활용하여 갑상선 결절의 정확한 인스턴스 분할(instance segmentation) 성능을 향상시키는 것을 목표로 합니다. 특히, 임상적으로 종종 배제되는 도플러 이미지(Doppler images) 가 분할 성능"},{"id":"2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights","title":"[논문리뷰] Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights","excerpt":"arXiv에 게시된 'Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights","tags":["Review","Multimodal AI","Text-to-Multi-Image","Causal Reasoning","World Knowledge","Benchmarking","Spatiotemporal Consistency","Generative Models","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Juanxi Tian¹, Siyuan Li¹, Conghui He¹, Lijun Wu¹, Cheng Tan¹ 핵심 연구 목표 현재 텍스트이미지(T2I) 모델이 정적 이미지 생성에는 뛰어나지만, 시간 경과에 따라 전개되는 동적, 인과적 프로세스 를 모델링하는 데 한계가 있음을 지적합니다. 이 논문은 모델이 정적 패턴 매"},{"id":"2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning","title":"[논문리뷰] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning","excerpt":"arXiv에 게시된 'Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning","tags":["Review","Diffusion Models","Image Generation","Distillation","Reinforcement Learning","Few-Step Sampling","Timestep-Aware","Pixel-GAN","Model Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Guanjie Chen, Shirui Huang, Kai Liu, Jianchen Zhu, Xiaoye Qu, Peng Chen, Yu Cheng, Yifu Sun 핵심 연구 목표 본 논문은 반복적인 샘플링 과정과 높은 훈련 비용으로 인해 computationally expensive한 확산 모델의 한계를 극복하는 것"},{"id":"2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence","title":"[논문리뷰] From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence","excerpt":"arXiv에 게시된 'From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence","tags":["Review","Code LLMs","Software Engineering Agents","Code Generation","Reinforcement Learning","Supervised Fine-tuning","Multimodal AI","Code Safety","Scaling Laws"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Yang, Xianglong Liu, Weifeng Lv, Ken Deng, Shawn Guo, Lin Jing, Yizhi Li, Shark Liu, Xianzhen Luo, Yuyu Luo, Changzai Pan, Ensheng Shi, Yingshui Tan, Renshuai Tao, Zili Wang"},{"id":"2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation","title":"[논문리뷰] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation","excerpt":"arXiv에 게시된 'GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation","tags":["Review","Robotic Manipulation","Reinforcement Learning","Vision-Language-Action","Dexterous Control","Long-Horizon Tasks","Data Filtering","Data Augmentation","Foundation Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunfei Li, Xiao Ma, Jiafeng Xu 핵심 연구 목표 본 논문은 일반적인 VisionLanguageAction (VLA) 파운데이션 모델 이 실제 환경에서 발생하는 긴 호라이즌의 정교하고 민첩한 로봇 조작 에서 겪는 한계를 해결하는 것을 목표로 합니다. 특히, 사람의 시범 데이터에 내재된 노이즈 및 "},{"id":"2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks","title":"[논문리뷰] Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks","excerpt":"arXiv에 게시된 'Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks","tags":["Review","Large Language Models","Clinical AI","Medical Benchmarks","AI Evaluation","Medical Decision Support","MedQA","HealthBench","Generalist AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Krithik Vishwanath, Mrigayu Ghosh, Anton Alyakin, Daniel Alexander Alber, Yindalon Aphinyanaphongs, Eric Karl Oermann 핵심 연구 목표 의료 분야에서 전문 임상 AI 도구들이 일반 목적의 대규모 언어 모델(LLM)보다 안전하고 "},{"id":"2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents","title":"[논문리뷰] HiconAgent: History Context-aware Policy Optimization for GUI Agents","excerpt":"Kaiwen Zhou이 arXiv에 게시한 'HiconAgent: History Context-aware Policy Optimization for GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents","tags":["Review","GUI Agents","Reinforcement Learning","Context-aware","History Compression","Policy Optimization","Multimodal LLM","Dynamic Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Xurui Zhou, Gongwei Chen, Yuquan Xie, Zaijing Li, Kaiwen Zhou, Shuai Wang, Shuo Yang, Zhuotao Tian, Rui Shao 핵심 연구 목표 GUI(Graphical User Interface) 에이전트가 순차적 탐색 작업을 수행할 때, 과도한 계산"},{"id":"2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents","title":"[논문리뷰] How Far Are We from Genuinely Useful Deep Research Agents?","excerpt":"Xinran Zhou이 arXiv에 게시한 'How Far Are We from Genuinely Useful Deep Research Agents?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents","tags":["Review","Deep Research Agents","Evaluation Benchmark","Failure Taxonomy","Report Generation","Information Retrieval","Reasoning Resilience","Content Fabrication","AI Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinran Zhou, Kangqi Song, He Zhu, Dingling Zhang, JinChengRen (OPPO AI Agent Team) 핵심 연구 목표 본 논문은 기존의 심층 연구 에이전트(DRA) 벤치마크가 질문 응답(QA) 또는 폐쇄형 작업 에 치중하여 종합적인 보고서 생성 능력을 제대로 평가하지 못하"},{"id":"2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages","title":"[논문리뷰] IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages","excerpt":"arXiv에 게시된 'IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages","tags":["Review","Low-resource Languages","Indic Languages","LLM Evaluation","Benchmark","Multilingual LLMs","Question Answering","Cross-lingual Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Ayush Maheshwari, Kaushal Sharma, Vivek Patel, Aditya Maheshwari 핵심 연구 목표 대규모 언어 모델(LLMs)이 고자원 다국어 작업에서 우수한 성능을 보이지만, 저자원 및 초저자원 인디언 언어에 대한 평가는 심각하게 부족합니다. 본 연구는 이러한 언어에서의 LLM 성능"},{"id":"2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout","title":"[논문리뷰] Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout","excerpt":"Pinar Yanardag이 arXiv에 게시한 'Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout","tags":["Review","Autoregressive Video Generation","Rotary Positional Embedding","Infinite Video Generation","Action Control","Cinematic Transitions","Video Diffusion Models","KV Cache"],"text":"링크: 논문 PDF로 바로 열기 저자: Hidir Yesiltepe, Tuna Han Salih Meral, Adil Kaan Akan, Kaan Oktay, Pinar Yanardag 핵심 연구 목표 본 논문은 기존의 autoregressive 비디오 diffusion 모델이 가진 세 가지 핵심 한계를 해결하는 것을 목표로 합니다. 이는 3D Rotary"},{"id":"2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision","title":"[논문리뷰] InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision","excerpt":"arXiv에 게시된 'InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision","tags":["Review","Video Foundation Models","Self-Supervised Learning","Masked Video Modeling","Video-Text Supervision-Free","Encoder-Predictor-Decoder","Diffusion Decoder","Semantic Alignment","Latent World Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenting Wang, Yuhan Zhu, Yicheng Xu, Jiange Yang, Ziang Yan, Yali Wang, Yi Wang, Limin Wang 핵심 연구 목표 본 논문은 노이즈 많고 제한적인 비디오텍스트 지도 학습의 한계와 저수준 픽셀 재구성에 머무르거나 숏컷 학습을 유도하는 기존 Masked "},{"id":"2025-12-02-LFM2-Technical-Report","title":"[논문리뷰] LFM2 Technical Report","excerpt":"arXiv에 게시된 'LFM2 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-LFM2-Technical-Report","tags":["Review","Edge AI","Foundation Models","Hybrid Architecture","Knowledge Distillation","Multimodal AI","On-device Deployment","Efficient Inference","LLM Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Liquid AI Team 핵심 연구 목표 본 논문은 LFM2 라는 Liquid Foundation Models 제품군을 소개하며, 효율적인 온디바이스 배포 와 강력한 태스크 수행 능력 을 동시에 달성하는 것을 목표로 합니다. 특히, CPU 및 이종 NPU 환경에서 엄격한 지연 시간, 메모리, 에너지 예산 제약 내에서"},{"id":"2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds","title":"[논문리뷰] Learning Eigenstructures of Unstructured Data Manifolds","excerpt":"arXiv에 게시된 'Learning Eigenstructures of Unstructured Data Manifolds' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds","tags":["Review","Spectral Basis Learning","Unstructured Data","Manifold Learning","Laplacian Operator","Optimal Approximation Theory","Neural Networks","Eigenstructure","Point Cloud Processing"],"text":"링크: 논문 PDF로 바로 열기 저자: Roy Velich, Arkadi Piven, David Bensaïd, Daniel Cremers, Thomas Dagès, Ron Kimmel 핵심 연구 목표 이 논문은 비정형 데이터(unstructured data)로부터 연산자 선택, 이산화, 고유값 해석기 없이 직접 스펙트럼 기저(spectral basis)를"},{"id":"2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling","title":"[논문리뷰] LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling","excerpt":"arXiv에 게시된 'LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling","tags":["Review","Long Video Understanding","Multimodal LLMs","Tool Calling","Reinforcement Learning","Chain-of-Thought","Temporal Grounding","Video Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Zuhao Yang, Sudong Wang, Kaichen Zhang, Keming Wu, Sicong Leng, Yifan Zhang, Chengwei Qin, Shijian Lu, Xingxuan Li, Lidong Bing 핵심 연구 목표 논문은 대규모 멀티모달 모델(LMMs)이 장시간 비디오(hourslong)"},{"id":"2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model","title":"[논문리뷰] Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model","excerpt":"Ying-Cong Chen이 arXiv에 게시한 'Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model","tags":["Review","Geometric Dense Prediction","Depth Estimation","Surface Normal Prediction","Diffusion Models","Rectified Flow","Generative Priors","Deterministic Inference","Two-Stage Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Jing He, Haodong Li, Mingzhi Sheng, YingCong Chen 핵심 연구 목표 본 논문은 단일 이미지에서 픽셀 단위의 기하학적 속성을 복구하는 고질적인 난제(illposed problem)를 해결하는 것을 목표로 합니다. 특히, 기존의 확률론적 생성 모델(stochastic generativ"},{"id":"2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion","title":"[논문리뷰] OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion","excerpt":"arXiv에 게시된 'OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion","tags":["Review","Multimodal Translation","Speech Translation","Simultaneous Translation","Large Language Models","Multimodal Foundation Models","Modular Fusion","End-to-End","Gated Fusion","OCR"],"text":"링크: 논문 PDF로 바로 열기 저자: Sai Koneru, Matthias Huck, Jan Niehues 핵심 연구 목표 본 논문은 텍스트 전용 번역 LLM이 겪는 지연 시간과 멀티모달 컨텍스트 활용 불가능성, 그리고 MMFM이 가진 다국어 번역 성능 및 커버리지의 한계를 해결하고자 합니다. 궁극적으로 MMFM과 번역 LLM을 융합 하여 동시 다국어 멀"},{"id":"2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic","title":"[논문리뷰] OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic","excerpt":"arXiv에 게시된 'OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic","tags":["Review","Autonomous Driving","Reinforcement Fine-tuning","LLM-as-Critic","Vision-Language Model","End-to-End Learning","Chain-of-Thought","Trajectory Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Songyan Zhang, Wenhui Huang, Zhan Chen, Chua Jiahao Collister, Qihang Huang, Chen Lv 핵심 연구 목표 자율 주행 시스템에서 기존 SFT(Supervised Finetuning) 기반 VLM(VisionLanguage Model) 의 제한된 추론 일반화 "},{"id":"2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models","title":"[논문리뷰] PromptBridge: Cross-Model Prompt Transfer for Large Language Models","excerpt":"Wei Wei이 arXiv에 게시한 'PromptBridge: Cross-Model Prompt Transfer for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models","tags":["Review","Large Language Models","Prompt Engineering","Model Drifting","Prompt Transfer","Cross-Model Adaptation","Training-Free","Prompt Optimization","MAP-RPE"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaxuan Wang, Quan Liu, Zhenting Wang, Zichao Li, Wei Wei, Yang Liu, Yujia Bao 핵심 연구 목표 본 논문은 LLM 시스템에서 모델이 교체되거나 업데이트될 때, 기존 모델에 최적화된 프롬프트의 성능이 다른 모델에서 크게 저하되는 현상인 모델 드리프팅(Model "},{"id":"2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization","title":"[논문리뷰] Rectifying LLM Thought from Lens of Optimization","excerpt":"Kai Chen이 arXiv에 게시한 'Rectifying LLM Thought from Lens of Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization","tags":["Review","LLM Reasoning","Chain-of-Thought","RLVR","Optimization Framework","Process-level Reward","Gradient Descent","Reasoning Efficiency","Suboptimal Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Junnan Liu, Hongwei Liu, Songyang Zhang, Kai Chen 핵심 연구 목표 본 논문은 Long ChainofThought (CoT) LLM이 흔히 보이는 과도한 추론 및 불필요하게 긴 추론 사슬과 같은 비최적 추론 행동 을 해결하여, 성능 저하 및 높은 계산 비용 문제를 개선하는 것을 목"},{"id":"2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling","title":"[논문리뷰] SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling","excerpt":"arXiv에 게시된 'SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling","tags":["Review","LLM Reasoning","Test-time Scaling","Resource Allocation","Dual-process Theory","Mathematical Reasoning","Adaptive Computation","Performance Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Xiao, Chunpu Xu, Ruifeng Yuan, Jiashuo Wang, Wenjie Li, Pengfei Liu 핵심 연구 목표 이 논문은 대규모 언어 모델(LLMs)의 수학적 추론 과정에서 발생하는 성능 병목 현상을 해결하는 것을 목표로 합니다. 기존 테스트시간 컴퓨팅 스케일링 방법론들이 모든 추론"},{"id":"2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models","title":"[논문리뷰] Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models","excerpt":"arXiv에 게시된 'Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models","tags":["Review","Multimodal Large Language Models (MLLMs)","Token Pruning","Graph-Structured Pruning (GSP)","Query-Conditioned Semantic Pruning (QCSP)","Determinantal Point Processes (DPP)","Model Efficiency","Visual Redundancy"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongyu Yang, Dannong Xu, Wei Pang, Yingfang Yuant 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLM)에서 고해상도 이미지 및 비디오 처리 시 발생하는 과도한 메모리 소비 및 추론 지연 시간 문제 를 해결하고자 합니다. 기존 토큰 가지치기(pruning) 방법들이 사"},{"id":"2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf","title":"[논문리뷰] Seeing the Wind from a Falling Leaf","excerpt":"Emily Yue-Ting Jia이 arXiv에 게시한 'Seeing the Wind from a Falling Leaf' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf","tags":["Review","Inverse Graphics","Differentiable Physics","Force Estimation","Video Generation","Material Point Method","3D Gaussians","Spatio-temporal Modeling","Vision-Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Gao, Jiageng Mao, HongXing Yu, Haozhe Lou, Emily YueTing Jia, Jernej Barbic, Jiajun Wu, Yue Wang 핵심 연구 목표 본 연구는 영상 데이터로부터 나뭇잎이 떨어지는 바람과 같이 눈에 보이지 않는 물리적 힘(invisible force"},{"id":"2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs","title":"[논문리뷰] SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs","excerpt":"arXiv에 게시된 'SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs","tags":["Review","LLMs","Long-context Reasoning","KV Cache Optimization","Speculative Sparsity","Knowledge Distillation","Adaptive Memory Management","Throughput"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaming Xu, Jiayi Pan, Hanzhen Wang, Yongkang Zhou, Jiancai Ye, Yu Wang, Guohao Dai 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 장문맥(longcontext) 추론 시 발생하는 KeyValue (KV) 캐시 관련 문제를 해결하는 것을 목표로 "},{"id":"2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices","title":"[논문리뷰] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices","excerpt":"arXiv에 게시된 'Stabilizing Reinforcement Learning with LLMs: Formulation and Practices' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices","tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Policy Gradient","REINFORCE","Mixture-of-Experts (MoE)","Training Stability","Importance Sampling","Routing Replay","Off-policy Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Chujie Zheng, Junrong Lin, Kai Dang, Bowen Yu, An Yang, Mingze Li, Huiqiang Jiang, Junyang Lin, Yuqiong Liu, Jingren Zhou 핵심 연구 목표 본 논문은 LLM 기반 RL의 불안정성 문제를 해결하고, 시퀀스 레벨 보상을 토큰 레"},{"id":"2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos","title":"[논문리뷰] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos","excerpt":"arXiv에 게시된 'StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos","tags":["Review","Streaming Video Understanding","Gaze-Guided AI","Temporal Reasoning","Proactive AI","MLLMs","Eye Tracking","Benchmark","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Daeun Lee, Subhojyoti Mukherjee, Branislav Kveton, Ryan A. Rossi, Viet Dac Lai, Seunghyun Yoon, Trung Bui, Franck Dernoncourt, Mohit Bansal 핵심 연구 목표 본 연구는 대규모 언어 모델(MLLMs)이 스트리밍 "},{"id":"2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models","title":"[논문리뷰] Structured Extraction from Business Process Diagrams Using Vision-Language Models","excerpt":"Barry Devereux이 arXiv에 게시한 'Structured Extraction from Business Process Diagrams Using Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models","tags":["Review","Vision-Language Models","BPMN Extraction","Structured Information Extraction","OCR Enrichment","Prompt Engineering","Diagram Understanding","Business Process Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Pritam Deka, Barry Devereux 핵심 연구 목표 이 논문은 비즈니스 프로세스 모델 및 표기법(BPMN) 다이어그램 이미지에서 원시 XML 파일이나 텍스트 주석 없이 직접 구조화된 JSON 표현 을 추출하는 것을 목표로 합니다. 이는 기존 방법론이 XML 의존성으로 인해 발생하는 하위 시스템 통합 및 "},{"id":"2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models","title":"[논문리뷰] TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models","excerpt":"arXiv에 게시된 'TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models","tags":["Review","Unified Multimodal Models","Visual Representation","VAE","Flow Matching","Multimodal Understanding","Multimodal Generation","Image Editing","State-of-the-Art"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiheng Liu, Weiming Ren, Haozhe Liu, Zijian Zhou, Shoufa Chen, Haonan Qiu, Xiaoke Huang, Zhaochong An, Fanny Yang, Aditya Patel, Viktar Atliha, Tony Ng, Xiao Han, Chuyan Zhu, Ch"},{"id":"2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models","title":"[논문리뷰] The Art of Scaling Test-Time Compute for Large Language Models","excerpt":"Tanmoy Chakraborty이 arXiv에 게시한 'The Art of Scaling Test-Time Compute for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models","tags":["Review","Test-Time Scaling","LLMs","Reasoning","Compute Efficiency","Inference Optimization","Decoding Strategies","Model Behavior"],"text":"링크: 논문 PDF로 바로 열기 저자: Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty 핵심 연구 목표 이 논문은 대규모 언어 모델(LLMs)의 추론 능력 향상을 위한 테스트타임 스케일링(TTS) 전략의 최적 선택 문제를 해결하는 것을 목표로 합니다. 특히, 모델의 훈련 방식, 문제 난이도, 사용 가능한 컴퓨팅 예"},{"id":"2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment","title":"[논문리뷰] The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment","excerpt":"arXiv에 게시된 'The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment","tags":["Review","Image Generation","Image Editing","Diffusion Models","Consistency Correction","Attention Mechanism","Reference-Guided","Agent Framework","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziheng Ouyang, Yiren Song, Yaoli Liu, Shihao Zhu, Qibin Hou, MingMing Cheng, Mike Zheng Shou 핵심 연구 목표 본 논문은 기존 참조 기반 이미지 생성 모델이 미세한 디테일에서 일관성을 유지하지 못하고, 텍스트 및 로고 영역에서 부정확하거나 흐릿하게"},{"id":"2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference","title":"[논문리뷰] VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference","excerpt":"arXiv에 게시된 'VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference","tags":["Review","Vision-Language-Action Models","Asynchronous Inference","Real-Time Robotics","Low-Latency Control","Future State Awareness","Action Quantization","Temporal Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaming Tang, Yufei Sun, Yilong Zhao, Shang Yang, Yujun Lin, Zhuoyang Zhang, James Hou, Yao Lu, Zhijian Liu, Song Han 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA) 모델의 실제 로봇 배포 시 발생하"},{"id":"2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","title":"[논문리뷰] What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards","excerpt":"arXiv에 게시된 'What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","tags":["Review","Video Generation","Diffusion Models","Newtonian Dynamics","Physics-aware AI","Post-Training","Verifiable Rewards","Optical Flow","Mass Estimation"],"text":"링크: 논문 PDF로 바로 열기 저자: MinhQuan Le¹, Yuanzhi Zhu², Vicky Kalogeiton², Dimitris Samaras¹ 핵심 연구 목표 최신 비디오 확산 모델이 시각적으로는 인상적이지만, 물체 부유, 가속도 불일치, 충돌 비현실성 등 기본적인 물리 법칙을 위반하는 문제점을 해결하는 것이 목표입니다. 시각적 사실성과 물리적"},{"id":"2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation","title":"[논문리뷰] Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation","excerpt":"Wenhua Wu이 arXiv에 게시한 'Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Cultural Consistency","Multilingual AI","Neuron Activation","Cultural Probing","Fine-Tuning","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhua Wu, Simiao Xie, Shiming Guo, Shangze Li, Chuancheng Shi 핵심 연구 목표 다국어 텍스트이미지(T2I) 모델이 다국어 프롬프트에 대해 문화적으로 중립적이거나 영어 편향적인 이미지를 생성하여 교차 언어 문화적 일관성(crosslingual cultural consis"},{"id":"2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models","title":"[논문리뷰] Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models","excerpt":"Mikhail Burtsev이 arXiv에 게시한 'Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models","tags":["Review","Knowledge Graphs","Large Language Models","Information Extraction","Wikidata Ontology","Question Answering","Entity Normalization","Retrieval Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Alla Chepurova, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev 핵심 연구 목표 본 논문은 LLM 기반 시스템에서 지식 그래프(KG)의 내재적 품질과 추론 능력이 충분히 활용되지 못하고, 개방형 정보 추출(OIE) KGs가 구조적 엄격성과 온톨로지 정합성 측면에서 한계를 보"},{"id":"2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing","title":"[논문리뷰] WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing","excerpt":"Wendong Bu이 arXiv에 게시한 'WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-02 00:00:00+0900+0900","permalink":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing","tags":["Review","Image Editing","Benchmarking","Cognitive AI","Creativity","Multimodal AI","Knowledge-based Reasoning","Diffusion Models","MLLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaihang Pan, Weile Chen, Haiyi Qiu, Qifan Yu, Wendong Bu, Zehan Wang, Yun Zhu, Juncheng Li, Siliang Tang 핵심 연구 목표 본 논문은 기존 이미지 편집 벤치마크가 인지 및 창의성 기반 이미지 편집 모델의 고급 능력을 평가하는 데 한계가 있"},{"id":"2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning","title":"[논문리뷰] Artemis: Structured Visual Reasoning for Perception Policy Learning","excerpt":"Piotr Koniusz이 arXiv에 게시한 'Artemis: Structured Visual Reasoning for Perception Policy Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning","tags":["Review","Visual Reasoning","Multimodal Large Language Models (MLLM)","Reinforcement Learning (RL)","Perception Policy Learning","Object Grounding","Object Detection","Structured Output"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Tang, Yanpeng Sun, Shan Zhang, Xiaofan Li, Piotr Koniusz, Wei Li, Na Zhao, Zechao Li 핵심 연구 목표 기존 멀티모달 대규모 언어 모델(MLLM)의 시각 지각 정책 학습에서 언어 기반의 추론이 공간적/객체 중심 추론이 필요한 시각 태스크에서 성능 "},{"id":"2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation","title":"[논문리뷰] BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation","excerpt":"arXiv에 게시된 'BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation","tags":["Review","Block Diffusion","Video Generation","Temporal Consistency","KV Cache","Semi-Autoregressive","Video Quality Metrics","Long Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyu Zhang¹, Shuning Chang¹, Yuanyu He12, Yizeng Han¹, Jiasheng Tang13, Fan Wang¹, Bohan Zhuang12 핵심 연구 목표 본 논문은 블록 확산 모델을 사용하여 분 단위 길이의 고품질 및 일관된 비디오를 생성하는 데 따르는 주요 과제들을 해결하는 것을"},{"id":"2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models","title":"[논문리뷰] C^2DLM: Causal Concept-Guided Diffusion Large Language Models","excerpt":"Xinpeng Dong이 arXiv에 게시한 'C^2DLM: Causal Concept-Guided Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models","tags":["Review","Diffusion Models","Large Language Models","Causality","Attention Mechanism","Reasoning","Natural Language Generation","Supervised Fine-Tuning","Concept-Guided"],"text":"링크: 논문 PDF로 바로 열기 저자: Kairong Han, Nuanqiao Shan, Ziyu Zhao, Zijing Hu, Xinpeng Dong 핵심 연구 목표 본 논문은 Autoregressive (AR) 및 Diffusion Language Models (DLMs)의 불충분한 추론 능력 문제를 해결하는 것을 목표로 합니다. 이는 어텐션 메커니즘의"},{"id":"2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning","title":"[논문리뷰] CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning","excerpt":"arXiv에 게시된 'CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning","tags":["Review","CUDA","Matrix Multiplication","Reinforcement Learning","LLMs","Kernel Optimization","HGEMM","GPU Performance","cuBLAS"],"text":"링크: 논문 PDF로 바로 열기 저자: Songqiao Su, Xiaofei Sun, Xiaoya Li, Albert Wang, Jiwei Li and Chris Shum 핵심 연구 목표 본 연구의 핵심 목표는 반정밀 일반 행렬 곱셈(HGEMM) CUDA 커널 의 수동 최적화가 어려운 문제를 해결하고, cuBLAS 와 같은 기존의 고도로 최적화된 라이브러리"},{"id":"2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click","title":"[논문리뷰] Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click","excerpt":"arXiv에 게시된 'Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click","tags":["Review","Panoptic Video Scene Graph Generation","Interactive AI","User Guidance","Promptable Segmentation","Video Understanding","Relational Reasoning","Human-in-the-Loop"],"text":"링크: 논문 PDF로 바로 열기 저자: Raphael Ruschel, Hardikkumar Prajapati, Md Awsafur Rahman, B. S. Manjunath 핵심 연구 목표 기존 Video Scene Graph Generation (VSGG) 및 Panoptic Video Scene Graph (PVSG) 시스템의 폐쇄적인 특성과, SAM/"},{"id":"2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization","title":"[논문리뷰] CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization","excerpt":"arXiv에 게시된 'CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization","tags":["Review","Vision-Language Models","Agentic Reasoning","Tool Use","Reinforcement Learning","Faithfulness Evaluation","Policy Optimization","Visual Search","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinhai Hou, Shaoyuan Xu, Manan Biyani, Moyan Li, Jia Liu, Todd C. Hollon, Bryan Wang 핵심 연구 목표 본 논문은 에이전트 시각언어 모델(VLMs)이 높은 최종 답변 정확도에도 불구하고 종종 \"불성실한\" 시각적 추론을 수행하는 문제를 해결하고자 합니다. "},{"id":"2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models","title":"[논문리뷰] DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models","excerpt":"arXiv에 게시된 'DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models","tags":["Review","Large Language Models","Sparse Attention","Reinforcement Learning","Agentic AI","Tool Use","Open-source LLM","DeepSeek"],"text":"링크: 논문 PDF로 바로 열기 저자: DeepSeekAI 핵심 연구 목표 본 논문은 오픈 소스 대규모 언어 모델(LLM)과 상업용 LLM 간의 성능 격차를 줄이고자 DeepSeekV3.2 를 소개합니다. 특히, 장문 컨텍스트 처리의 비효율성, 후처리 단계에서의 불충분한 컴퓨팅 투자, 그리고 에이전트의 일반화 및 지시 이해 능력 부족이라는 세 가지 주요 한"},{"id":"2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models","title":"[논문리뷰] DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models","excerpt":"arXiv에 게시된 'DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models","tags":["Review","VLA Models","Flow Matching","Robotics","Robustness","Distribution Shift","Wasserstein Distance","Geometric Regularization","Representation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Wanpeng Zhang, Ye Wang, Hao Luo, Haoqi Yuan, Yicheng Feng, Sipeng Zheng, Qin Jin, Zongqing Lu 핵심 연구 목표 VisionLanguageAction (VLA) 모델이 분포 변화 및 복잡한 다단계 로봇 조작 태스크에서 성능 저하를 겪는 문제를 해결"},{"id":"2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","title":"[논문리뷰] Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation","excerpt":"arXiv에 게시된 'Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","tags":["Review","Video Generation","Audio-Video Multimodal","Joint Denoising","Diffusion Models","Transformer Architecture","World Models","Physical Commonsense","Multimodal Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianzong Wu, Hao Lian, Dachao Hao, Ye Tian, Qingyu Shi, Biaolong Chen, Hao Jiang, Yunhai Tong 핵심 연구 목표 본 연구는 오디오비디오 공동 노이즈 제거 훈련이 비디오 품질에만 중점을 둘 때도 비디오 생성 성능을 향상시키는 근본적인 질문에 답하는 "},{"id":"2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","title":"[논문리뷰] DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation","excerpt":"Zixin Zhang이 arXiv에 게시한 'DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","tags":["Review","Diffusion Models","Video Generation","Camera Control","Depth Estimation","Dual-Branch Architecture","Geometric Awareness","Semantic Alignment","Multi-modal Fusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongfei Zhang, Kanghao Chen, Zixin Zhang, Harold H. Chen, Yuanhuiyi Lyu, Yuqi Zhang, Shuai Yang, Kun Zhou, Yingcong Chen 핵심 연구 목표 본 논문은 기존의 카메라 제어 비디오 생성 모델들이 겪는 장면 이해 및 기하학적 인식 "},{"id":"2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning","title":"[논문리뷰] GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning","excerpt":"Kaijun Tan이 arXiv에 게시한 'GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning","tags":["Review","GUI Agents","Screen Navigation","Reinforcement Learning","Multi-Turn RL","Simulation","Supervised Fine-tuning","Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Haolong Yan, Yeqing Shen, Xin Huang, Jia Wang, Kaijun Tan, Zhixuan Liang, Hongxin Li, Zheng Ge, Osamu Yoshie, Si Li, Xiangyu Zhang, Daxin Jiang 핵심 연구 목표 본 연구는 GUI(Graphical User "},{"id":"2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample","title":"[논문리뷰] Glance: Accelerating Diffusion Models with 1 Sample","excerpt":"Linjie Li이 arXiv에 게시한 'Glance: Accelerating Diffusion Models with 1 Sample' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample","tags":["Review","Diffusion Models","Acceleration","Distillation","LoRA","Few-shot Learning","Phase-aware","Image Generation","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuobai Dong, Rui Zhao, Songjie Wu, Junchao Yi, Linjie Li 핵심 연구 목표 본 논문은 이미지 생성 확산 모델의 높은 계산 비용과 많은 추론 단계를 해결하고자 합니다. 특히, 모델의 재훈련 비용과 일반화 성능 저하 없이, 단일 샘플만으로도 효율적인 가속화와 강력한 일반화 능력"},{"id":"2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision","title":"[논문리뷰] Guided Self-Evolving LLMs with Minimal Human Supervision","excerpt":"arXiv에 게시된 'Guided Self-Evolving LLMs with Minimal Human Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision","tags":["Review","Self-Evolving LLMs","Self-Play","Reinforcement Learning","Curriculum Learning","Few-shot Learning","Human Supervision","Concept Drift","Diversity Collapse"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu 핵심 연구 목표 본 논문은 기존의 자율 진화(selfevolving) 언어 모델(LLM)이 겪는 불안정성, 성능 정체, 개념 표류(concept dr"},{"id":"2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory","title":"[논문리뷰] MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory","excerpt":"arXiv에 게시된 'MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory","tags":["Review","Visual Navigation","Dual-Scale Framework","Sparse Spatial Memory Graph","Memory-Guided Planning","Geometry-Enhanced Control","Zero-Shot Navigation","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Bo Wang, Jiehong Lin, Chenzhi Liu, Xinting Hu, Yifei Yu, Tianjia Liu, Zhongrui Wang, Xiaojuan Qi 핵심 연구 목표 이 논문은 동적이고 이전에 본 적 없는 환경에서 강건한 제로샷 시각 내비게이션(zeroshot visual navigation) "},{"id":"2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models","title":"[논문리뷰] Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models","excerpt":"arXiv에 게시된 'Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Masked Diffusion Language Models","Context Comprehension","Locality Bias","Mask Tokens","Fine-tuning","Mask-agnostic Loss","Long-context Processing"],"text":"링크: 논문 PDF로 바로 열기 저자: Julianna Piskorz, Cristina Pinneri, Alvaro Correia, Motasem Alfarra, Risheek Garrepalli, Christos Louizos 핵심 연구 목표 본 연구는 Masked Diffusion Language Models (MDLMs) 의 컨텍스트 이해 능력을 체계"},{"id":"2025-12-03-Mixture-of-Horizons-in-Action-Chunking","title":"[논문리뷰] Mixture of Horizons in Action Chunking","excerpt":"Zelong Sun이 arXiv에 게시한 'Mixture of Horizons in Action Chunking' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Mixture-of-Horizons-in-Action-Chunking","tags":["Review","Vision-Language-Action Models","Action Chunking","Robotic Manipulation","Multi-horizon Planning","Transformer Architecture","Gated Fusion","Dynamic Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Dong Jing, Gang Wang, Jiaqi Liu, Weiliang Tang, Zelong Sun, Yunchao Yao, Zhenyu Wei, Yunhui Liu, Zhiwu Lu, Mingyu Ding 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA) 모델 에서 고정된 액션 청크 길"},{"id":"2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework","title":"[논문리뷰] MultiShotMaster: A Controllable Multi-Shot Video Generation Framework","excerpt":"arXiv에 게시된 'MultiShotMaster: A Controllable Multi-Shot Video Generation Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework","tags":["Review","Multi-Shot Video Generation","Controllable Video Generation","Diffusion Models","RoPE","Spatiotemporal Consistency","Reference Injection","Data Curation Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Qinghe Wang, Xiaoyu Shi, Baolu Li, Weikang Bian, Quande Liu, Huchuan Lu, Xintao Wang, Pengfei Wan, Kun Gai, Xu Jia 핵심 연구 목표 본 논문은 단일 샷(singleshot) 비디오 생성 기술의 한계를 넘어, 유연한 샷 배열, 일관"},{"id":"2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI","title":"[논문리뷰] PAI-Bench: A Comprehensive Benchmark For Physical AI","excerpt":"Humphrey Shi이 arXiv에 게시한 'PAI-Bench: A Comprehensive Benchmark For Physical AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI","tags":["Review","Physical AI","Benchmark","Video Generation","Conditional Video Generation","Video Understanding","Multimodal LLMs","Physical Plausibility","Embodied Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Fengzhe Zhou, Jiannan Huang, Jialuo Li, Deva Ramanan, Humphrey Shi 핵심 연구 목표 현재 다중 모달 대규모 언어 모델( MLLM )과 비디오 생성 모델( VGM )이 실제 물리적 역학을 인지하고 예측하는 능력을 충분히 지원하는지 이해하는 데 한계가 있습니다. 본 연구"},{"id":"2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization","title":"[논문리뷰] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization","excerpt":"arXiv에 게시된 'Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization","tags":["Review","Chain-of-Thought (CoT)","Vision-Language Models (VLMs)","Visual Reasoning","Generalization","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Grounding CoT","Maze Solving"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Du, Kun Zhou, Yingqian Min, Yue Ling, Wayne Xin Zhao, Youbin Wu 핵심 연구 목표 본 논문은 VisionLanguage Models (VLMs)에서 일반화 가능한 시각적 추론 능력을 습득하는 데 다양한 ChainofThought (CoT) 설계 방식 이 어떻게"},{"id":"2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale","title":"[논문리뷰] SimScale: Learning to Drive via Real-World Simulation at Scale","excerpt":"arXiv에 게시된 'SimScale: Learning to Drive via Real-World Simulation at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale","tags":["Review","Autonomous Driving","Simulation","Neural Rendering","3D Gaussian Splatting","Sim-to-Real","Data Scaling","End-to-End Planning","Pseudo-Expert"],"text":"링크: 논문 PDF로 바로 열기 저자: Haochen Tian, Tianyu Li, Haochen Liu, Jiazhi Yang, Yihang Qiu, Guang Li, Junli Wang, Yinfeng Gao, Zhang Zhang, Liang Wang, Hangjun Ye, Tieniu Tan, Long Chen, Hongyang Li 핵심 연구 목표"},{"id":"2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds","title":"[논문리뷰] SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds","excerpt":"Xuhong He이 arXiv에 게시한 'SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds","tags":["Review","Autonomous Agents","Realistic Simulator","Unreal Engine 5","LLM/VLM Agents","Procedural Generation","Multi-Agent Systems","Physical Simulation","Social Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuhong He, Lingjun Mao, Xiaokang Ye, Yan Zhuang, Jiawei Ren 핵심 연구 목표 본 논문은 기존 시뮬레이터들의 한계(제한된 환경, 비현실적인 물리/사회 규칙, LLM/VLM 에이전트 미지원)를 극복하고, 현실적이고 개방적인 환경에서 자율 에이전트의 개발 및 평가를 위한 SIM"},{"id":"2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch","title":"[논문리뷰] Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch","excerpt":"arXiv에 게시된 'Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch","tags":["Review","Multimodal AI","Agentic Models","Interleaved Reasoning","Image Manipulation","DeepSearch","Supervised Fine-tuning (SFT)","Tool-Augmented LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Multimodality Team, Skywork AI 핵심 연구 목표 기존 멀티모달 에이전트 시스템의 한계, 즉 이미지 조작과 웹 검색의 분리, 값비싼 강화 학습(RL) 의존성, 실제 도구 실행과 괴리된 계획 수립 문제를 해결하는 것을 목표로 합니다. SkyworkR1V4 는 멀티모달 계획, 능동적 이미지 조작, 심"},{"id":"2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead","title":"[논문리뷰] SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead","excerpt":"arXiv에 게시된 'SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead","tags":["Review","Vision-Language-Action (VLA)","Lightweight Models","Spatiotemporal Dynamics","4D Features","Masked Autoencoding","Robotics","Edge AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaojun Ni, Cheng Chen, Xiaofeng Wang, Zheng Zhu, Wenzhao Zheng, Boyuan Wang, Tianrun Chen, Guosheng Zhao, Haoyun Li, Zhehao Dong, Qiang Zhang, Yun Ye, Yang Wang, Guan Huang, Wen"},{"id":"2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition","title":"[논문리뷰] TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition","excerpt":"Zichen Wen이 arXiv에 게시한 'TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition","tags":["Review","Table Recognition","Self-supervised Learning","Vision-Language Models","Reinforcement Learning","Question Answering","Data Augmentation","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyuan Zhang, Bin Wang, Qintong Zhang, Fan Wu, Zichen Wen, Jialin Lu, Junjie Shan, Ziqi Zhao, Shuya Yang, Ziling Wang, Ziyang Miao, Huaping Zhong, Yuhang Zang, Xiaoyi Dong, KaHo"},{"id":"2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models","title":"[논문리뷰] The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models","excerpt":"arXiv에 게시된 'The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models","tags":["Review","Analogical Reasoning","Large Language Models","Mechanistic Interpretability","Proportional Analogies","Story Analogies","Structural Alignment","Attention Knockout","Patchscopes"],"text":"링크: 논문 PDF로 바로 열기 저자: Taewhoo Lee, Minju Song, Chanwoong Yoon, Jungwoo Park, Jaewoo Kang 핵심 연구 목표 본 연구는 대규모 언어 모델(LLMs)의 내재된 메커니즘을 탐구하여 LLM이 유추 추론을 수행하는 방식을 이해하는 것을 목표로 합니다. 특히, LLM이 관계형 개념을 추출하고 새로운 "},{"id":"2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation","title":"[논문리뷰] ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation","excerpt":"arXiv에 게시된 'ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation","tags":["Review","Binaural Audio Generation","Spatial Audio","Video-Driven","End-to-End","Conditional Flow Matching","Multimodal AI","Deep Learning","Audio-Visual Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengchen Zhang, Qi Chen, Tong Wu, Zihan Liu, Dahua Lin 핵심 연구 목표 본 논문은 기존 비디오오디오 생성 모델이 모노 출력에 국한되어 공간적 몰입감이 부족하며, 기존 바이노럴 접근 방식이 2단계 파이프라인(모노 생성 후 공간화)으로 인한 오류 누적과 시공간 불일치 문제를 겪는"},{"id":"2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","title":"[논문리뷰] Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation","excerpt":"Yu Ning이 arXiv에 게시한 'Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","tags":["Review","Video Generation","Spatial Reasoning","Visuospatial Intelligence","Diffusion Models","Context-Guided Generation","Scene Navigation","Object Grounding","Out-of-Domain Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeqi Xiao, Yiwei Zhao, Lingxiao Li, Yushi Lan, Yu Ning 핵심 연구 목표 본 논문은 비디오 생성 모델이 시각 데이터(비디오 컨텍스트) 만을 사용하여 인간의 인지와 유사한 시공간 지능(Visuospatial Intelligence) 을 발휘할 수 있는지 탐구하는 것을 목표로 합니"},{"id":"2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning","title":"[논문리뷰] WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning","excerpt":"arXiv에 게시된 'WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning","tags":["Review","Long Video Reasoning","Multimodal Memory","Adaptive Retrieval","Video Large Language Models","Knowledge Graph","Multiscale Temporal Reasoning","Episodic Memory","Semantic Memory"],"text":"링크: 논문 PDF로 바로 열기 저자: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang 핵심 연구 목표 본 논문은 기존 비디오 LLM이 긴 비디오(수 시간수 일)를 처리할 때 직면하는 제한된 컨텍스트 용량 및 시각적 세부 정보 손실 문제를 해결하고자 합니다. 특히 기존 메모리 증강 방식이 텍스트 의존적 "},{"id":"2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation","title":"[논문리뷰] YingVideo-MV: Music-Driven Multi-Stage Video Generation","excerpt":"Chaofan Ding이 arXiv에 게시한 'YingVideo-MV: Music-Driven Multi-Stage Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-03 00:00:00+0900+0900","permalink":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation","tags":["Review","Music-Driven Video Generation","Diffusion Models","Multi-Stage Framework","Camera Control","Lip-Sync","Temporal Coherence","Video Diffusion Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahui Chen, Weida Wang, Runhua Shi, Huan Yang, Chaofan Ding, Zihao Chen 핵심 연구 목표 본 논문은 기존 오디오 기반 아바타 비디오 생성 모델에서 잘 다루어지지 않았던 음악 공연 비디오 생성 및 카메라 모션 제어의 한계를 극복하고자 합니다. 고품질의 장편 음악 "},{"id":"2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models","title":"[논문리뷰] Adversarial Confusion Attack: Disrupting Multimodal Large Language Models","excerpt":"Artur Janicki이 arXiv에 게시한 'Adversarial Confusion Attack: Disrupting Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models","tags":["Review","Adversarial Attack","Multimodal Large Language Models (MLLMs)","Entropy Maximization","Confusion Attack","Black-box Transfer","PGD","AI Agent Safety"],"text":"링크: 논문 PDF로 바로 열기 저자: Jakub Hoscilowicz, Artur Janicki 핵심 연구 목표 본 논문은 기존의 오분류나 탈옥(jailbreak) 공격과 달리, 멀티모달 대규모 언어 모델(MLLMs)이 일관성 없거나 자신감 있게 틀린 출력을 생성하도록 유도하여 시스템적인 혼란(confusion)을 야기하는 새로운 유형의 적대적 공격인 A"},{"id":"2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs","title":"[논문리뷰] AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs","excerpt":"Tosho Hirasawa이 arXiv에 게시한 'AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs","tags":["Review","Image-Text Alignment","Multimodal Benchmarking","Hallucination Detection","Vision-Language Models","Synthetic Data Generation","Fine-Grained Analysis","Captioning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kuniaki Saito, Risa Shinoda, Shohei Tanaka, Tosho Hirasawa, Fumio Okura, Yoshitaka Ushiku 핵심 연구 목표 기존 벤치마크들이 규칙 기반 교란이나 짧은 캡션에 의존하여 미세한 이미지텍스트 정렬 능력을 측정하는 데 한계가 있음을 지적하며, AlignBe"},{"id":"2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation","title":"[논문리뷰] CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation","excerpt":"Yi Yao이 arXiv에 게시한 'CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation","tags":["Review","Multi-step Image Generation","Recipe Illustration","Diffusion Models","Consistent Generation","Regional Control","Positional Encoding","Ingredient Consistency","Procedural Content Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruoxuan Zhang, Bin Wen, Hongxia Xie, Yi Yao, Songhan Zuo, JianYu JiangLin, HongHan Shuai, WenHuang Cheng 핵심 연구 목표 본 논문은 기존 확산 모델이 구조화된 다단계 시나리오, 특히 가변 길이 레시피 이미지 생성에서 일관성 및 유연성 부"},{"id":"2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment","title":"[논문리뷰] Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment","excerpt":"arXiv에 게시된 'Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment","tags":["Review","Normalizing Flows","Representation Alignment","Generative Models","TARFlow","Image Generation","Classification","Training Acceleration","Reverse Pass"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Chen, Xiaowei Xu, Shuai Wang, Chenhui Zhu, Ruxue Wen, Xubin Li, Tiezheng Ge, Limin Wang 핵심 연구 목표 본 논문은 Normalizing Flows (NFs) 의 생성 품질이 학습된 의미론적 표현의 부족으로 제한되는 문제를 해결하고자 합니다."},{"id":"2025-12-04-In-Context-Representation-Hijacking","title":"[논문리뷰] In-Context Representation Hijacking","excerpt":"yossig이 arXiv에 게시한 'In-Context Representation Hijacking' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-In-Context-Representation-Hijacking","tags":["Review","LLM Jailbreak","In-Context Learning","Representation Hijacking","Mechanistic Interpretability","LLM Safety","Adversarial Attack","Semantic Shift"],"text":"링크: 논문 PDF로 바로 열기 저자: Itay Yona, Amir Sarid, Michael Karasik, Yossi Gandelsman 핵심 연구 목표 본 논문은 LLM의 내부 표현을 조작하여 안전 장치를 우회하는 새로운 형태의 탈옥(jailbreak) 공격인 \"Doublespeak\"을 소개합니다. 기존 표면 수준의 공격과는 달리, LLM의 문맥 내 "},{"id":"2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model","title":"[논문리뷰] Jina-VLM: Small Multilingual Vision Language Model","excerpt":"arXiv에 게시된 'Jina-VLM: Small Multilingual Vision Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model","tags":["Review","Vision-Language Model","Multilingual VLM","Small VLM","Visual Question Answering","Attention Pooling","Image Tiling","SigLIP","Qwen"],"text":"링크: 논문 PDF로 바로 열기 저자: Andreas Koukounas, Georgios Mastrapas, Florian Hönicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao 핵심 연구 목표 본 연구는 VLM의 실용적 배포를 저해하는 두 가지 주요 과제를 해결하는 것을 목표로 합니다. "},{"id":"2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video","title":"[논문리뷰] OneThinker: All-in-one Reasoning Model for Image and Video","excerpt":"Kaixuan Fan이 arXiv에 게시한 'OneThinker: All-in-one Reasoning Model for Image and Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video","tags":["Review","Multimodal LLMs","Reinforcement Learning","Visual Reasoning","Generalist Model","Image Understanding","Video Understanding","Multitask Learning","EMA-GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, et al. 핵심 연구 목표 기존 MLLM(Multimodal Large Language Models)이 단일 태스크나 단일 모달리티(이미지 또는 비디오)에 국한되는 한계를 넘어, 이미지와 비디오 이해를 아우르는 다양한 시각 "},{"id":"2025-12-04-PretrainZero-Reinforcement-Active-Pretraining","title":"[논문리뷰] PretrainZero: Reinforcement Active Pretraining","excerpt":"Guoqi Li이 arXiv에 게시한 'PretrainZero: Reinforcement Active Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining","tags":["Review","Reinforcement Learning","Active Learning","Pretraining","Large Language Models","Self-Supervised Learning","Masked Language Modeling","Generalization","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingrun Xing, Zhiyuan Fan, Jie Lou, Guoqi Li, Jiajun Zhang, Debing Zhang 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 사전 훈련 과정에서 강화 학습(RL) 을 활용하여 일반적인 추론 능력을 향상하고, 도메인 특정적인 검증 가능한 보상에 대한 의존성을 "},{"id":"2025-12-04-Qwen3-VL-Technical-Report","title":"[논문리뷰] Qwen3-VL Technical Report","excerpt":"arXiv에 게시된 'Qwen3-VL Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-Qwen3-VL-Technical-Report","tags":["Review","Vision-Language Model","Multimodal Reasoning","Long-Context","Interleaved Data","Mixture-of-Experts","DeepStack","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Qwen Team 핵심 연구 목표 Qwen3VL은 기존 Qwen 시리즈 중 가장 강력한 VisionLanguage Model (VLM) 을 개발하여 광범위한 멀티모달 벤치마크에서 뛰어난 성능을 달성하는 것을 목표로 합니다. 텍스트, 이미지, 비디오를 최대 256K 토큰 의 인터리브드 컨텍스트로 통합하여 순수 텍스트 이"},{"id":"2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","title":"[논문리뷰] RELIC: Interactive Video World Model with Long-Horizon Memory","excerpt":"Chongjian Ge이 arXiv에 게시한 'RELIC: Interactive Video World Model with Long-Horizon Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","tags":["Review","Interactive World Model","Video Generation","Long-Horizon Memory","Real-Time Streaming","Diffusion Models","Autoregressive Models","Spatial Consistency","Unreal Engine"],"text":"링크: 논문 PDF로 바로 열기 저자: Yicong Hong, Yiqun Mei, Chongjian Ge, et al. 핵심 연구 목표 논문은 실시간 장기 스트리밍, 일관된 공간 메모리, 정밀한 사용자 제어라는 세 가지 핵심 요소를 동시에 만족하는 상호작용 가능한 비디오 월드 모델 을 구축하는 것을 목표로 합니다. 기존 접근 방식들이 이 중 하나만을 다루거"},{"id":"2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment","title":"[논문리뷰] SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment","excerpt":"Yi Yang이 arXiv에 게시한 'SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment","tags":["Review","LLM Alignment","Stable Rank","Intrinsic Reward","Reinforcement Learning","Geometric Properties","Group Relative Policy Optimization","Annotation-Free Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yixuan Tang, Yi Yang 핵심 연구 목표 본 논문은 LLM을 인간의 선호도에 맞춰 정렬하는 과정에서 발생하는 외부 감독(인간 주석의 희소성, 보상 모델 해킹, 프롬프트 민감도)의 한계를 극복하는 것을 목표로 합니다. 구체적으로, 모델의 내부 표현(hidden states) 에서 직접 텍스트 품질을 측정할 "},{"id":"2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors","title":"[논문리뷰] SkillFactory: Self-Distillation For Learning Cognitive Behaviors","excerpt":"Manya Wadhwa이 arXiv에 게시한 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors","tags":["Review","Self-Distillation","Cognitive Skills","Reinforcement Learning","Supervised Fine-Tuning","Language Models","Reasoning","Verification","Retrying"],"text":"링크: 논문 PDF로 바로 열기 저자: Zayne Sprague, Jack Lu, Manya Wadhwa, Sedrick Keh, Mengye Ren, Greg Durrett 핵심 연구 목표 본 논문은 기반 언어 모델(LLM)이 처음부터 갖추지 못한 인지적 스킬(예: 검증, 백트래킹, 재시도) 을 외부의 더 강력한 모델 없이 스스로 학습하도록 하는 프레임워"},{"id":"2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL","title":"[논문리뷰] SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL","excerpt":"arXiv에 게시된 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL","tags":["Review","Spatial Reasoning","Vision Language Models","Reinforcement Learning","Tool Augmentation","Robotics","Multi-Tool Use","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyi Chen, Mikaela Angelina Uy, Chan Hee Song, Faisal Ladhak, Adithyavairavan Murali, Qing Qu, Stan Birchfield, Valts Blukis, Jonathan Tremblay 핵심 연구 목표 본 논문은 시각언어 모델(VLM)이 실제 로봇"},{"id":"2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach","title":"[논문리뷰] Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach","excerpt":"Xiu Li이 arXiv에 게시한 'Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach","tags":["Review","Vision-Language-Action Models","Anti-Exploration","Test-Time Scaling","Pseudo-Count","Coin Flipping Network","Offline Reinforcement Learning","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyuan Yang, Yang Zhang, Haoran He, Ling Pan, Xiu Li, Chenjia Bai, Xuelong Li 핵심 연구 목표 사전 학습된 VisionLanguageAction (VLA) 모델 을 지도 미세 조정(SFT)한 후 추론 시 발생하는 불안정성 문제를 해결하는 것이 목표입니다. 이"},{"id":"2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images","title":"[논문리뷰] Thinking with Programming Vision: Towards a Unified View for Thinking with Images","excerpt":"Tao Jin이 arXiv에 게시한 'Thinking with Programming Vision: Towards a Unified View for Thinking with Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images","tags":["Review","Multimodal LLM","Tool Learning","Code Generation","Reinforcement Learning","Image Manipulation","Robustness","Error Recovery","Programming Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Zirun Guo, Minjie Hong, Feng Zhang, Kai Jia, Tao Jin 핵심 연구 목표 본 논문은 기존 MLLM이 단순한 이미지 변형(방향 전환, 뒤집기 등)에 취약하며, 제한적이고 유연하지 못한 도구 사용으로 인해 시각적 추론 성능 향상이 미미하다는 문제를 제기합니다. 이러한 한계를 극복하고,"},{"id":"2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs","title":"[논문리뷰] UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs","excerpt":"arXiv에 게시된 'UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs","tags":["Review","LLM Compression","Quantization","Pruning","Edge AI","Adaptive Deployment","Transformer","State Space Models","Hybrid Models","One-shot Compression"],"text":"링크: 논문 PDF로 바로 열기 저자: HungYueh Chiang, ChiChih Chang, Natalia Frumkin, KaiChiang Wu, Mohamed S. Abdelfattah, Diana Marculescu, et al. 핵심 연구 목표 본 논문은 제한된 리소스를 가진 엣지 디바이스에서 대규모 언어 모델(LLM)의 효율적인 배포를 가능하게"},{"id":"2025-12-04-ViDiC-Video-Difference-Captioning","title":"[논문리뷰] ViDiC: Video Difference Captioning","excerpt":"jiakaiW이 arXiv에 게시한 'ViDiC: Video Difference Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-04 00:00:00+0900+0900","permalink":"/ai/review/2025-12-04-ViDiC-Video-Difference-Captioning","tags":["Review","Video Difference Captioning","Multimodal Large Language Models","Video Understanding","Comparative Reasoning","Evaluation Benchmark","LLM-as-a-Judge","ViDiC-1K"],"text":"링크: 논문 PDF로 바로 열기 저자: jiakaiW, heyween, wrz123, LongoXC, Leexeo 핵심 연구 목표 본 논문은 동적 비디오 시퀀스 간의 시각적 차이를 이해하고 설명하는 Video Difference Captioning (ViDiC) 이라는 새로운 태스크를 제안합니다. 기존 Image Difference Captioning (I"},{"id":"2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer","title":"[논문리뷰] 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer","excerpt":"arXiv에 게시된 '4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer","tags":["Review","4D Scene Understanding","Language Grounding","Transformer","Feed-forward Network","Semantic Field","Geometry Reconstruction","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Xianfeng Wu, Yajing Bai, Minghan Li, Xianzu Wu, Xueqi Zhao, Zhongyuan Lai, Wenyu Liu & Xinggang Wang 핵심 연구 목표 기존 4D 시맨틱 필드 구축 방식이 Gaussian Splatting 에 의존하여 장면별 최적화가 필요하고 일반화 및 확장"},{"id":"2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning","title":"[논문리뷰] ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning","excerpt":"arXiv에 게시된 'ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning","tags":["Review","Multimodal Reward Models","Agentic AI","Tool Use","Reinforcement Learning","Visual Reasoning","Multimodal LLMs","Instruction Following","Evaluation Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang 핵심 연구 목표 본 논문은 기"},{"id":"2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models","title":"[논문리뷰] Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models","excerpt":"arXiv에 게시된 'Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models","tags":["Review","Text-to-Image","LVLM","Social Bias","System Prompts","Bias Mitigation","Meta-Prompting","Fairness","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: NaHyeon Park, Na Min An, Kunhee Kim, Soyeon Yoon, Jiahao Huo, Hyunjung Shim 핵심 연구 목표 본 연구는 최근 LVLM(Large VisionLanguage Model) 기반 텍스트투이미지(T2I) 모델 이 이미지 생성에서 높은 품질을 달성했음에도 불구하고, 사"},{"id":"2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","title":"[논문리뷰] BulletTime: Decoupled Control of Time and Camera Pose for Video Generation","excerpt":"Jan Ackermann이 arXiv에 게시한 'BulletTime: Decoupled Control of Time and Camera Pose for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","tags":["Review","Video Generation","Diffusion Models","4D Control","Camera Pose Control","Time Control","Positional Encoding","Adaptive Normalization","Synthetic Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiming Wang, Qihang Zhang, Shengqu Cai, Zhengfei Kuang, Yang Zheng, Frano Rajič, Tong Wu, Jan Ackermann, Siyu Tang, Gordon Wetzstein 핵심 연구 목표 본 논문은 기존 비디오 확산 모델의 고질적인 문제점인 장면 역학과"},{"id":"2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle","title":"[논문리뷰] DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle","excerpt":"arXiv에 게시된 'DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle","tags":["Review","Data Agents","Benchmarking","Data Engineering","Data Analysis","LLM-as-Judge","Full Data Intelligence Lifecycle","Repository-Level","Open-Ended Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangyu Lei, Jinxiang Meng, Yiming Huang, Junjie Zhao, Yitong Zhang, Jianwen Luo, Xin Zou, Ruiyi Yang, Wenbo Shi, Yan Gao, Shizhu He, Zuo Wang, Qian Liu, Yang Wang, Ke Wang, Jun Z"},{"id":"2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation","title":"[논문리뷰] DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation","excerpt":"Ziyu Guo이 arXiv에 게시한 'DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation","tags":["Review","Text-to-Image Generation","Chain-of-Thought (CoT)","Multimodal Large Language Models (MLLMs)","Visual Planning","Rare Concept Generation","Drafting","Classifier-Free Guidance (CFG)","Image Refinement"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li 핵심 연구 목표 본 논문은 기존 MLLM 기반 텍스트투이미지(T"},{"id":"2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling","title":"[논문리뷰] DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling","excerpt":"arXiv에 게시된 'DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling","tags":["Review","4D World Modeling","Multimodal Data","Dynamic Scenes","Metric-Scale","Bundle Adjustment","Foundation Models","Video Analysis","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Kairun Wen, Yuzhi Huang, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, Jian Zhang, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin"},{"id":"2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion","title":"[논문리뷰] EgoLCD: Egocentric Video Generation with Long Context Diffusion","excerpt":"arXiv에 게시된 'EgoLCD: Egocentric Video Generation with Long Context Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion","tags":["Review","Egocentric Video Generation","Long-Context Diffusion","Long-Short Memory","Sparse KV Cache","Memory Regulation Loss","Structured Narrative Prompting","World Models","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Liuzhou Zhang, Jiarui Ye, Yuanlei Wang, Ming Zhong, Mingju Gao, Wanke Xia, Bowen Zeng, Zeyu Zhang, Hao Tang 핵심 연구 목표 논문은 장기적으로 일관된 1인칭 시점(egocentric) 비디오를 생성하는 데 있어 콘텐츠 드리프트(cont"},{"id":"2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring","title":"[논문리뷰] FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring","excerpt":"Munchurl Kim이 arXiv에 게시한 'FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring","tags":["Review","Video Super-Resolution","Video Deblurring","Joint Restoration","Exposure-Aware","Motion Compensation","Transformer Architecture","Dynamic Filtering","Real-World Degradations"],"text":"링크: 논문 PDF로 바로 열기 저자: Geunhyuk Youk, Jihyong Oh, Munchurl Kim 핵심 연구 목표 본 논문은 실제 환경에서 발생하는 동적으로 변화하는 노출 과 모션에 의한 복합적인 비디오 열화 문제를 해결하여, 고해상도(HR) 및 선명한 비디오를 복원하는 것을 목표로 합니다. 기존 비디오 복원 방법론들이 고정된 노출 시간을 가정"},{"id":"2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces","title":"[논문리뷰] GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces","excerpt":"Sezer Karaoglu이 arXiv에 게시한 'GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces","tags":["Review","3D Gaussian Splatting","Text-to-3D Stylization","Latent Diffusion Models","Disentangled Latent Spaces","Feed-forward Editing","Geometry Preservation","Multi-view Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Melis Ocal, Xiaoyan Xing, Yue Li, Ngo Anh Vien, Sezer Karaoglu, Theo Gevers 핵심 연구 목표 본 논문은 기존 textto3D 스타일 변환 방법의 느린 최적화 시간과 멀티뷰 불일치 문제를 해결하여, 3D Gaussian Splatting (3DGS) 자산에 대한"},{"id":"2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior","title":"[논문리뷰] Generative Neural Video Compression via Video Diffusion Prior","excerpt":"arXiv에 게시된 'Generative Neural Video Compression via Video Diffusion Prior' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior","tags":["Review","Neural Video Compression","Diffusion Models","Generative Models","Video Compression","Temporal Coherence","Perceptual Quality","Flow Matching","Video Diffusion Transformer (VideoDiT)"],"text":"링크: 논문 PDF로 바로 열기 저자: Qi Mao, Hao Cheng, Tinghan Yang, Libiao Jin, Siwei Ma 핵심 연구 목표 본 논문은 기존 비디오 압축 방식이 초저비트레이트 환경에서 발생하는 흐릿함, 세부 정보 손실, 그리고 지각적 깜빡임(perceptual flickering) 문제를 해결하는 것을 목표로 합니다. 특히, 이미"},{"id":"2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale","title":"[논문리뷰] LATTICE: Democratize High-Fidelity 3D Generation at Scale","excerpt":"Qingxiang Lin이 arXiv에 게시한 'LATTICE: Democratize High-Fidelity 3D Generation at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale","tags":["Review","3D Generation","High-Fidelity","Latent Representation","Voxel Grid","Diffusion Models","Transformer","Scalable AI","Asset Creation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Qingxiang Lin, Jingwei Huang, Chunchao Guo, Xiangyu Yue 핵심 연구 목표 본 논문은 고품질 3D 에셋 생성에 있어 3D 및 2D 생성 모델 간의 품질과 확장성 격차를 해소하는 것을 목표로 "},{"id":"2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length","title":"[논문리뷰] Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length","excerpt":"Shifeng Zhang이 arXiv에 게시한 'Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length","tags":["Review","Audio-Driven Avatar Generation","Real-time Streaming","Diffusion Models","Infinite Length","Pipeline Parallelism","Temporal Consistency","Model Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yubo Huang, Hailong Guo, Qijun Gan, Lin Liu, Fangtai Wu, Shifeng Zhang, Shijie Huang, Jiaming Liu, Sirui Zhao, Enhong Chen, Steven Hoi 핵심 연구 목표 본 논문은 기존 확산 모델 기반 비디오 생성 방법론의 순차적 "},{"id":"2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates","title":"[논문리뷰] Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates","excerpt":"Nikolaos Aletras이 arXiv에 게시한 'Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates","tags":["Review","Large Language Models (LLMs)","Catastrophic Forgetting","Language Adaptation","Continual Pre-training","Parameter Freezing","Low-Resource Languages","Source Knowledge Preservation"],"text":"링크: 논문 PDF로 바로 열기 저자: Atsuki Yamaguchi, Terufumi Morishita, Aline Villavicencio, Nikolaos Aletras 핵심 연구 목표 이 논문은 instruct LLM을 비용이 많이 드는 특화된 레이블링된 데이터 없이 비레이블링된 타겟 언어 데이터만으로 새로운 언어에 적응 시킬 때 발생하는 재앙적 망"},{"id":"2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment","title":"[논문리뷰] Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment","excerpt":"arXiv에 게시된 'Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment","tags":["Review","Multimodal LLMs","Video Understanding","Hallucination Mitigation","Object Hallucination","Action Hallucination","Contrastive Learning","Self-Augmentation","Tracklet-Phrase Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: KaiPo Chang, WeiYuan Cheng, ChiPin Huang, FuEn Yang, and YuChiang Frank Wang 핵심 연구 목표 본 논문은 비디오 이해 태스크에서 멀티모달 LLM(MLLM)이 생성하는 설명문의 시각적 객체 및 시간적 행동 환각 문제를 공동으로 완화하는 것을 목표로 합니다. 언어"},{"id":"2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing","title":"[논문리뷰] Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing","excerpt":"Jun Wang이 arXiv에 게시한 'Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing","tags":["Review","Sphere Packing","Mathematical Discovery","Semidefinite Programming (SDP)","Bayesian Optimization (BO)","Monte Carlo Tree Search (MCTS)","Sample-Efficient AI","Model-Based Learning","Geometric Constraints"],"text":"링크: 논문 PDF로 바로 열기 저자: Rasul Tutunov, Alexandre Maraval, Antoine Grosnit, Xihan Li, Jun Wang, Haitham BouAmmar 핵심 연구 목표 본 논문은 계산 비용이 매우 높은(각 평가에 며칠 소요) 문제인 구 채우기(sphere packing) 문제에서 AI를 활용하여 새로운 수학적 상"},{"id":"2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation","title":"[논문리뷰] NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation","excerpt":"Vitor Guizilini이 arXiv에 게시한 'NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation","tags":["Review","Diffusion Models","Phase Preservation","Frequency Domain","Structure-Aligned Generation","Image-to-Image Translation","Sim-to-Real","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Zeng, Charles Ochoa, Mingyuan Zhou, Vishal M. Patel, Vitor Guizilini, Rowan McAllister 핵심 연구 목표 기존 확산 모델이 데이터의 공간적 구조를 파괴하는 문제를 해결하고, 아키텍처 변경이나 추가 파라미터 없이 이미지의 위상을 보존하여 구조 정렬 "},{"id":"2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction","title":"[논문리뷰] Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction","excerpt":"arXiv에 게시된 'Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction","tags":["Review","Agentic Models","Large Language Models (LLMs)","Agentic Scaling","Environment Construction","NexAU","NexA4A","NexGAP","Interactive Environments"],"text":"링크: 논문 PDF로 바로 열기 저자: Tao Gui, Yining Zheng, Xinchi Chen, Jie Zhou, et al. (NexAGI Team) 핵심 연구 목표 본 논문은 LLM이 수동적 응답자에서 자율 에이전트로 발전 하는 데 필요한 확장 가능한 고품질 상호작용 신호 인프라의 부족 문제를 해결하고자 합니다. 특히, 기존 학습 패러다임이 가진"},{"id":"2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral","title":"[논문리뷰] On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral","excerpt":"Christos Thrampoulidis이 arXiv에 게시한 'On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral","tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Tool-Integrated Reasoning (TIR)","GRPO","Training Stability","Lazy Likelihood Displacement (LLD)","Regularization","Search-R1"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenlong Deng, Yushu Li, Boying Gong, Yi Ren, Christos Thrampoulidis, Xiaoxiao Li 핵심 연구 목표 본 논문은 GRPO(Group Relative Policy Optimization) 기반의 툴 통합 강화 학습(TIRL) , 특히 SearchR1 프레임워크에"},{"id":"2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing","title":"[논문리뷰] PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing","excerpt":"arXiv에 게시된 'PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing","tags":["Review","LLM Agents","Academic Writing","In-editor Assistant","Multi-agent System","Overleaf Integration","Chrome Extension","Kubernetes","XtraMCP"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyi Hou, Andre Lin Huikai, Nuo Chen, Yiwei Gong, Bingsheng He 핵심 연구 목표 기존 LLM 기반 글쓰기 보조 도구가 편집기 외부에 존재하여 발생하는 컨텍스트 전환, 상호작용 기록 단절, 문서 상태와의 심층적 상호작용 부족 문제를 해결하는 것을 목표로 합니다. 특히 O"},{"id":"2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory","title":"[논문리뷰] QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory","excerpt":"Nan-Yow Chen이 arXiv에 게시한 'QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory","tags":["Review","Quantum Machine Learning","Kolmogorov-Arnold Networks","Long Short-Term Memory (LSTM)","Time Series Forecasting","Hybrid Quantum-Classical Learning","Quantum-inspired","Recurrent Neural Networks"],"text":"링크: 논문 PDF로 바로 열기 저자: YuChao Hsu, JiunCheng Jiang, ChunHua Lin, KuoChung Peng, NanYow Chen, Samuel YenChi Chen, EnJui Kuo, HsiSheng Goan 핵심 연구 목표 본 연구는 기존 LSTM 모델 의 높은 파라미터 중복성과 제한된 비선형 표현력 문제를 해결하고, "},{"id":"2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance","title":"[논문리뷰] REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance","excerpt":"Yaxin Fan이 arXiv에 게시한 'REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance","tags":["Review","Fact-Checking","Explainable AI (XAI)","Large Language Models (LLMs)","Self-Refinement","Latent Space","Disentanglement","Steering Vectors","Misinformation"],"text":"링크: 논문 PDF로 바로 열기 저자: Chuyi Kong, Gao Wei, Hongzhan Lin, Jing Ma, Yaxin Fan 핵심 연구 목표 소셜 미디어의 가짜 뉴스 확산으로 인한 신뢰 저하 문제를 해결하기 위해, 기존 LLM 기반 팩트 체크 시스템의 외부 지식 의존성, 높은 지연 시간, 환각 현상, 낮은 해석 가능성 등의 한계를 극복하는 것을 "},{"id":"2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation","title":"[논문리뷰] Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation","excerpt":"Hao Ouyang이 arXiv에 게시한 'Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation","tags":["Review","Streaming Video Generation","Video Diffusion Models","Distribution Matching Distillation","Reinforcement Learning","Autoregressive Models","Attention Sink","Real-time"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunhong Lu, Hao Ouyang, Haobo Li, Yanhong Zeng, Ka Leong Cheng, Jiapeng Zhu, Hengyuan Cao, Xing Zhu, Yujun Shen, Min Zhang, Qiuyu Wang, Zhipeng Zhang 핵심 연구 목표 효율적인 스트리밍 비디오 생성 시 "},{"id":"2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds","title":"[논문리뷰] SIMA 2: A Generalist Embodied Agent for Virtual Worlds","excerpt":"arXiv에 게시된 'SIMA 2: A Generalist Embodied Agent for Virtual Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds","tags":["Review","Embodied AI","Generalist Agent","Virtual Worlds","Foundation Models","Gemini","Self-Improvement","Dialogue","Reasoning","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: SIMA Team, Google DeepMind 핵심 연구 목표 SIMA 2는 다양한 3D 가상 세계에서 광범위하게 이해하고 행동하는 제너럴리스트 임베디드 에이전트 를 개발하는 것을 목표로 합니다. SIMA 1과 같은 기존 작업의 한계를 넘어, 고수준 목표를 추론하고, 사용자와 대화하며, 언어와 이미지로 주어지는 복잡"},{"id":"2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization","title":"[논문리뷰] SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization","excerpt":"arXiv에 게시된 'SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization","tags":["Review","Vision-Language Navigation","Large Vision-Language Models","Visual Prompt","Reinforcement Fine-Tuning","Policy Optimization","Embodied AI","Spatial Reasoning","Perception Errors"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengcheng Wang, Zichuan Lin, Yijun Yang, Haobo Fu, Deheng Ye 핵심 연구 목표 기존 LVLM(Large VisionLanguage Models) 기반의 VLN(VisionLanguage Navigation) 에이전트가 겪는 지각, 추론, 계획 오류로 인한 낮은 내비게이션"},{"id":"2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion","title":"[논문리뷰] Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion","excerpt":"arXiv에 게시된 'Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion","tags":["Review","Latent Diffusion Models","Asynchronous Denoising","Semantic Modeling","Texture Modeling","Image Generation","Vision Transformer","VAE","Fast Convergence"],"text":"링크: 논문 PDF로 바로 열기 저자: Yueming Pan, Ruoyu Feng, Qi Dai, Yuqi Wang, Wenfeng Lin, Mingyu Guo, Chong Luo, Nanning Zheng 핵심 연구 목표 본 논문은 Latent Diffusion Models (LDMs)의 내재적인 문제점인 고수준 의미론(semantics)과 저수준 텍스처"},{"id":"2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs","title":"[논문리뷰] SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs","excerpt":"arXiv에 게시된 'SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs","tags":["Review","Post-Training Quantization (PTQ)","Large Language Models (LLMs)","Low-Bit Quantization","Mixed-Precision Quantization","Sensitivity Metric","Quantization Scale Initialization","Accuracy Preservation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhua Cheng, Weiwei Zhang, Heng Guo, Haihao Shen (Intel) 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)을 극단적인 저비트 양자화(예: 2비트, 4비트 MXFP4) 시 발생하는 심각한 성능 저하 문제를 해결하는 것을 목표로 합니다. 특히 PostTraining Qu"},{"id":"2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting","title":"[논문리뷰] Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting","excerpt":"Yu-Lun Liu이 arXiv에 게시한 'Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting","tags":["Review","Monocular 3D Reconstruction","Mannequin Challenge","Dynamic Gaussian Splatting","Freeze-Time Video","Temporal Consistency","Artifact Suppression","Regularization"],"text":"링크: 논문 PDF로 바로 열기 저자: HaoJen Chien, YiChuan Huang, ChungHo Wu, WeiLun Chao, YuLun Liu 핵심 연구 목표 본 논문의 핵심 목표는 단안 카메라로 촬영된 불완전한 마네킹 챌린지(MannequinChallenge, MC) 영상 에서 미세한 움직임으로 인해 발생하는 고스팅(ghosting) 및 블러("},{"id":"2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation","title":"[논문리뷰] TV2TV: A Unified Framework for Interleaved Language and Video Generation","excerpt":"arXiv에 게시된 'TV2TV: A Unified Framework for Interleaved Language and Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation","tags":["Review","Video Generation","Language Modeling","Multimodal AI","Interleaved Generation","Flow Matching","Transformer","Controllability","World Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, ShangWen Li, Sreya Dutta Ro"},{"id":"2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers","title":"[논문리뷰] UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers","excerpt":"arXiv에 게시된 'UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-05 00:00:00+0900+0900","permalink":"/ai/review/2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers","tags":["Review","Diffusion Transformers","Resolution Extrapolation","Positional Encoding","Frequency Analysis","Adaptive Attention","High-Resolution Image Generation","Image Quality","Content Repetition"],"text":"링크: 논문 PDF로 바로 열기 저자: Min Zhao, Bokai Yan, Xue Yang, Hongzhou Zhu, Jintao Zhang, Shilong Liu, Chongxuan Li, Jun Zhu 핵심 연구 목표 본 논문은 이미지 diffusion transformer 모델이 훈련된 해상도를 넘어선 이미지를 생성할 때 발생하는 콘텐츠 반복 및 품"},{"id":"2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence","title":"[논문리뷰] AI & Human Co-Improvement for Safer Co-Superintelligence","excerpt":"arXiv에 게시된 'AI & Human Co-Improvement for Safer Co-Superintelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence","tags":["Review","AI Safety","Superintelligence","Human-AI Collaboration","Self-Improving AI","Co-Improvement","Alignment","AI Research Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Jason Weston, Jakob Foerster 핵심 연구 목표 이 논문은 AI가 스스로 개선하는 자율적 자기 개선(SelfImproving AI)의 목표가 위험하고 최적의 경로가 아니라고 주장하며, 대신 인간과 AI의 협력적 공동 개선(CoImprovement) 을 통해 더 안전하고 빠른 공동 초지능(CoSupe"},{"id":"2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence","title":"[논문리뷰] COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence","excerpt":"Jiawei Sheng이 arXiv에 게시한 'COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence","tags":["Review","Multimodal Large Language Models (MLLMs)","Spatial Reasoning","Perception Enhancement","Auxiliary Modalities","Adaptive Interleaved Reasoning","Reinforcement Learning","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawei Sheng, Zhenyu Zhang, Hengzhu Tang, CUDAOUTOFMEMORY, Starrrrrry 핵심 연구 목표 본 연구는 기존 MLLM이 3D 공간 추론 및 객체 속성 이해에 어려움을 겪는 문제를 해결하고자 합니다. 단일 통합 MLLM이 공간 지각 능력을 내재적으로 향상 시키고, 적응형의"},{"id":"2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor","title":"[논문리뷰] EditThinker: Unlocking Iterative Reasoning for Any Image Editor","excerpt":"Ziyu Guo이 arXiv에 게시한 'EditThinker: Unlocking Iterative Reasoning for Any Image Editor' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor","tags":["Review","Image Editing","Iterative Reasoning","Multimodal Large Language Model (MLLM)","Reinforcement Learning (RL)","Instruction Following","Critique-Refine-Repeat Cycle","Think-while-Edit"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu 핵심 연구 목표 본 논문은 "},{"id":"2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning","title":"[논문리뷰] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning","excerpt":"Zijia Lin이 arXiv에 게시한 'Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Policy Optimization","Trust Region","Entropy Clipping","Large Language Models","Training Stability","Distributional Shift"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijia Lin, Tiehua Mei, Minxuan Lv, Leiyu Pan, Suu 핵심 연구 목표 대규모 언어 모델(LLMs)을 위한 강화 학습(RL)은 trustregion deviation 과 훈련 불안정성 문제에 직면해 있습니다. 기존 PPOclip 이 샘플링된 액션에만 제약을 가하여 샘플링되지 않은 액션"},{"id":"2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks","title":"[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks","excerpt":"Yang Li이 arXiv에 게시한 'From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks","tags":["Review","Reinforcement Learning","Large Language Models","Curriculum Learning","Advantage Function","Reasoning Tasks","Multimodal AI","Policy Optimization","Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Changpeng Yang, Jinyang Wu, Yuchen Liu, Shuai Zhang, Yang Li, Qiliang Liang, Hongzhen Wang, Shuai Nie, Jiaming Xu, Runyu Shi, Ying Huang, Guoquan Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(L"},{"id":"2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image","title":"[논문리뷰] Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image","excerpt":"arXiv에 게시된 'Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image","tags":["Review","4D Synthesis","3D Reconstruction","Motion Generation","Single Image","Diffusion Model","Point Cloud","Dataset Curation","View Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanran Zhang, Ziyi Wang, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu 핵심 연구 목표 논문은 단일 정적 이미지로부터 물리적으로 그럴듯하고 시간적으로 일관된 동적인 4D 장면(3D 기하학과 시간적 역학) 을 생성하는 핵심적인 문제를 해결하는 것을 목표로 합니다. 기"},{"id":"2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation","title":"[논문리뷰] ProPhy: Progressive Physical Alignment for Dynamic World Simulation","excerpt":"Yuhao Cheng이 arXiv에 게시한 'ProPhy: Progressive Physical Alignment for Dynamic World Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation","tags":["Review","Video Generation","Physics-aware","World Simulation","Progressive Alignment","Mixture-of-Experts","Vision-Language Models","Token-level Routing"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijun Wang, Panwen Hu, Jing Wang, Terry Jingchen Zhang, Yuhao Cheng, Long Chen, Yiqiang Yan, Zutao Jiang, Hanhui Li, Xiaodan Liang 핵심 연구 목표 기존 비디오 생성 모델들이 대규모 또는 복잡한 다이내믹스에서 물리적으"},{"id":"2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning","title":"[논문리뷰] ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning","excerpt":"Shengju Qian이 arXiv에 게시한 'ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning","tags":["Review","Video Object Segmentation","Reinforcement Learning","Vision-Language Models","Reasoning Chain","Explainable AI","Multi-step Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Li, Yingda Yin, Lingting Zhu, Weikai Chen, Shengju Qian, Xin Wang, Yanwei Fu 핵심 연구 목표 본 논문은 복잡한 추론 중심 비디오 객체 분할 (Reasoning VOS) 태스크에서 기존 VisionLanguage Models (VLMs) 의 불투명한"},{"id":"2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards","title":"[논문리뷰] RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards","excerpt":"Zilong Huang이 arXiv에 게시한 'RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards","tags":["Review","Text-to-Image Generation","Photorealism","Reinforcement Learning","Diffusion Models","Adversarial Learning","Detector-Guided Rewards","LLM Prompt Optimization","Image Quality Assessment"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyan Ye, Leiqi Zhu, Yuncheng Guo, Dongzhi Jiang, Zilong Huang, Yifan Zhang, Zhiyuan Yan, Haohuan Fu, Conghui He, Weijia Li 핵심 연구 목표 본 논문은 기존 텍스트이미지(T2I) 생성 모델들이 보이는 \"가짜 같은\" AI "},{"id":"2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations","title":"[논문리뷰] SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations","excerpt":"arXiv에 게시된 'SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations","tags":["Review","Character Animation","3D Pose Representation","In-Context Learning","Diffusion Transformer","Studio-Grade Animation","Spatio-Temporal Reasoning","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhao Yan, Sheng Ye, Zhuoyi Yang, Jiayan Teng, ZhenHui Dong, Kairui Wen, Xiaotao Gu, YongJin Liu, Jie Tang 핵심 연구 목표 기존 캐릭터 애니메이션 방법론이 복잡한 모션, 크로스아이덴티티 애니메이션, 다중 캐릭터 상호작용 등 스튜디오 "},{"id":"2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs","title":"[논문리뷰] SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs","excerpt":"Minghui Yu이 arXiv에 게시한 'SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs","tags":["Review","LLM Quantization","Sparsification","Hardware Acceleration","Mixed-Precision","Post-Training Quantization","Data Format","GPU Optimization","AI Accelerator"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruixuan Huang, Hao Zeng, Hantao Huang, Jinyuan Shi, Minghui Yu, Ian EnHsu Yen, Shuai Wang 핵심 연구 목표 대규모 언어 모델(LLMs)의 배포에 있어 저비트 양자화(lowbit quantization) 와 희소화(sparsification) 기술이 "},{"id":"2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations","title":"[논문리뷰] Self-Improving VLM Judges Without Human Annotations","excerpt":"arXiv에 게시된 'Self-Improving VLM Judges Without Human Annotations' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations","tags":["Review","Vision-Language Models","Self-Improvement","Judge Models","Synthetic Data Generation","Iterative Refinement","Reward Modeling","Human-free Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Wanyin Lin, Yushi Hu, Stella Li, Scott Geng, Pang Wei Koh, Luke Zettlemoyer, Tim Althoff, Marjan Ghazvininejad 핵심 연구 목표 본 논문은 VLM (VisionLanguage Model) judge 를 훈련하기 위해 필요한 고비용의 "},{"id":"2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling","title":"[논문리뷰] SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling","excerpt":"Marc Pollefeys이 arXiv에 게시한 'SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling","tags":["Review","3D Generative Models","Spatial Control","Test-Time Guidance","Rectified Flow","Superquadrics","Training-Free","Trellis"],"text":"링크: 논문 PDF로 바로 열기 저자: Elisabetta Fedele, Francis Engelmann, Ian Huang, Or Litany, Marc Pollefeys, Leonidas Guibas 핵심 연구 목표 본 연구는 3D 에셋 생성에서 직관적이고 정밀한 기하학적 제어가 부족하다는 문제에 주목합니다. 기존의 텍스트 또는 이미지 기반 제어 방식이"},{"id":"2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation","title":"[논문리뷰] TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation","excerpt":"Salih Tileylioglu이 arXiv에 게시한 'TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation","tags":["Review","Strong Motion Generation","Deep Learning","TimesNet","Conditional Generation","Site Effects","Seismology","HVSR","Time Series"],"text":"링크: 논문 PDF로 바로 열기 저자: Salih Tileylioglu, Erdem Akagündüz, Bevan Deniz Cilgin, Baris Yilmaz 핵심 연구 목표 논문은 지진 시 지반 운동의 시간주파수 특성 을 효과적으로 포착하는 딥러닝 모델의 부재 문제를 해결하고자 합니다. 특히, 지역 지반 조건에 따른 지진파의 복잡한 시공간 및 스펙트럼"},{"id":"2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows","title":"[논문리뷰] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows","excerpt":"arXiv에 게시된 'TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows","tags":["Review","Generative Models","One-step Generation","Self-Adversarial Learning","Flow Matching","Large Language Models","Text-to-Image","Efficient Inference","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenglin Cheng, Peng Sun, Jianguo Li, Tao Lin 핵심 연구 목표 현재 다단계 생성 모델(Diffusion, Flow Matching)의 느린 추론 속도 (40100 NFE) 문제를 해결하는 것을 목표로 합니다. 기존 소수 단계(fewstep) 가속화 방법들이 겪는 훈련 불안정성, 복잡"},{"id":"2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty","title":"[논문리뷰] World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty","excerpt":"Anirudha Majumdar이 arXiv에 게시한 'World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-08 00:00:00+0900+0900","permalink":"/ai/review/2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty","tags":["Review","Controllable Video Generation","Uncertainty Quantification","Video Models","Calibration","Out-of-Distribution Detection","Proper Scoring Rules","Latent Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, Anirudha Majumdar 핵심 연구 목표 본 논문은 최첨단 제어 가능한 비디오 모델이 흔히 겪는 환각 현상과 불확실성 표현 능력 부족 문제를 해결하고자 합니다. 특히, 비디오 모델이 생성된 프레임 내에서 서브패치 수준의 "},{"id":"2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs","title":"[논문리뷰] Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs","excerpt":"arXiv에 게시된 'Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs","tags":["Review","Rotary Position Embedding","Long-Context LLMs","Complex-Valued Neural Networks","Self-Attention","Positional Encoding","Information Loss","Length Extrapolation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoran Liu, Yuerong Song, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Zhaoxiang Liu, Shiguo Lian, Ziwei He, Xipeng Qiu 핵심 연구 목표 현재 RoPE(Rotary Position Embeddings) 구현이 어텐션 스코어 계산 시"},{"id":"2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning","title":"[논문리뷰] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning","excerpt":"Jiacheng Chen이 arXiv에 게시한 'Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning","tags":["Review","Decoding-based Regression","Reinforcement Learning","Numerical Prediction","Large Language Models","Policy Gradient","Tokenization","Sequence Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Chen, Sheng Tang, RongXi Tan, Ziniu Li, Jiacheng Chen, Ke Xue, Chao Qian 핵심 연구 목표 이 논문은 디코딩 기반 회귀 모델이 개별 토큰 수준의 목표(예: crossentropy)와 연속적인 수치 값 사이의 불일치로 인해 겪는 한계를 해결하고자 합니다. "},{"id":"2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue","title":"[논문리뷰] DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue","excerpt":"YijunLiao이 arXiv에 게시한 'DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue","tags":["Review","Long-Context Dialogue","Mutable State Tracking","Temporal Alignment","Preference Optimization","Attention Mechanism","State Inertia","Non-Destructive Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yijun Liao 핵심 연구 목표 본 논문은 긴 컨텍스트 대화 시스템에서 모델이 오래된 이력에 과도하게 집중하여 새로운 충돌 정보가 있을 때 내부 상태를 업데이트하지 못하는 \"State Inertia\" 문제를 해결하고자 합니다. 특히, 기존 정렬 방법론의 \"Static Alignment Constraint\"로 인한 "},{"id":"2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning","title":"[논문리뷰] Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning","excerpt":"arXiv에 게시된 'Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning","tags":["Review","Vision-Language Models","Reinforcement Learning","Self-Evolving Learning","Data-Scarce Domains","Context-First Learning","Reward Hacking Mitigation","Multimodal Reasoning","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tingyu Li, Zheng Sun, Jingxuan Wei, Siyuan Li, Conghui He, Lijun Wu, Cheng Tan 핵심 연구 목표 본 논문은 데이터 부족 및 보상 해킹(reward hacking) 문제로 인해 강화 학습(RL) 기반 VisionLanguage Models (VLMs) 의 전문"},{"id":"2025-12-09-Distribution-Matching-Variational-AutoEncoder","title":"[논문리뷰] Distribution Matching Variational AutoEncoder","excerpt":"arXiv에 게시된 'Distribution Matching Variational AutoEncoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder","tags":["Review","Variational Autoencoder (VAE)","Distribution Matching","Diffusion Models","Latent Space","Self-supervised Learning (SSL) Features","Generative Models","ImageNet","Tokenizer"],"text":"링크: 논문 PDF로 바로 열기 저자: Sen Ye, Jianning Pei, Mengde Xu, Shuyang Gu, Chunyu Wang, Liwei Wang, Han Hu 핵심 연구 목표 본 논문은 시각적 생성 모델에서 VAE 및 파운데이션 모델 인코더가 잠재 공간의 분포를 명시적으로 형성하지 못하는 문제를 해결합니다. 재구성 충실도와 모델링 효율성 "},{"id":"2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems","title":"[논문리뷰] DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems","excerpt":"arXiv에 게시된 'DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems","tags":["Review","LLM Multi-Agent Systems","Debugging","Intervention-Driven","Failure Attribution","Automated Debugging","Verification","AI Agents","Reliability"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Ma, Jue Zhang, Fangkai Yang, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang 핵심 연구 목표 LLM 기반 다중 에이전트 시스템의 복잡한 디버깅 문제를 해결하는 것을 목표로 합니다. 특히, 긴 상호작용 추적에서 발생하는 실패의 원인을 찾는 "},{"id":"2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","title":"[논문리뷰] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing","excerpt":"arXiv에 게시된 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","tags":["Review","Egocentric Video Editing","Real-Time Streaming","Augmented Reality","Video Generation","Dataset","Benchmark","Diffusion Models","Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Runjia Li, Moayed Haji Ali, Ashkan Mirzaei, Chaoyang Wang, Arpit Sahni, Ivan Skorokhodov, Aliaksandr Siarohin, Tomas Jakab, Junlin Han, Sergey Tulyakov, Philip Torr, Willi Menapa"},{"id":"2025-12-09-Group-Representational-Position-Encoding","title":"[논문리뷰] Group Representational Position Encoding","excerpt":"arXiv에 게시된 'Group Representational Position Encoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Group-Representational-Position-Encoding","tags":["Review","Positional Encoding","Group Theory","Transformer","RoPE","ALiBi","Lie Groups","Multiplicative PE","Additive PE"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Zhang, Zixiang Chen, Yifeng Liu, Zhen Qin, Huizhuo Yuan, Kangping Xu, Yang Yuan, Quanquan Gu, Andrew ChiChih Yao 핵심 연구 목표 Transformer 모델의 필수 요소인 위치 인코딩(Positional Encoding)"},{"id":"2025-12-09-LongCat-Image-Technical-Report","title":"[논문리뷰] LongCat-Image Technical Report","excerpt":"arXiv에 게시된 'LongCat-Image Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-LongCat-Image-Technical-Report","tags":["Review","Image Generation","Text-to-Image","Image Editing","Diffusion Model","Multilingual Text Rendering","Photorealism","Efficiency","Open-Source"],"text":"링크: 논문 PDF로 바로 열기 저자: Meituan LongCat Team 핵심 연구 목표 컴퓨터 비전 분야에서 다국어 텍스트 렌더링, 사실주의, 배포 효율성, 개발자 접근성 등 기존 주요 모델들의 핵심 과제를 해결하고자 합니다. LongCatImage 는 브루트 포스 스케일링에 대한 의존성에서 벗어나 최첨단 성능과 효율성 간의 최적의 균형을 이루는 경량"},{"id":"2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader","title":"[논문리뷰] Multi-view Pyramid Transformer: Look Coarser to See Broader","excerpt":"Jungwoo Kim이 arXiv에 게시한 'Multi-view Pyramid Transformer: Look Coarser to See Broader' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader","tags":["Review","Multi-view Transformer","3D Reconstruction","Hierarchical Attention","Computational Efficiency","3D Gaussian Splatting","Novel View Synthesis","Scalability"],"text":"링크: 논문 PDF로 바로 열기 저자: Gyeongjin Kang, Seungkwon Yang, Seungtae Nam, Younggeun Lee, Jungwoo Kim, Eunbyung Park 핵심 연구 목표 본 논문은 대규모 3D 장면을 수십에서 수백 개의 이미지로부터 단일 순방향 패스로 재구성하는 트랜스포머 아키텍처의 확장성 문제 를 해결하는 것을 "},{"id":"2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning","title":"[논문리뷰] Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning","excerpt":"arXiv에 게시된 'Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning","tags":["Review","Large Language Models (LLMs)","Parallel Reasoning","Self-Distilled Reinforcement Learning","Policy Optimization","Inference Acceleration","Structured Output","Agentic Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tong Wu, Yang Liu, Jun Bai, Zixia Jia†, Shuyi Zhang, Ziyong Lin, Yanting Wang, SongChun Zhu, and Zilong Zheng† 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 순차적 모방 에서 벗어나 진정한 병렬 추론 능력 을 자기 진화할 "},{"id":"2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation","title":"[논문리뷰] OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation","excerpt":"Simeng Qin이 arXiv에 게시한 'OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation","tags":["Review","Multimodal LLMs","Jailbreak Attack","Attack-Defense Evaluation","Benchmark","Safety Alignment","Vulnerability Analysis","Risk Taxonomy","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Simeng Qin, Teng Ma, Qi Guo, Jie Liao, Xiaojun Jia 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLM)의 안전성 정렬을 우회하는 탈옥(jailbreak) 공격 에 대한 통합적인 벤치마크 및 툴박스 를 구축하는 것을 목표로 합니다. 기존 벤치마크가 가진 제한적인 공격 "},{"id":"2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","title":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models","excerpt":"arXiv에 게시된 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","tags":["Review","Reinforcement Learning (RL)","Pre-training","Mid-training","Reasoning LMs","Generalization","Synthetic Reasoning Tasks","Process-level Supervision"],"text":"링크: 논문 PDF로 바로 열기 저자: Charlie Zhang, Graham Neubig, Xiang Yue 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)에서 사전 훈련(pretraining), 중간 훈련(midtraining), 강화 학습(RL) 기반 후처리 훈련(posttraining)이 추론 능력의 일반화에 미치는 상호작용과 인과적 영향을 "},{"id":"2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","title":"[논문리뷰] ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation","excerpt":"Taojun Ding이 arXiv에 게시한 'ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","tags":["Review","Video Generation","Camera Control","Novel Trajectory","3D Gaussian Splatting (3DGS)","LiDAR-Free","Diffusion Models","Autonomous Driving","Scene Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaokun Li, Shuaixian Wang, Mantang Guo, Jiehui Huang, Taojun Ding, Mu Hu, Kaixuan Wang, Shaojie Shen, Guang Tan 핵심 연구 목표 본 연구는 자율 주행 환경에서 고품질의 카메라 제어 기반 신규 궤적 비디오 생성 문제를 해결하고자 합니"},{"id":"2025-12-09-Relational-Visual-Similarity","title":"[논문리뷰] Relational Visual Similarity","excerpt":"Jing Shi이 arXiv에 게시한 'Relational Visual Similarity' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Relational-Visual-Similarity","tags":["Review","Relational Similarity","Visual Similarity","Vision-Language Models","Anonymous Captioning","Image Retrieval","Analogical Reasoning","Dataset Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Thao Nguyen, Sicheng Mo, Krishna Kumar Singh, Yilin Wang, Jing Shi 핵심 연구 목표 본 연구는 기존 이미지 유사성 모델들이 시각적 속성(perceptual attribute)에만 집중하여, 인간이 인지하는 추상적이고 관계적인 시각 유사성(relational visua"},{"id":"2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation","title":"[논문리뷰] Rethinking Training Dynamics in Scale-wise Autoregressive Generation","excerpt":"arXiv에 게시된 'Rethinking Training Dynamics in Scale-wise Autoregressive Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation","tags":["Review","Autoregressive Generation","Visual Synthesis","Exposure Bias","Student Forcing","Self-Autoregressive Refinement","Scale-wise Prediction","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Gengze Zhou, Chongjian Ge, Hao Tan, Feng Liu, Yicong Hong 핵심 연구 목표 본 연구는 스케일별 자동회귀(AR) 생성 모델이 겪는 (1) 훈련추론 불일치(exposure bias) 와 (2) 스케일별 학습 난이도 불균형 문제로 인해 저하되는 생성 품질을 해결하는 것을 목표로 "},{"id":"2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation","title":"[논문리뷰] Scaling Zero-Shot Reference-to-Video Generation","excerpt":"arXiv에 게시된 'Scaling Zero-Shot Reference-to-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation","tags":["Review","Reference-to-Video Generation","Zero-Shot Learning","Diffusion Models","Masked Training","Video-Text Pairs","Identity Preservation","Scalability","Attention Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijian Zhou, Shikun Liu, Haozhe Liu, Haonan Qiu, Zhaochong An, Weiming Ren, Zhiheng Liu, Xiaoke Huang, Kam Woh Ng, Tian Xie, Xiao Han, Yuren Cong, Hang Li, Chuyan Zhu, Aditya Pat"},{"id":"2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","title":"[논문리뷰] Unified Video Editing with Temporal Reasoner","excerpt":"arXiv에 게시된 'Unified Video Editing with Temporal Reasoner' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","tags":["Review","Video Editing","Diffusion Models","Temporal Reasoning","Chain-of-Thought","In-Context Learning","ROPE","Multi-instance Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangpeng Yang¹, Ji Xie², Yiyuan Yang¹, Yan Huang¹, Min Xu¹, Qiang Wu¹ ¹University of Technology Sydney ²Zhejiang University 핵심 연구 목표 기존 비디오 편집 모델들이 겪는 정밀도(expert models)와 통합성/마스"},{"id":"2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","title":"[논문리뷰] UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation","excerpt":"arXiv에 게시된 'UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","tags":["Review","Video Generation","Multi-modal Learning","Multi-task Learning","Zero-shot Generalization","Diffusion Models","World Models","Video Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiehui Huang, Yuechen Zhang, Xu He, Yuan Gao, Zhi Cen, Bin Xia, Yan Zhou, Xin Tao, Pengfei Wan, Jiaya Jia 핵심 연구 목표 기존 비디오 생성 모델들이 단일 모달리티 조건화 및 제한된 모달 다양성으로 인해 세계를 총체적으로 이해하는 데 한"},{"id":"2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning","title":"[논문리뷰] VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning","excerpt":"Yansong Tang이 arXiv에 게시한 'VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning","tags":["Review","Tool-integrated Visual Reasoning","Referring Grounded Reasoning","Agentic Reinforcement Learning","Self-Correction","Large Vision-Language Models","Chain-of-Thought","Tool Refinement"],"text":"링크: 논문 PDF로 바로 열기 저자: Yansong Tang, Haoji Zhang, Jingxuan Niu, Wenlong Liu, VoyageWang 핵심 연구 목표 이 논문은 기존 Toolintegrated Visual Reasoning (TiVR) 패러다임이 부정확하거나 오류 있는 도구 출력에 취약하여 환각적인 추론으로 이어지는 문제를 해결하고자 "},{"id":"2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators","title":"[논문리뷰] VideoVLA: Video Generators Can Be Generalizable Robot Manipulators","excerpt":"Yaobo Liang이 arXiv에 게시한 'VideoVLA: Video Generators Can Be Generalizable Robot Manipulators' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators","tags":["Review","Robot Manipulation","Video Generation Models","Vision-Language-Action (VLA)","Diffusion Transformer","Generalization","Action Prediction","Visual Imagination"],"text":"링크: 논문 PDF로 바로 열기 저자: Yichao Shen, Fangyun Wei, Zhiying Du, Yaobo Liang, Yan Lu, Jiaolong Yang, Nanning Zheng, Baining Guo 핵심 연구 목표 본 논문은 로봇 조작 분야에서 기존 VLA 모델의 제한적인 일반화 능력을 극복하고, 새로운 태스크, 객체, 환경에 대한 강"},{"id":"2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering","title":"[논문리뷰] Voxify3D: Pixel Art Meets Volumetric Rendering","excerpt":"Yu-Lun Liu이 arXiv에 게시한 'Voxify3D: Pixel Art Meets Volumetric Rendering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-09 00:00:00+0900+0900","permalink":"/ai/review/2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering","tags":["Review","Voxel Art","Volumetric Rendering","3D Stylization","Neural Radiance Fields","Discrete Optimization","Gumbel-Softmax","CLIP Loss"],"text":"링크: 논문 PDF로 바로 열기 저자: YiChuan Huang, Jiewen Chan, HaoJen Chien, YuLun Liu 핵심 연구 목표 3D 메시에서 고품질 복셀 아트를 자동 생성하는 과정에서 발생하는 기하학적 추상화, 의미 보존, 그리고 이산적인 색상 일관성 간의 상충하는 요구사항을 해결하는 것이 목표입니다. 기존 방법들은 기하학적 구조를 과"},{"id":"2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training","title":"[논문리뷰] Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training","excerpt":"Dim P. Papadopoulos이 arXiv에 게시한 'Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training","tags":["Review","Unsupervised Video Instance Segmentation","Self-Training","Quality Assessment","Pseudo-labeling","Domain Adaptation","VideoMask2Former","YouTubeVIS"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaixuan Lu, Mehmet Onurcan Kaya, Dim P. Papadopoulos 핵심 연구 목표 이 논문은 비디오 인스턴스 분할(VIS)에서 발생하는 합성실제(synthetictoreal) 도메인 간극 과 높은 주석 비용 문제를 해결하고자 합니다. 특히, 인간 주석 없이 실제 비디오에 대한 다중 인스턴스"},{"id":"2025-12-10-DeepCode-Open-Agentic-Coding","title":"[논문리뷰] DeepCode: Open Agentic Coding","excerpt":"Chao Huang이 arXiv에 게시한 'DeepCode: Open Agentic Coding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-DeepCode-Open-Agentic-Coding","tags":["Review","Agentic Coding","LLM","Code Generation","Repository Synthesis","Information Flow Management","Code Memory","CodeRAG","Automated Verification","Scientific Reproduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Zongwei Li, Zhonghang Li, Zirui Guo, Xubin Ren, Chao Huang 핵심 연구 목표 대규모 언어 모델(LLM) 기반 코드 에이전트들이 정보 과부하 와 컨텍스트 병목 현상 으로 인해 과학 논문과 같은 복잡한 문서로부터 고품질의 코드베이스를 생성하는 데 어려움을 겪는 문제를 해결하는 "},{"id":"2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce","title":"[논문리뷰] EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce","excerpt":"arXiv에 게시된 'EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce","tags":["Review","E-commerce","Foundation Agents","LLM Agents","Benchmark","Agent Evaluation","Tool Use","Multi-step Reasoning","Real-world Scenarios"],"text":"링크: 논문 PDF로 바로 열기 저자: Rui Min, Zile Qiao, Ze Xu, Jiawen Zhai, Wenyu Gao, Xuanzhong Chen, Haozhen Sun, Zhen Zhang, Xinyu Wang, Hong Zhou, Wenbiao Yin, Xuan Zhou, Yong Jiang, Haicheng Liu, Liang Ding, L"},{"id":"2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time","title":"[논문리뷰] Efficiently Reconstructing Dynamic Scenes One D4RT at a Time","excerpt":"arXiv에 게시된 'Efficiently Reconstructing Dynamic Scenes One D4RT at a Time' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time","tags":["Review","Dynamic Scene Reconstruction","4D Reconstruction","Point Tracking","Transformer Architecture","Feedforward Model","Query-based Inference","Computer Vision","Geometric Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Chuhan Zhang, Guillaume Le Moing, Skanda Koppula, Ignacio Rocco, Liliane Momeni, Junyu Xie, Shuyang Sun, Rahul Sukthankar, Joëlle K. Barral, Raia Hadsell, Zoubin Ghahramani, Andr"},{"id":"2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs","title":"[논문리뷰] From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs","excerpt":"arXiv에 게시된 'From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs","tags":["Review","Diffusion Language Models","LLM Adaptation","Block-Diffusion","Autoregressive Models","Attention Masks","Parallel Generation","Transfer Learning","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchuan Tian, Yuchen Liang, Jiacheng Sun, Shuo Zhang, Guangwen Yang, Yingte Shu, Sibo Fang, Tianyu Guo, Kai Han, Chao Xu, Hanting Chen, Xinghao Chen, Yunhe Wang (Peking Universit"},{"id":"2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation","title":"[논문리뷰] Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation","excerpt":"arXiv에 게시된 'Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation","tags":["Review","Vision-Language Navigation","Dual-System Architecture","Foundation Models","Diffusion Policies","Robotics","Real-time Control","Generalization","Autonomous Navigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Meng Wei, Chenyang Wan, Jiaqi Peng, Xiqian Yu, Yuqiang Yang, Delin Feng, Wenzhe Cai, Chenming Zhu, Tai Wang, Jiangmiao Pang, Xihui Liu 핵심 연구 목표 기존 VisionLanguage Navigation (VLN)"},{"id":"2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning","title":"[논문리뷰] LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning","excerpt":"arXiv에 게시된 'LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning","tags":["Review","Early Exit","Confidence Control","Reasoning Models","Conformal Prediction","LLM Optimization","Dynamic Exits","Hidden States","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Ömer Faruk Akgül, Yusuf Hakan Kalaycı, Rajgopal Kannan, Willie Neiswanger, Viktor Prasanna 핵심 연구 목표 대규모 추론 모델(LLM)이 불필요하게 긴 사고 과정을 생성하여 컴퓨팅 자원을 낭비하고 때로는 정확도를 저해하는 \"과잉 사고(overthin"},{"id":"2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","title":"[논문리뷰] MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment","excerpt":"arXiv에 게시된 'MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","tags":["Review","Video Generation","Robotic Manipulation","Hierarchical Framework","Reinforcement Learning","Diffusion Models","World Models","Cognitive Science","Physical Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruicheng Zhang, Mingyang Zhang, Jun Zhou, Zhangrui Guo, Xiaofan Liu, Zunnan Xu, Zhizhou Zhong, Puxin Yan, Haocheng Luo, Xiu Li 핵심 연구 목표 본 논문은 다양한 장기 로봇 조작 데이터의 부족과 기존 비디오 생성 모델의 "},{"id":"2025-12-10-Modular-Neural-Image-Signal-Processing","title":"[논문리뷰] Modular Neural Image Signal Processing","excerpt":"Michael S. Brown이 arXiv에 게시한 'Modular Neural Image Signal Processing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Modular-Neural-Image-Signal-Processing","tags":["Review","Neural ISP","Modular Architecture","Raw Image Processing","Photo-Editing","Camera Agnostic","Generalization","Deep Learning","Image Enhancement"],"text":"링크: 논문 PDF로 바로 열기 저자: Mahmoud Afifi, Zhongling Wang, Ran Zhang, Michael S. Brown 핵심 연구 목표 본 논문은 기존의 단일 신경망 ISP(Image Signal Processing)가 가지는 카메라 일반화 능력 부족, 높은 계산 비용, 그리고 낮은 해석 가능성이라는 한계를 극복하고자 합니다. 이를"},{"id":"2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory","title":"[논문리뷰] OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory","excerpt":"arXiv에 게시된 'OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory","tags":["Review","Multi-Shot Video Generation","Adaptive Memory","Long-Range Context","Frame Selection","Diffusion Models","Image-to-Video","Autoregressive Generation","Narrative Coherence"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaochong An, Menglin Jia, Haonan Qiu, Zijian Zhou, Xiaoke Huang, Zhiheng Liu, Weiming Ren, Kumara Kahatapitiya, Ding Liu, Sen He, Chenyang Zhang, Tao Xiang, Fanny Yang, Serge Be"},{"id":"2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks","title":"[논문리뷰] Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks","excerpt":"arXiv에 게시된 'Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks","tags":["Review","Neural Operators","Time-Dependent Flow","Complex Geometries","DeepONet","Signed Distance Field","Autoregressive Prediction","Computational Fluid Dynamics","FlowBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Ali Rabeha, Suresh Murugaiyana, Adarsh Krishnamurthya, Baskar Ganapathysubramaniana 핵심 연구 목표 본 논문은 복잡한 형상 주변의 시간 의존적 유동장(velocity fields) 을 빠르고 정확하게 예측하는 것을 목표로 합니다. 특히, 기존 전산유체역"},{"id":"2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality","title":"[논문리뷰] Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality","excerpt":"arXiv에 게시된 'Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality","tags":["Review","Face Swapping","Video Editing","Diffusion Models","Reference-guided Generation","Temporal Consistency","Keyframe Conditioning","Cinematic Quality","Dataset Construction"],"text":"링크: 논문 PDF로 바로 열기 저자: Zekai Luo, Zongze Du, Zhouhang Zhu, Hao Zhong, Muzhi Zhu, Wen Wang, Yuling Xi, Chenchen Jing, Hao Chen, Chunhua Shen 핵심 연구 목표 본 논문은 기존의 얼굴 교체(face swapping) 기술들이 장시간의 복잡한 비디오 시퀀스"},{"id":"2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting","title":"[논문리뷰] SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting","excerpt":"Sung-Ho Bae이 arXiv에 게시한 'SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting","tags":["Review","3D Gaussian Splatting (3DGS)","Gaussian Compression","Model Efficiency","Novel View Synthesis","Dynamic Scenes","Parameter Compression","Restructuring Compression","Real-time Rendering"],"text":"링크: 논문 PDF로 바로 열기 저자: Seokhyun Youn, Soohyun Lee, Geonho Kim, Weeyoung Kwon, SungHo Bae, Jihyong Oh 핵심 연구 목표 본 논문은 3D Gaussian Splatting (3DGS) 의 방대한 메모리 사용량과 높은 연산 오버헤드 문제를 해결하고, 특히 4D 다이내믹 씬 에서의 실용적"},{"id":"2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs","title":"[논문리뷰] Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs","excerpt":"arXiv에 게시된 'Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs","tags":["Review","Multimodal Large Language Models (MLLMs)","Cross-Modal Consistency","Reasoning Inconsistency","OCR Performance","Modality Gap","Benchmarking","Render Equivalence"],"text":"링크: 논문 PDF로 바로 열기 저자: Angela van Sprang, Laurens Samson, Ana Lucic, Erman Acar, Sennay Ghebreab, Yuki M. Asano 핵심 연구 목표 본 논문은 MLLM이 시각 및 언어 모달리티에 걸쳐 동일한 의미를 가진 정보에 대해 일관된 추론 능력 을 보이는지 체계적으로 평가하는 것을 목표"},{"id":"2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models","title":"[논문리뷰] ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models","excerpt":"Xiuyu Li이 arXiv에 게시한 'ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models","tags":["Review","LLM","Parallel Reasoning","Inference Latency","Chain-of-Thought","Reinforcement Learning","Adaptive Threading","Mathematical Reasoning","Speedup"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiuyu Li, TsuJui Fu, Felix JuefeiXu, Sida Wang, Long Lian 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 순차적 추론 과정에서 발생하는 높은 지연 시간 문제를 해결하고자 합니다. 특히, 기존 적응형 병렬 추론 방식들이 정확도 손실이 크거나 배포가 어려운 맞춤형 추론"},{"id":"2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels","title":"[논문리뷰] TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels","excerpt":"Tianyu Huang이 arXiv에 게시한 'TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels","tags":["Review","Monocular 3D Tracking","World-centric Coordinates","Dense Tracking","Camera Pose Estimation","Dynamic Object Tracking","Optimization","2D Track Upsampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahao Lu, Weitao Xiong, Jiacheng Deng, Peng Li, Tianyu Huang, Zhiyang Dou, Cheng Lin, SaiKit Yeung, Yuan Liu 핵심 연구 목표 기존 단안 3D 트래킹 방법론의 한계인 카메라 움직임과 전경 동적 객체 움직임의 분리 미흡 및 새롭게 출현"},{"id":"2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models","title":"[논문리뷰] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models","excerpt":"Weirui Ye이 arXiv에 게시한 'TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models","tags":["Review","Reinforcement Learning","Diffusion Models","Generative Models","Tree Search","Sample Efficiency","Credit Assignment","GRPO","Visual Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zheng Ding, Weirui Ye 핵심 연구 목표 본 논문은 시각적 생성 모델의 RL 후학습(posttraining) 시 발생하는 막대한 계산 비용 문제를 해결하고, 기존 방법론들의 낮은 샘플 효율성 과 투박한 신용 할당 한계를 극복하여 인간의 선호도에 더 잘 부합하는 모델을 효율적으로 정렬하는 것을 목표로 합니"},{"id":"2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform","title":"[논문리뷰] Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform","excerpt":"Muyao Niu이 arXiv에 게시한 'Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform","tags":["Review","Neural Rendering","3D Gaussian Splatting","WebGPU","ONNX Inference","World Models","Real-time Rendering","Browser-based","Dynamic Scenes"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuning Gong, Yifei Liu, Yifan Zhan, Muyao Niu, Xueying Li, Yuanjun Liao, Jiaming Chen, Yuanyuan Gao, Jiaqi Chen, Minming Chen, Li Zhou, Yuning Zhang, Wei Wang, Xiaoqing Hou, Huax"},{"id":"2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","title":"[논문리뷰] Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance","excerpt":"arXiv에 게시된 'Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-10 00:00:00+0900+0900","permalink":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","tags":["Review","Video Generation","Motion Control","Latent Trajectory Guidance","Image-to-Video","Diffusion Models","Neural Networks","MoveBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruihang Chu, Yefei He, Zhekai Chen, Shiwei Zhang, Xiaogang Xu, Bin Xia, Dingdong Wang, Xihui Liu, Hongwei Yi, Hengshuang Zhao, Yu Liu, Yingya Zhang, Yujiu Yang 핵심 연구 목표 기존 모션 제어 "},{"id":"2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS","title":"[논문리뷰] Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS","excerpt":"Morteza Abolghasemi이 arXiv에 게시한 'Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS","tags":["Review","TTS","Phonemization","G2P","Low Latency","Real-time","Service-Oriented Architecture","Context-Aware","Persian Language"],"text":"링크: 논문 PDF로 바로 열기 저자: Mahta Fetrat, Donya Navabi, Zahra Dehghanian, Morteza Abolghasemi, Hamid R. Rabiee 핵심 연구 목표 경량화된 실시간 TTS 시스템에서 문맥 인지 phonemization의 품질과 추론 속도 간의 근본적인 트레이드오프를 해결하는 것이 목표입니다. 특히 페르"},{"id":"2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain","title":"[논문리뷰] BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain","excerpt":"tamarott이 arXiv에 게시한 'BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain","tags":["Review","fMRI","Brain Mapping","Visual Representation","Interpretability","Sparse Autoencoders","Vision-Language Models","Unsupervised Learning","Neuroscience"],"text":"링크: 논문 PDF로 바로 열기 저자: Navve Wasserman, Matias Cosarinsky, Yuval Golbari, Aude Oliva, Antonio Torralba, Tamar Rott Shaham, Michal Irani 핵심 연구 목표 본 논문은 인간 뇌에서 시각적 개념 표현을 대규모로 발견하고 해석하는 자동화된 프레임워크인 Brain"},{"id":"2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding","title":"[논문리뷰] Composing Concepts from Images and Videos via Concept-prompt Binding","excerpt":"arXiv에 게시된 'Composing Concepts from Images and Videos via Concept-prompt Binding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding","tags":["Review","Visual Concept Composition","Diffusion Models","Text-to-Video Generation","Concept Binding","Hierarchical Binder","Diversify-and-Absorb Mechanism","Temporal Disentanglement","One-shot Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xianghao Kong¹, Zeyu Zhang¹, Yuwei Guo², Zhuoran Zhao1,3, Songchun Zhang¹, Anyi Rao¹ 핵심 연구 목표 본 논문은 복잡한 시각적 개념(예: 스타일, 모션)을 이미지 및 비디오 입력에서 정확하게 추출하고, 이를 유연하게 조합하여 일관된 시각적 출력을 생성하"},{"id":"2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing","title":"[논문리뷰] EtCon: Edit-then-Consolidate for Reliable Knowledge Editing","excerpt":"Chenglin Li이 arXiv에 게시한 'EtCon: Edit-then-Consolidate for Reliable Knowledge Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing","tags":["Review","Knowledge Editing","Large Language Models","Lifelong Learning","Reinforcement Learning","Trust Region Policy Optimization","Chain-of-Thought","Catastrophic Forgetting"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruilin Li, Yibin Wang, Wenhong Zhu, Chenglin Li, Jinghao Zhang, Chenliang Li, Junchi Yan, Jiaqi Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 지식 편집 방법론이 제어된 환경에서는 높은 성능을 보이나, 실제 자율 회귀 생성 및"},{"id":"2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules","title":"[논문리뷰] Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules","excerpt":"Yang Zhang이 arXiv에 게시한 'Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules","tags":["Review","Diffusion Language Models","Decoding Efficiency","Early Exit","Confidence Schedules","Training-free","Model-agnostic","Progress-aware"],"text":"링크: 논문 PDF로 바로 열기 저자: Amr Mohamed, Yang Zhang, Michalis Vazirgiannis, Guokan Shang 핵심 연구 목표 본 논문은 확산 언어 모델(dLLM)이 오토회귀 모델에 비해 가지는 잠재력에도 불구하고, 느리고 반복적인 샘플링 과정으로 인해 실용성이 저해되는 문제를 해결하고자 합니다. 모델의 품질 저하 없이"},{"id":"2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models","title":"[논문리뷰] HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models","excerpt":"arXiv에 게시된 'HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models","tags":["Review","Vision-Language-Action","Motion Representation","Temporal Reasoning","Long-Horizon Manipulation","Hindsight","Foresight","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Minghui Lin, Pengxiang Ding, Shu Wang, Zifeng Zhuang, Yang Liu, Xinyang Tong, Wenxuan Song, Shangke Lyu, Siteng Huang, Donglin Wang 핵심 연구 목표 대부분의 VisionLanguageAction (VLA) 모델이 M"},{"id":"2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting","title":"[논문리뷰] IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting","excerpt":"arXiv에 게시된 'IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting","tags":["Review","Multimodal Large Language Models (MLLMs)","Infrared Image Understanding","Benchmark Dataset","Visual Question Answering (VQA)","Generative Visual Prompting (GenViP)","Domain Adaptation","Image-to-Image Translation"],"text":"링크: 논문 PDF로 바로 열기 저자: Tao Zhang, Yuyang Hong, Yang Xia, Kun Ding, Zeyu Zhang, Ying Wang, Shiming Xiang, Chunhong Pan 핵심 연구 목표 본 연구는 주로 자연 이미지에 훈련된 Multimodal Large Language Models (MLLMs) 의 적외선 이미지 이해"},{"id":"2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models","title":"[논문리뷰] InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models","excerpt":"arXiv에 게시된 'InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models","tags":["Review","Vision-Language Models","Linear Attention","Sliding Window Attention","Gated DeltaNet","Long-Context Understanding","Efficiency","Hybrid Architecture","Multimodal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongyuan Tao, Bencheng Liao, Shaoyu Chen, Haoran Yin, Qian Zhang, Wenyu Liu, Xinggang Wang 핵심 연구 목표 본 연구는 기존 VLM의 이차적인 계산 복잡성과 증가하는 KV 캐시로 인한 장기 컨텍스트 이해 능력 및 배포 제약 문제를 해결하는 것을 목표"},{"id":"2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models","title":"[논문리뷰] Learning Unmasking Policies for Diffusion Language Models","excerpt":"arXiv에 게시된 'Learning Unmasking Policies for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Reinforcement Learning","Masked Diffusion","Sampling Policy","Inference Optimization","Markov Decision Process","Generative AI","Text Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Metod Jazbec, Theo X. Olausson, Louis Béthune, Pierre Ablin, Michael Kirchhof, Joao Monterio, Victor Turrisi, Jason Ramapuram, Marco Cuturi 핵심 연구 목표 마스킹된 이산 확산 언어 모델(dLLMs)에서 토큰 "},{"id":"2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer","title":"[논문리뷰] OmniPSD: Layered PSD Generation with Diffusion Transformer","excerpt":"Cheng Liu이 arXiv에 게시한 'OmniPSD: Layered PSD Generation with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer","tags":["Review","Diffusion Transformer","PSD Generation","Image Decomposition","RGBA-VAE","In-Context Learning","Text-to-PSD","Image-to-PSD"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Liu, Yiren Song, Haofan Wang, Mike Zheng Shou 핵심 연구 목표 본 논문은 기존 생성 모델의 한계인 단일 평면 이미지 출력 문제를 해결하고, 투명한 알파 채널을 포함하는 레이어드 PSD 파일 을 생성 및 재구성하는 통합 프레임워크인 OmniPSD 를 제안합니다. 이를 통해 "},{"id":"2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models","title":"[논문리뷰] Pay Less Attention to Function Words for Free Robustness of Vision-Language Models","excerpt":"arXiv에 게시된 'Pay Less Attention to Function Words for Free Robustness of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models","tags":["Review","Vision-Language Models","Adversarial Robustness","Function Words","Cross-Attention","Adversarial Attacks","Differential Attention","Vision-Language Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Chao Shen 핵심 연구 목표 VisionLanguage Model (VLM)의 견고성과 성능 간의 상충 관계를 해결하고, 특히 함수어(function words) 가 교차모달 적대적 공격에 대한 VLM의 취약성을 유발한다는 가설을 검증하고자 합"},{"id":"2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication","title":"[논문리뷰] Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication","excerpt":"Hengshu Zhu이 arXiv에 게시한 'Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication","tags":["Review","Clinical Dialogue","LLM Agents","Healthcare AI","Agentic Paradigm","Medical Decision Support","Knowledge Grounding","AI Safety","Workflow Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: XIAOQUAN ZHI, HONGKE ZHAO, LIKANG WU, CHUANG ZHAO, HENGSHU ZHU 핵심 연구 목표 임상 대화에서 기존 LLM 의 반응적, 무상태적 특성 및 환각 문제의 한계를 극복하고, LLM 을 자율적인, 목표 지향적 시스템으로 전환하는 'Agentic Paradigm'을 제안합니다. "},{"id":"2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","title":"[논문리뷰] StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation","excerpt":"Guixun Luo이 arXiv에 게시한 'StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","tags":["Review","Monocular-to-Stereo","Video Generation","Diffusion Models","Geometry-Aware","XR","IPD-aligned Dataset","Novel View Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Guixun Luo, Hanwen Liang, Longfei Li, Yuyang Yin, KXingLab 핵심 연구 목표 기존 단안 비디오 생성 모델의 스테레오 기능 부재 및 취약한 pose estimation/multistage warping 파이프라인으로 인한 스테레오 비디오 생성의 한계를 극복하는 것이 목표입니다"},{"id":"2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression","title":"[논문리뷰] TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression","excerpt":"arXiv에 게시된 'TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression","tags":["Review","4D Gaussian Splatting","Dynamic Scene Compression","Rate-Distortion Optimization","Temporal Activation","Embedding-based Deformation","Neural Compression","3D Gaussian Splatting"],"text":"링크: 논문 PDF로 바로 열기 저자: ChengYuan Ho, HeBi Yang, JuiChiu Chiang, YuLun Liu, WenHsiao Peng (National Yang Ming Chiao Tung University, Taiwan), JuiChiu Chiang (National Chung Cheng University, Taiwan) 핵심 "},{"id":"2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","title":"[논문리뷰] UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving","excerpt":"arXiv에 게시된 'UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","tags":["Review","Autonomous Driving","End-to-End Learning","Vision-Language Models","World Model","Chain-of-Thought","Video Generation","Trajectory Planning","Multimodal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, YingCong Chen 핵심 연구 목표 자율 주행 시스템이 제한된 세계 지식 과 시각적 동적 모델링 부족 으로 인해 롱테일 시나리오에서 겪는 어려움을 해결하는 것이 목표입니다. 기"},{"id":"2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory","title":"[논문리뷰] VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory","excerpt":"arXiv에 게시된 'VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory","tags":["Review","Autoregressive Video Generation","Diffusion Models","Hybrid Memory","State-Space Models (SSM)","Long Video Synthesis","Temporal Consistency","Interactive AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifei Yu, Xiaoshan Wu, Xiaoyang Lyu, Bo Wang, Lin Ma, Xinting Hu, Yuewen Ma, Tao Hu, YangTian Sun, Zhongrui Wang, Xiaojuan Qi et al. 핵심 연구 목표 본 논문은 AR(Autoregressive) 비디오 확산 모델의 "},{"id":"2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation","title":"[논문리뷰] WonderZoom: Multi-Scale 3D World Generation","excerpt":"Jiajun Wu이 arXiv에 게시한 'WonderZoom: Multi-Scale 3D World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-11 00:00:00+0900+0900","permalink":"/ai/review/2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation","tags":["Review","Multi-Scale 3D Generation","Gaussian Surfel","Progressive Synthesis","Neural Rendering","Scale-Adaptive","Content Creation","Zoom-in"],"text":"링크: 논문 PDF로 바로 열기 저자: Jin Cao, HongXing Yu, Jiajun Wu 핵심 연구 목표 본 논문은 단일 이미지로부터 다양한 공간 스케일에 걸쳐 일관된 3D 세계를 생성하는 다중 스케일 3D 세계 생성 의 핵심 문제를 해결하고자 합니다. 기존 3D 생성 모델들이 단일 스케일 합성에 국한되고 스케일 인식 3D 표현이 부족하여 상호작용적"},{"id":"2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning","title":"[논문리뷰] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning","excerpt":"arXiv에 게시된 'Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning","tags":["Review","LLM Agents","Geometry Problem Solving","Reinforcement Learning","Curriculum Learning","Auxiliary Construction","Symbolic Reasoning","IMO"],"text":"링크: 논문 PDF로 바로 열기 저자: Haiteng Zhao, Junhao Shen, Yiming Zhang, Songyang Gao, Kuikun Liu, Tianyou Ma, Fan Zheng, Dahua Lin, Wenwei Zhang, Kai Chen 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM) 에이전트가 국제 수학 올림피아드(IMO) 수"},{"id":"2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation","title":"[논문리뷰] Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation","excerpt":"arXiv에 게시된 'Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation","tags":["Review","Reinforcement Learning","Text-to-3D Generation","Autoregressive Models","Reward Modeling","Hierarchical RL","3D Benchmarking","ShapeLLM-Omni"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiwen Tang, Zoey Guo, Kaixin Zhu, Ray Zhang, Qizhi Chen, Dongzhi Jiang, Junli Liu, Bohan Zeng, Haoming Song, Delin Qu, Tianyi Bai, Dan Xu, Wentao Zhang, Bin Zhao 핵심 연구 목표 텍스트3D 자"},{"id":"2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale","title":"[논문리뷰] Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale","excerpt":"arXiv에 게시된 'Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale","tags":["Review","AI Agent","Software Engineering","Open-Source","LLM","Orchestrator","Context Management","Long-term Memory","Meta-agent"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaodong Wang, Zhenting Qi, Sherman Wong, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Yining Yang, Ben Maurer, Wenlin Chen, David Recordon, Yilun Du, Minlan Yu, Ying Zhang 핵심 연구 목표"},{"id":"2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator","title":"[논문리뷰] Evaluating Gemini Robotics Policies in a Veo World Simulator","excerpt":"arXiv에 게시된 'Evaluating Gemini Robotics Policies in a Veo World Simulator' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator","tags":["Review","Robotics","Policy Evaluation","World Model","Video Generation","Out-of-Distribution (OOD)","Safety","Gemini Robotics","Veo Simulator"],"text":"링크: 논문 PDF로 바로 열기 저자: Coline Devin, Yilun Du, Debidatta Dwibedi, Ruiqi Gao, Abhishek Jindal, Thomas Kipf, Sean Kirmani, Fangchen Liu, Anirudha Majumdar, Andrew Marmon, Carolina Parada, Yulia Rubanova,"},{"id":"2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents","title":"[논문리뷰] Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents","excerpt":"Xiaodong Gu이 arXiv에 게시한 'Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents","tags":["Review","Federated Learning (FL)","LLM Agents","Self-Evolution","Privacy-Preserving","Multi-Environment","Parameter-Efficient Fine-Tuning","Low-Rank Aggregation","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiang Chen, Yuling Shi, Qizhen Lan, Yuchao Qiu, Xiaodong Gu 핵심 연구 목표 본 논문은 복잡한 인터랙티브 태스크에서 LLM 에이전트가 직면하는 프라이버시 제약으로 인해 중앙 집중식 최적화 및 동적 환경 간 공동 진화가 어려운 문제를 해결하고자 합니다. 특히 이질적인 태스크"},{"id":"2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models","title":"[논문리뷰] From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models","excerpt":"arXiv에 게시된 'From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models","tags":["Review","Vision-Language Models","Microscopic Spatial Intelligence","Molecular Structures","Benchmarking","PDBbind Dataset","Spatial Reasoning","Drug Discovery"],"text":"링크: 논문 PDF로 바로 열기 저자: Zongzhao Li, Xiangzhe Kong, Jiahui Su, Zongyang Ma, Mingze Li, Songyou Li, Yuelin Zhang, Yu Rong, Tingyang Xu, Deli Zhao, Wenbing Huang 핵심 연구 목표 본 논문은 눈에 보이지 않는 미세한 엔티티(원자, 분자)의 "},{"id":"2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos","title":"[논문리뷰] H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos","excerpt":"Mike Zheng Shou이 arXiv에 게시한 'H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos","tags":["Review","Video-to-Video Translation","Robot Learning","Human-Robot Transfer","Diffusion Models","Unpaired Data Learning","Pose-Guided Generation","Embodiment Gap Bridging"],"text":"링크: 논문 PDF로 바로 열기 저자: Hai Ci, Xiaokang Liu, Pei Yang, Yiren Song, Mike Zheng Shou 핵심 연구 목표 본 논문은 일상적인 인간객체 상호작용 비디오를 물리적으로 접지된 로봇 조작 비디오 로 변환하여 로봇이 인간 비디오로부터 조작 기술을 학습할 수 있도록 하는 것을 목표로 합니다. 특히, 페어링된 인"},{"id":"2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving","title":"[논문리뷰] Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving","excerpt":"arXiv에 게시된 'Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving","tags":["Review","Mathematical Reasoning","Long-Horizon Reasoning","Multi-Agent System","Reinforcement Learning","Olympiad Problems","Lemma Memory","Context Length","OREAL-H"],"text":"링크: 논문 PDF로 바로 열기 저자: Songyang Gao, Yuzhe Gu, Zijian Wu, Lingkai Kong, Wenwei Zhang, Zhongrui Cai, Fan Zheng, Tianyou Ma, Junhao Shen, Haiteng Zhao, Duanyang Zhang, Huilun Zhang, Kuikun Liu, Chengqi L"},{"id":"2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents","title":"[논문리뷰] MOA: Multi-Objective Alignment for Role-Playing Agents","excerpt":"Yongbin Li이 arXiv에 게시한 'MOA: Multi-Objective Alignment for Role-Playing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents","tags":["Review","Role-Playing Agents","Multi-Objective Reinforcement Learning","LLM Alignment","Persona Consistency","Dialogue Generation","Reward Shaping","Off-Policy Guidance"],"text":"링크: 논문 PDF로 바로 열기 저자: Chonghua Liao, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li 핵심 연구 목표 본 논문은 역할극 에이전트(RPA)가 다중 턴 지시 따르기, 도메인 지식 습득, 일관된 언어 스타일 유지 등 여러 상충하는 기술들을 동시에 습득해야 하는 문제를 해결하고자 합니다. 기존 지도 학습(S"},{"id":"2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos","title":"[논문리뷰] MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos","excerpt":"Qi Wang이 arXiv에 게시한 'MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos","tags":["Review","3D Motion Capture","Monocular Video","Arbitrary Skeletons","Motion Retargeting","Deep Learning","Inverse Kinematics","Transformer Architecture","Category-Agnostic"],"text":"링크: 논문 PDF로 바로 열기 저자: Kehong Gong, Zhengyu Wen, Weixia He, Mingxi Xu, Qi Wang, Ning Zhang, Zhengyu Li, Dongze Lian, Wei Zhao, Xiaoyu He, Mingyuan Zhang 핵심 연구 목표 본 논문은 기존 모션 캡처 파이프라인의 종(species) 또는 템플릿"},{"id":"2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification","title":"[논문리뷰] OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification","excerpt":"arXiv에 게시된 'OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification","tags":["Review","LLM Verification","Chain-of-Thought","Process-based Verifier","Outcome-based Verifier","Active Learning","Reinforcement Learning","Mathematical Reasoning","AI Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijian Wu, Lingkai Kong, Wenwei Zhang, Songyang Gao, Yuzhe Gu, Zhongrui Cai, Tianyou Ma, Yuhong Liu, Zhi Wang, Runyuan Ma, Guangyu Wang, Wei Li, Conghui He, Dahua Lin, and Kai Ch"},{"id":"2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning","title":"[논문리뷰] ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning","excerpt":"Yujin Han이 arXiv에 게시한 'ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning","tags":["Review","Video Editing","Reasoning","Unified Models","Self-Reflective Learning","Vision-Language Models (VLMs)","Diffusion Models","RVE-Bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyu Liu, Hangjie Yuan, Yujie Wei, Jiazheng Xing, Yujin Han 핵심 연구 목표 본 논문은 강력한 VisionLanguage Model (VLM) 을 탑재한 최신 비디오 통합 모델들이 추론 기반 시각 편집(reasoninformed visual editing) 에서 어려움을"},{"id":"2025-12-12-Stronger-Normalization-Free-Transformers","title":"[논문리뷰] Stronger Normalization-Free Transformers","excerpt":"Zhuang Liu이 arXiv에 게시한 'Stronger Normalization-Free Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Stronger-Normalization-Free-Transformers","tags":["Review","Normalization-Free Transformers","Point-wise Functions","Error Function","Deep Learning","Transformer Architecture","Generalization","Normalization Layers"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingzhi Chen, Taiming Lu, Jiachen Zhu, Mingjie Sun, Zhuang Liu 핵심 연구 목표 본 논문은 트랜스포머 아키텍처에서 필수적이었던 정규화 계층(Normalization Layers)의 의존성을 제거 하고, 단순히 기존 정규화 계층의 성능에 필적하는 것을 넘어 이를 능가하는 "},{"id":"2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground","title":"[논문리뷰] T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground","excerpt":"arXiv에 게시된 'T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground","tags":["Review","Russian LLM","Hybrid Reasoning","Speculative Decoding","Cyrillic Tokenizer","Instruction Tuning","Reward Modeling","T-Math Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: GenT Team (Anatolii Potapov, et al.) 핵심 연구 목표 논문은 러시아어 오픈소스 LLM의 한계, 특히 추론 능력과 효율적인 추론을 위한 통합 생태계의 부재를 해결하고자 합니다. 이를 위해 Tpro 2.0 이라는 개방형 러시아어 하이브리드 추론 LLM과 상호작용 가능한 데모 플랫폼을 도입하여,"},{"id":"2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality","title":"[논문리뷰] The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality","excerpt":"arXiv에 게시된 'The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality","tags":["Review","LLM Evaluation","Factuality Benchmark","Multimodal AI","Knowledge Grounding","Parametric Knowledge","Retrieval Augmented Generation","Automated Scoring"],"text":"링크: 논문 PDF로 바로 열기 저자: Aileen Cheng, Alon Jacovi, Amir Globerson, Ben Golan, Charles Kwong, Chris Alberti, et al. 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)이 다양한 시나리오에서 사실적으로 정확한 텍스트를 생성하는 능력을 포괄적으로 평가하기 위한 새로운 온라인"},{"id":"2025-12-12-Thinking-with-Images-via-Self-Calling-Agent","title":"[논문리뷰] Thinking with Images via Self-Calling Agent","excerpt":"Qixiang Ye이 arXiv에 게시한 'Thinking with Images via Self-Calling Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Thinking-with-Images-via-Self-Calling-Agent","tags":["Review","Multimodal LLMs","Self-Calling Chain-of-Thought","Reinforcement Learning","Visual Reasoning","Agentic AI","Tool Calling","Group Relative Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenxi Yang, Yuzhong Zhao, Fang Wan, Qixiang Ye 핵심 연구 목표 본 논문은 희소한 고품질 추론 데이터로 인해 강화 학습을 통한 MLLM의 Interleaved Multimodal ChainofThought (iMCoT) 최적화가 어렵다는 문제점을 해결하고자 합니다. 복잡한 iMCoT"},{"id":"2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task","title":"[논문리뷰] Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task","excerpt":"arXiv에 게시된 'Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task","tags":["Review","VideoQA","MLLMs","Tool Learning","Spatiotemporal Reasoning","Video Toolkit","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Sunqi Fan, Jiashuo Cui, MengHao Guo, Shuojin Yang 핵심 연구 목표 본 논문은 기존 MLLM(Multimodal Large Language Models) 이 복잡한 VideoQA(Video Question Answering) 태스크에서 시공간적 관계 모델링 및 시간적 진화의 인과적"},{"id":"2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction","title":"[논문리뷰] VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction","excerpt":"arXiv에 게시된 'VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-12 00:00:00+0900+0900","permalink":"/ai/review/2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction","tags":["Review","Multimodal Learning","Vector Quantization","Autoencoder","Unified Tokenizer","Image Generation","Image Reconstruction","Vision Transformers","Semantic Features"],"text":"링크: 논문 PDF로 바로 열기 저자: Sinan Du, Jiahao Guo, Bo Li, Shuhao Cui, Zhengzhuo Xu, Yifu Luo, Yongxian Wei, Kun Gai, Xinggang Wang, Kai Wu, Chun Yuan 핵심 연구 목표 멀티모달 이해, 생성 및 재구성 표현을 단일 토크나이저 내에서 통합하는 핵심 과제를 해"},{"id":"2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images","title":"[논문리뷰] CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images","excerpt":"Enzo Ferrante이 arXiv에 게시한 'CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images","tags":["Review","Uncertainty Quantification","Landmark Segmentation","Chest X-ray","VAE","Graph Neural Networks","Out-of-Distribution Detection","Medical Imaging"],"text":"링크: 논문 PDF로 바로 열기 저자: Matias Cosarinsky, Nicolas Gaggion, Rodrigo Echeveste, Enzo Ferrante 핵심 연구 목표 본 논문은 의료 영상 분할 시스템의 안전한 임상 배포를 위해 랜드마크 기반 해부학적 분할 에서 불확실성 추정을 연구합니다. 기존 픽셀 기반 불확실성 연구와 달리, 내재적 토폴로지 "},{"id":"2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry","title":"[논문리뷰] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry","excerpt":"Yanchao Li이 arXiv에 게시한 'DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry","tags":["Review","Multimodal Large Language Model","Dental Imaging","Complex Reasoning","Domain Adaptation","Reinforcement Learning","Medical VQA","Dental Healthcare"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenyang Cai, Jiaming Zhang, Junjie Zhao, Ziyi Zeng, Yanchao Li, et al. 핵심 연구 목표 본 논문은 기존 MLLM이 치과 영상 데이터의 미세한 시각적 특징을 포착하고 정밀한 진단을 위한 충분한 추론 능력을 갖추지 못하는 한계를 해결하고자 합니다. 이를 위해 치과 "},{"id":"2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video","title":"[논문리뷰] EgoX: Egocentric Video Generation from a Single Exocentric Video","excerpt":"arXiv에 게시된 'EgoX: Egocentric Video Generation from a Single Exocentric Video' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video","tags":["Review","Egocentric Video Generation","Exocentric-to-Egocentric","Video Diffusion Models","3D Scene Reconstruction","Geometry-Guided Attention","View Synthesis","Camera Pose Estimation","LoRA Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Taewoong Kang, Kinam Kim, Dohyeon Kim, Minho Park, Junha Hyung, Jaegul Choo 핵심 연구 목표 본 연구는 단일 외부 시점(exocentric) 비디오 입력으로부터 사실적이고 일관성 있는 내부 시점(egocentric) 비디오를 생성하는 것을 목표로 합니다. 극심"},{"id":"2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","title":"[논문리뷰] Exploring MLLM-Diffusion Information Transfer with MetaCanvas","excerpt":"arXiv에 게시된 'Exploring MLLM-Diffusion Information Transfer with MetaCanvas' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","tags":["Review","Multimodal Large Language Models (MLLMs)","Diffusion Models","Image Generation","Video Generation","Image Editing","Video Editing","Latent Space Planning","Canvas Tokens","Information Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Han Lin, Xichen Pan, Ziqi Huang, Ji Hou, Jialiang Wang, Weifeng Chen, Zecheng He, Felix JuefeiXu, Junzhe Sun, Zhipeng Fan, Ali Thabet, Mohit Bansal, Chu Wang 핵심 연구 목표 MLLM이 복잡한 시"},{"id":"2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator","title":"[논문리뷰] LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator","excerpt":"arXiv에 게시된 'LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator","tags":["Review","Robotic Agent","Large Language Models (LLMs)","Embodied AI","Task Planning","Human-Robot Interaction","General-purpose Robotics","ROS"],"text":"링크: 논문 PDF로 바로 열기 저자: Lihuang Chen, Xiangyu Luo, and Jun Meng 핵심 연구 목표 본 논문은 다양한 유형의 로봇이 예측 불가능한 복잡한 작업을 수행할 수 있도록 하는 일반 목적의 언어 기반 지능형 로봇 에이전트 프레임워크인 를 제안합니다. 이는 기존 로봇 작업 계획 연구의 제한적인 일반화 및 복잡한 구조 문제를 "},{"id":"2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes","title":"[논문리뷰] MeshSplatting: Differentiable Rendering with Opaque Meshes","excerpt":"Matheus Gadelha이 arXiv에 게시한 'MeshSplatting: Differentiable Rendering with Opaque Meshes' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes","tags":["Review","Differentiable Rendering","Novel View Synthesis","Mesh Reconstruction","3D Gaussian Splatting","Opaque Meshes","Real-time Rendering","Game Engines"],"text":"링크: 논문 PDF로 바로 열기 저자: Jan Held, Sanghyun Son, Renaud Vandeghen, Daniel Rebain, Matheus Gadelha, Yi Zhou, Anthony Cioppa, Ming C. Lin, Marc Van Droogenbroeck, Andrea Tagliasacchi 핵심 연구 목표 본 논문은 3D Gaus"},{"id":"2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming","title":"[논문리뷰] PersonaLive! Expressive Portrait Image Animation for Live Streaming","excerpt":"Jue Wang이 arXiv에 게시한 'PersonaLive! Expressive Portrait Image Animation for Live Streaming' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming","tags":["Review","Live Streaming","Portrait Animation","Diffusion Models","Real-time AI","Appearance Distillation","Micro-chunk Streaming","Motion Control","Low Latency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Li, ChiMan Pun, Chen Fang, Jue Wang, Xiaodong Cun 핵심 연구 목표 기존 확산 모델 기반 초상화 애니메이션이 시각적 품질과 표현 사실성에 중점을 두어 높은 계산 비용 과 지연 시간 으로 인해 라이브 스트리밍에 부적합하다는 문제를 해결하고자 합니다. 본 논문은 저지연 "},{"id":"2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder","title":"[논문리뷰] SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder","excerpt":"arXiv에 게시된 'SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder","tags":["Review","Text-to-Image Generation","Latent Diffusion Model","Visual Foundation Model","DINOv3","Flow Matching","High-Resolution Synthesis","VAE-free Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng, Ziyang Yuan, Xiaoshi Wu, Yuanxing Zhang, Huan Yang, Xintao Wang, Pengfei Wan, Kun Gai, Jie Zhou, Jiwen Lu 핵심 연구 "},{"id":"2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models","title":"[논문리뷰] Scaling Behavior of Discrete Diffusion Language Models","excerpt":"arXiv에 게시된 'Scaling Behavior of Discrete Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models","tags":["Review","Discrete Diffusion Models","Scaling Laws","Language Models","Masked Diffusion","Uniform Diffusion","Hyperparameter Tuning","Compute-Optimal Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Dimitri von Rütte, Janis Fluri, Omead Pooladzandi, Bernhard Schölkopf, Thomas Hofmann, Antonio Orvieto 핵심 연구 목표 본 논문은 Discrete Diffusion Language Models (DLMs) 의 스케일링 행동을 체계적으로 연"},{"id":"2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second","title":"[논문리뷰] Sharp Monocular View Synthesis in Less Than a Second","excerpt":"arXiv에 게시된 'Sharp Monocular View Synthesis in Less Than a Second' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second","tags":["Review","View Synthesis","3D Gaussian Splatting","Single Image","Neural Rendering","Real-time","Feedforward Network","Monocular Depth Estimation","AR/VR"],"text":"링크: 논문 PDF로 바로 열기 저자: Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, Amaël Delaunoy, Tian Fang, Yanghai Tsin, Stephan R. Richter, Vladlen Koltu"},{"id":"2025-12-15-Sliding-Window-Attention-Adaptation","title":"[논문리뷰] Sliding Window Attention Adaptation","excerpt":"arXiv에 게시된 'Sliding Window Attention Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation","tags":["Review","Large Language Models","Sliding Window Attention","Model Adaptation","Long Context","Inference Optimization","Fine-tuning","Chain-of-Thought","Sparse Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Yijiong Yuª, Jiale Liub, Qingyun Wub, Huazheng Wanga, and Ji Pei 핵심 연구 목표 본 논문은 Transformer 기반 LLM의 SelfAttention 메커니즘 이 입력 길이의 제곱에 비례하여 발생하는 높은 연산 비용 문제를 해결하고자 합니다. 특히, Full Att"},{"id":"2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","title":"[논문리뷰] Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation","excerpt":"Qifeng Chen이 arXiv에 게시한 'Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","tags":["Review","Video Generation","Motion Tracking","Diffusion Models","Structure Preservation","SAM2","Feature Distillation","Local Gram Flow"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Fei, George Stoica, Jingyuan Liu, Qifeng Chen, Ranjay Krishna, Xiaojuan Wang, Benlin Liu 핵심 연구 목표 본 논문은 비디오 생성 모델, 특히 diffusion 모델 이 관절형 및 변형 가능한 객체에 대해 물리적으로 그럴듯하고 구조를 보존하는"},{"id":"2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge","title":"[논문리뷰] Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge","excerpt":"Akash Karnatak이 arXiv에 게시한 'Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge","tags":["Review","Vision-Language-Action (VLA) models","Flow Matching","Embodied AI","Robot Manipulation","BEHAVIOR Challenge","Correlated Noise","Stage Tracking","Multi-Task Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Ilia Larchenko, Gleb Zaring, Akash Karnatak 핵심 연구 목표 본 논문은 2025 BEHAVIOR Challenge에서 1위를 차지한 비전액션 정책을 제시하며, 50가지의 다양하고 장기적인 가정용 작업을 포토리얼리스틱 시뮬레이션 에서 수행하는 것을 목표로 합니다. 특히, 누적 오류, 비"},{"id":"2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","title":"[논문리뷰] V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties","excerpt":"arXiv에 게시된 'V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-15 00:00:00+0900+0900","permalink":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","tags":["Review","Video Editing","Intrinsic Decomposition","Video Generation","Diffusion Models","Keyframe Editing","Inverse Rendering","Temporal Consistency","Physically Based Rendering"],"text":"링크: 논문 PDF로 바로 열기 저자: Ye Fang, Tong Wu, Valentin Deschaintre, Duygu Ceylan, Iliyan Georgiev, ChunHao Paul Huang, Yiwei Hu, Xuelin Chen, Tuanfeng Yang Wang (Fudan University, Adobe Research, Stanford U"},{"id":"2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver","title":"[논문리뷰] Image Diffusion Preview with Consistency Solver","excerpt":"arXiv에 게시된 'Image Diffusion Preview with Consistency Solver' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver","tags":["Review","Diffusion Models","Efficient Sampling","Reinforcement Learning","ODE Solvers","Image Generation","Consistency","Diffusion Preview"],"text":"링크: 논문 PDF로 바로 열기 저자: FuYun Wang, Hao Zhou, Liangzhe Yuan, Sanghyun Woo, Boqing Gong, Bohyung Han, MingHsuan Yang, Han Zhang, Yukun Zhu, Ting Liu, Long Zhao 핵심 연구 목표 본 논문은 이미지 Diffusion 모델의 느린 추론 속도로 "},{"id":"2025-12-16-KlingAvatar-2-0-Technical-Report","title":"[논문리뷰] KlingAvatar 2.0 Technical Report","excerpt":"arXiv에 게시된 'KlingAvatar 2.0 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-KlingAvatar-2-0-Technical-Report","tags":["Review","Avatar Generation","Video Diffusion","Multi-modal LLM","Long-duration Video","High-resolution Video","Lip Synchronization","Multi-character Control","Spatio-temporal Cascade"],"text":"링크: 논문 PDF로 바로 열기 저자: Kling Team, Kuaishou Technology 핵심 연구 목표 본 연구는 장시간 고해상도 아바타 비디오 생성 시 발생하는 효율성 부족, 시간적 드리프트, 품질 저하, 프롬프트 불일치 문제를 해결하는 것을 목표로 합니다. 특히, 멀티모달 지침에 효과적으로 부합하며 시각적 선명도, 사실적인 립싱크, 강한 정체성"},{"id":"2025-12-16-Memory-in-the-Age-of-AI-Agents","title":"[논문리뷰] Memory in the Age of AI Agents","excerpt":"Yanwei Yue이 arXiv에 게시한 'Memory in the Age of AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-Memory-in-the-Age-of-AI-Agents","tags":["Review","AI Agents","Memory Systems","LLMs","Taxonomy","Continual Learning","Self-Evolution","Multimodality","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanwei Yue, Yuyang Hu, zstanjj, KYLN24, Liusc2020 핵심 연구 목표 이 서베이 논문은 급증하는 AI 에이전트 메모리 연구 분야의 파편화된 개념적 명확성 부족을 해결하고, 기존 분류 체계의 한계 를 극복하고자 합니다. LLM 기반 에이전트의 장기 추론, 지속적 적응, 복잡한 환경과의"},{"id":"2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents","title":"[논문리뷰] NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents","excerpt":"chongyang09이 arXiv에 게시한 'NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents","tags":["Review","Coding Agents","LLMs","Software Engineering","Repository Generation","Long-Horizon Reasoning","Benchmark","Python Development","Autonomous Systems"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingzhe Ding, Yue Hou, Chongyao Tao, et al. 핵심 연구 목표 이 논문은 기존 코딩 에이전트 벤치마크들이 완전한 소프트웨어 시스템을 구축하는 데 필요한 장기적인 추론 능력 을 엄격하게 평가하지 못하는 문제를 해결하고자 합니다. 주로 국지적인 코드 생성이나 단기 버그 수정에 초점을 맞추던"},{"id":"2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge","title":"[논문리뷰] Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge","excerpt":"Jinwei Gu이 arXiv에 게시한 'Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge","tags":["Review","Embodied AI","Long-horizon Tasks","Vision-Language-Action Models (VLA)","BEHAVIOR Challenge","Offline RL","Pre-training","Rejection Sampling Fine-Tuning (RFT)","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Delin Qu, Qizhi Chen, Shangkun Sun, Zhaoshuo Li, YuWei Chao, Xiaohui Zeng, Xuan Li, Junjie Bai, TsungYi Lin, MingYu Liu 핵심 연구 목표 2025 BEHAVIOR Challenge에서 물리적 에이전트 가 시뮬레이션 환경에서 장"},{"id":"2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection","title":"[논문리뷰] Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection","excerpt":"arXiv에 게시된 'Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection","tags":["Review","Active Perception","Vision-Language Models (VLMs)","Embodied AI","View Selection","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Visual Question Answering (VQA)","3D Environments"],"text":"링크: 논문 PDF로 바로 열기 저자: Juil Koo, Daehyeon Choi, Sangwoo Youn, Phillip Y. Lee, Minhyuk Sung 핵심 연구 목표 본 논문은 정적인 이미지에 국한된 기존 VisionLanguage Models (VLMs) 의 Visual Question Answering (VQA) 한계를 극복하고, 앰뷸러토리 "},{"id":"2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans","title":"[논문리뷰] Towards Interactive Intelligence for Digital Humans","excerpt":"Yifei Huang이 arXiv에 게시한 'Towards Interactive Intelligence for Digital Humans' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans","tags":["Review","Digital Human","Interactive Intelligence","Multimodal Interaction","LLM Agent","Real-time Animation","Persona Fidelity","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifei Huang, Sitong Gong, Xiwei Gao, Xuangeng Chu, Yiyi Cai 핵심 연구 목표 본 논문은 기존의 모방적인 디지털 휴먼이 가지는 상호작용 논리 및 자율성 부족 문제를 해결하고, 개성정렬 표현, 적응적 상호작용, 자가 진화 능력 을 갖춘 '상호작용 지능(Interactive I"},{"id":"2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation","title":"[논문리뷰] Towards Scalable Pre-training of Visual Tokenizers for Generation","excerpt":"arXiv에 게시된 'Towards Scalable Pre-training of Visual Tokenizers for Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation","tags":["Review","Visual Tokenizers","Pre-training","Latent Diffusion Models","Generative Models","Vision Transformer","Contrastive Learning","Self-Supervised Learning","Scaling Laws"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingfeng Yao¹, Yuda Song², Yucong Zhou¹, Xinggang Wang¹† (¹Huazhong University of Science and Technology, ²MiniMax) 핵심 연구 목표 본 논문은 시각 토크나이저(예: VAE)의 잠재 공간이 저수준 정보에 편향되어 고품질 생성으로 "},{"id":"2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions","title":"[논문리뷰] V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions","excerpt":"Kwesi Cobbina이 arXiv에 게시한 'V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-16 00:00:00+0900+0900","permalink":"/ai/review/2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions","tags":["Review","Visual Reasoning","Multi-step Exploration","Chain-of-Questions (CoQ)","Vision-Language Models (VLMs)","Benchmarking","Planning","Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenrui Fan, Yijun Liang, Shweta Bhardwaj, Kwesi Cobbina, Ming Li, Tianyi Zhou 핵심 연구 목표 본 논문은 기존 VLM이 복잡하고 개방형인 시각 추론 태스크에서 다단계 탐색 및 동적 계획 수립에 어려움을 겪는 문제를 해결하고자 합니다. 대규모 탐색 공간으로 "},{"id":"2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning","title":"[논문리뷰] A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning","excerpt":"Hongfei Zhang이 arXiv에 게시한 'A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning","tags":["Review","Affordance Prediction","Zero-Shot Learning","Agentic AI","Foundation Models","Multimodal Reasoning","Visual Grounding","Image Generation","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Zixin Zhang, Kanghao Chen, Harold H. Chen, Chenfei Liao, Hanqing Wang, Hongfei Zhang, Litao Guo, YingCong Chen 핵심 연구 목표 이 논문은 고수준 추론과 저수준 그라운딩이 긴밀하게 결합된 기존 endtoend 어포던스 예측 모델들이 "},{"id":"2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference","title":"[논문리뷰] Janus: Disaggregating Attention and Experts for Scalable MoE Inference","excerpt":"arXiv에 게시된 'Janus: Disaggregating Attention and Experts for Scalable MoE Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference","tags":["Review","MoE Inference","Disaggregated Architecture","Resource Management","Scalability","Load Balancing","GPU Utilization","Communication Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhexiang Zhang1, Ye Wang1,2, Xiangyu Wang2, Yumiao Zhao³, Jingzhe Jiang¹, Qizhen Weng2, Shaohuai Shi⁴, Yin Chen², Minchen Yu¹† 핵심 연구 목표 본 연구는 대규모 MixtureofExperts (MoE) 모델 추론 시 발"},{"id":"2025-12-17-MMGR-Multi-Modal-Generative-Reasoning","title":"[논문리뷰] MMGR: Multi-Modal Generative Reasoning","excerpt":"Haozhe Zhao이 arXiv에 게시한 'MMGR: Multi-Modal Generative Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-MMGR-Multi-Modal-Generative-Reasoning","tags":["Review","Multi-Modal Generative Models","Reasoning Evaluation","World Models","Physical Commonsense","Abstract Reasoning","Embodied Navigation","VLM-based Evaluation","Temporal Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Haozhe Zhao, Haoyi Qiu, Zefan Cai, ZGZzz, SueMintony 핵심 연구 목표 본 논문은 대규모 텍스트투비디오 모델 평가의 한계, 특히 인지적 충실도를 넘어선 추론 능력 을 평가하는 문제를 해결하고자 합니다. 생성 모델이 단순히 시각적으로 그럴듯한 콘텐츠를 만드는 것을 넘어, 현실을 지"},{"id":"2025-12-17-Olmo-3","title":"[논문리뷰] Olmo 3","excerpt":"arXiv에 게시된 'Olmo 3' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-Olmo-3","tags":["Review","Large Language Models","Open-Source AI","Model Flow","Long-Context Reasoning","Instruction Following","Function Calling","Thinking Models","Data Curation","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, Jacob Morrison, Jake Poznanski, Kyle"},{"id":"2025-12-17-RecGPT-V2-Technical-Report","title":"[논문리뷰] RecGPT-V2 Technical Report","excerpt":"Dian Chen이 arXiv에 게시한 'RecGPT-V2 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-RecGPT-V2-Technical-Report","tags":["Review","Recommender Systems","Large Language Models","Multi-Agent Systems","Reinforcement Learning","Dynamic Prompting","Hybrid Representation","Agentic Evaluation","Explanation Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Dian Chen, Chao Yi, Zhujin Gao, Jiakai Tang, Haoyi Hu 핵심 연구 목표 RecGPTV2는 기존 RecGPTV1의 LLM 기반 추천 시스템 이 겪던 계산 비효율성, 설명 다양성 부족, 제한된 일반화 능력, 단순한 평가 방식의 네 가지 근본적인 한계를 해결하는 것을 목표로 합니다."},{"id":"2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement","title":"[논문리뷰] ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement","excerpt":"Zhaohe Liao이 arXiv에 게시한 'ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement","tags":["Review","Table Visualization","Infographic Generation","Multi-modal Large Language Models (MLLMs)","Diffusion Models","Self-Correction","Reinforcement Learning","Graphic Design","Data-to-Visual Mapping"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhihang Liu, Xiaoyi Bao, Pandeng Li, Junjie Zhou, Zhaohe Liao, Yefei He, Kaixun Jiang, ChenWei Xie, Yun Zheng, Hongtao Xie 핵심 연구 목표 논문은 기존 이미지 생성 및 통합 모델이 깊은 추론, 계획, 그리고 데이터시각 매핑"},{"id":"2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models","title":"[논문리뷰] Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models","excerpt":"arXiv에 게시된 'Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models","tags":["Review","Discrete Diffusion Models","Multimodal Models","Sparse Parameterization","KV Caching","Token Truncation","Image Generation","Image Editing","Visual Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shufan Li et al. 핵심 연구 목표 본 논문은 Masked Diffusion Models (MDMs)의 주요 비효율성, 즉 KV 캐싱 미지원 과 불필요한 마스크 토큰 처리 로 인한 느린 추론 속도 문제를 해결하고자 합니다. 특히, 멀티모달 태스크 전반에서 성능 저하 없이 효율성을 크게 향상시키는 새로운 모델"},{"id":"2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans","title":"[논문리뷰] Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?","excerpt":"Ming Hu이 arXiv에 게시한 'Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-17 00:00:00+0900+0900","permalink":"/ai/review/2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans","tags":["Review","AIGC Detection","ASMR Videos","VLM Evaluation","VGM Realism","Audio-Visual Consistency","Perceptual Fidelity","Adversarial Benchmark","Deepfake Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaqi Wang, Weijia Wu, Yi Zhan, Rui Zhao, Ming Hu, James Cheng, Wei Liu, Philip Torr, Kevin Qinghong Lin 핵심 연구 목표 본 논문은 최근 AI 생성 비디오의 높은 현실성으로 인해 야기되는 진위 판별 문제를 해결하고자 합니다. 특히 몰입감"},{"id":"2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning","title":"[논문리뷰] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning","excerpt":"arXiv에 게시된 'Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning","tags":["Review","Reinforcement Learning","Large Language Models","Exploration Strategy","Gradient-Guided","Reward Shaping","Reasoning","PPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu 핵심 연구 목표 본 논문은 LLM의 강화 학습(RL) 탐색 메커니즘이 모델의 실제 학습 방식과 근본적으로 일치하지 않는다는 문제를 제기합니다. 기존의 엔트로피 보너스나 "},{"id":"2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models","title":"[논문리뷰] DEER: Draft with Diffusion, Verify with Autoregressive Models","excerpt":"Zhijie Deng이 arXiv에 게시한 'DEER: Draft with Diffusion, Verify with Autoregressive Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models","tags":["Review","Speculative Decoding","Diffusion LLM","Autoregressive Model","Inference Acceleration","Model Alignment","Code Generation","Block Regeneration"],"text":"링크: 논문 PDF로 바로 열기 저자: Zicong Cheng, GuoWei Yang, Jia Li, Zhijie Deng, MengHao Guo, ShiMin Hu 핵심 연구 목표 본 논문은 autoregressive (AR) 디코딩의 내재된 지연으로 인해 발생하는 LLM 기반 에이전트 및 추론 시스템의 효율성 문제를 해결하고자 합니다. 특히, 기존 AR"},{"id":"2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models","title":"[논문리뷰] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models","excerpt":"arXiv에 게시된 'DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models","tags":["Review","Diffusion Models","Vision Language Models","Autoregressive Models","Diffusion Finetuning","Block Diffusion","Multimodal AI","KV Cache"],"text":"링크: 논문 PDF로 바로 열기 저자: Lunbin Zeng¹, Jingfeng Yao¹,, Bencheng Liao¹, Hongyuan Tao¹, Wenyu Liu¹, Xinggang Wang1,† 핵심 연구 목표 본 논문은 기존 확산 비전 언어 모델(dVLMs)의 성능 저하와 가변 길이 생성 및 KV 캐시 재사용의 비효율성 문제를 해결하고자 합니다. 특"},{"id":"2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing","title":"[논문리뷰] Fast and Accurate Causal Parallel Decoding using Jacobi Forcing","excerpt":"Tajana Rosing이 arXiv에 게시한 'Fast and Accurate Causal Parallel Decoding using Jacobi Forcing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing","tags":["Review","Parallel Decoding","Causal LLM","Jacobi Decoding","Consistency Distillation","Transformer Inference","Latency Reduction","Rejection Recycling","Multi-block Decoding"],"text":"링크: 논문 PDF로 바로 열기 저자: Lanxiang Hu, Siqi Kou, Yichao Fu, Samyam Rajbhandari, Tajana Rosing, Yuxiong He, Zhijie Deng, Hao Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 순차적(autoregressive, AR) 디코딩으로 인한 높은 지연 시간을 "},{"id":"2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices","title":"[논문리뷰] HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices","excerpt":"Yuhang Dong이 arXiv에 게시한 'HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices","tags":["Review","Multimodal Large Language Model","Edge AI","Efficient Inference","Visual Resolution Compressor","Dual Consistency Learning","Vision Transformer","Quantization","Low-Latency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhang Dong, Zhiqiang Xia, Kaiyang Han, Yuchen Liu, HyperAI Team 핵심 연구 목표 현재 멀티모달 대규모 언어 모델(MLLM)이 가진 높은 연산 및 메모리 요구사항으로 인한 온디바이스 배포의 어려움을 해결하는 것을 목표로 합니다. 특히 표준 Vision Transform"},{"id":"2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning","title":"[논문리뷰] IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning","excerpt":"arXiv에 게시된 'IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning","tags":["Review","Video VFX Editing","In-Context Learning","Diffusion Transformers","Few-Shot Learning","LoRA","Spatiotemporal Tokenization","Instruction-Guided"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuanhang Li, Yiren Song, Junzhe Bai, Xinran Liang, Hu Yang, Libiao Jin, Qi Mao 핵심 연구 목표 논문은 기존 비디오 편집 모델이 겪는 배경 무결성 유지, 제한된 데이터에서의 효과 학습, 픽셀 수준 일관성 부족 등의 문제를 해결하여, 텍스트 지시에 따라 정확하"},{"id":"2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training","title":"[논문리뷰] In Pursuit of Pixel Supervision for Visual Pre-training","excerpt":"Dong Wang이 arXiv에 게시한 'In Pursuit of Pixel Supervision for Visual Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training","tags":["Review","Pixel Supervision","Self-Supervised Learning","Masked Autoencoders (MAE)","Visual Pre-training","Foundation Models","Representation Learning","Web-Scale Data","Computer Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Dong Wang, Xinjie Lei, Yang Li, ShangWen Li, Lihe Yang 핵심 연구 목표 본 논문은 기존 자기 지도 학습(SelfSupervised Learning) 패러다임이 잠재 공간 목표(latentspace objectives)에 의존하거나 과도한 휴먼 큐레이션을 통해 편향을 도입하는 "},{"id":"2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","title":"[논문리뷰] MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence","excerpt":"Peizhou Cao이 arXiv에 게시한 'MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","tags":["Review","Video-Based Spatial Intelligence","MLLM Benchmark","Spatial Reasoning","Multi-Modal Learning","Perception","Planning","Prediction","Cross-Video Reasoning","Human-AI Gap"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingli Lin, Runsen Xu, Shaohao Zhu, Sihan Yang, Peizhou Cao, Yunlong Ran, Miao Hu, Chenming Zhu, Yiman Xie, Yilin Long, Wenbo Hu, Dahua Lin, Tai Wang, Jiangmiao Pang 핵심 연구 목표 본 논"},{"id":"2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition","title":"[논문리뷰] Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition","excerpt":"Xiao Xu이 arXiv에 게시한 'Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition","tags":["Review","Image Editing","Diffusion Models","Layer Decomposition","RGBA Layers","Variational Autoencoder (VAE)","Multi-stage Training","Photoshop Documents (PSD)","Inherent Editability"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengming Yin, Zekai Zhang, Zecheng Tang, Kaiyuan Gao, Xiao Xu, Kun Yan, Jiahao Li, Yilei Chen, Yuxiang Chen, HeungYeung Shum, Lionel M. Ni, Jingren Zhou, Junyang Lin, Chenfei Wu"},{"id":"2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content","title":"[논문리뷰] Robust and Calibrated Detection of Authentic Multimedia Content","excerpt":"arXiv에 게시된 'Robust and Calibrated Detection of Authentic Multimedia Content' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content","tags":["Review","Deepfake Detection","Content Authenticity","Generative Models","Adversarial Robustness","Image Inversion","Plausible Deniability","Diffusion Models","Multimedia Forensics"],"text":"링크: 논문 PDF로 바로 열기 저자: Sarim Hashmi, Abdelrahman Elsayed, Mohammed Talha Alam, Samuele Poppi, Nils Lukas 핵심 연구 목표 본 논문은 기존 딥페이크 탐지 방법론의 한계, 즉 생성 모델의 재합성 가능성(resynthesis indistinguishability) 으로 인한 높은 오"},{"id":"2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning","title":"[논문리뷰] SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning","excerpt":"arXiv에 게시된 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning","tags":["Review","Video Reasoning","Reinforcement Learning","Multi-Turn Reasoning","Agent System","Long Videos","Synthetic Data","Any-Horizon Reasoning","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jitesh Jain, Jialuo Li, Zixian Ma, Jieyu Zhang, Chris Dongjoo Kim, Sangho Lee, Rohun Tripathi, Tanmay Gupta, Christopher Clark, Humphrey Shi 핵심 연구 목표 본 논문은 기존 SOTA 비디오 추론 모델이 단일 "},{"id":"2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness","title":"[논문리뷰] SCOPE: Prompt Evolution for Enhancing Agent Effectiveness","excerpt":"Yunhe Wang이 arXiv에 게시한 'SCOPE: Prompt Evolution for Enhancing Agent Effectiveness' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness","tags":["Review","LLM Agents","Prompt Optimization","Context Management","Online Learning","Agent Effectiveness","Self-Evolving Prompts","Trace-Based Learning","Dual-Stream Routing"],"text":"링크: 논문 PDF로 바로 열기 저자: Zehua Pei, HuiLing Zhen, Shixiong Kai, Sinno Jialin Pan, Yunhe Wang, Mingxuan Yuan, Bei Yu 핵심 연구 목표 대규모 언어 모델(LLM) 에이전트가 방대한 동적 컨텍스트에 직면했을 때 정적인 프롬프트로 인해 발생하는 '수정(Corrective)' 및 "},{"id":"2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning","title":"[논문리뷰] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning","excerpt":"arXiv에 게시된 'Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning","tags":["Review","AI-Generated Video Detection","Multimodal Large Language Model (MLLM)","Artifact Reasoning","Explainable AI","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Video Forensics"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifei Li, Wenzhao Zhen†, Yanran Zhang, Runze Sun, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu 핵심 연구 목표 본 논문은 기존의 AI 생성 비디오 탐지 모델이 이진 분류에만 초점을 맞추고 설명 가능성이 부족하다는 한계를 해결하고자 합니다. 인간이 인지할 "},{"id":"2025-12-18-Step-GUI-Technical-Report","title":"[논문리뷰] Step-GUI Technical Report","excerpt":"arXiv에 게시된 'Step-GUI Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-Step-GUI-Technical-Report","tags":["Review","GUI Automation","Self-Evolving Pipeline","Reinforcement Learning","Multimodal LLMs","Privacy-Preserving AI","Human-Computer Interaction","Model Context Protocol","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: GELabTeam, StepFun 핵심 연구 목표 논문은 GUI 자동화 분야에서 고품질 훈련 데이터를 효율적이고 신뢰성 있게 확보하는 근본적인 문제를 해결하고자 합니다. 또한, 이종 기기 간의 표준화된 인터페이스를 구축하여 사용자 개인 정보를 보호하고, 실제 일상적인 사용 패턴에 기반한 평가 벤치마크를 통해 에이전트의"},{"id":"2025-12-18-Universal-Reasoning-Model","title":"[논문리뷰] Universal Reasoning Model","excerpt":"arXiv에 게시된 'Universal Reasoning Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-Universal-Reasoning-Model","tags":["Review","Universal Transformer","Recurrent Neural Networks","ARC-AGI","Reasoning Tasks","Nonlinearity","Convolutional Gating","Truncated Backpropagation","Model Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zitian Gao, Lynx Chen, Yihao Xiao, He Xing, Ran Tao, Haoming Luo, Joey Zhou, Bryan Dai 핵심 연구 목표 본 연구는 Universal Transformer (UT) 모델이 ARCAGI 와 같은 복잡한 추론 태스크에서 성능 향상을 보이는 근본적인 원인을 "},{"id":"2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression","title":"[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?","excerpt":"arXiv에 게시된 'VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression","tags":["Review","Vision-Text Compression (VTC)","Long Context Understanding","Vision-Language Models (VLMs)","Benchmark","Information Retrieval","Associative Reasoning","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 컨텍스트 창 확장과 관련된 계산 및 메모리 오버헤드 문제를 해결하기 위해 VisionTe"},{"id":"2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory","title":"[논문리뷰] WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory","excerpt":"Sung Won Han이 arXiv에 게시한 'WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-18 00:00:00+0900+0900","permalink":"/ai/review/2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory","tags":["Review","AIS data","vessel destination estimation","deep learning","transformer","channel attention","trajectory analysis","Gradient Dropout","maritime surveillance"],"text":"링크: 논문 PDF로 바로 열기 저자: Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Dongil Park, Sung Won Han 핵심 연구 목표 이 논문은 기존의 AIS 데이터 기반 선박 목적지 예측 모델 이 직면했던 ROI(관심 지역) 제약 , 불규칙한 시공간적 데이터 로 인한 편향 문제, 그리고 장거리 예측 능력 부족 "},{"id":"2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos","title":"[논문리뷰] AdaTooler-V: Adaptive Tool-Use for Images and Videos","excerpt":"Zhixun Li이 arXiv에 게시한 'AdaTooler-V: Adaptive Tool-Use for Images and Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos","tags":["Review","Multimodal LLM","Adaptive Tool-Use","Reinforcement Learning","Chain-of-Thought","Vision-Language Models","Visual Reasoning","AT-GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue 핵심 연구 목표 본 논문은 기존 멀티모달 대규모 언어 모델(ML"},{"id":"2025-12-19-Adaptation-of-Agentic-AI","title":"[논문리뷰] Adaptation of Agentic AI","excerpt":"Zhiyi Shi이 arXiv에 게시한 'Adaptation of Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Adaptation-of-Agentic-AI","tags":["Review","Agentic AI","Adaptation","Agent Adaptation","Tool Adaptation","Reinforcement Learning","Fine-tuning","Modular AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengcheng Jiang, Jiacheng Lin, Zhiyi Shi, Zifeng Wang, Luxi He, Yichen Wu, Ming Zhong, Peiyang Song, Qizheng Zhang, Heng Wang, Xueqiang Xu, Hanwen Xu, Pengrui Han, Dylan Zhang, J"},{"id":"2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection","title":"[논문리뷰] Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection","excerpt":"Jiarong Ou이 arXiv에 게시한 'Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection","tags":["Review","Text-to-Image","Data Selection","Meta-Learning","Meta-Gradient","Data Efficiency","Generative Models","Coreset Selection","Data Pruning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao 핵심 연구 목표 TexttoImage(T2I) 생성 모델(예: Imagen, Stable Diffusion, FLUX)의 훈련 효율성을 개선하고 시각적 품질"},{"id":"2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers","title":"[논문리뷰] DeContext as Defense: Safe Image Editing in Diffusion Transformers","excerpt":"arXiv에 게시된 'DeContext as Defense: Safe Image Editing in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers","tags":["Review","Diffusion Transformers","Image Editing","Privacy Protection","Adversarial Attack","Attention Mechanism","Identity Preservation","Deepfake Defense","In-context Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Linghui Shen, Mingyue Cui, Xingyi Yang 핵심 연구 목표 본 논문은 대규모 Diffusion Transformer(DiT) 기반 이미지 편집 모델 의 심각한 프라이버시 문제를 해결하고자 합니다. 사용자의 동의 없이 개인 이미지가 신원 사칭, 허위 정보 생성 등 악의적인 목적으로 조작되는 것"},{"id":"2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation","title":"[논문리뷰] Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation","excerpt":"Wenxuan Lu이 arXiv에 게시한 'Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation","tags":["Review","Panoramic Depth Estimation","Foundation Model","Semi-Supervised Learning","Pseudo-Labeling","Data-in-the-Loop","DINOv3","Metric Depth","360-degree Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, MingHsuan Yang, Truong Nguyen, Lu Qi 핵심 연구 목표 본 연구는 파노라마 깊이 추정의 핵심 과제인 다양한 장면과 거리에서의 일반화 및 측정 일관성 부족 문제를 해결하는 것을 "},{"id":"2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification","title":"[논문리뷰] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification","excerpt":"arXiv에 게시된 'Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification","tags":["Review","MLLM","Model Auditing","Capability Gaps","Failure Mode Discovery","Reinforcement Learning","Data Rectification","Counterfactual Generation","VQA"],"text":"링크: 논문 PDF로 바로 열기 저자: Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, WenSheng Chu 핵심 연구 목표 본 논문은 기존 MLLM 평가 방법론의 해석력 부족 과 중요한 능력 격차를 포착하지 못하는 한계 를 해결하고자 합니다. 특히 모델의 고질적인 약점 과 실패 모드 를 자동으로 식별하고 해석하며"},{"id":"2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward","title":"[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward","excerpt":"arXiv에 게시된 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward","tags":["Review","Reinforcement Learning","Large Language Models","Exploration-Exploitation","Clipping","Policy Entropy","Spurious Rewards","Mathematical Reasoning","RLVR"],"text":"링크: 논문 PDF로 바로 열기 저자: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin 핵심 연구 목표 RLVR(Reinforcement Learning with Verifiable Rewards) 환경에서 탐색활용 트레이드오프 를 재해석하고, 특히 클리핑(clipping), 정책 엔트로"},{"id":"2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","title":"[논문리뷰] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction","excerpt":"arXiv에 게시된 'FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","tags":["Review","Portrait Animation","Diffusion Models","Inference Acceleration","Identity Preservation","Video Generation","Latent Prediction","Sliding Window"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu 핵심 연구 목표 본 논문은 확산 모델 기반의 기존 장시간 인물 애니메이션 방법론이 겪는 신원(ID) 불일치 및 높은 추론 지연 시간 문제"},{"id":"2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering","title":"[논문리뷰] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering","excerpt":"Hendrik P. A. Lensch이 arXiv에 게시한 'FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering","tags":["Review","Neural Rendering","Diffusion Models","G-Buffer","Autoregressive Generation","Temporal Consistency","ControlNet","ControlLoRA","Interactive Applications"],"text":"링크: 논문 PDF로 바로 열기 저자: Ole Beisswenger, JanNiklas Dihlmann, Hendrik Lensch 핵심 연구 목표 본 논문은 인터랙티브 애플리케이션을 위한 Gbuffer 조건부 신경망 포워드 프레임 렌더링에서 시간적 일관성 을 유지하는 동시에 사실적인 이미지를 프레임별로 자동회귀적으로 생성 하는 문제를 해결하는 것을 목표로"},{"id":"2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image","title":"[논문리뷰] Generative Refocusing: Flexible Defocus Control from a Single Image","excerpt":"Yu-Lun Liu이 arXiv에 게시한 'Generative Refocusing: Flexible Defocus Control from a Single Image' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image","tags":["Review","Generative AI","Image Refocusing","Defocus Deblurring","Bokeh Synthesis","Depth of Field Control","Semi-Supervised Learning","Diffusion Models","Aperture Shape Control"],"text":"링크: 논문 PDF로 바로 열기 저자: ChunWei Tuan Mu, JiaBin Huang, YuLun Liu 핵심 연구 목표 본 논문은 단일 이미지로부터 촬영 후 유연한 초점 및 심도 제어를 가능하게 하는 생성적 리포커싱(Generative Refocusing) 시스템을 개발하는 것을 목표로 합니다. 특히, 기존 방법론이 겪는 모든 초점 입력 의존성, "},{"id":"2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs","title":"[논문리뷰] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs","excerpt":"Carlos Escolano이 arXiv에 게시한 'Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs","tags":["Review","Speech-to-Text Translation","Multimodal LLMs","Speech Foundation Models","Cascaded Systems","Benchmarking","Speech Modality Integration","Robustness","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Carlos Escolano, Vilém Zouhar, Zachary Hopton, javi8979, spapi 핵심 연구 목표 이 논문은 음성 양식이 LLM(Large Language Model) 에 직접 통합될 때 음성텍스트 번역(ST) 품질이 향상되는지, 아니면 기존의 계단식(cascaded) 또는 직접(dire"},{"id":"2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language","title":"[논문리뷰] Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language","excerpt":"arXiv에 게시된 'Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language","tags":["Review","Time Series Analysis","Multimodal Language Models","Natural Language Generation","Dataset Creation","Instruction Tuning","GPT-4","LLaVA","Cross-Domain Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunkai Zhang, Kezhen Chen, Ruian Ge, Amine Jelloul, Yawen Zhang, Ming Zheng, Chongyang Gao, Siyuan Teng, Jinmeng Rao, ChiangWei Fang, Jie Yang 핵심 연구 목표 본 논문은 시계열 데이터로부터 통찰력을 추출하는"},{"id":"2025-12-19-Kling-Omni-Technical-Report","title":"[논문리뷰] Kling-Omni Technical Report","excerpt":"arXiv에 게시된 'Kling-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Kling-Omni-Technical-Report","tags":["Review","Video Generation","Multimodal Visual Language","Generative AI","Video Editing","Reasoning-enhanced Generation","Diffusion Transformer","Multi-modal World Simulators"],"text":"링크: 논문 PDF로 바로 열기 저자: Kling Team, Kuaishou Technology 핵심 연구 목표 논문은 단편적인 비디오 생성, 편집, 추론 태스크들을 통합하여 멀티모달 시각 언어(MVL) 입력 으로부터 고품질 비디오를 직접 합성하는 범용 생성 프레임워크인 KlingOmni 를 개발하는 것을 목표로 합니다. 기존 모델들의 한계인 단일 태스크 "},{"id":"2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image","title":"[논문리뷰] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image","excerpt":"arXiv에 게시된 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image","tags":["Review","Reward Models","Multimodal LLMs","Benchmark","Text-to-Image Generation","Image Editing","Interleaved Generation","Multimodal Reasoning","MLLM-as-a-judge"],"text":"링크: 논문 PDF로 바로 열기 저자: Yushi Hu, Reyhane AskariHemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad 핵심 연구 목표 본 논문은 이미지와 텍스트가 혼합된 시퀀스를 처리하는 옴니 모델(Omni Models)을 위한 보상 모델(Reward Model"},{"id":"2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models","title":"[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models","excerpt":"arXiv에 게시된 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models","tags":["Review","3D Grounding","Spatial Reasoning","Vision-Language Models","Depth Estimation","3D Object Detection","Chain-of-Thought","Data Generation","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxin Wang, Lei Ke, Boqiang Zhang, Zhenpeng Huang, Meng Yu, Tianyuan Qu, Hanxun Yu, Dan Xu, Dong Yu 핵심 연구 목표 본 연구는 기존 멀티모달 모델이 2D 이미지에 의존하여 3D 공간 이해 능력이 부족하다는 한계를 해결하는 것을 목표로 합니다"},{"id":"2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners","title":"[논문리뷰] Next-Embedding Prediction Makes Strong Vision Learners","excerpt":"arXiv에 게시된 'Next-Embedding Prediction Makes Strong Vision Learners' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners","tags":["Review","Self-supervised Learning","Generative Pretraining","Vision Transformer","Next-Embedding Prediction","Autoregressive Model","Image Classification","Semantic Segmentation","Causal Masking"],"text":"링크: 논문 PDF로 바로 열기 저자: Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu 핵심 연구 목표 본 논문은 자연어 처리 분야의 생성적 사전 훈련(generative pretraining) 성공 사례에서 영감을 받아, 다음 "},{"id":"2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion","title":"[논문리뷰] REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion","excerpt":"Giorgos Sfikas이 arXiv에 게시한 'REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion","tags":["Review","Latent Diffusion Models","Vision Foundation Models","Semantic Compression","Global-Local Semantics","Image Generation","Representation Entanglement","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Giorgos Petsangourakis, Christos Sgouropoulos, Bill Psomas, Theodoros Giannakopoulos, Giorgos Sfikas, Ioannis Kakogeorgiou 핵심 연구 목표 본 논문은 최신 이미지 생성 모델인 Latent Diffusion Models (L"},{"id":"2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing","title":"[논문리뷰] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing","excerpt":"Yuqi Liu이 arXiv에 게시한 'RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing","tags":["Review","Image Editing","Vision-Language Models","Diffusion Models","Region-aligned Guidance","Reinforcement Learning","Instruction-Visual Complexity","Attention Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyuan Qu, Lei Ke, Xiaohang Zhan, Longxiang Tang, Yuqi Liu, Bohao Peng, Bei Yu, Dong Yu, Jiaya Jia 핵심 연구 목표 본 논문은 기존 지시 기반 이미지 편집 모델들이 InstructionVisual Complexity (IVComplexit"},{"id":"2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model","title":"[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model","excerpt":"arXiv에 게시된 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model","tags":["Review","Audio-Visual Generation","Diffusion Transformer","Multimodal AI","Speech Synchronization","Video Generation","Reinforcement Learning from Human Feedback","Inference Acceleration"],"text":"링크: 논문 PDF로 바로 열기 저자: ByteDance Seed Team 핵심 연구 목표 본 논문은 오디오와 비디오를 통합적으로 생성하는 기반 모델(foundation model) 인 Seedance 1.5 pro를 소개합니다. 이 모델은 탁월한 오디오비디오 동기화 및 최고 수준의 생성 품질 을 달성하며, 네이티브 조인트 오디오비디오 생성 을 위한 전문적"},{"id":"2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","title":"[논문리뷰] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors","excerpt":"arXiv에 게시된 'StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","tags":["Review","Monocular-to-Stereo Conversion","Video Generation","Diffusion Models","Feed-Forward Architecture","Domain Switcher","Cycle Consistency","Unified Dataset","Depth Ambiguity"],"text":"링크: 논문 PDF로 바로 열기 저자: Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang, Donghao Zhou, Zhen Yang, Luozhou Wang, Xin Tao, YingCong Chen 핵심 연구 목표 본 논문은 스테레오 비디오 변환 시 기존의 다단계 “DepthWarpInpaint” (D"},{"id":"2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","title":"[논문리뷰] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text","excerpt":"arXiv에 게시된 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","tags":["Review","World Models","Video Generation","Multimodal Control","Trajectory Guidance","Reference Images","Promptable Events","Cross-Attention","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Che"},{"id":"2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks","title":"[논문리뷰] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks","excerpt":"arXiv에 게시된 'VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-19 00:00:00+0900+0900","permalink":"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks","tags":["Review","GUI Grounding","Multi-Platform","Benchmark","MLLM","Hierarchical Evaluation","Human-in-the-Loop Annotation","GUI Agents","Multilingual Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Beitong Zhou, Zhexiao Huang, Yuan Guo, Zhangxuan Gu, Tianyu Xia, Zichen Luo, Fei Tang, Dehan Kong, Yanyi Shang, Suling Ou, Zhenlin Guo, Changhua Meng, Shuheng Shen 핵심 연구 목표 기존 GU"},{"id":"2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework","title":"[논문리뷰] 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework","excerpt":"Hendrik P. A. Lensch이 arXiv에 게시한 '3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework","tags":["Review","3D Reconstruction","Generative AI","Indoor Scenes","Compositional Framework","Differentiable Rendering","Image-to-3D","VFX","Game Development"],"text":"링크: 논문 PDF로 바로 열기 저자: Tobias Sautter, JanNiklas Dihlmann, Hendrik Lensch 핵심 연구 목표 본 논문은 단일 2D 이미지로부터 시각 효과(VFX) 및 게임 개발에 즉시 활용 가능한, 수정 가능한 생산 준비 완료(productionready) 3D 텍스처 메시 장면 을 재구성하는 것을 목표로 합니다. 기존"},{"id":"2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation","title":"[논문리뷰] 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation","excerpt":"arXiv에 게시된 '4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation","tags":["Review","Multimodal LLMs","4D Understanding","Perceptual Distillation","Region-level VQA","Video Question Answering","Temporal Perception","Depth Perception"],"text":"링크: 논문 PDF로 바로 열기 저자: ChiaoAn Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, YuChiang Frank Wang, MinHung Chen 핵심 연구 목표 본 논문은 기존 MLLM이 3D 구조와 시간적 역학(4D)을 추론하는 능력이 부족하며, 특히 4D"},{"id":"2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","title":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges","excerpt":"arXiv에 게시된 'An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","tags":["Review","Vision-Language-Action Models","Embodied Intelligence","Robotics","Foundation Models","Multi-modal Learning","Reinforcement Learning","Sim-to-Real Transfer","Human-Robot Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Chao Xu, Suyu Zhang, Yang Liu, Baigui Sun, Weihong Chen, Bo Xu, Qi Liu, Juncheng Wang, Shujun Wang, Shan Luo, Jan Peters, Athanasios V. Vasilakos, Stefanos Zafeiriou, Jiankang De"},{"id":"2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge","title":"[논문리뷰] Are We on the Right Way to Assessing LLM-as-a-Judge?","excerpt":"arXiv에 게시된 'Are We on the Right Way to Assessing LLM-as-a-Judge?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge","tags":["Review","LLM-as-a-Judge","Evaluation Metrics","Consistency","Robustness","Positional Bias","Transitivity","Situational Preference","Multi-agent Systems"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuanning Feng, Sinan Wang, Zhengxiang Cheng, Yao Wan, Dongping Chen 핵심 연구 목표 본 논문은 현재 LLMasaJudge 평가 방법론이 인간 주석에 과도하게 의존하여 발생하는 편향, 불일치성, 확장성 문제를 해결하고자 합니다. 인간 주석 없는 내재적(intrinsi"},{"id":"2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing","title":"[논문리뷰] Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing","excerpt":"arXiv에 게시된 'Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing","tags":["Review","Text-to-Image Generation","Image Editing","Representation Encoders","Latent Diffusion Models","Variational Autoencoder (VAE)","Semantic Reconstruction","Off-manifold Latents","DINOv2"],"text":"링크: 논문 PDF로 바로 열기 저자: Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo 핵심 연"},{"id":"2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation","title":"[논문리뷰] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation","excerpt":"arXiv에 게시된 'GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation","tags":["Review","Visual Grounding","MLLMs","Benchmark","Multi-Dimensional Evaluation","Rejection Capability","Test-Time Scaling","Data Mixture Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, Jingyuan Ma, Zhifang Sui, Fuli Luo 핵심 연구 목표 본 연구는 기존 벤치마크에서 MLLM(Mult"},{"id":"2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering","title":"[논문리뷰] HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering","excerpt":"arXiv에 게시된 'HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering","tags":["Review","Video Question Answering","Multi-evidence Integration","Video-LLMs","Benchmark","Temporal Reasoning","Frame Selection","Evidential Requirement","MRFS"],"text":"링크: 논문 PDF로 바로 열기 저자: Dan BenAmi, Gabriele Serussi, Kobi Cohen, Chaim Baskin 핵심 연구 목표 기존 VideoQA 벤치마크가 단일 단서나 언어 사전 지식에 의존하는 경향이 있어 다중 증거 통합 능력을 제대로 평가하지 못하는 문제를 해결하고자 합니다. HERBench 라는 새로운 벤치마크를 통해 시간"},{"id":"2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents","title":"[논문리뷰] Meta-RL Induces Exploration in Language Agents","excerpt":"Maria Brbic이 arXiv에 게시한 'Meta-RL Induces Exploration in Language Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents","tags":["Review","Meta-RL","LLM Agents","Exploration","Reinforcement Learning","Policy Adaptation","In-context Learning","Self-reflection","Multi-episode tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbić 핵심 연구 목표 본 논문은 기존 강화 학습(RL) 기반의 대규모 언어 모델(LLM) 에이전트가 환경에서 능동적인 탐색과 시행착오 경험으로부터 효율적인 정책 적응에 어려움을 겪는 문제를 해결하고자 합"},{"id":"2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence","title":"[논문리뷰] PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence","excerpt":"arXiv에 게시된 'PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence","tags":["Review","Egocentric Data","Physical Intelligence","VLM","Robot Control","Embodied AI","VQA Supervision","Human-Robot Interaction","Zero-shot Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, Kai Chen 핵심 연구 목표 본 연구는 시점 불일치 문제로 인해 로봇 일반화에 한계가 있는 기존 VLM"},{"id":"2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers","title":"[논문리뷰] Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers","excerpt":"arXiv에 게시된 'Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers","tags":["Review","Language Models","Transformer Architecture","Canon Layers","Synthetic Pretraining","Reasoning Depth","Linear Attention","State-Space Models","NoPE"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyuan AllenZhu 핵심 연구 목표 언어 모델 아키텍처 간의 성능 차이를, 특히 학술 규모의 사전 훈련에서 발생하는 높은 노이즈와 비용 문제 없이 신뢰성 있게 평가하고 이해하는 것을 목표로 합니다. 이를 위해, 핵심 모델 역량을 격리하고 평가할 수 있는 제어된 합성 사전 훈련 태스크 를 도입하고, 토큰 간 수"},{"id":"2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows","title":"[논문리뷰] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows","excerpt":"Yuhao Zhou이 arXiv에 게시한 'Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows","tags":["Review","Scientific General Intelligence (SGI)","LLMs","Benchmarking","Scientist-Aligned Workflows","Practical Inquiry Model","Multi-modal Reasoning","Code Generation","Test-Time Reinforcement Learning (TTRL)"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhao Zhou, SciYu, VitaCoco, BoKelvin, CoCoOne 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 과학적 일반 지능(SGI) 평가를 위한 체계적인 프레임워크와 정의가 부족하다는 문제를 해결합니다. 과학적 탐구의 완전하고 반복적인 주기를 자율적으로 탐색하고 수행하는 AI의 능력을"},{"id":"2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras","title":"[논문리뷰] RadarGen: Automotive Radar Point Cloud Generation from Cameras","excerpt":"Or Litany이 arXiv에 게시한 'RadarGen: Automotive Radar Point Cloud Generation from Cameras' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras","tags":["Review","Radar Point Cloud Generation","Diffusion Models","Camera-to-Radar","BEV Representation","Autonomous Driving","Multi-modal Generative Models","Scene Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany 핵심 연구 목표 본 연구는 자동차 레이더 포인트 클라우드 생성이 지닌 고유한 데이터 특성(희소성, 무질서성, RCS/Doppler 속성)으로 인한 어려움을 해결하고자 합니다. 기존 시뮬레이터가 "},{"id":"2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding","title":"[논문리뷰] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding","excerpt":"Runtao Liu이 arXiv에 게시한 'Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding","tags":["Review","Multimodal Large Language Models (MLLMs)","Visual Degradation","Robustness","Reasoning Chains","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Degradation-Aware Reasoning","Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Runtao Liu, Xiaogang Xu, Wei Wei, Jianmin Chen, Jiaqihkust 핵심 연구 목표 본 논문은 Multimodal Large Language Models (MLLMs)가 실제 환경의 극심한 시각적 열화(visual degradations) 조건에서 성능이 크게 저하되는 문제를 해결"},{"id":"2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories","title":"[논문리뷰] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories","excerpt":"arXiv에 게시된 'SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories","tags":["Review","Software Engineering Benchmarks","Large Language Models (LLMs)","Code Generation","Automated Benchmark Generation","Multilingual","GitHub Pull Requests","Test Oracle","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe 핵심 연구 목표 이 논문은 기존의 LLM 기반 소프트웨어 엔지니어링 벤"},{"id":"2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience","title":"[논문리뷰] Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience","excerpt":"arXiv에 게시된 'Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience","tags":["Review","Formal Theorem Proving","Large Language Models","Reinforcement Learning","Agentic Prover","Lean Theorem Prover","Mathematical Reasoning","Test-Time Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: ByteDance Seed AI4Math 핵심 연구 목표 본 논문은 학부 및 대학원 수준 이상의 수학 문제에 대한 형식적 정리 증명(Formal Theorem Proving)의 효율성과 성능을 개선하는 것을 목표로 합니다. 특히, LLM 기반의 형식적 증명에서 나타나는 높은 계산 비용과 도전 과제를 해결하며, 자연어 "},{"id":"2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models","title":"[논문리뷰] StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models","excerpt":"arXiv에 게시된 'StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models","tags":["Review","Visual Autoregressive Models","Image Generation","Model Acceleration","Low-Rank Approximation","Semantic Irrelevance","Stage-Aware Optimization","Text-to-Image Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Senmao Li, Kai Wang, Salman Khan, Fahad Shahbaz Khan, Jian Yang, Yaxing Wang 핵심 연구 목표 Visual Autoregressive (VAR) 모델은 고품질 이미지 생성을 가능하게 하지만, 특히 대규모 스케일 단계에서 상당한 연산 복잡도와 긴 런타임으로 어려"},{"id":"2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs","title":"[논문리뷰] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs","excerpt":"Lihong Li이 arXiv에 게시한 'Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs","tags":["Review","Multi-Turn Reinforcement Learning","LLM Agents","Proximal Policy Optimization (PPO)","Turn-Level MDP","Advantage Estimation","Generative AI","Deep Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Junbo Li, Peng Zhou, Rui Meng, Meet P. Vadera, Lihong Li, Yang Li 핵심 연구 목표 본 논문은 멀티턴 LLM 에이전트 학습에서 기존 GRPO(Group Relative Policy Optimization) 의 불안정성과 비효율성을 해결하고자 합니다. 특히 긴 추론이 필"},{"id":"2025-12-22-When-Reasoning-Meets-Its-Laws","title":"[논문리뷰] When Reasoning Meets Its Laws","excerpt":"Liu Ziyin이 arXiv에 게시한 'When Reasoning Meets Its Laws' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-22 00:00:00+0900+0900","permalink":"/ai/review/2025-12-22-When-Reasoning-Meets-Its-Laws","tags":["Review","Large Reasoning Models","Reasoning Behaviors","Compute Law","Accuracy Law","Monotonicity","Compositionality","Fine-tuning","LORE-BENCH"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang 핵심 연구 목표 이 논문은 대규모 추론 모델(LRMs) 의 비직관적이고 최적화되지 않은 추론 행동을 체계적으로 이론화하고, 바람직한 추론 패턴을 특성화하기 위한"},{"id":"2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States","title":"[논문리뷰] Brain-Grounded Axes for Reading and Steering LLM States","excerpt":"Sandro Andric이 arXiv에 게시한 'Brain-Grounded Axes for Reading and Steering LLM States' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States","tags":["Review","LLM Interpretability","Brain-Grounded AI","MEG","Phase-Locking Value","ICA","LLM Steering","Neural Decoding","Latent Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Sandro Andric 핵심 연구 목표 본 연구는 LLM(대규모 언어 모델)의 해석 가능성 방향이 종종 외부 접지(external grounding)가 부족하다는 문제에 주목합니다. 이를 해결하기 위해 인간의 뇌 활동을 LLM의 내부 상태를 해석하고 조종하기 위한 안정적이고 외부적으로 접지된 좌표계로 정의하는 것을 "},{"id":"2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction","title":"[논문리뷰] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction","excerpt":"Hong Jiao이 arXiv에 게시한 'Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction","tags":["Review","Large Language Models","Item Difficulty Prediction","Human-AI Alignment","Proficiency Simulation","Metacognition","Curse of Knowledge","Educational Assessment","Zero-shot Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao, Tianyi Zhou 핵심 연구 목표 본 논문은 LLM이 인간이 인지하는 문항(질문 또는 과제) 난이도를 정확하게 예측할 수 있는지, 특히 초기 데이터 부족 문제(coldstart problem) 상황에서 인간AI 난이도 정"},{"id":"2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI","title":"[논문리뷰] DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI","excerpt":"arXiv에 게시된 'DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI","tags":["Review","LLM Data Preparation","Workflow Automation","Data-Centric AI","Synthetic Data","Multi-Agent System","Framework","Reproducibility"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Liang, Xiaochen Ma, Zhou Liu, Zhen Hao Wong, Zhengyang Zhao, Zimo Meng, Runming He, Chengyu Shen, Qifeng Cai, Zhaoyang Han, Meiyi Qiang, Yalin Feng, Tianyi Bai, Zewei Pan, Zi"},{"id":"2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital","title":"[논문리뷰] Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital","excerpt":"arXiv에 게시된 'Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital","tags":["Review","Legal AI","Venture Capital","Due Diligence","Capitalization Table","Multi-document Reasoning","Knowledge Graph","World Model","Neuro-Symbolic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Pierre Colombo, Malik Boudiaf, Allyn Sweet, Michael Desa, Hongxi Wang, Kevin Candra, Syméon del Marmol 핵심 연구 목표 본 연구는 벤처 캐피탈 자본금 내역 검증(\"cap table tieout\")이라는 복잡한 법률 워크플로우를 자동화하는 "},{"id":"2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators","title":"[논문리뷰] GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators","excerpt":"arXiv에 게시된 'GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators","tags":["Review","LLM Agents","Environment Simulation","Co-evolution","Curriculum Learning","Data Efficiency","Reinforcement Learning","Adaptive Simulation","Difficulty Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang, Xinzhe Juan, Jiahao Qiu, Ke Shen, Mengdi Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트 훈련의 주요 병목인 높은 비용과 실세계 상호작용 데이터의 정적인"},{"id":"2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","title":"[논문리뷰] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation","excerpt":"arXiv에 게시된 'Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","tags":["Review","Video Generation","Camera Control","Homography","Diffusion Models","Data Augmentation","Novel View Synthesis","Pose Fidelity"],"text":"링크: 논문 PDF로 바로 열기 저자: MinJung Kim, Jeongho Kim, Hoiyeong Jin, Junha Hyung, Jaegul Choo 핵심 연구 목표 논문은 카메라 제어 가능한 동적 장면 비디오 생성에서 높은 카메라 포즈 충실도 와 뷰 일관성 을 유지하며, 가려진 기하학에 대해 추론하는 문제를 해결하는 것을 목표로 합니다. 특히, 기존"},{"id":"2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry","title":"[논문리뷰] LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry","excerpt":"Yuan Shen이 arXiv에 게시한 'LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry","tags":["Review","Autonomous Navigation","End-to-end Learning","Localization Grounded","Visual Geometry","Metric-aware Perception","Diffusion Policy","RGB-D"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuan Shen, Tai Wang, Yuqiang Yang, Wenzhe Cai, Jiaqi Peng 핵심 연구 목표 이 논문은 전통적인 모듈형 내비게이션 파이프라인의 지연 시간과 오류 누적 문제를 해결하고, 기존 endtoend 방식의 명시적 localization 의존성 한계를 극복하는 것을 목표로 합니다. Lo"},{"id":"2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding","title":"[논문리뷰] LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding","excerpt":"arXiv에 게시된 'LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding","tags":["Review","dLLM","Parallel Decoding","Lookahead","Inference Acceleration","Token Filling Order","Branch Parallelism","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenkai Xu, Yijie Jin, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Mingcong Song, Hongjie Si, Tianqi Hou, Junchi Yan, Zhijie Deng 핵심 연구 목표 Diffusion Large Language Models (dLLM)은 "},{"id":"2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry","title":"[논문리뷰] MatSpray: Fusing 2D Material World Knowledge on 3D Geometry","excerpt":"arXiv에 게시된 'MatSpray: Fusing 2D Material World Knowledge on 3D Geometry' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry","tags":["Review","3D Reconstruction","Material Estimation","Diffusion Models","Gaussian Splatting","Inverse Rendering","PBR","Relighting","Neural Merger"],"text":"링크: 논문 PDF로 바로 열기 저자: Philipp Langsteiner, JanNiklas Dihlmann, Hendrik Lensch 핵심 연구 목표 본 논문은 2D 이미지 기반의 물질 예측 모델을 활용하여 3D 형상에 물리 기반 렌더링(PBR) 속성을 부여하고, 여러 시점(multiview)에서 일관성을 유지하며 다시 조명 가능한(relightabl"},{"id":"2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments","title":"[논문리뷰] MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments","excerpt":"arXiv에 게시된 'MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments","tags":["Review","Mobile Agents","GUI Benchmarking","Agent-User Interaction","Tool-Augmented Agents","Model Context Protocol (MCP)","Long-Horizon Tasks","Reproducible Evaluation","Android Environment"],"text":"링크: 논문 PDF로 바로 열기 저자: Quyu Kong, Xu Zhang, Zhenyu Yang, Nolan Gao, Chen Liu, Panrong Tong, Chenglin Cai, Hanzhang Zhou, Jianan Zhang, Liangyu Chen, Zhidan Liu, Steven HOI, Yue Wang 핵심 연구 목표 기존 모바일 GUI"},{"id":"2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming","title":"[논문리뷰] Name That Part: 3D Part Segmentation and Naming","excerpt":"Alan Yuille이 arXiv에 게시한 'Name That Part: 3D Part Segmentation and Naming' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming","tags":["Review","3D Semantic Segmentation","Part Naming","Open-Vocabulary","LLM","Set Alignment","Geometric Deep Learning","Annotation Engine","Affordance Description"],"text":"링크: 논문 PDF로 바로 열기 저자: Soumava Paul, Prakhar Kaushik, Ankit Vaidya, Anand Bhattad, Alan Yuille 핵심 연구 목표 본 논문은 3D 객체를 의미론적으로 명명된 부분으로 분해하는 시맨틱 3D 파트 분할(semantic 3D part segmentation) 문제를 해결하는 것을 목표로 합니다"},{"id":"2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation","title":"[논문리뷰] QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation","excerpt":"Lu Cheng이 arXiv에 게시한 'QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation","tags":["Review","Dynamic RAG","Hallucination Detection","Corpus Statistics","Uncertainty Quantification","Pre-training Data","LLM Calibration","Infini-gram","Multi-hop QA"],"text":"링크: 논문 PDF로 바로 열기 저자: Dehai Min, Kailin Zhang, Tongtong Wu, Lu Cheng 핵심 연구 목표 대규모 언어 모델(LLM)의 내부 신호(예: logits, 엔트로피)가 부정확한 예측에 대해 종종 높은 확신을 보이는 등 신뢰할 수 없다는 문제점을 해결하고자 합니다. 이를 위해 동적 RAG(RetrievalAugmen"},{"id":"2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface","title":"[논문리뷰] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface","excerpt":"Liliang Chen이 arXiv에 게시한 'Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface","tags":["Review","Robotics","Demonstration Generation","3D Control Interface","Data Efficiency","Visuomotor Policy Learning","Spatial Generalization","Depth Map","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong 핵심 연구 목표 본 연구는 로봇 학습에서 공간 일반화 및 정책 견고성을 제한하는 다양한 로봇 시연 데이터 수집의 높은 비용 문제를 해결하고자 합"},{"id":"2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs","title":"[논문리뷰] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs","excerpt":"arXiv에 게시된 'Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs","tags":["Review","Latent Variable Models","Variational Autoencoder (VAE)","Reinforcement Learning (RL)","Exploration","Large Language Models (LLMs)","Vision-Language Models (VLMs)","Controllable Generation","Reasoning Strategies"],"text":"링크: 논문 PDF로 바로 열기 저자: Rujiao Long, Yang Li, Xingyao Zhang, Xi Zhao, Yuchi Xu, Wenbo Su, Weixun Wang, Tianqianjin Lin, Junchi Yan, Bo Zheng 핵심 연구 목표 본 논문은 대규모 (비전) 언어 모델(LLMs/VLMs)의 추론 및 강화 학습(RL) 훈련 과"},{"id":"2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing","title":"[논문리뷰] Region-Constraint In-Context Generation for Instructional Video Editing","excerpt":"arXiv에 게시된 'Region-Constraint In-Context Generation for Instructional Video Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing","tags":["Review","Video Editing","In-Context Learning","Diffusion Models","Region-Constraint","Instruction-based Editing","Latent Space Regularization","Attention Space Regularization","Large-scale Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongwei Zhang†, Fuchen Long, Wei Li†, Zhaofan Qiu, Wu Liu†, Ting Yao, and Tao Mei 핵심 연구 목표 본 논문은 텍스트 지시만으로 비디오 콘텐츠를 정밀하게 수정 하는 인컨텍스트 비디오 편집 과정에서 발생하는 문제를 해결하고자 합니다. 구체적으로, 편집 영역"},{"id":"2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory","title":"[논문리뷰] StoryMem: Multi-shot Long Video Storytelling with Memory","excerpt":"arXiv에 게시된 'StoryMem: Multi-shot Long Video Storytelling with Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory","tags":["Review","Video Storytelling","Multi-shot Video Generation","Memory Mechanism","Diffusion Models","Cross-shot Consistency","Latent Video Diffusion","ROPE Shift","Keyframe Selection"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiwen Zhang, Liming Jiang, Angtian Wang, Jacob Zhiyuan Fang, Tiancheng Zhi, Qing Yan, Hao Kang, Xin Lu, Xingang Pan 핵심 연구 목표 본 논문은 영화적 품질과 장거리 일관성을 갖춘 다중 샷 장편 비디오 스토리텔링을 생성하는 문제"},{"id":"2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding","title":"[논문리뷰] The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding","excerpt":"Ziwei Liu이 arXiv에 게시한 'The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding","tags":["Review","Unified Autoencoding","Prism Hypothesis","Semantic Representations","Pixel Representations","Frequency-Band Modulator","Foundation Models","Spectral Bias","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu 핵심 연구 목표 본 논문은 최신 파운데이션 모델에서 추상적 의미(semantic abstraction)와 시각적 충실도(pixellevel fidelity) 사이의 근본적인 불일치를 해결하는 것을 목표로 합니다. 다"},{"id":"2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models","title":"[논문리뷰] UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models","excerpt":"Yuqing Ma이 arXiv에 게시한 'UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models","tags":["Review","Unsupervised Learning","Code Generation","Large Language Models (LLMs)","Internal Probing","Self-Bootstrapping","Consensus Clustering","Code Intelligence"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiajun Wu, Jian Yang, Wei Zhang, Lin Jing, Yuqing Ma, Ensheng Shi, Yuchi Ma, Zhoujun Li, Xianglong Liu 핵심 연구 목표 본 연구는 대규모 언어 모델(LLMs)의 코드 생성 능력이 값비싼 감독 학습 데이터에 크게 의존하는 문제점을 해결하고자"},{"id":"2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives","title":"[논문리뷰] Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives","excerpt":"Sujata Ghosh이 arXiv에 게시한 'Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives","tags":["Review","Syllogistic Reasoning","Large Language Models (LLMs)","Belief Bias","Natural Language Understanding (NLU)","Formal Logic","Prompt Engineering","Self-Consistency","Cognitive Psychology"],"text":"링크: 논문 PDF로 바로 열기 저자: Aheli Poddar, Saptarshi Sahoo, Sujata Ghosh 핵심 연구 목표 본 연구는 LLM의 연역적 추론 능력 을 논리적(형식적) 및 직관적(자연어) 관점에서 깊이 이해하는 것을 목표로 합니다. 특히, LLM이 인간의 추론에서 흔히 나타나는 믿음 편향(belief bias) 을 보이는지, 아니면 "},{"id":"2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion","title":"[논문리뷰] WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion","excerpt":"arXiv에 게시된 'WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-23 00:00:00+0900+0900","permalink":"/ai/review/2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion","tags":["Review","Novel View Synthesis","3D Geometry Propagation","Video Diffusion Models","Gaussian Splatting","Autoregressive Generation","Spatio-Temporal Noise","Geometric Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang 핵심 연구 목표 논문은 단일 이미지로부터 장범위(longrange) 및 기하학적으로 일관된 새로운 시점 비디오를 생성하는 근본적인 문제를 해결하고자 합니다. 특히, 기존 생성 모델이 3D 기하학을 효과적으로 활용하지 못하고"},{"id":"2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling","title":"[논문리뷰] Active Intelligence in Video Avatars via Closed-loop World Modeling","excerpt":"Cheng Meng이 arXiv에 게시한 'Active Intelligence in Video Avatars via Closed-loop World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling","tags":["Review","Video Avatars","Active Intelligence","World Models","Closed-loop Reasoning","POMDP","Generative AI","Hierarchical Planning","Cognitive Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Cheng Meng, Qifeng Chen 핵심 연구 목표 기존 비디오 아바타 생성 방식이 단순한 애니메이션을 넘어 자율적인 에이전시 를 가지지 못하고 장기 목표를 달"},{"id":"2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies","title":"[논문리뷰] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies","excerpt":"arXiv에 게시된 'Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies","tags":["Review","Reinforcement Learning","Large Language Models","Policy Optimization","Interpretability","Transformer","Internal Policy","Entropy Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu 핵심 연구 목표 본 논문은 기존 RL 접근 방식이 LLM을 단일 블랙박스 정책으로 취급하는 한계를 극복하고자 합니다. L"},{"id":"2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination","title":"[논문리뷰] FaithLens: Detecting and Explaining Faithfulness Hallucination","excerpt":"arXiv에 게시된 'FaithLens: Detecting and Explaining Faithfulness Hallucination' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination","tags":["Review","LLM Hallucination Detection","Explainable AI","Faithfulness Evaluation","Data Augmentation","Reinforcement Learning","Fact-Checking"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, and Maosong Sun. 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) "},{"id":"2025-12-24-INTELLECT-3-Technical-Report","title":"[논문리뷰] INTELLECT-3: Technical Report","excerpt":"arXiv에 게시된 'INTELLECT-3: Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-INTELLECT-3-Technical-Report","tags":["Review","Reinforcement Learning","Large Language Models","Mixture-of-Experts","Asynchronous Training","Distributed Systems","Agentic AI","Code Execution","Model Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mika Senghaas, Jack Min Ong, Andrew Baker, Fares Obeid, Sami Jaghouar, Daniel Auras, Matej Sirovatka, William Brown, Jannik Straube, Sebastian Müller, Justus Mattern, Manveer Bas"},{"id":"2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos","title":"[논문리뷰] LongVideoAgent: Multi-Agent Reasoning with Long Videos","excerpt":"Renjie Pi이 arXiv에 게시한 'LongVideoAgent: Multi-Agent Reasoning with Long Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos","tags":["Review","Multi-Agent System","Long Video Understanding","Video Question Answering","Reinforcement Learning","Large Language Models","Temporal Grounding","Multimodal Reasoning","Tool-Augmented AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen 핵심 연구 목표 본 논문은 기존 MLLM(Multimodal Large Language Models)이 긴 길이의 비디오에서 발생하는 정보 압축 손실, 제한된 도구 세트, 그리고"},{"id":"2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems","title":"[논문리뷰] MemEvolve: Meta-Evolution of Agent Memory Systems","excerpt":"Junhao Wang이 arXiv에 게시한 'MemEvolve: Meta-Evolution of Agent Memory Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems","tags":["Review","LLM Agents","Memory Systems","Meta-Evolution","Self-Evolving AI","Memory Architecture","EvolveLab","Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Junhao Wang, Zhenhong Zhou, Chong Zhan, Haotian Ren, Guibin Zhang 핵심 연구 목표 본 논문은 LLM 기반 에이전트의 고정된 메모리 시스템 아키텍처가 다양한 태스크 컨텍스트에 메타 적응할 수 없는 근본적인 한계 를 해결하고자 합니다. 에이전트가 단순히 경험을 축적하는 "},{"id":"2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation","title":"[논문리뷰] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation","excerpt":"arXiv에 게시된 'Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation","tags":["Review","Thematic Analysis","Large Language Models","Qualitative Research","Cohen's Kappa","Semantic Similarity","Reliability Metrics","Ensemble Validation","Prompt Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Nilesh Jain, Seyi Adeyinka, Aza Allsop, Leor Roseman 핵심 연구 목표 본 연구는 질적 연구에서 LLM 기반 주제 분석의 신뢰성 문제를 해결하고, 기존의 시간 소모적이며 비용이 많이 드는 인간 코더 기반 방식의 한계를 극복하는 것을 목표로 합니다. 특히, LLM 출력의 신뢰도를 "},{"id":"2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models","title":"[논문리뷰] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models","excerpt":"arXiv에 게시된 'QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models","tags":["Review","Vision-Language Models","Physical Reasoning","Quantitative Benchmark","Kinematics","Mean Relative Accuracy","Video-Text","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Li Puyin, Tiange Xiang, Ella Mao, Shirley Wei, Xinye Chen, Adnan Masood, Li FeiFei, Ehsan Adeli 핵심 연구 목표 본 논문은 최신 VisionLanguage Models (VLMs) 이 물리적 특성을 정량적으로 추론하는 능력에 대한 불확실성을 해"},{"id":"2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library","title":"[논문리뷰] Reinforcement Learning for Self-Improving Agent with Skill Library","excerpt":"Soumya Smruti Mishra이 arXiv에 게시한 'Reinforcement Learning for Self-Improving Agent with Skill Library' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library","tags":["Review","Reinforcement Learning (RL)","LLM Agents","Skill Library","Self-Improvement","Sequential Rollout","AppWorld dataset","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트가 복잡한 환경에서 지속"},{"id":"2025-12-24-SAM-Audio-Segment-Anything-in-Audio","title":"[논문리뷰] SAM Audio: Segment Anything in Audio","excerpt":"arXiv에 게시된 'SAM Audio: Segment Anything in Audio' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-SAM-Audio-Segment-Anything-in-Audio","tags":["Review","Audio Source Separation","Foundation Models","Multimodal Prompting","Diffusion Transformers","Flow Matching","Self-Supervised Learning","Reference-Free Evaluation","Audio-Visual Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Shi, Andros Tjandra, John Hoffman, Helin Wang, YiChiao Wu, Luya Gao, Julius Richter, Matt Le, Apoorv Vyas, Sanyuan Chen, Christoph Feichtenhofer, Piotr Dollár, WeiNing Hsu,"},{"id":"2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","title":"[논문리뷰] SemanticGen: Video Generation in Semantic Space","excerpt":"arXiv에 게시된 'SemanticGen: Video Generation in Semantic Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","tags":["Review","Video Generation","Semantic Space","Diffusion Models","VAE Latents","Long Video Generation","Semantic Encoders","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianhong Bai, Xiaoshi Wu, Xintao Wang, Xiao Fu, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai 핵심 연구 목표 기존 비디오 생성 모델의 느린 수렴 속도 와"},{"id":"2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems","title":"[논문리뷰] Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems","excerpt":"Luisa Bentivogli이 arXiv에 게시한 'Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems","tags":["Review","Streaming Speech-to-Text Translation","StreamST","Evaluation Toolkit","Open-Source Framework","Re-translation","Incremental Decoding","Latency Metrics","Quality Metrics","Real-time Demonstration"],"text":"링크: 논문 PDF로 바로 열기 저자: Marco Gaido, Sara Papi, Mauro Cettolo, Matteo Negri, Luisa Bentivogli 핵심 연구 목표 스트리밍 음성텍스트 번역(StreamST) 시스템의 평가 및 시연을 위한 통일된 오픈 소스 프레임워크가 부재하며, 기존 도구의 한계(유지보수 중단, 재번역 미지원, 짧은 오디오 "},{"id":"2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs","title":"[논문리뷰] SpatialTree: How Spatial Abilities Branch Out in MLLMs","excerpt":"arXiv에 게시된 'SpatialTree: How Spatial Abilities Branch Out in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs","tags":["Review","Spatial Intelligence","Multimodal LLMs","Cognitive Hierarchy","Benchmark","Reinforcement Learning","Supervised Fine-tuning","Spatial Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang 핵심 연구 목표 멀티모달 대규모 언어 모델(MLLM) 내에서 공간 능력의 계층적 구조가 제대로 이해되지 않고 단편적으로 연구되는 문제를 해결하는 것"},{"id":"2025-12-24-Step-DeepResearch-Technical-Report","title":"[논문리뷰] Step-DeepResearch Technical Report","excerpt":"arXiv에 게시된 'Step-DeepResearch Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-Step-DeepResearch-Technical-Report","tags":["Review","Deep Research Agents","LLMs","Reinforcement Learning","Supervised Fine-tuning","Agentic AI","Multi-hop Reasoning","Benchmarking","Cost-effectiveness"],"text":"링크: 논문 PDF로 바로 열기 저자: AgentTeam at StepFun 핵심 연구 목표 본 논문은 Deep Research —개방형, 장기적, 복잡한 정보 탐색 작업—를 수행할 수 있는 견고한 자율 에이전트 구축의 문제를 다룹니다. 기존 LLM 기반 시스템이 단순 검색이나 파편화된 멀티홉 질의응답에 의존하는 한계를 극복하고, 복잡한 인지 과정을 내재화"},{"id":"2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub","title":"[논문리뷰] Toxicity Ahead: Forecasting Conversational Derailment on GitHub","excerpt":"Kostadin Damevski이 arXiv에 게시한 'Toxicity Ahead: Forecasting Conversational Derailment on GitHub' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-24 00:00:00+0900+0900","permalink":"/ai/review/2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub","tags":["Review","Conversational AI","Toxicity Detection","LLM","Prompt Engineering","Open Source Software","GitHub","Derailment Forecasting"],"text":"링크: 논문 PDF로 바로 열기 저자: Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, Kostadin Damevski 핵심 연구 목표 본 연구는 오픈 소스 소프트웨어(OSS) 커뮤니티의 건강을 해치는 유해한 대화(toxic interactions)가 발생하기 전에 이를 사전"},{"id":"2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models","title":"[논문리뷰] Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models","excerpt":"Yu-Lun Liu이 arXiv에 게시한 'Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models","tags":["Review","Vision-Language Models (VLMs)","Popularity Bias","Ordinal Regression","Building Age Estimation","Multi-modal Learning","Benchmark Dataset","Explainable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: LiZhong SzuTu, TingLin Wu, ChiaJui Chang, He Syu, YuLun Liu 핵심 연구 목표 본 논문은 최신 에 내재된 을 탐구하고 노출하는 것을 목표로 합니다. 연구는 VLMs가 건축 양식을 진정으로 이해하는지, 아니면 단지 유명한 랜드마크를 하는지에 대한 근본적인 질문에 답하고, 이를"},{"id":"2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation","title":"[논문리뷰] DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation","excerpt":"arXiv에 게시된 'DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation","tags":["Review","Video Generation","One-Shot Video","Diffusion Transformer (DiT)","Frame-Guided Generation","Auto-Regressive Generation","Supervised Fine-Tuning (SFT)","Direct Preference Optimization (DPO)"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu 핵심 연구 목표 본 "},{"id":"2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming","title":"[논문리뷰] HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming","excerpt":"arXiv에 게시된 'HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming","tags":["Review","High-Resolution Video Generation","Diffusion Models","Autoregressive","Efficiency","Caching","Attention Mechanisms","Video Streaming","Temporal Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, JuanManuel PerezRua 핵심 연구 목표 고해상"},{"id":"2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics","title":"[논문리뷰] LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics","excerpt":"arXiv에 게시된 'LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics","tags":["Review","LLM Evaluation","Competitive Ranking","Swiss-System","Monte Carlo Simulation","Failure Sensitivity Analysis","Robustness","Multi-Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong 핵심 연구 목표 논문은 LLM 평가가 파편화된 태스크별 지표에 의존하고 있음을 지적하며, 이는 다양한 벤치마크 간의 적절한 가중치"},{"id":"2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations","title":"[논문리뷰] Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations","excerpt":"arXiv에 게시된 'Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations","tags":["Review","Autoregressive Model","Video Modeling","Generative Pretraining","Representation Learning","Flow-Matching Decoder","Context Isolation","Masked Next-Frame Prediction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinghan Li, Yang Jin, Hao Jiang, Yadong Mu, Yang Song, Kun Xu (Peking University) 핵심 연구 목표 기존 시각 생성 사전 훈련 방법론이 비디오의 핵심적인 시간 정보를 간과하거나, 자기회귀 방식이 의미론적 부정확성 및 낮은 생성 품질을 겪는 문제를 해결합니다"},{"id":"2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models","title":"[논문리뷰] Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models","excerpt":"arXiv에 게시된 'Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models","tags":["Review","Dynamic Spatial Reasoning","Vision-Language Models","4D Understanding","Automated Data Generation","Geometry Selection Module","Video Analysis","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi 핵심 연구 목표 본 논문은 VisionLanguage Models (VLMs)이 동적 공간 추론(DSR)에 취약하다는 문제점을 해결하고자 합니다. 특히, 3D 공간에"},{"id":"2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment","title":"[논문리뷰] Multi-hop Reasoning via Early Knowledge Alignment","excerpt":"Xuanjing Huang이 arXiv에 게시한 'Multi-hop Reasoning via Early Knowledge Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment","tags":["Review","Retrieval-Augmented Generation (RAG)","Multi-hop Reasoning","Reinforcement Learning (RL)","Knowledge Alignment","Iterative RAG","Entropy Analysis","Plan Failure"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuanjing Huang, Qi Luo, Bo Wang, Shicheng Fang, Yuxin Wang 핵심 연구 목표 본 논문은 복잡한 다중 홉(multihop) 질문을 처리하는 반복적 RAG(Iterative RAG) 시스템 의 비효율적인 검색 및 추론 문제, 특히 초기 계획 단계에서의 '계획 실패(plan fa"},{"id":"2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence","title":"[논문리뷰] NVIDIA Nemotron 3: Efficient and Open Intelligence","excerpt":"arXiv에 게시된 'NVIDIA Nemotron 3: Efficient and Open Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence","tags":["Review","Hybrid Mamba-Transformer","Mixture-of-Experts","LatentMoE","NVFP4 Training","Multi-Token Prediction","Long Context","Reinforcement Learning","Open Models"],"text":"링크: 논문 PDF로 바로 열기 저자: NVIDIA 핵심 연구 목표 Nemotron 3 가족 모델(Nano, Super, Ultra)을 공개하여 강력한 agentic, 추론, 대화 능력 을 제공하는 효율적인 오픈 모델을 구축하는 것이 목표입니다. 최대 1M 토큰의 긴 컨텍스트 길이 와 최고 수준의 처리량을 지원하며, 모델 가중치, 데이터셋, 훈련 레시피 및"},{"id":"2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning","title":"[논문리뷰] Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning","excerpt":"arXiv에 게시된 'Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning","tags":["Review","Mixture-of-Experts","Mamba-Transformer","Agentic Reasoning","Long Context LLM","FP8 Quantization","Supervised Fine-Tuning","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: NVIDIA 핵심 연구 목표 본 논문은 오픈 소스 로 제공되며, 효율적 이면서도 에이전트적 추론 능력이 뛰어난 MixtureofExperts (MoE) 하이브리드 MambaTransformer 언어 모델 인 Nemotron 3 Nano를 개발하는 것을 목표로 합니다. 특히, 기존 모델 대비 향상된 정확도와 추론 처리량"},{"id":"2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios","title":"[논문리뷰] SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios","excerpt":"Nghi D. Q. Bui이 arXiv에 게시한 'SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios","tags":["Review","Coding Agents","Software Evolution","Benchmarking","Long-Horizon Tasks","Large Language Models (LLMs)","Software Engineering","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy N. Phan, Nghi D. Q. Bui 핵심 연구 목표 이 논문은 기존 AI 코딩 에이전트 벤치마크(예: SWEBench )가 단일 이슈 해결 에 초점을 맞춰 실제 소프트웨어 진화의 복잡성을 포착하지 못하는 한계를 해결하고자 합니다"},{"id":"2025-12-25-Streaming-Video-Instruction-Tuning","title":"[논문리뷰] Streaming Video Instruction Tuning","excerpt":"Kaiyang Zhou이 arXiv에 게시한 'Streaming Video Instruction Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-Streaming-Video-Instruction-Tuning","tags":["Review","Streaming Video Understanding","Large Language Models (LLMs)","Instruction Tuning","Multi-task Learning","Real-time AI Assistant","Temporal Reasoning","Focal Loss","Video Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou 핵심 연구 목표 이 논문은 실시간 비디오 스트림을 이해하고 동적인 지시에 반응하는 일반 목적의 대화형 AI 어시스턴트인 Streamo 를 개발하는 것을 목표로 합니다. 기존 오프라인 비디오 모델의 한계인 실"},{"id":"2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation","title":"[논문리뷰] T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation","excerpt":"arXiv에 게시된 'T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation","tags":["Review","Text-to-Audio-Video Generation","Multimodal Evaluation","Benchmark","MLLM-as-a-Judge","Cross-modal Alignment","Instruction Following","Perceptual Realism","Audio Realism"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhe Cao, Tao Wang, Jiaming Wang, Yanghai Wang, Yuanxing Zhang, Jialu Chen, Miao Deng, Jiahao Wang, Yubin Guo, Chenxi Liao, Yize Zhang, Zhaoxiang Zhang, Jiaheng Liu 핵심 연구 목표 텍스트오디"},{"id":"2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior","title":"[논문리뷰] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior","excerpt":"arXiv에 게시된 'TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior","tags":["Review","Tokenizer","Language Models (LMs)","Robustness","Multilingual NLP","Benchmark","Subword Segmentation","Pre-training","Tokenization Impact"],"text":"링크: 논문 PDF로 바로 열기 저자: Gül Sena Altıntaş, Malikeh Ehghaghi, Brian Lester, Fengyuan Liu, Wanru Zhao, Marco Ciccone, Colin Raffel 핵심 연구 목표 언어 모델(LM) 성능 및 동작에 대한 토크나이저 선택의 영향 을 체계적으로 측정하고 이해하는 것을 목표로 합니다."},{"id":"2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","title":"[논문리뷰] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times","excerpt":"arXiv에 게시된 'TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-25 00:00:00+0900+0900","permalink":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","tags":["Review","Video Generation","Diffusion Models","Acceleration","Quantization","Attention","Step Distillation","Performance Optimization","RTX 5090"],"text":"링크: 논문 PDF로 바로 열기 저자: Jintao Zhang, Kaiwen Zheng, Kai Jiang, Haoxu Wang, Ion Stoica, Joseph E. Gonzalez, Jianfei Chen, Jun Zhu 핵심 연구 목표 본 논문은 비디오 확산 모델의 엔드투엔드 생성 속도를 100200배 가속화하면서도 비디오 품질을 유지하는 것을 목표"},{"id":"2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training","title":"[논문리뷰] GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training","excerpt":"Yuanchun Shi이 arXiv에 게시한 'GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","permalink":"/ai/review/2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training","tags":["Review","Multi-turn Reinforcement Learning","Vision-Language Models (VLMs)","Agentic AI","Knowledge Distillation","Model Merging","PPO","Thought Guidance","Cost Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Tong Wei, Yijun Yang, Changhao Zhang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye 핵심 연구 목표 멀티턴 강화 학습(RL) 기반 VLM(VisionLanguage Model) 에이전트 훈련 의 주요 문제점인 희소한 보상, 긴 신용 할당 문제"},{"id":"2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode","title":"[논문리뷰] How Much 3D Do Video Foundation Models Encode?","excerpt":"arXiv에 게시된 'How Much 3D Do Video Foundation Models Encode?' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","permalink":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode","tags":["Review","Video Foundation Models","3D Understanding","3D Reconstruction","Model Agnostic","Feature Probing","Diffusion Models","Temporal Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg 핵심 연구 목표 본 논문은 대규모 비디오 데이터로 사전 훈련된 Video Foundation Models (VidFMs) 내에 글로벌 3D 이해도가 자연스럽게 내재되어 있는지를 정량적으로 탐구하는 것을 목표로 합니다. 명시적인 "},{"id":"2025-12-26-Latent-Implicit-Visual-Reasoning","title":"[논문리뷰] Latent Implicit Visual Reasoning","excerpt":"arXiv에 게시된 'Latent Implicit Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","permalink":"/ai/review/2025-12-26-Latent-Implicit-Visual-Reasoning","tags":["Review","Large Multimodal Models (LMMs)","Visual Reasoning","Latent Tokens","Visual Bottlenecking","Implicit Learning","Task-agnostic","Attention Mechanisms"],"text":"링크: 논문 PDF로 바로 열기 저자: Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig 핵심 연구 목표 본 논문은 현재 대규모 멀티모달 모델(LMMs) 이 텍스트 중심적 추론에 치우쳐 있어 시각적 정보 처리가 많이 필요한 추론 태스크에서 한계를 보이는 문"},{"id":"2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models","title":"[논문리뷰] Schoenfeld's Anatomy of Mathematical Reasoning by Language Models","excerpt":"Tianyi Zhou이 arXiv에 게시한 'Schoenfeld's Anatomy of Mathematical Reasoning by Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","permalink":"/ai/review/2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models","tags":["Review","LLM Reasoning","Cognitive Science","Schoenfeld's Episode Theory","Mathematical Problem Solving","Reasoning Dynamics","Interpretable AI","Behavioral Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou 핵심 연구 목표 대규모 언어 모델(LLM)의 추론 과정은 표면적인 통계 외에는 그 인지 구조와 단계를 파악하기 어렵습니다. 본 논문은 Schoenfeld의 에피소드 이론을 기반으로 이라는 프레임워크를 제안하여 L"},{"id":"2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","title":"[논문리뷰] Spatia: Video Generation with Updatable Spatial Memory","excerpt":"arXiv에 게시된 'Spatia: Video Generation with Updatable Spatial Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","permalink":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","tags":["Review","Video Generation","Spatial Memory","3D Scene Point Cloud","Spatial Consistency","Camera Control","Interactive Editing","Diffusion Models","Visual SLAM"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu 핵심 연구 목표 기존 비디오 생성 모델들이 직면한 장기적인 공간 및 시간적 일관성 유지의 어려움 을 해결하는 것을 목표로 합니다. 이를 위해 업데이트 가능한 3D 장면 포인트 클라우드 를 "},{"id":"2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation","title":"[논문리뷰] VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation","excerpt":"Yicong Li이 arXiv에 게시한 'VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-26 00:00:00+0900+0900","permalink":"/ai/review/2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation","tags":["Review","Autoregressive Generation","Pixel-Aware Alignment","Variational Optimization","Reinforcement Learning","Visual Tokenizers","Image Quality","ELBO","Post-Training Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li, Wei Wei, Angela Yao 핵심 연구 목표 본 논문은 Autoregressive (AR) 시각 생성 모델이 토큰 수준에서만 최적화되어 픽셀 공간에서 낮은 품질의 이미지를 생성하는 문제를 해결하고자 합니다. 토크나이"},{"id":"2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication","title":"[논문리뷰] A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication","excerpt":"A. I. Perminov이 arXiv에 게시한 'A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication","tags":["Review","Matrix Multiplication","Additive Complexity","Algorithm Optimization","Ternary Flip-Graph","Heuristic Search","Common Subexpression Elimination","BLAS"],"text":"링크: 논문 PDF로 바로 열기 저자: Andrew I. Perminov 핵심 연구 목표 본 논문의 핵심 목표는 일반적인 비가환 링(noncommutative rings) 환경에서 3x3 행렬 곱셈 을 위한 랭크23(rank23) 알고리즘 의 가산 복잡도(additive complexity)를 최적화하는 것입니다. 기존의 최고 기록인 60회 덧셈보다 더 적"},{"id":"2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search","title":"[논문리뷰] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search","excerpt":"Jierun Chen이 arXiv에 게시한 'InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search","tags":["Review","Multimodal AI","Visual Search","Foundation Models","Multi-agent Systems","Reinforcement Learning","Benchmarking","Visual Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaican Li¹, Lewei Yao2, Jiannan Wu2, Tiezheng Yu², Jierun Chen², Haoli Bai², Lu Hou², Lanqing Hong², Wei Zhang2, Nevin L. Zhang¹† 핵심 연구 목표 본 논문은 최신 개방형 멀티모달 에이전트가 복잡한 실세계 시각적 추론 "},{"id":"2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion","title":"[논문리뷰] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion","excerpt":"arXiv에 게시된 'InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion","tags":["Review","Video Object Insertion (VOI)","4D Scene Geometry","Diffusion Models","Mask Generation","Temporal Consistency","Occlusion Handling","Illumination Synthesis","ROSE++ Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo 핵심 연구 목표 본 논문은 상업적 활용에 적합한 수준의 사실적인 비디오 객체 삽입(VOI) 을 달성하는 것을 목표"},{"id":"2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents","title":"[논문리뷰] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents","excerpt":"arXiv에 게시된 'MAI-UI Technical Report: Real-World Centric Foundation GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents","tags":["Review","GUI Agents","Foundation Models","Reinforcement Learning","Device-Cloud Collaboration","Mobile Navigation","Tool Augmentation","User Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven HOI 핵심 연구 목표 본 연구는 사용자 상호작용 부족, UI 전용 작업의 한계,"},{"id":"2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding","title":"[논문리뷰] Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding","excerpt":"arXiv에 게시된 'Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding","tags":["Review","Retrieval Augmented Generation","Long Context Understanding","Mindscape-Aware","Hierarchical Summarization","Context-Aware Embeddings","Integrative Reasoning","LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuqing Li, Jiangnan Li, Zheng Lin, Ziyan Zhou, Junjie Wu, Weiping Wang, Jie Zhou, Mo Yu 핵심 연구 목표 본 논문은 현재 RAG(RetrievalAugmented Generation) 시스템이 인간의 \"마인드스케이프(mindscapeaware)\" 능력"},{"id":"2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding","title":"[논문리뷰] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding","excerpt":"Yixin Chen이 arXiv에 게시한 'Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding","tags":["Review","Foundation Model","Multimodal AI","Weather Nowcasting","Radar Inversion","Weather Understanding","Chain-of-Thought","Shared Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, et al. 핵심 연구 목표 기존의 날씨 모델들이 예측(예: nowcasting, inversion)과 이해(예: 진단적 추론, 질의응답) 태스크를 개별적으로 다루는 문제를 해결하고자 합니다. 이 논문은 단일 아"},{"id":"2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right","title":"[논문리뷰] ProEdit: Inversion-based Editing From Prompts Done Right","excerpt":"Kun-Yu Lin이 arXiv에 게시한 'ProEdit: Inversion-based Editing From Prompts Done Right' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right","tags":["Review","Inversion-based Editing","Text-to-Image Editing","Text-to-Video Editing","Diffusion Models","Flow-based Models","Attention Mechanism","Latent Space Manipulation","Plug-and-Play"],"text":"링크: 논문 PDF로 바로 열기 저자: KunYu Lin, JianJian Jiang, XiaoMing Wu, Dian Zheng, Zhi Ouyang, et al. 핵심 연구 목표 본 논문은 기존의 inversionbased visual editing 방법론들이 소스 이미지 정보를 과도하게 주입하여, 대상 이미지의 편집 영역에서 주체의 속성(자세, 수, "},{"id":"2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning","title":"[논문리뷰] SVBench: Evaluation of Video Generation Models on Social Reasoning","excerpt":"Xiaojie Xu이 arXiv에 게시한 'SVBench: Evaluation of Video Generation Models on Social Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning","tags":["Review","Video Generation","Social Reasoning","Benchmark","Evaluation","Agent-based Pipeline","Vision-Language Models","Social Cognition"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang 핵심 연구 목표 현재 텍스트투비디오(T2V) 생성 모델이 시각적 사실성과 모션 충실도에서 발전했음에도 불구하고, 사회적으로 일관된 행동 을 생성하는 데 근본"},{"id":"2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents","title":"[논문리뷰] SWE-RM: Execution-free Feedback For Software Engineering Agents","excerpt":"X. W.이 arXiv에 게시한 'SWE-RM: Execution-free Feedback For Software Engineering Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents","tags":["Review","Software Engineering Agents","Execution-free Feedback","Reward Model","Reinforcement Learning","Test-Time Scaling","Calibration","AUC","SWE-Bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Kashun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, X. W., Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He 핵심 연구 목표 본 논문은 소프트웨어 엔지니어링(SWE) 에이전트 개발에서 실행 기반 피드백(executionbased feed"},{"id":"2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning","title":"[논문리뷰] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning","excerpt":"arXiv에 게시된 'See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning","tags":["Review","Multimodal Reasoning","Vision-Language Models (VLMs)","Perceptual Shaping","KL-Divergence","Chart Understanding","Data Augmentation","Reinforcement Learning (RL)","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang 핵심 연구 목표 본 논문은 대규모 시각언어 모델(VLM)이 미세한 시각적 증거(finegrained visual evidence) 를 놓치고, 도메인 간 일반화 능력"},{"id":"2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers","title":"[논문리뷰] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers","excerpt":"arXiv에 게시된 'SlideTailor: Personalized Presentation Slide Generation for Scientific Papers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers","tags":["Review","Personalized Slide Generation","Preference Learning","Large Language Models","Multimodal AI","Chain-of-Speech","Agentic Framework","Document-to-Slides"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng 핵심 연구 목표 이 논문은 기존 자동 슬라이드 생성 시스템이 사용자 선호도를 충분히 반영하지 못하여 만족스럽지 못한 결과물을 초래하는 문제를 해결하고자 합니다. 사용자 정의된 콘텐츠 및 시각적 선호도를 기반으로 과학 논"},{"id":"2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models","title":"[논문리뷰] TimeBill: Time-Budgeted Inference for Large Language Models","excerpt":"Yehan Ma이 arXiv에 게시한 'TimeBill: Time-Budgeted Inference for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models","tags":["Review","LLM Inference","Time Budgeting","KV Cache Eviction","Response Length Prediction","Execution Time Estimation","Real-time AI","Performance Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Qi Fan, An Zou, Yehan Ma 핵심 연구 목표 시간 제약이 있는 시스템(예: 로봇 공학, 자율 주행)에서 대규모 언어 모델(LLM)의 응답 성능을 유지하면서 주어진 시간 예산 내에 추론을 완료하는 문제를 해결하는 것이 목표입니다. 기존 LLM 추론 방식이 자동 회귀 특성으로 인해 실행 시간을 예측하기 어"},{"id":"2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture","title":"[논문리뷰] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture","excerpt":"Kaiwen Zhu이 arXiv에 게시한 'UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-29 00:00:00+0900+0900","permalink":"/ai/review/2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture","tags":["Review","Perceptual Understanding","Image Aesthetics","Image Quality","Image Structure","Image Texture","MLLM Benchmark","Visual Question Answering","Reward Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiwen Zhu, Yuandong Pu, Xiaohui Li, Jiayang Li, Shuo Cao 핵심 연구 목표 본 연구는 Multimodal Large Language Models (MLLMs) 이 이미지의 미학, 품질, 구조, 텍스처와 같은 지각 수준의 특성을 이해하는 데 어려움을 겪는 문제를 해결하고자 합"},{"id":"2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy","title":"[논문리뷰] Act2Goal: From World Model To General Goal-conditioned Policy","excerpt":"arXiv에 게시된 'Act2Goal: From World Model To General Goal-conditioned Policy' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy","tags":["Review","Goal-Conditioned Policy","World Models","Robotic Manipulation","Multi-Scale Temporal Hashing","Online Adaptation","Hindsight Experience Replay","LoRA Finetuning","Zero-shot Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengfei Zhou¹,, Liliang Chen¹,, Shengcong Chen¹, Di Chen¹, Wenzhi Zhao¹, Rongjun Jin¹, Guanghui Ren¹, Jianlan Luo¹,† Agibot Research 핵심 연구 목표 본 논문은 장기 로봇 조작(longhorizon robotic m"},{"id":"2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design","title":"[논문리뷰] An Information Theoretic Perspective on Agentic System Design","excerpt":"arXiv에 게시된 'An Information Theoretic Perspective on Agentic System Design' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design","tags":["Review","Agentic Systems","Language Models","Mutual Information","Rate-Distortion Theory","Compute Efficiency","Scaling Laws","Compressor-Predictor Architecture","On-device AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman 핵심 연구 목표 논문은 에이전트형 언어 모델(LM) 시스템, 특히 컴프레서프레딕터(compressorpredictor) 아키텍처 의 설계에 대한 체계적인"},{"id":"2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss","title":"[논문리뷰] Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss","excerpt":"arXiv에 게시된 'Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss","tags":["Review","Mixture-of-Experts (MoE)","Router-Expert Coupling","Auxiliary Loss","Expert Specialization","Large Language Models (LLMs)","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Ang Lv, Jin Ma, Yiyuan Ma, Siyuan Qiao 핵심 연구 목표 본 논문은 MixtureofExperts (MoE) 모델에서 라우터의 결정이 개별 전문가의 실제 역량과 충분히 연동되지 않아 발생하는 성능 한계를 해결하고자 합니다. 라우터와 전문가 간의 약한 결합 문제를 개선하여 모델 성능을 향상시"},{"id":"2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models","title":"[논문리뷰] DiRL: An Efficient Post-Training Framework for Diffusion Language Models","excerpt":"arXiv에 게시된 'DiRL: An Efficient Post-Training Framework for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Post-Training","Reinforcement Learning","GRPO","FlexAttention","LMDeploy","Math Reasoning","SFT"],"text":"링크: 논문 PDF로 바로 열기 저자: Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu 핵심 연구 목표 Diffusion Language Models (dLLMs)의 미흡한 posttraining "},{"id":"2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation","title":"[논문리뷰] Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation","excerpt":"arXiv에 게시된 'Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation","tags":["Review","Video Diffusion Model","Depth Estimation","Normal Estimation","Transparent Objects","Robotics","Data Generation","LoRA Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao 핵심 연구 "},{"id":"2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone","title":"[논문리뷰] Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone","excerpt":"arXiv에 게시된 'Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone","tags":["Review","Diffusion Models","Vision-Language Models","Vision-Language-Action Models","Robotics","Multimodal AI","Action Planning","Long-Horizon Planning","Bidirectional Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong 핵심 연구 목표 본 논문은 기존 Autoregressive (AR) 기반 대규모 시각언어 모델(VLM) 및 시각언어액션 모"},{"id":"2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models","title":"[논문리뷰] GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models","excerpt":"arXiv에 게시된 'GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models","tags":["Review","Text Encoder","Diffusion Models","Text Embedding","Evaluation Benchmark","MLLM Fine-tuning","Layer-wise Weighting","Text-to-Image Generation","Text-to-Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Bozhou Li, Sihan Yang, Yushuo Guan, Ruichuan An, Xinlong Chen, Yang Shi, Pengfei Wan, Wentao Zhang, Yuanxing Zhang 핵심 연구 목표 본 논문은 텍스트이미지(T2I) 및 텍스트비디오(T2V) 확산 모델에서 핵심 구성 요소인 텍스트 "},{"id":"2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation","title":"[논문리뷰] LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation","excerpt":"Steffi Chern이 arXiv에 게시한 'LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation","tags":["Review","Real-time Video Generation","Multimodal Diffusion","On-Policy Distillation","Interactive AI Avatars","Video Streaming","Anchor-Heavy Identity Sinks","Lip Synchronization"],"text":"링크: 논문 PDF로 바로 열기 저자: Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu 핵심 연구 목표 본 논문은 기존 확산 모델의 느린 추론 속도와 양방향 어텐션으로 인한 실시간 상호작용의 어려움을 해결하고자 합니다. 특히, 멀티모달 조건(텍스트, 이미"},{"id":"2025-12-30-Monadic-Context-Engineering","title":"[논문리뷰] Monadic Context Engineering","excerpt":"arXiv에 게시된 'Monadic Context Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Monadic-Context-Engineering","tags":["Review","Monads","Functional Programming","AI Agents","State Management","Error Handling","Concurrency","Monad Transformers","Meta-Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Zhang, Mengdi Wang 핵심 연구 목표 본 논문은 현재 AI 에이전트 아키텍처가 겪는 상태 관리, 오류 처리, 동시성 문제로 인한 취약성을 해결하고자 합니다. Monadic Context Engineering (MCE) 이라는 새로운 아키텍처 패러다임을 도입하여, Functors, Applicat"},{"id":"2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking","title":"[논문리뷰] Nested Browser-Use Learning for Agentic Information Seeking","excerpt":"arXiv에 게시된 'Nested Browser-Use Learning for Agentic Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking","tags":["Review","Agentic Information Seeking","LLM Agents","Browser Automation","Nested Framework","Tool Learning","Context Efficiency","Deep Web"],"text":"링크: 논문 PDF로 바로 열기 저자: Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang 핵심 연구 목표 정보 탐색(IS) 에이전트의 현재 브라우저 도구 사"},{"id":"2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding","title":"[논문리뷰] OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding","excerpt":"Jian Liu이 arXiv에 게시한 'OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding","tags":["Review","Omnimodal Understanding","Audio-Guided Perception","Active Learning Agents","Cross-Modal Alignment","Tool-Use","Video Understanding","Multimodal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang 핵심 연구 목표 기존 옴니모달 대규모 언어 모델(OmniLLMs) 이 겪는 미세한 크로스모달 이해(finegrained crossmodal understanding) 및 멀티모달 정렬(multimoda"},{"id":"2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting","title":"[논문리뷰] Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting","excerpt":"arXiv에 게시된 'Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting","tags":["Review","3D Gaussian Splatting","Open-vocabulary Segmentation","Neural Rendering","High-dimensional Features","Quantile Sampling","Real-time Rendering","Feature Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yoonwoo Jeong 1,2,†, Cheng Sun¹, Frank Wang¹, Minsu Cho², Jaesung Choe¹ 핵심 연구 목표 이 논문은 3D Gaussian Splatting (3DGS) 기반의 Openvocabulary segmentation (OVS)에서 512차원 CLIP 특징 과 같은 고차원"},{"id":"2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents","title":"[논문리뷰] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents","excerpt":"arXiv에 게시된 'SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents","tags":["Review","Agentic RL","Self-Verifying Agents","GUI Automation","Evidence Curation","LLM-as-a-Judge","Reward Shaping","AndroidLab"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaofei Cai, Yulei Qin, Yong Mao, Siqi Cai, Xiaoyu Tan, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yitao Liang, Ke Li, Xing Sun (Tencent Youtu Lab 및 Peking University)"},{"id":"2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers","title":"[논문리뷰] SpotEdit: Selective Region Editing in Diffusion Transformers","excerpt":"arXiv에 게시된 'SpotEdit: Selective Region Editing in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers","tags":["Review","Diffusion Transformer","Image Editing","Selective Editing","Computational Efficiency","Training-Free","Region-Aware","Perceptual Similarity"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang 핵심 연구 목표 본 논문은 기존 Diffusion Transformer 기반 이미지 편집 모델들이 변경되지 않은 영역까지 포함하여 전체 이미지를 일관적으로 처리하고 디노이징하는 방식의 비효율성과 품질 "},{"id":"2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion","title":"[논문리뷰] Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion","excerpt":"Po-Fan Yu이 arXiv에 게시한 'Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion","tags":["Review","Video Super-Resolution","Diffusion Models","Low-Latency","Streamable","Auto-Regressive","Model Distillation","Temporal Consistency","Perceptual Quality"],"text":"링크: 논문 PDF로 바로 열기 저자: HauShiang Shiu, ChinYang Lin, Zhixiang Wang, ChiWei Hsiao, PoFan Yu, YuChih Chen, YuLun Liu 핵심 연구 목표 본 논문은 기존 확산 모델 기반 비디오 초해상화(VSR) 방법들이 높은 지각 품질(perceptual quality)을 제공함에도 불구하고"},{"id":"2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling","title":"[논문리뷰] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling","excerpt":"arXiv에 게시된 'SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling","tags":["Review","Surgical Robotics","World Models","Video Generation","Imitation Learning","Inverse Dynamics Model","Synthetic Data","Vision-Language-Action Models","Data Scarcity"],"text":"링크: 논문 PDF로 바로 열기 저자: Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu 핵심 연구 목표 본 논"},{"id":"2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards","title":"[논문리뷰] Training AI Co-Scientists Using Rubric Rewards","excerpt":"arXiv에 게시된 'Training AI Co-Scientists Using Rubric Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards","tags":["Review","AI Co-Scientists","Research Plan Generation","Reinforcement Learning (RL)","Self-Grading","Rubric Rewards","Language Models (LLMs)","Scientific Discovery"],"text":"링크: 논문 PDF로 바로 열기 저자: Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse 핵심 연"},{"id":"2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs","title":"[논문리뷰] VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs","excerpt":"Xihui Liu이 arXiv에 게시한 'VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs","tags":["Review","Embodied AI","Vision and Language Navigation","Instance Object Navigation","Active Dialog","Large Language Models (LLMs)","Benchmark","Human-Robot Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Xihui Liu, Jinming Xu, Meng Wei, Shaohao Zhu, Wensi Huang 핵심 연구 목표 이 논문은 에이전트가 모호한 자연어 지시를 받아 복잡하고 장거리인 환경에서 특정 객체 인스턴스를 찾아내는 Interactive Instance Object Navigation (IION) 태스크를 도"},{"id":"2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web","title":"[논문리뷰] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web","excerpt":"Kaixin Liang이 arXiv에 게시한 'Video-BrowseComp: Benchmarking Agentic Video Research on Open Web' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web","tags":["Review","Agentic AI","Video Understanding","Web Browsing","Benchmark","Multimodal LLMs","Temporal Grounding","Cross-Source Reasoning","Information Seeking"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng Liu, Lizi Liao 핵심 연구 목표 본 논문은 기존 벤치마크들이 텍스트 및 정적 멀티모달 정보 탐색에 초점을 맞추고 동적인 웹 비디오 콘텐츠"},{"id":"2025-12-30-Web-World-Models","title":"[논문리뷰] Web World Models","excerpt":"arXiv에 게시된 'Web World Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Web-World-Models","tags":["Review","Web World Model","LLM","Neuro-symbolic AI","Procedural Generation","Hybrid Architecture","Deterministic Generation","Persistent Environments","TypeScript"],"text":"링크: 논문 PDF로 바로 열기 저자: Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang 핵심 연구 목표 본 논문은 고정된 컨텍스트의 웹 프레임워크와 완전히 생성형 세계 모델(World Model) 사이의 간극을 메우는 Web World Model (WWM) 개념을 제안"},{"id":"2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model","title":"[논문리뷰] Yume-1.5: A Text-Controlled Interactive World Generation Model","excerpt":"Kaining Ying이 arXiv에 게시한 'Yume-1.5: A Text-Controlled Interactive World Generation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-30 00:00:00+0900+0900","permalink":"/ai/review/2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model","tags":["Review","Interactive World Generation","Video Diffusion Models","Text-to-Video","Image-to-Video","Real-time Generation","Temporal-Spatial-Channel Modeling","Self-Forcing"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang 핵심 연구 목표 본 논문은 대규모 파라미터 크기, 긴 추론 단계, 빠르게 증가하는 히스토리컬 컨텍스트, 그리고 텍스트 기반"},{"id":"2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation","title":"[논문리뷰] DreamOmni3: Scribble-based Editing and Generation","excerpt":"arXiv에 게시된 'DreamOmni3: Scribble-based Editing and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","permalink":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation","tags":["Review","Image Editing","Image Generation","Scribble-based Control","Multimodal AI","Diffusion Models","Data Synthesis","Human-Computer Interaction","Instruction-based Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Bin Xia, Bohao Peng, Jiyang Liu, Sitong Wu, Jingyao Li, Junjia Huang, Xu Zhao, Yitong Wang, Ruihang Chu, Bei Yu, Jiaya Jia 핵심 연구 목표 본 논문은 통합 생성 및 편집 모델에서 텍스트 프롬프트의 한계, 즉 사용자의 의도된"},{"id":"2025-12-31-End-to-End-Test-Time-Training-for-Long-Context","title":"[논문리뷰] End-to-End Test-Time Training for Long Context","excerpt":"Marcel Rød이 arXiv에 게시한 'End-to-End Test-Time Training for Long Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","permalink":"/ai/review/2025-12-31-End-to-End-Test-Time-Training-for-Long-Context","tags":["Review","Long-Context Language Modeling","Test-Time Training (TTT)","Meta-Learning","Continual Learning","Transformer","Sliding-Window Attention","Inference Efficiency","MLP Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød 핵심 연구 목표 본 논문은 트랜스포머의 전체 어텐션이 긴 컨텍스트에서 선형적인 비용 증가로 비효율적인 문제를 해결하고자 합니다. 기존 RNN 기반 모델들이 긴 컨텍스트에서 성능 저하를 겪는 한계를 극복하며"},{"id":"2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR","title":"[논문리뷰] Evaluating Parameter Efficient Methods for RLVR","excerpt":"arXiv에 게시된 'Evaluating Parameter Efficient Methods for RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","permalink":"/ai/review/2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR","tags":["Review","Parameter-Efficient Fine-Tuning (PEFT)","Reinforcement Learning with Verifiable Rewards (RLVR)","Low-Rank Adaptation (LoRA)","Mathematical Reasoning","LLM Adaptation","SVD Initialization"],"text":"링크: 논문 PDF로 바로 열기 저자: Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu 핵심 연구 목표 본 논문은 Reinforcement Learning with Verifiable Rewards (RLVR"},{"id":"2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs","title":"[논문리뷰] GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs","excerpt":"arXiv에 게시된 'GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","permalink":"/ai/review/2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs","tags":["Review","MoE LLM","Safety Alignment","Adversarial Attack","Neuron Pruning","Gate-level Profiling","Transfer Attack","Vision Language Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Lichao Wu, Sasha Behrouzi, Mohamadreza Rostami, Stjepan Picek, AhmadReza Sadeghi 핵심 연구 목표 본 논문은 MixtureofExperts (MoE) LLM 의 고유한 안전 특성과 취약점이 기존 Dense LLM 에 비해 충분히 연구되지 않았다는 문제의식을"},{"id":"2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization","title":"[논문리뷰] GraphLocator: Graph-guided Causal Reasoning for Issue Localization","excerpt":"Wei Zhang이 arXiv에 게시한 'GraphLocator: Graph-guided Causal Reasoning for Issue Localization' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","permalink":"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization","tags":["Review","Issue Localization","Causal Reasoning","Graph-guided","Large Language Models","Software Engineering","Defect Analysis","Repository Mining"],"text":"링크: 논문 PDF로 바로 열기 저자: WEI LIU, CHAO PENG, PENGFEI GAO, AOFAN LIU, WEI ZHANG, HAIYAN ZHAO, ZHI JIN 핵심 연구 목표 본 논문은 소프트웨어 이슈 로컬라이제이션의 근본적인 문제인 '증상원인 불일치(symptomtocause mismatch)'와 '일대다 불일치(onetomany mism"},{"id":"2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement","title":"[논문리뷰] UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement","excerpt":"Kaiyi Zhang이 arXiv에 게시한 'UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement' 논문에 대한 자세한 리뷰입니다.","date":"2025-12-31 00:00:00+0900+0900","permalink":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement","tags":["Review","3D Shape Generation","Diffusion Models","Geometric Refinement","Data Curation","Watertight Mesh","Voxel-based","Scalability","High-Fidelity"],"text":"링크: 논문 PDF로 바로 열기 저자: Tanghui Jia, Dongyu Yan, Dehao Hao, Yang Li, Kaiyi Zhang, Xianyi He, Lanjiong Li, Yuhan Wang, Jinnan Chen, Lutao Jiang, Qishen Yin, Long Quan, YingCong Chen, Li Yuan 핵심 연구 목표 본 논"},{"id":"2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey","title":"[논문리뷰] Adapting Vision-Language Models Without Labels: A Comprehensive Survey","excerpt":"Eleni Chatzi이 arXiv에 게시한 'Adapting Vision-Language Models Without Labels: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey","tags":["Review","Vision-Language Models (VLMs)","Unsupervised Adaptation","Test-Time Adaptation (TTA)","Domain Transfer","Multimodal Learning","Label-Free Learning","Zero-Shot Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Dong, Lijun Sheng, Jian Liang, Ran He, Eleni Chatzi, Olga Fink 핵심 연구 목표 본 서베이 논문은 레이블링된 데이터 없이 사전 훈련된 VisionLanguage Models (VLMs) 를 특정 다운스트림 태스크에 적용할 때 발생하는 성능 저하 문제를 해결하고자 "},{"id":"2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing","title":"[논문리뷰] GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing","excerpt":"Przemysław Spurek이 arXiv에 게시한 'GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing","tags":["Review","Neural Radiance Fields (NeRF)","Gaussian Splatting (GS)","Interactive Editing","3D Scene Representation","Physics Simulation","Hybrid Model","Real-time Rendering","Ray Tracing"],"text":"링크: 논문 PDF로 바로 열기 저자: Mikołaj Zieliński, Krzysztof Byrski, Tomasz Szczepanik, Przemysław Spurek 핵심 연구 목표 본 논문은 NeRF 의 사실적인 렌더링 품질과 Gaussian Splatting (GS) 의 편집 가능성 및 구조적 표현의 장점을 결합하여, 물리 기반 상호작용 이 가능한"},{"id":"2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models","title":"[논문리뷰] GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models","excerpt":"GLM-4. 5 Team이 arXiv에 게시한 'GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models","tags":["Review","Large Language Model","Mixture-of-Experts","Agentic AI","Reasoning","Code Generation","Reinforcement Learning","Foundation Model"],"text":"링크: 논문 PDF로 바로 열기 저자: GLM4.5 Team (Zhipu AI & Tsinghua University) 핵심 연구 목표 본 논문은 오픈소스 MoE(MixtureofExperts) 기반 대규모 언어 모델인 GLM4.5 를 소개합니다. 핵심 목표는 에이전트, 추론, 코딩(ARC) 태스크 전반에서 강력한 성능을 달성하고, 사고 및 직접 응답 모드"},{"id":"2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization","title":"[논문리뷰] InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization","excerpt":"Pengxiang Li이 arXiv에 게시한 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization","tags":["Review","GUI Grounding","MLLMs","Reinforcement Learning","Policy Optimization","Exploration Strategy","Semantic Alignment","Adaptive Exploration Reward","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xavier Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, Shengyu Zhang, Hongxia Yang, Fei Wu 핵심 연구 목표 본 논문"},{"id":"2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion","title":"[논문리뷰] LightSwitch: Multi-view Relighting with Material-guided Diffusion","excerpt":"Shubham Tulsiani이 arXiv에 게시한 'LightSwitch: Multi-view Relighting with Material-guided Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion","tags":["Review","Multi-view Relighting","Diffusion Models","Material-guided","Inverse Rendering","3D Scene Reconstruction","Image Synthesis","Consistent Relighting"],"text":"링크: 논문 PDF로 바로 열기 저자: Yehonathan Litman, Fernando De la Torre, Shubham Tulsiani 핵심 연구 목표 논문은 기존의 2D 이미지 리라이팅(relighting) 생성 모델들이 대상의 내재적 특성을 활용하지 못하거나 다중 뷰 데이터를 확장성 있게 고려하지 못해 불충분한 리라이팅 결과를 초래하는 문제를 해"},{"id":"2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs","title":"[논문리뷰] MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs","excerpt":"Guohang Yan이 arXiv에 게시한 'MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs","tags":["Review","Multimodal Large Language Models","Low-Resource Languages","Cultural Groundedness","Linguistic Capability","Dataset Creation","Multilingual AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yufei Gao, Jiaying Fei, Nuo Chen, Ruirui Chen, Guohang Yan, Yunshi Lan, Botian Shi 핵심 연구 목표 본 논문은 고자원 언어에 집중되어 저자원 언어에서 성능이 저하되는 기존 다중 모드 대규모 언어 모델(MLLM) 의 한계를 해결하고자 합니다. 특히, 기존 "},{"id":"2025-8-11-Memp-Exploring-Agent-Procedural-Memory","title":"[논문리뷰] Memp: Exploring Agent Procedural Memory","excerpt":"Shuofei Qiao이 arXiv에 게시한 'Memp: Exploring Agent Procedural Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory","tags":["Review","Procedural Memory","LLM Agents","Memory Management","Task Automation","Lifelong Learning","Experience Replay","Agent Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang 핵심 연구 목표 논문은 대규모 언어 모델(LLM) 기반 에이전트가 겪는 취약한 절차적 메모리 문제를 해결하고, 에"},{"id":"2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh","title":"[논문리뷰] MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh","excerpt":"Yi Yang이 arXiv에 게시한 'MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh","tags":["Review","3D Mesh Generation","LLMs","Mesh Understanding","Text-to-3D","Primitive-Mesh Decomposition","Progressive Training","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuangkang Fang, IChao Shen, Yufeng Wang, YiHsuan Tsai, Yi Yang, Shuchang Zhou, Wenrui Ding, Takeo Igarashi, MingHsuan Yang 핵심 연구 목표 본 연구는 기존 대규모 언어 모델(LLM) 기반의 3D 메시 처리 방식이 갖는 데"},{"id":"2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal","title":"[논문리뷰] Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal","excerpt":"Chengcheng Wan이 arXiv에 게시한 'Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal","tags":["Review","Code Reasoning","CoT Compression","LLMs","Efficiency","Surprisal","Pruning","Fine-tuning","Large Reasoning Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, Xiaodong Gu 핵심 연구 목표 본 논문은 대규모 추론 모델(LRMs)의 ChainofThought(CoT) 추론 과정에서 발생하는 과도하게 긴 추론 트레이스 문제를 해결하여"},{"id":"2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding","title":"[논문리뷰] UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding","excerpt":"Bingqi Chen이 arXiv에 게시한 'UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding","tags":["Review","GUI Agents","Reinforcement Learning","Grounding","MLLMs","Reward Function","Resampling","Visual Noise Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuquan Lian, Yuhang Wu, Jia Ma, Zihan Song, Bingqi Chen, Xiawu Zheng, Hui Li 핵심 연구 목표 본 논문은 기존 GUI 에이전트 훈련 및 추론 방식의 세 가지 한계점인 추론 설계 딜레마(P1) , 비효율적인 보상(P2) , 그리고 고해상도 디스플레이에서의 시각"},{"id":"2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off","title":"[논문리뷰] Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off","excerpt":"jgkwak이 arXiv에 게시한 'Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-11 13:13:28+0900","permalink":"/ai/review/2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off","tags":["Review","Virtual Try-On","Virtual Try-Off","Diffusion Transformer","Bidirectional Learning","Generative AI","Fashion Synthesis","Attention Mechanism","Self-Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Seungyong Lee, Jeonggi Kwak 핵심 연구 목표 가상 의류 착용(tryon) 및 탈의(tryoff) 시 사람의 자세 및 외형 변화에 따른 의류신체 일치성 모델링과 세부 묘사의 정확성 유지라는 고질적인 문제를 해결하는 것입니다. 단일 Diffusion Transformer(DiT) 프레임워크를 통해 이"},{"id":"2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems","title":"[논문리뷰] A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems","excerpt":"Xinhao Yi이 arXiv에 게시한 'A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems","tags":["Review","Self-Evolving AI Agents","Lifelong Learning","Foundation Models","Multi-Agent Systems","Agent Optimization","Prompt Engineering","Tool Use","AI Safety","Survey"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, Zhaochun Ren, Nikos Aletras, Xi Wang, Han Zhou, Zaiqiao Meng 핵심 연구 "},{"id":"2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents","title":"[논문리뷰] Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents","excerpt":"Mohit Bansal이 arXiv에 게시한 'Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents","tags":["Review","Multimodal LLM","Diffusion Model","CLIP Latent","Image Generation","Multimodal Understanding","ControlNet","Training Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Han Lin, Jaemin Cho, Amir Zadeh, Chuan Li, Mohit Bansal 핵심 연구 목표 본 연구는 강력한 추론 능력을 유지하면서도 고품질 시각적 합성 기능을 LLM에 통합하는 것을 목표로 합니다. 특히, 기존 방식들이 높은 훈련 비용을 수반하고 백본 LLM의 이미지 표현 학습 부족으로 어려"},{"id":"2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent","title":"[논문리뷰] BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent","excerpt":"Kai Zou이 arXiv에 게시한 'BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent","tags":["Review","Benchmarking","Deep-Research Agents","LLMs","Retrieval","Curated Corpus","Evaluation","Fairness","Transparency","Reproducibility"],"text":"링크: 논문 PDF로 바로 열기 저자: Kai Zou, Ping Nie, Shengyao Zhuang, Xueguang Ma, Zijian Chen 핵심 연구 목표 현재 DeepResearch 에이전트 평가 벤치마크(예: BrowseComp)는 라이브 웹 검색 API 에 의존하여 공정성, 재현성 및 투명성 측면에서 중대한 한계를 가집니다. 이는 동적이고 불"},{"id":"2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy","title":"[논문리뷰] Compressing Chain-of-Thought in LLMs via Step Entropy","excerpt":"Zhijian Xu이 arXiv에 게시한 'Compressing Chain-of-Thought in LLMs via Step Entropy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy","tags":["Review","LLM","Chain-of-Thought","CoT Compression","Step Entropy","Reinforcement Learning","SFT","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu 핵심 연구 목표 Large Language Models(LLMs)의 ChainofThought(CoT) 추론 과정에서 발생하는 과도한 상세함"},{"id":"2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs","title":"[논문리뷰] Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs","excerpt":"Robert Kirk이 arXiv에 게시한 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs","tags":["Review","LLMs","데이터 필터링","사전 학습","변조 저항성","바이오위협","AI 안전","서킷 브레이킹","머신 언러닝"],"text":"링크: 논문 PDF로 바로 열기 저자: Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman 핵심 연구 목표 본 논문은 오픈웨이트 대규모 언어 모델("},{"id":"2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System","title":"[논문리뷰] Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System","excerpt":"Reynold Cheng이 arXiv에 게시한 'Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System","tags":["Review","Adversarial Attack","Poisoning Attack","Fact-checking","LLM Agent","Retrieval Augmented Generation","Misinformation","System Security"],"text":"링크: 논문 PDF로 바로 열기 저자: Haorui He, Yupeng Li, Bin Benjamin Zhu, Dacheng Wen, Reynold Cheng, Francis C. M. Lau 핵심 연구 목표 본 연구는 최신 LLM 기반 에이전트 팩트체킹 시스템 이 잘못된 정보를 확산시키거나 진실을 훼손할 수 있는 포이즈닝 공격에 취약함을 지적합니다. 기존"},{"id":"2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control","title":"[논문리뷰] Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control","excerpt":"Hongyu Liu이 arXiv에 게시한 'Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control","tags":["Review","Image Editing","Shape Transformation","Rectified Flow","Trajectory Divergence Map","Region Control","Generative Models","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, Yue Ma 핵심 연구 목표 이 논문은 기존 flow기반 이미지 편집 모델이 대규모 형상 변환(largescale shape tr"},{"id":"2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks","title":"[논문리뷰] GLiClass: Generalist Lightweight Model for Sequence Classification Tasks","excerpt":"Alexander Yavorskyi이 arXiv에 게시한 'GLiClass: Generalist Lightweight Model for Sequence Classification Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks","tags":["Review","Sequence Classification","Zero-shot Learning","Few-shot Learning","Transformer","Multi-label Classification","PPO","GLiNER","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko 핵심 연구 목표 본 연구는 기존 제로샷 텍스트 분류 모델(생성형 LLM, 크로스 인코더, 임베딩 기반 모델)의 한계점"},{"id":"2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts","title":"[논문리뷰] Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts","excerpt":"Tieyuan Chen이 arXiv에 게시한 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts","tags":["Review","Mixture of Experts","LLMs","MoE Architecture","Dynamic Activation","Adjugate Experts","Upcycling Strategy","Load Balancing"],"text":"링크: 논문 PDF로 바로 열기 저자: Tieyuan Chen, Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Haoyuan Wu 핵심 연구 목표 본 논문은 기존 MoE (Mixture of Experts) LLM의 한계인 고정된 파라미터 활성화와 이로 인한 비효율적인 계산 문제를 해결하는 것을 목표로 합니다. 특히, 입력 "},{"id":"2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization","title":"[논문리뷰] Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization","excerpt":"Guanting Dong이 arXiv에 게시한 'Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization","tags":["Review","Reasoning LLMs","Reinforcement Learning","PPO","Gradient Clipping","Supervised Fine-tuning","Math Reasoning","Code Generation","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Fuzheng Zhang, Guorui Zhou 핵심 연구 목표 본 논문은 고성능 추론 모델의 훈련 세부사항이 불완전하게 공개되어 재현이 어려운 문제를 해결하고, "},{"id":"2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning","title":"[논문리뷰] Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning","excerpt":"Baihong Yuan이 arXiv에 게시한 'Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning","tags":["Review","Sparse Attention","LLMs","Reasoning Tasks","Efficiency","Training-Free","Global Locality","KV Cache Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Lijie Yang, Zhihao Zhang, Arti Jain, Shijie Cao, Baihong Yuan, Yiwei Chen, Zhihao Jia, Ravi Netravali 핵심 연구 목표 본 논문은 대규모 추론 모델(LRMs)의 긴 토큰 생성 과정에서 발생하는 막대한 계산 오버헤드를 해결하는 것을 목표로 합"},{"id":"2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs","title":"[논문리뷰] MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs","excerpt":"Jianguo Li이 arXiv에 게시한 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs","tags":["Review","Mixture-of-Experts (MoE)","LLM Compression","Matrix Decomposition","Parameter Efficiency","Deep Learning","Memory Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaodong Chen, Mingming Ha, Zhenzhong Lan, Jing Zhang, Jianguo Li 핵심 연구 목표 대규모 MoE 기반 LLM(예: DeepSeekV30324 , KimiK2Instruct )의 막대한 메모리 요구사항으로 인한 배포 병목 현상을 해결하고자 합니다. 기존 MoE 압축 방"},{"id":"2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space","title":"[논문리뷰] MolmoAct: Action Reasoning Models that can Reason in Space","excerpt":"Shuo Liu이 arXiv에 게시한 'MolmoAct: Action Reasoning Models that can Reason in Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space","tags":["Review","Robotics","Action Reasoning","Vision-Language Models","Spatial Planning","Depth Perception","Trajectory Generation","Explainable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu 핵심 연구 목표 기존 로봇 파운데이션 모델들이 지각과 명령을 직접 제어로 매핑하여 적응성, 일반화, 의미론적 기반이 부족한 문제를 해결하는 것을 목표로 합니다. 본 연구는 Action Reasoning Model"},{"id":"2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","title":"[논문리뷰] Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation","excerpt":"Xiaokun Feng이 arXiv에 게시한 'Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","tags":["Review","Visual Effects","Video Generation","LoRA","Mixture of Experts","Spatial Control","Diffusion Models","Multi-VFX"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Xiangxiang Chu 핵심 연구 목표 본 논문은 기존 비디오 생성 모델들이 개별 효과에 특화된 LoRA 훈련으로 인해 복합 시각 효과"},{"id":"2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks","title":"[논문리뷰] OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks","excerpt":"Hongxing Li이 arXiv에 게시한 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks","tags":["Review","Embodied AI","Agent Reasoning","LLM","Benchmarking","Tool Use","Multi-Agent Systems","Physical Interaction","Constraint Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongxing Li, Dingming Li, tricktreat, yanyc, wangzx1210 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)이 물리적 상호작용, 도구 사용, 다중 에이전트 협업이 필요한 구체화된(embodied) 태스크 에서 얼마나 잘 추론하는지 평가하기 위한 종합적인 프레임워크인 Omni"},{"id":"2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning","title":"[논문리뷰] Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning","excerpt":"Jiaheng Liu이 arXiv에 게시한 'Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning","tags":["Review","Reinforcement Learning","Large Language Models","LLM Reasoning","Policy Optimization","Normalization","Clipping","Loss Aggregation","Overlong Filtering"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaheng Liu, Weixun Wang, Yancheng He, Jiashun Liu, Zihe Liu 핵심 연구 목표 본 논문은 LLM 추론을 위한 강화 학습(RL) 기술의 급속한 발전으로 인해 발생하는 파편화된 이해, 불일치한 실험 설정 및 모호한 가이드라인 문제를 해결하고자 합니다. RL 기술의 내부 메커니"},{"id":"2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability","title":"[논문리뷰] ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability","excerpt":"Yuchen Li이 arXiv에 게시한 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability","tags":["Review","Passage Ranking","Reasoning Models","Large Language Models","Data Synthesis","Reinforcement Learning","Listwise Reranking","Information Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou 핵심 연구 목표 기존 패시지 랭킹 모델들이 추론 집약적(reasoningintensive) 훈련 데이터 부족 으로 인해 복잡한 검색 시나리오에서 낮은 성능을 보이는 문제를 해결"},{"id":"2025-8-12-Reinforcement-Learning-in-Vision-A-Survey","title":"[논문리뷰] Reinforcement Learning in Vision: A Survey","excerpt":"Qingwei Meng이 arXiv에 게시한 'Reinforcement Learning in Vision: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey","tags":["Review","Reinforcement Learning (RL)","Computer Vision (CV)","Multimodal Large Language Models (MLLMs)","Visual Generation","Vision-Language-Action (VLA) Models","Policy Optimization","Reward Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou 핵심 연구 목표 본 연구는 강화 학습(RL)과 시각 지능의 교차점에서 발전한 에이전트의 현황을 체계적으로 종합"},{"id":"2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation","title":"[논문리뷰] Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation","excerpt":"Hengtao Shen이 arXiv에 게시한 'Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation","tags":["Review","Robot Learning","Generalization","Shortcut Learning","Dataset Diversity","Dataset Fragmentation","Data Augmentation","Imitation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, Jingkuan Song 핵심 연구 목표 본 논문은 일반 로봇 정책의 제한된 일반화 능력의 근본 원인을 규명하고자 합니다. 특히, 이 일반화의 주요 장애물인지 조사합니다. 개별 서브 데이터셋 내의 과 서브 데이"},{"id":"2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences","title":"[논문리뷰] Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences","excerpt":"Matvey Skripkin이 arXiv에 게시한 'Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences","tags":["Review","Speech-to-LaTeX","ASR","Language Models","Multimodal AI","Dataset Creation","Mathematical Expression Recognition","LaTeX Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Dmitrii Korzh, Dmitrii Tarasov, Artyom Iudin, Elvir Karimov, Matvey Skripkin 핵심 연구 목표 본 연구는 음성으로 표현된 수학 방정식과 문장을 LaTeX 형식으로 변환하는 도전적인 문제를 해결하고자 합니다. 기존 연구의 한계점(예: 이중 ASR 전사 의존성, "},{"id":"2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future","title":"[논문리뷰] Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future","excerpt":"Qiufeng Wang이 arXiv에 게시한 'Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future","tags":["Review","Self-Rewarding LLMs","Direct Preference Optimization (DPO)","Preference Learning","Generative AI","Gradient Collapse","LLM Alignment","Iterative Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, Shikun Zhang 핵심 연구 목표 본 논문은 기존의 SelfRewarding"},{"id":"2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents","title":"[논문리뷰] UserBench: An Interactive Gym Environment for User-Centric Agents","excerpt":"Jianguo Zhang이 arXiv에 게시한 'UserBench: An Interactive Gym Environment for User-Centric Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents","tags":["Review","User-Centric AI","LLM Evaluation","Interactive Agents","Gym Environment","Preference Elicitation","Multi-turn Dialogue","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Qian, Zuxin Liu, Akshara Prabhakar, Zhiwei Liu, Jianguo Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트가 사용자의 모호하고, 변화하며, 간접적으로 표현되는 목표 에 대해 능동적으로 협력하는 능력을 평가하고자 합니다. 기존 에이전트 평"},{"id":"2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding","title":"[논문리뷰] VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding","excerpt":"Tong Yu이 arXiv에 게시한 'VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding","tags":["Review","Multimodal Retrieval","Retrieval-Augmented Generation","Long Document Understanding","Multilingual NLP","Visual QA","Benchmark","MLLMs","Table Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Chen, Ming Li, Jihyung Kil, Chenguang Wang, Tong Yu, Ryan Rossi, Tianyi Zhou, Changyou Chen, Ruiyi Zhang 핵심 연구 목표 본 논문은 기존 벤치마크의 영어 단일 언어 및 단일 페이지 제한을 넘어, 다국어 장문 문서 에서 질문 기반"},{"id":"2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs","title":"[논문리뷰] When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs","excerpt":"Dasol Choi이 arXiv에 게시한 'When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs","tags":["Review","Audio-Language Models","Jailbreak Attack","Adversarial Audio","Reinforcement Learning","Projected Gradient Descent","Native Payload Discovery","Multimodal AI Safety"],"text":"링크: 논문 PDF로 바로 열기 When Good Sounds Go Adversarial: Jailbreaking AudioLanguage Models with Benign Inputs 저자: Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, "},{"id":"2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking","title":"[논문리뷰] WideSearch: Benchmarking Agentic Broad Info-Seeking","excerpt":"Yan Gao이 arXiv에 게시한 'WideSearch: Benchmarking Agentic Broad Info-Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-12 13:29:09+0900","permalink":"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking","tags":["Review","Agentic Search","LLM","Benchmark","Information Seeking","Structured Output","Evaluation Metrics","Multi-agent Systems"],"text":"링크: 논문 PDF로 바로 열기 저자: Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, Ke Wang 핵심 연구 목표 본 논문은 광범위한 정보 탐색(WideSearch"},{"id":"2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval","title":"[논문리뷰] Adversarial Video Promotion Against Text-to-Video Retrieval","excerpt":"Shuai Liu이 arXiv에 게시한 'Adversarial Video Promotion Against Text-to-Video Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval","tags":["Review","Adversarial Attack","Video Promotion","Text-to-Video Retrieval","Modality Refinement","Black-box Attack","Video Manipulation","Transferability"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Shuai Liu, Qian Li, Chao Shen 핵심 연구 목표 본 논문은 텍스트비디오 검색(T2VR) 모델의 간과된 취약점인 적대적 비디오 프로모션 공격 을 탐구합니다. 기존 공격이 비디오 순위를 하락시키는 데 초점을 맞춘 것과 달리, 재정적"},{"id":"2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math","title":"[논문리뷰] Aryabhata: An exam-focused language model for JEE Math","excerpt":"Sandeep Varma이 arXiv에 게시한 'Aryabhata: An exam-focused language model for JEE Math' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math","tags":["Review","Language Model","Math Reasoning","JEE","Supervised Fine-Tuning","Reinforcement Learning","Model Merging","Chain-of-Thought","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Sandeep Varma, Sachin Dharashivkar, RitvikPW 핵심 연구 목표 본 논문은 인도 입학 시험(JEE) 수학 영역에 최적화된 7B 파라미터 의 경량 언어 모델인 Aryabhata 1.0 을 제안합니다. 기존 대규모 언어 모델(LLM)이 교육적 활용에 부적합했던 문제를 해결하고, 학생 이해를"},{"id":"2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators","title":"[논문리뷰] AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators","excerpt":"Tao Zhang이 arXiv에 게시한 'AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators","tags":["Review","코드 생성","대규모 언어 모델","코드 벤치마크","다국어 프로그래밍","자동화된 데이터 생성","샌드박스 평가","멀티모달 AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jason Chou, Ao Liu, Yuchi Deng, Zhiying Zeng, Tao Zhang 핵심 연구 목표 기존 코드 생성 벤치마크의 한계(수동 어노테이션 의존, Python 중심, 난이도 및 다양성 부족)를 해결하고, LLM의 코드 생성 능력을 포괄적으로 평가하기 위해 높은 난이도를 가진 다국어 코드 생성 "},{"id":"2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL","title":"[논문리뷰] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL","excerpt":"Chuyi He이 arXiv에 게시한 'Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL","tags":["Review","Reinforcement Learning","LLM Agents","Agentic Search","Asynchronous RL","Long-Horizon Planning","Tool Use","Data Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu 핵심 연구 목표 본 논문은 기존 오픈소스 LLM 기반 에이전트의 '검색 인텔리전스'가 전문가 수준에 미치지 못하며, 모호한 질의 해결, 정확한 검색 생성, 결과 분"},{"id":"2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them","title":"[논문리뷰] BiasGym: Fantastic Biases and How to Find (and Remove) Them","excerpt":"Arnav Arora이 arXiv에 게시한 'BiasGym: Fantastic Biases and How to Find (and Remove) Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them","tags":["Review","Bias Mitigation","LLMs","Mechanistic Interpretability","Fine-tuning","Attention Steering","Stereotype Analysis","Safety Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Sekh Mainul Islam, Nadav Borenstein, Siddhesh Milind Pawar, Haeun Yu, Arnav Arora, Isabelle Augenstein 핵심 연구 목표 대규모 언어 모델(LLM)에 인코딩된 편향과 고정관념을 신뢰할 수 있게 감지하고 완화하기 위한 간단하고 비용 효율적이며"},{"id":"2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware","title":"[논문리뷰] Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware","excerpt":"Jhon Alejandro Andrade이 arXiv에 게시한 'Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware","tags":["Review","Quantum Game Theory","NISQ Hardware","Error Mitigation","Battle of the Sexes","Qiskit","Quantum Computing","Strategic Coordination","Payoff Maximization"],"text":"링크: 논문 PDF로 바로 열기 저자: Germán Díaz Agreda, Carlos Andres Duran Paredes, Mateo Buenaventura Samboni, Jhon Alejandro Andrade, Sebastián Cajas Ordoñez 핵심 연구 목표 본 논문은 양자 게임 이론의 \"Battle of the Sexes\" 게임을 실제"},{"id":"2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation","title":"[논문리뷰] CharacterShot: Controllable and Consistent 4D Character Animation","excerpt":"Fei Shen이 arXiv에 게시한 'CharacterShot: Controllable and Consistent 4D Character Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation","tags":["Review","4D Character Animation","Diffusion Models","Gaussian Splatting","Pose Control","Multi-view Synthesis","Temporal Consistency","Character Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyao Gao, Jiaxing Li, Wenran Liu, Yanhong Zeng, Fei Shen, Kai Chen, Yanan Sun, Cairong Zhao 핵심 연구 목표 본 논문은 단일 캐릭터 이미지와 2D 포즈 시퀀스를 입력으로 받아, 사용자가 제어할 수 있는 동적인 3D 캐릭터(4D 캐릭터 애니메이션"},{"id":"2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning","title":"[논문리뷰] Cut2Next: Generating Next Shot via In-Context Tuning","excerpt":"Yu Qiao이 arXiv에 게시한 'Cut2Next: Generating Next Shot via In-Context Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning","tags":["Review","Next Shot Generation","In-Context Tuning","Diffusion Transformer","Cinematic Continuity","Hierarchical Prompting","Video Generation","Shot Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingwen He, Hongbo Liu, Jiajun Li, Ziqi Huang, Yu Qiao, Wanli Ouyang, Ziwei Liu 핵심 연구 목표 본 논문은 기존 비디오 생성 모델이 간과했던 영화적 내러티브 흐름과 편집 패턴(예: Shot/Reverse Shot , CutOut , Cutaway )을 준수"},{"id":"2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition","title":"[논문리뷰] DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition","excerpt":"Lukáš Burget이 arXiv에 게시한 'DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition","tags":["Review","Speech Recognition","Encoder-Decoder","Regularization","Decoder-Centric","Intermediate Supervision","Out-of-Domain Generalization","Internal Language Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexander Polok, Santosh Kesiraju, Karel Beneš, Bolaji Yusuf, Lukáš Burget, Jan Černocký 핵심 연구 목표 본 논문은 EncoderDecoder 기반 자동 음성 인식(ASR) 모델의 내부 언어 모델(ILM) 견고성을 향상시켜 도메인 내외(in and "},{"id":"2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy","title":"[논문리뷰] Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy","excerpt":"Elizabeth Karpinski이 arXiv에 게시한 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy","tags":["Review","Large Language Models","Diplomacy Game","Multi-agent Systems","Strategic Reasoning","LLM Evaluation","Prompt Engineering","Behavioral Analysis","Game AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexander Duffy, Samuel J Paech, Ishana Shastri, Elizabeth Karpinski, Baptiste AllouiCros, Tyler Marques, Matthew Lyle Olson 핵심 연구 목표 본 연구는 복잡한 전략적 추론 능력 을 요구하는 외교(Diplomacy) 게임에"},{"id":"2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments","title":"[논문리뷰] Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments","excerpt":"Xuesong Yao이 arXiv에 게시한 'Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments","tags":["Review","Large Language Models (LLMs)","Tool Use","Reinforcement Learning (RL)","Automated Environment Generation","Feedback-Driven Training","Reward Mechanism","Contextual Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Xuanjing Huang, Jiecao Chen 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 효율적인 도구 사용(tool use)"},{"id":"2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay","title":"[논문리뷰] GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay","excerpt":"Yang Fan이 arXiv에 게시한 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay","tags":["Review","Continual Learning","Large Language Models (LLMs)","Catastrophic Forgetting","Replay","Knowledge Distillation","Activation States","Anti-forgetting","Threshold-based Margin Loss"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen 핵심 연구 목표 대규모 언어 모델(LLM)의 연속 학습 시 발생하는 파국적 망각(catastrophic forgetting) 문제를 해결하는 것이 주된 목"},{"id":"2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches","title":"[논문리뷰] HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches","excerpt":"Qiang Ju이 arXiv에 게시한 'HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches","tags":["Review","Hierarchical Reinforcement Learning","Deep Search","Multi-source RAG","Agentic AI","Knowledge Integration","Enterprise Search","Large Reasoning Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, JiRong Wen 핵심 연구 목표 이 논문은 기업 환경에서 로컬(사내 문서/지식 그래프) 및 웹 지식 소스 를 동시에 활용하는 딥 서치 시스템의 필요성에 주목합니다. 기존 단일 소스 딥 서치나 평"},{"id":"2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation","title":"[논문리뷰] Matrix-3D: Omnidirectional Explorable 3D World Generation","excerpt":"Yuqi Li이 arXiv에 게시한 'Matrix-3D: Omnidirectional Explorable 3D World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation","tags":["Review","3D World Generation","Panoramic Video Generation","3D Reconstruction","Diffusion Models","Gaussian Splatting","Dataset","Camera Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongqi Yang, Wenhang Ge, Yuqi Li, Jiaqi Chen, Haoyuan Li, Mengyin An, Fei Kang, Hua Xue, Baixin Xu, Yuyang Yin, Eric Li, Yang Liu, Yikai Wang, HaoXiang Guo, Yahui Zhou 핵심 연구 목표 "},{"id":"2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations","title":"[논문리뷰] NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations","excerpt":"Haoyue Zhan이 arXiv에 게시한 'NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations","tags":["Review","Paralinguistic Vocalizations","Speech Recognition","Text-to-Speech","Speech Synthesis","Data Annotation","Mandarin Speech","Expressive Speech"],"text":"링크: 논문 PDF로 바로 열기 저자: Huan Liao, Qinke Ni, Yuancheng Wang, Yiheng Lu, Haoyue Zhan 핵심 연구 목표 본 연구는 자연스러운 음성 의사소통에 필수적인 웃음, 호흡, 감탄사 등의 비언어적 발성(paralinguistic vocalizations) 이 기존 ASR 및 TTS 시스템에서 간과되는 문제를 "},{"id":"2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents","title":"[논문리뷰] OpenCUA: Open Foundations for Computer-Use Agents","excerpt":"Tianbao Xie이 arXiv에 게시한 'OpenCUA: Open Foundations for Computer-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents","tags":["Review","Computer-Use Agents","Vision-Language Models","Chain-of-Thought Reasoning","Large-scale Dataset","Open-source Framework","Desktop Automation","Agent Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianbao Xie, Junlin Yang, Dunjie Lu, Bowen Wang, Xinyuan Wang et al. 핵심 연구 목표 본 논문은 상업용 컴퓨터 사용 에이전트(CUA) 시스템의 핵심 세부 정보가 비공개인 현 상황에서, 연구 커뮤니티가 CUA의 역량, 한계, 위험을 연구할 수 있는 포괄적인 오픈 소스"},{"id":"2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency","title":"[논문리뷰] Test-Time Reinforcement Learning for GUI Grounding via Region Consistency","excerpt":"Zhengxi Lu이 arXiv에 게시한 'Test-Time Reinforcement Learning for GUI Grounding via Region Consistency' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency","tags":["Review","GUI Grounding","Test-Time Scaling","Reinforcement Learning","Region Consistency","Spatial Voting","Self-Supervised Learning","Vision-Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, Yongliang Shen 핵심 연구 목표 이 논문은 픽셀 수준 주석의 높은 비용 과 기존 훈련 방식의 한계 로 인해 GUI 접지(grounding)의 성능 확장성에 제약"},{"id":"2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models","title":"[논문리뷰] Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models","excerpt":"Chenchen Jing이 arXiv에 게시한 'Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Temporal Oscillation","Self-Consistency Voting","Reinforcement Learning","Temporal Semantic Entropy","Text Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen 핵심 연구 목표 본 논문은 확산 언어 모델(dLLMs)이 텍스트를 생성하는 반복적인 디노이징 과정에서 \"시간적 진동"},{"id":"2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation","title":"[논문리뷰] TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation","excerpt":"Rachel Bawden이 arXiv에 게시한 'TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation","tags":["Review","Low-Resource MT","Data Augmentation","Large Language Models (LLMs)","Back-Translation","In-Context Learning (ICL)","Fine-Tuning","Topic-Guided Generation","Parallel Data Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Armel Zebaze, Benoît Sagot, Rachel Bawden 핵심 연구 목표 본 연구는 저자원 언어(LRL) 기계 번역(MT) 모델의 성능 향상을 위해, 고품질의 주제 다양성(topicdiverse) 을 가진 병렬 데이터를 자동으로 생성하는 방법을 제시합니다. 기존의 병렬 데이터 부족 문제를 해결하고, "},{"id":"2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors","title":"[논문리뷰] Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors","excerpt":"Haoran Xu이 arXiv에 게시한 'Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors","tags":["Review","Robotic Dexterous Grasping","Affordance-Aware","Human-like Priors","Reinforcement Learning","Vision-Language Models","Two-Stage Training","Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou 핵심 연구 목표 이 논문은 로봇의 능숙한 "},{"id":"2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning","title":"[논문리뷰] Train Long, Think Short: Curriculum Learning for Efficient Reasoning","excerpt":"Marzyeh Ghassemi이 arXiv에 게시한 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning","tags":["Review","Curriculum Learning","Reinforcement Learning","Large Language Models","Reasoning Efficiency","Token Budget Control","Group Relative Policy Optimization","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie BouZeid, Marzyeh Ghassemi 핵심 연구 목표 대규모 언어 모델(LLMs)의 추론 능력 향상 과정에서 발생하는 비효율성, 즉 고정된 토큰 예산의 한계와 과도하게 긴 추론 과정의 문제를 해"},{"id":"2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation","title":"[논문리뷰] UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation","excerpt":"Kevin Galim이 arXiv에 게시한 'UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Masked Generative Transformers","Compositional Generation","Attention Guidance","Unmasking Strategy","Contrastive Learning","Training-Free","Attribute Binding"],"text":"링크: 논문 PDF로 바로 열기 저자: Wonjun Kang, Byeongkeun Ahn, Minjae Lee, Kevin Galim, Seunghyuk Oh, Hyung Il Koo, Nam Ik Cho 핵심 연구 목표 본 논문은 Masked Generative Transformers (MGTs)를 사용한 텍스트이미지(T2I) 생성 시 발생하는 조합적 충"},{"id":"2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail","title":"[논문리뷰] VertexRegen: Mesh Generation with Continuous Level of Detail","excerpt":"Jakob Engel이 arXiv에 게시한 'VertexRegen: Mesh Generation with Continuous Level of Detail' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail","tags":["Review","Mesh Generation","Level of Detail (LOD)","Progressive Meshes","Vertex Split","Autoregressive Models","Transformer","3D Graphics"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiang Zhang, Yawar Siddiqui, Armen Avetisyan, Chris Xie, Jakob Engel, Henry HowardJenkins 핵심 연구 목표 기존 자동회귀 메쉬 생성 모델들이 부분완료 방식으로 동작하여, 유효한 메쉬를 얻기 위해 전체 시퀀스를 생성해야만 하고 중간 단계에서는 불완전한"},{"id":"2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion","title":"[논문리뷰] WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion","excerpt":"Rachid Nedjai이 arXiv에 게시한 'WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-13 13:29:23+0900","permalink":"/ai/review/2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion","tags":["Review","Spatio-Temporal Fusion","Land Surface Temperature","Generative Adversarial Network","Weakly-Supervised Learning","Remote Sensing","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Sofiane Bouaziz, Adel Hafiane, Raphaël Canals, Rachid Nedjai 핵심 연구 목표 현재 원격 감지 위성은 지표면 온도(LST) 데이터의 공간 및 시간 해상도 간 트레이드오프 문제를 겪고 있으며, 특히 일별 10m 해상도 LST 추정은 어렵습니다. 본 연구는 이러한 한계를 극복"},{"id":"2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance","title":"[논문리뷰] AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance","excerpt":"Yong Li이 arXiv에 게시한 'AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance","tags":["Review","Large Language Models","Fine-tuning","Reinforcement Learning","Meta-learning","Adaptive Control","Imitation Learning","Exploration","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Lixuan He, Jie Feng, Yong Li 핵심 연구 목표 대규모 언어 모델(LLM)이 추론 태스크에서 겪는 catastrophic forgetting 및 모방(imitation) 과 탐색(exploration) 간의 최적화되지 않은 트레이드오프 문제를 해결하는 것이 목표입니다. 기존의 이단계(SFT 후 RL"},{"id":"2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving","title":"[논문리뷰] AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving","excerpt":"Jinjie Gu이 arXiv에 게시한 'AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving","tags":["Review","Multi-Agent System","Agent Stability","LLM","Tool Use","GAIA Benchmark","Robustness","Dynamic Supervision","Maneuvering"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinjie Gu, Chenyi Zhuang, Chengyue Yu, Qintong Wu, Zhitian Xie 핵심 연구 목표 대규모 언어 모델(LLM) 기반 에이전트가 외부 도구를 활용할 때 발생하는 확장된 컨텍스트 및 노이즈/관련성 없는 도구 출력 으로 인한 시스템 신뢰성 및 정확도 저하 문제를 해결하고, 에이전"},{"id":"2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study","title":"[논문리뷰] Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study","excerpt":"Gjergji Kasneci이 arXiv에 게시한 'Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study","tags":["Review","Explainable NLP","Natural Language Explanations","Large Language Models","Pre-trained Language Models","Natural Language Inference","Model Performance Enhancement","Text Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mahdi Dhaini, Juraj Vladika, Ege Erdogan, Zineb Attaoui, Gjergji Kasneci 핵심 연구 목표 본 연구는 비용이 많이 들고 확장성이 낮은 인간 주석 기반 설명의 한계를 극복하기 위해, LLM이 생성한 텍스트 설명 이 자연어 추론(NLI)과 같은 다운스트림 예측 태스크"},{"id":"2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models","title":"[논문리뷰] Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models","excerpt":"Guiyang Hou이 arXiv에 게시한 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models","tags":["Review","Reinforcement Learning","Large Language Models","Reward Model","Policy Optimization","Reward Hacking","Hybrid Annotation","Mathematical Reasoning","Verifiable Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Haitao Hong, Yuchen Yan, Xingyu Wu, Guiyang Hou, Wenqi Zhang, Weiming Lu, Yongliang Shen, Jun Xiao 핵심 연구 목표 대규모 언어 모델(LLMs)의 추론 능력 강화를 위한 강화 학습(RL) 시, 기존 보상 모델(Reward Model, RM)이"},{"id":"2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing","title":"[논문리뷰] Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing","excerpt":"Hao Zhang이 arXiv에 게시한 'Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing","tags":["Review","Diffusion LLMs","Faster Inference","Discrete Diffusion Forcing (D2F)","Autoregressive Generation","KV Cache Optimization","Parallel Decoding","Text Generation","Model Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, Zhijie Deng 핵심 연구 목표 본 논문은 기존 오픈소스 Diffusion Large Language Models (dLLMs)가 Autoregressive (AR) LLMs에 비해 추론 속도에서 우위를 점하지 못"},{"id":"2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation","title":"[논문리뷰] Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation","excerpt":"Zhenghao Hu이 arXiv에 게시한 'Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation","tags":["Review","Synthetic Data","Image Generation","GPT-4o","Multimodal Models","Instruction Following","Surreal Image Generation","Dataset","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenghao Hu, Leqi Zhu, Zihao Wang, Dongzhi Jiang, Junyan Ye 핵심 연구 목표 본 논문은 GPT4o 로 생성된 합성 이미지 데이터를 활용하여 오픈소스 이미지 생성 모델이 겪는 성능 격차를 해소하는 것을 목표로 합니다. 특히, 실제 데이터셋에서 부족한 초현실적 판타지 시나리오"},{"id":"2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors","title":"[논문리뷰] GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors","excerpt":"Qingnan Fan이 arXiv에 게시한 'GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors","tags":["Review","3D Gaussian Splatting","Novel View Synthesis","Diffusion Model","Artifact Restoration","Sparse-view 3D Reconstruction","Reference-Guided"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingyilang Yin, Qi Zhang, Jiahao Chang, Ying Feng, Qingnan Fan, Xi Yang, ChiMan Pun, Huaqi Zhang, Xiaodong Cun 핵심 연구 목표 본 논문은 적은 수의 입력 영상으로 3D Gaussian Splatting (3DGS) 장면을 재구성할 "},{"id":"2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding","title":"[논문리뷰] IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding","excerpt":"Di Zhang이 arXiv에 게시한 'IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding","tags":["Review","Backdoor Attack","Vision-Language Models (VLMs)","Visual Grounding","Input-aware Trigger","Adversarial Attack","Security","U-Net","Open-vocabulary"],"text":"링크: 논문 PDF로 바로 열기 저자: Junxian Li, Beining Xu, Di Zhang 핵심 연구 목표 이 연구는 시각적 그라운딩(Visual Grounding) 태스크를 수행하는 VisionLanguage Models (VLMs) 에 대한 새로운 입력 인지(Inputaware) 백도어 공격(Backdoor Attack) 시나리오와 방법론인 IA"},{"id":"2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment","title":"[논문리뷰] Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment","excerpt":"Lei Fan이 arXiv에 게시한 'Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment","tags":["Review","LLM Alignment","Reinforcement Learning from Human Feedback","Preference Learning","Group Relative Alignment Optimization","Self-Optimization","Mixture-of-Experts","Imitation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Haowen Wang, Yun Yue, Zhiling Ye, Shuowen Zhang, Lei Fan, Jiaxin Liang, Jiadi Jiang, Cheng Wei, Jingyuan Deng, Xudong Han, Ji Li, Chunxiao Guo, Peng Wei, Jian Wang, Jinjie Gu 핵심 "},{"id":"2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models","title":"[논문리뷰] MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models","excerpt":"Zhihan Zhou이 arXiv에 게시한 'MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models","tags":["Review","Multimodal Large Language Models (MLLMs)","Math Reasoning","Real-World Benchmark","Visual Perception","Robustness","K-12 Education","Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Jun Feng, Zixin Wang, Zhentao Zhang, Yue Guo, Zhihan Zhou, Xiuyi Chen, Zhenyang Li, Dawei Yin 핵심 연구 목표 기존 MLLM 수학 추론 벤치마크들이 대부분 깨끗하거나 전처리된 이미지를 사용하는 한계를 극복하고자 합니다. 실제 K12 교육 환경에서"},{"id":"2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery","title":"[논문리뷰] Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery","excerpt":"Di Zhang이 arXiv에 게시한 'Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery","tags":["Review","Molecule Discovery","Chain-of-Thought","Large Language Models","Reinforcement Learning","Supervised Fine-tuning","Molecular Generation","Explainable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiatong Li, Weida Wang, Qinggang Zhang, Junxian Li, Di Zhang 핵심 연구 목표 본 논문은 Large Language Models (LLMs) 의 분자 발견 분야 적용 시 나타나는 설명 가능성 및 추론 성능 한계를 해결하는 것을 목표로 합니다. 특히, 텍스트 기반 분자 생성"},{"id":"2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models","title":"[논문리뷰] Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models","excerpt":"Zeynep Akata이 arXiv에 게시한 'Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models","tags":["Review","Diffusion Models","Hypernetworks","Test-Time Optimization","Reward-Guided Generation","Latent Space Optimization","LoRA","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, Zeynep Akata 핵심 연구 목표 본 논문은 확산 모델에서 추론 시 계산 비용을 크게 증가시키는 테스트시간 스케일링(testtime scaling) 의 문제점을 해결하고자 합니다. 모델이 추가"},{"id":"2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory","title":"[논문리뷰] Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory","excerpt":"Yuan Lin이 arXiv에 게시한 'Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory","tags":["Review","Multimodal Agent","Long-Term Memory","Episodic Memory","Semantic Memory","Reinforcement Learning","Video Question Answering","Entity-Centric Memory"],"text":"링크: 논문 PDF로 바로 열기 저자: Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li 핵심 연구 목표 본 논문은 실시간 멀티모달 입력(시각, 청각)을 지속적으로 처리하여 장기 기억을 구축하고 업데이트하며, 이를 기반으로 추론하여 복잡한 지시를 완료할 수 있는"},{"id":"2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","title":"[논문리뷰] Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation","excerpt":"Chen Li이 arXiv에 게시한 'Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","tags":["Review","Video Generation","Identity Preservation","Plug-and-Play","Diffusion Models","Self-Attention","Lightweight AI","Conditional Image Branch"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Xue, Qixin Yan, Wenjing Wang, Hao Liu, Chen Li 핵심 연구 목표 이 논문은 비디오 생성에서 사용자가 지정한 정체성을 고품질로 일관되게 유지하면서도, 기존 방법론의 과도한 훈련 파라미터 및 다른 AI 생성 모델과의 호환성 부족 문제를 해결하는 것을 목표로 합니다. 특히, 경"},{"id":"2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation","title":"[논문리뷰] Story2Board: A Training-Free Approach for Expressive Storyboard Generation","excerpt":"Dani Lischinski이 arXiv에 게시한 'Story2Board: A Training-Free Approach for Expressive Storyboard Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation","tags":["Review","Storyboard Generation","Text-to-Image","Diffusion Models","Training-Free","Character Consistency","Scene Diversity","Visual Storytelling"],"text":"링크: 논문 PDF로 바로 열기 저자: David Dinkevich, Matan Levy, Omri Avrahami, Dvir Samuel, Dani Lischinski 핵심 연구 목표 논문은 자연어 프롬프트로부터 표현력이 풍부하고 시각적으로 일관된 스토리보드를 생성하는 훈련 불필요(trainingfree) 프레임워크인 Story2Board를 제시합니다. "},{"id":"2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models","title":"[논문리뷰] VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models","excerpt":"Dongdong Zhang이 arXiv에 게시한 'VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-14 13:19:02+0900","permalink":"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models","tags":["Review","Multimodal LLM","Code Generation","Model Merging","Task Vectors","Vision-Language Model","Coding LLM","Instruction Tuning","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei 핵심 연구 목표 논문은 멀티모달 대규모 언어 모델(MLLM)이 시각적 입력으로부터 기능적인 코드를 생성하는 데 있어 한계가 있음을 지적합니다. 이를 해결하기 위해 시각적 이해와 고급 코딩 "},{"id":"2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP","title":"[논문리뷰] When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing","excerpt":"Gjergji Kasneci이 arXiv에 게시한 'When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP","tags":["Review","Natural Language Processing (NLP)","Explainable AI (XAI)","Post-hoc Explainability","Differential Privacy (DP)","Privacy-Utility Trade-off","Model Faithfulness","Text Privatization"],"text":"링크: 논문 PDF로 바로 열기 저자: Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci 핵심 연구 목표 이 논문은 NLP 분야에서 사후 설명 가능성(Posthoc Explainability) 과 차등 프라이버시(Differential Privacy) 의 교차점"},{"id":"2025-8-15-A-Survey-on-Diffusion-Language-Models","title":"[논문리뷰] A Survey on Diffusion Language Models","excerpt":"Zhiqiang Shen이 arXiv에 게시한 'A Survey on Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Generative AI","Parallel Decoding","Text Generation","Multimodal AI","Model Compression","Reinforcement Learning from Human Feedback","Inference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen 핵심 연구 목표 본 설문조사는 지배적인 자기회귀(AR) 패러다임 에 대한 강력하고 유망한 대안으로 부상하고 있는 확산 언어 모델(DLM) 의 전체 생태계를 체계적으로 포괄적으로 조명하는 것을 목표로 합니다. DLM의 근본 원"},{"id":"2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms","title":"[논문리뷰] From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms","excerpt":"Ziyin Zhang이 arXiv에 게시한 'From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms","tags":["Review","Automated Interpreting Assessment","Explainable AI","Data Augmentation","Variational Autoencoder","SHAP","Interpreting Quality","Natural Language Processing"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaokun Jiang, Ziyin Zhang 핵심 연구 목표 본 논문은 기존의 수동 통역 평가 방식의 한계(편향, 불일치)와 자동 평가 시스템의 불투명성 및 데이터 불균형 문제를 해결하고자 합니다. 특히 모델 예측에 대한 설명 가능성(Explainability) 을 강조하며, 통역 품질 평가를 위한 투명하고 다차원"},{"id":"2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs","title":"[논문리뷰] HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs","excerpt":"Yi Yuan이 arXiv에 게시한 'HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs","tags":["Review","Multimodal LLMs","Human-Centered AI","Empathy","Context-Awareness","MLLM Benchmark","Reinforcement Learning","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang 핵심 연구 목표 본 논문은 인간 중심 시나리오에서 MLLM(Multimodal Large Language Models) 의 심층적인 이해 및 공감적, 상황 인지적 응답 능력"},{"id":"2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale","title":"[논문리뷰] NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale","excerpt":"Quan Sun이 arXiv에 게시한 'NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale","tags":["Review","Autoregressive Models","Text-to-Image Generation","Continuous Latent Tokens","Flow Matching","Image Editing","Multimodal Learning","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Quan Sun, Jingwei Wu, Guopeng Li, Chunrui Han, NextStep Team 핵심 연구 목표 이 논문은 텍스트이미지 생성 분야에서 기존 autoregressive (AR) 모델이 직면한 양자화 손실 및 무거운 확산 모델 의존성 의 한계를 극복하고자 합니다. NextStep1 을 통해 연"},{"id":"2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts","title":"[논문리뷰] PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts","excerpt":"Rui Lu이 arXiv에 게시한 'PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts","tags":["Review","Long-Context Understanding","Reasoning Benchmark","LLMs Evaluation","Natural Language Processing","Global Comprehension","Fluid Intelligence","Prequel Entailment","RAG"],"text":"링크: 논문 PDF로 바로 열기 저자: Rui Lu, Tong Li, Chulun Zhou, Tsz Ting Chung, Mo Yu 핵심 연구 목표 이 논문은 기존 장문 컨텍스트 이해 벤치마크의 한계(기억력 의존, 얕은 추론, 전역적 의존성 부족 등)를 해결하고, 대규모 언어 모델(LLMs)의 전역적 이해(global comprehension) 및 심층 추"},{"id":"2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models","title":"[논문리뷰] Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models","excerpt":"Qinghao Ye이 arXiv에 게시한 'Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models","tags":["Review","Reinforcement Learning","Large Language Models","Exploration-Exploitation","Reward Design","Reasoning Tasks","Pass@k","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi 핵심 연구 목표 본 논문은 RLVR(Verifiable Rewards를 사용한 강화 학습) 환경에서 Pass@1 기반 훈련이 겪는 탐색활용 균형 문제, 즉 정책이 보수"},{"id":"2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera","title":"[논문리뷰] Processing and acquisition traces in visual encoders: What does CLIP know about your camera?","excerpt":"Giorgos Tolias이 arXiv에 게시한 'Processing and acquisition traces in visual encoders: What does CLIP know about your camera?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera","tags":["Review","Visual Encoders","Metadata","Image Processing","Image Acquisition","Robustness","CLIP","Foundation Models","Distribution Shift"],"text":"링크: 논문 PDF로 바로 열기 저자: Ryan Ramos, Vladan Stojnić, Giorgos KordopatisZilos, Yuta Nakashima, Giorgos Tolias, Noa Garcia 핵심 연구 목표 본 연구는 파운데이션 시각 인코더(Foundation Visual Encoders)가 이미지 처리(예: JPEG 압축) 및 획득(예"},{"id":"2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer","title":"[논문리뷰] STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer","excerpt":"Honghua Chen이 arXiv에 게시한 'STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer","tags":["Review","3D Reconstruction","Causal Transformer","Sequential Modeling","Streaming Data","Pointmap Prediction","Online Perception","KVCache"],"text":"링크: 논문 PDF로 바로 열기 저자: Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan 핵심 연구 목표 논문은 기존 다중 뷰 3D 재구성 방법론들이 높은 연산 비용을 요구하"},{"id":"2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing","title":"[논문리뷰] ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing","excerpt":"Xiaoyu Li이 arXiv에 게시한 'ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing","tags":["Review","Cartoon Generation","Video Diffusion Models","DiT","Post-Keyframing","Low-Rank Adaptation","Sparse Control","Generative AI","Animation"],"text":"링크: 논문 PDF로 바로 열기 저자: Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Qi Dou, Jinwei Gu, Tianfan Xue, Yaowei Li, Xiaoyu Li, Ying Shan 핵심 연구 목표 이 논문은 전통적인 카툰 제작 파이프라인의 핵심적인 병목 현상인 인비트위닝(inbetweening) 과 컬러라이제"},{"id":"2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT","title":"[논문리뷰] UI-Venus Technical Report: Building High-performance UI Agents with RFT","excerpt":"Shuheng Shen이 arXiv에 게시한 'UI-Venus Technical Report: Building High-performance UI Agents with RFT' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT","tags":["Review","UI Agent","MLLM","RFT","UI Grounding","UI Navigation","GRPO","Data Cleaning","Self-Evolving Trajectory"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuheng Shen, Xingran Zhou, Zhenyu Xu, Zhengwen Zeng, Zhangxuan Gu 핵심 연구 목표 본 논문은 스크린샷만을 입력으로 받는 고성능 UI 에이전트인 UIVenus 를 구축하는 것을 목표로 합니다. 기존 지도 미세 조정(SFT) 방식의 한계인 일반화 능력 부족과 높은 데이"},{"id":"2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning","title":"[논문리뷰] We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning","excerpt":"Xiaowan Wang이 arXiv에 게시한 'We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-15 13:09:31+0900","permalink":"/ai/review/2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning","tags":["Review","Visual Mathematical Reasoning","MLLMs","Knowledge System","Reinforcement Learning","Curriculum Learning","Dataset Construction","Mathematical Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang 핵심 연구 목표 복잡한 시각 수학적 추론에서 Multimodal Large Language Models (MLLMs) 의 한계를 극복하는 것을 목표로 합니다. 기존 연구가 데이터셋 구축과 모델 최적화에 집중하"},{"id":"2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding","title":"[논문리뷰] Controlling Multimodal LLMs via Reward-guided Decoding","excerpt":"Michal Drozdzal이 arXiv에 게시한 'Controlling Multimodal LLMs via Reward-guided Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding","tags":["Review","Multimodal LLMs","Reward Models","Guided Decoding","Visual Grounding","Hallucination Mitigation","Object Precision","Object Recall","Inference-time Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Oscar Mañas, Pierluca D'Oro, Koustuv Sinha, Adriana RomeroSoriano, Michal Drozdzal, Aishwarya Agrawal 핵심 연구 목표 본 논문은 MLLM(Multimodal Large Language Models)이 다양한 사용자 요구에 맞춰 동작을 조절"},{"id":"2025-8-18-DINOv3","title":"[논문리뷰] DINOv3","excerpt":"Maxime Oquab이 arXiv에 게시한 'DINOv3' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-DINOv3","tags":["Review","Self-supervised Learning","Foundation Models","Vision Transformer","Dense Feature Maps","Gram Anchoring","Model Distillation","Geospatial AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, et al. 핵심 연구 목표 본 연구는 수동 데이터 주석 없이 대규모 데이터셋 과 대규모 아키텍처 에 맞춰 모델을 확장하고, 단일 알고리즘으로 다양한 소스(자연 이미지부터"},{"id":"2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation","title":"[논문리뷰] FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation","excerpt":"Mu Xu이 arXiv에 게시한 'FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation","tags":["Review","Audio-Driven Animation","Preference Optimization","Diffusion Models","Reward Modeling","Human Feedback","Multi-Objective Optimization","Timestep-Layer Adaptive"],"text":"링크: 논문 PDF로 바로 열기 저자: MengChao Wang, Qiang Wang, Fan Jiang, Mu Xu 핵심 연구 목표 오디오 기반 인물 애니메이션에서 모션 자연스러움, 립싱크 정확도, 시각적 품질 과 같은 다양한 인간 선호도를 동시에 만족시키지 못하는 문제를 해결하는 것이 목표입니다. 기존 방식의 상충하는 선호도 목표와 대규모 다차원 선호도"},{"id":"2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data","title":"[논문리뷰] MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data","excerpt":"Nicolas Gonthier이 arXiv에 게시한 'MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data","tags":["Review","Self-supervised Learning","Masked Autoencoder","Earth Observation","Multimodal","Multitemporal","Multispectral","Fusion Strategies","Target Normalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Antoine Labatie, Michael Vaccaro, Nina Lardiere, Anatol Garioud, Nicolas Gonthier 핵심 연구 목표 본 논문은 지구 관측(EO) 데이터 의 고유한 다중 모달, 다중 시간, 다중 스펙트럼 특성을 효율적으로 처리하기 위해 Masked Autoencoder (M"},{"id":"2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing","title":"[논문리뷰] PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing","excerpt":"Xianpei Han이 arXiv에 게시한 'PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing","tags":["Review","논문 검색","계층적 인덱싱","유연한 검색","대규모 언어 모델","정보 추출","뷰 인식","강화 학습"],"text":"링크: 논문 PDF로 바로 열기 저자: Xianpei Han, Yaojie Lu, Hongyu Lin, Xuanang Chen, lzq2021 핵심 연구 목표 이 논문은 기존 논문 검색 시스템이 추상 기반 인덱싱에 의존하여 세분화된 쿼리(flexiblegrained queries) 를 효과적으로 처리하지 못하는 한계를 해결하는 것을 목표로 합니다. 논문의 "},{"id":"2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation","title":"[논문리뷰] SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation","excerpt":"Paolo Soda이 arXiv에 게시한 'SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation","tags":["Review","Semi-supervised Learning","Few-shot Learning","Medical Imaging","GAN-based Methods","Image-to-image Translation","Pseudo-labeling","Ensemble Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda 핵심 연구 목표 의료 영상 분야에서 레이블링된 학습 데이터의 부족 으로 인한 딥러닝 모델의 한계를 극복하고, 특히 5개에서 50개 사이의 매우 적은 레이블링된 샘플 만 사용 가능한 저데이터(lowdata) 환경 에"},{"id":"2025-8-18-SSRL-Self-Search-Reinforcement-Learning","title":"[논문리뷰] SSRL: Self-Search Reinforcement Learning","excerpt":"Yanxu Chen이 arXiv에 게시한 'SSRL: Self-Search Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Large Language Models","Self-Search","Sim-to-Real Transfer","Agentic AI","Knowledge Retrieval","Reward Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai Zhu, Che Jiang, Yuchen Zhang, Li Kang, Gang Chen, Cheng Huang, Zhizhou He, Bingning Wang, L"},{"id":"2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation","title":"[논문리뷰] StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation","excerpt":"Junyong Noh이 arXiv에 게시한 'StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation","tags":["Review","3D Morphable Model","Face Stylization","Text-to-Image Translation","Diffusion Model","Attribute Preservation","Generative AI","Computer Graphics"],"text":"링크: 논문 PDF로 바로 열기 저자: Seungmi Lee, Kwan Yun, Junyong Noh 핵심 연구 목표 본 논문은 기존 3D Morphable Model (3DMM)의 한계, 즉 일관된 메쉬 구조, 분리된 제어, 그리고 사실적 범위를 넘어서는 스타일화라는 세 가지 핵심 요구사항을 동시에 충족하지 못하는 문제를 해결하고자 합니다. 사용자 정의 "},{"id":"2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures","title":"[논문리뷰] TexVerse: A Universe of 3D Objects with High-Resolution Textures","excerpt":"Nan Cao이 arXiv에 게시한 'TexVerse: A Universe of 3D Objects with High-Resolution Textures' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures","tags":["Review","3D Dataset","High-Resolution Textures","Physically Based Rendering (PBR)","3D Animation","Data Curation","GPT-5 Annotations","Sketchfab"],"text":"링크: 논문 PDF로 바로 열기 저자: Yibo Zhang, Li Zhang, Rui Ma, Nan Cao 핵심 연구 목표 본 연구의 핵심 목표는 고해상도 텍스처와 PBR(Physically Based Rendering) 재료를 특징으로 하는 대규모 3D 객체 데이터셋의 부족 문제를 해결하는 것입니다. 기존 3D 데이터셋(예: Objaverse)이 고해상도"},{"id":"2025-8-18-Thyme-Think-Beyond-Images","title":"[논문리뷰] Thyme: Think Beyond Images","excerpt":"Wei Chen이 arXiv에 게시한 'Thyme: Think Beyond Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-Thyme-Think-Beyond-Images","tags":["Review","Multimodal LLMs","Code Generation","Image Processing","Reinforcement Learning","Supervised Fine-Tuning","Visual Reasoning","Sandbox"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Chen, Chaoyou Fu, Shukang Yin, Xingyu Lu, YiFan Zhang 핵심 연구 목표 본 논문은 기존의 \"이미지로 생각하기\" 방식의 멀티모달 대규모 언어 모델(MLLM) 이 가진 이미지 조작 기능의 제한성과 논리적 추론 능력의 한계를 극복하는 것을 목표로 합니다. 특히, OpenAI의"},{"id":"2025-8-18-X-Node-Self-Explanation-is-All-We-Need","title":"[논문리뷰] X-Node: Self-Explanation is All We Need","excerpt":"Islem Rekik이 arXiv에 게시한 'X-Node: Self-Explanation is All We Need' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-18 13:14:38+0900","permalink":"/ai/review/2025-8-18-X-Node-Self-Explanation-is-All-We-Need","tags":["Review","Graph Neural Networks","Explainable AI","Self-Explanation","Node Classification","Medical Imaging","Natural Language Processing","Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Prajit Sengupta and Islem Rekik 핵심 연구 목표 그래프 신경망(GNN)의 불투명한 의사결정 문제를 해결하고, 특히 신뢰성이 필수적인 고위험 임상 환경에서 개별 노드 수준의 충실한 자체 설명(selfexplanation) 을 제공하는 것을 목표로 합니다. 기존의 사후(posthoc) 전역(glo"},{"id":"2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy","title":"[논문리뷰] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy","excerpt":"Zeng Tao이 arXiv에 게시한 '4DNeX: Feed-Forward 4D Generative Modeling Made Easy' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy","tags":["Review","4D Generation","Dynamic 3D","Generative Models","Diffusion Models","Single Image Input","Video Synthesis","Point Clouds","Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, Ziwei Liu 핵심 연구 목표 본 논문은 단일 이미지로부터 4D(동적 3D) 장면 표현을 효율적으로 생성하는 피드포워드 프레임워크 인 4DNeX 를 제"},{"id":"2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information","title":"[논문리뷰] Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information","excerpt":"Xi Yang이 arXiv에 게시한 'Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information","tags":["Review","Large Reasoning Models (LRMs)","Information Seeking","Incomplete Problems","Mathematical Reasoning","Supervised Fine-tuning (SFT)","Overthinking","Hallucination","CRITIC-math"],"text":"링크: 논문 PDF로 바로 열기 저자: Youcheng Huang, Xi Yang, Bowen Qin, Chen Huang, Duanyu Feng, Wenqiang Lei, Jiancheng Lv 핵심 연구 목표 본 논문은 기존 수학 벤치마크가 잘 정의된 문제 해결 능력에만 초점을 맞추는 한계를 지적하며, Large Reasoning Models (LRMs"},{"id":"2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning","title":"[논문리뷰] ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning","excerpt":"Yufeng Wang이 arXiv에 게시한 'ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning","tags":["Review","Cognitive-Inspired RAG","Stateful Reasoning","Long Narrative Comprehension","Dynamic Memory","Metacognitive Regulation","Multi-step Retrieval","Hierarchical Knowledge Source"],"text":"링크: 논문 PDF로 바로 열기 저자: Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, Liyan Xu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 제한된 컨텍스트 길이와 높은 연산 비용 문제, 그리고 기존 RAG(RetrievalAugmented Generat"},{"id":"2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration","title":"[논문리뷰] G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration","excerpt":"Evgeny Burnaev이 arXiv에 게시한 'G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration","tags":["Review","3D Reconstruction","Deep Learning","Multi-Modal Fusion","Camera Pose Estimation","Depth Estimation","Transformer Networks","Prior Information"],"text":"링크: 논문 PDF로 바로 열기 저자: Ramil Khafizov, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, Evgeny Burnaev 핵심 연구 목표 본 논문은 기존의 피드포워드(feedforward) 3D 재구성 모델들이 RGB 이미지에만 의존하여 보조 데이터(깊이 맵, 카메라 내/외부 파라미터)를 활용하지 "},{"id":"2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study","title":"[논문리뷰] Has GPT-5 Achieved Spatial Intelligence? An Empirical Study","excerpt":"Ruisi Wang이 arXiv에 게시한 'Has GPT-5 Achieved Spatial Intelligence? An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study","tags":["Review","Spatial Intelligence","Multimodal LLMs","Benchmark Evaluation","GPT-5","Cognitive AI","AGI"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, et al. 핵심 연구 목표 이 연구는 최신 MLLM(Multimodal Large Language Model) , 특히 GPT5 가 인공 일반 지능(AGI)의 핵심 역량인 공간 이해 및 추론 능력을 얼마나 달성했는지 실증적으로"},{"id":"2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds","title":"[논문리뷰] HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds","excerpt":"Artyom Sorokin이 arXiv에 게시한 'HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds","tags":["Review","Long-Horizon Planning","Structured Reasoning","LLM Evaluation","Virtual Worlds","RPG","Benchmark","Agent Systems","Combat Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Petr Anokhin, Roman Khalikov, Stefan Rebrikov, Viktor Volkov, Artyom Sorokin, Vincent Bissonnette 핵심 연구 목표 본 논문의 핵심 연구 목표는 복잡한 가상 세계 내에서 대규모 언어 모델(LLM) 의 장기 계획 및 구조화된 추론 능력을 평가하는"},{"id":"2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping","title":"[논문리뷰] Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping","excerpt":"Tyler Derr이 arXiv에 게시한 'Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping","tags":["Review","Multimodal Learning","Vision-Language Models","Alignment Pre-training","Text-to-Vision Mapping","Continuous Representations","Computational Efficiency","LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuhui Zhan, Tyler Derr 핵심 연구 목표 기존 대규모 시각언어 모델(LVLM)의 핵심 병목인 고비용의 정렬 사전 훈련(alignment pretraining) 단계를 제거 하고, 시각 정보를 이산적인 텍스트 토큰 공간에 강제로 매핑함으로써 발생하는 정보 손실 문제 를 해결하는 것을 목표로 합니다. 대신"},{"id":"2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models","title":"[논문리뷰] Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models","excerpt":"Zixiang Gao이 arXiv에 게시한 'Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models","tags":["Review","Video Relighting","Background Replacement","Generative Models","Diffusion Models","Temporal Consistency","Dataset Generation","Video Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianshu Zeng, Yuxuan Liu, Yutong Feng, Chenxuan Miao, Zixiang Gao, Jiwang Qu, Jianzhang Zhang, Bin Wang, Kun Yuan 핵심 연구 목표 본 연구는 비디오에서 배경을 교체하고 동시에 포그라운드의 조명을 조화롭게 조정하는 비디오 리라이팅 "},{"id":"2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model","title":"[논문리뷰] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model","excerpt":"Yifan Zhang이 arXiv에 게시한 'Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model","tags":["Review","World Model","Interactive Video Generation","Real-Time AI","Diffusion Models","Auto-Regressive Generation","Data Pipeline","Self-Forcing","KV Caching"],"text":"링크: 논문 PDF로 바로 열기 저자: Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, Baixin Xu, HaoXiang Guo, Kaixiong Gong, Cyrus Wu, Wei Li"},{"id":"2025-8-19-Next-Visual-Granularity-Generation","title":"[논문리뷰] Next Visual Granularity Generation","excerpt":"Kang Liao이 arXiv에 게시한 'Next Visual Granularity Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Next-Visual-Granularity-Generation","tags":["Review","Image Generation","Granularity Control","Structured Representation","Hierarchical Generation","Coarse-to-fine","Visual Tokenization","Latent Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Yikai Wang, Zhouxia Wang, Zhonghua Wu, Qingyi Tao, Kang Liao, Chen Change Loy 핵심 연구 목표 본 논문은 기존 이미지 생성 모델들이 이미지를 평면적이거나 비구조적인 데이터로 취급하여 미세한 제어 및 오류 누적에 한계가 있다는 문제점을 해결하고자 합니다. 이를"},{"id":"2025-8-19-Ovis2-5-Technical-Report","title":"[논문리뷰] Ovis2.5 Technical Report","excerpt":"Yang Li이 arXiv에 게시한 'Ovis2.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Ovis2-5-Technical-Report","tags":["Review","Multimodal LLMs","Native Resolution Vision","Deep Reasoning","Chart Analysis","OCR","Visual Grounding","Training Efficiency","Preference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Li, cqgwin, Suikong, xxyyy123, runninglsy 핵심 연구 목표 Ovis2.5는 이전 Ovis 버전의 한계, 특히 고정 해상도 이미지 처리와 선형 사고 체인(CoT) 기반 추론의 문제를 해결하고자 합니다. 이를 위해 네이티브 해상도 시각 인코더 를 통합하여 세부 정보 및 전역 레이아"},{"id":"2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts","title":"[논문리뷰] Precise Action-to-Video Generation Through Visual Action Prompts","excerpt":"Minghan Qin이 arXiv에 게시한 'Precise Action-to-Video Generation Through Visual Action Prompts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts","tags":["Review","Action-to-Video Generation","Visual Action Prompts","Skeleton Representation","Human-Object Interaction","Robotic Manipulation","Cross-Domain Transfer","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuang Wang, Chao Wen, Haoyu Guo, Sida Peng, Minghan Qin, Hujun Bao, Xiaowei Zhou, Ruizhen Hu 핵심 연구 목표 본 논문은 복잡하고 고자유도(highDoF)의 상호작용(예: 인간의 손 또는 로봇 그리퍼 동작)을 위한 비디오 생성에서 정밀성과 범용성 "},{"id":"2025-8-19-Reinforcement-Learning-with-Rubric-Anchors","title":"[논문리뷰] Reinforcement Learning with Rubric Anchors","excerpt":"Haokai Xu이 arXiv에 게시한 'Reinforcement Learning with Rubric Anchors' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors","tags":["Review","Reinforcement Learning","Large Language Models","Rubric-based Reward","RLVR Extension","Human-centric AI","Controllable Generation","Reward Hacking Mitigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, et al. 핵심 연구 목표 이 논문은 확인 가능한 보상(RLVR) 을 사용하는 기존 강화 학습 패러다임이 자동 검증이 가능한 특정 도메인(예: 수학, 코딩)에 국한되는 한계를 해결하고자 합니다. 본 연구는 본"},{"id":"2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens","title":"[논문리뷰] Representing Speech Through Autoregressive Prediction of Cochlear Tokens","excerpt":"Daniel L. K. Yamins이 arXiv에 게시한 'Representing Speech Through Autoregressive Prediction of Cochlear Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens","tags":["Review","Speech Representation Learning","Autoregressive Models","Cochlear Tokens","Biologically Inspired AI","Self-Supervised Learning","Audio Processing","Transformer Networks"],"text":"링크: 논문 PDF로 바로 열기 저자: Greta Tuckute, Klemen Kotar, Evelina Fedorenko, Daniel L. K. Yamins 핵심 연구 목표 본 논문은 인간의 청각 처리 계층에서 영감을 받아, 유연하고 효율적으로 음성 정보를 이해하고 상호작용하는 인공 신경망 모델을 개발하는 것을 목표로 합니다. 특히, 인간의 달팽이관에서"},{"id":"2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models","title":"[논문리뷰] S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models","excerpt":"Meiqi Wu이 arXiv에 게시한 'S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models","tags":["Review","Diffusion Models","Classifier-free Guidance","Self-Guidance","Training-Free","Stochastic Block-Dropping","Generative Models","Text-to-Image"],"text":"링크: 논문 PDF로 바로 열기 저자: Chubin Chen, Jiashu Zhu, Xiaokun Feng, Nisha Huang, Meiqi Wu, Fangyuan Mao, Jiahong Wu, Xiangxiang Chu, Xiu Li 핵심 연구 목표 본 논문은 확산 모델에서 널리 사용되는 Classifierfree Guidance (CFG) 가 종종 의"},{"id":"2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models","title":"[논문리뷰] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models","excerpt":"Jusen Du이 arXiv에 게시한 'Speed Always Wins: A Survey on Efficient Architectures for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models","tags":["Review","Large Language Models","Efficient Architectures","Transformer Optimization","Linear Attention","State Space Models","Mixture-of-Experts","Sparse Attention","Diffusion LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng 핵심 연구 "},{"id":"2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs","title":"[논문리뷰] When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs","excerpt":"Elena Tutubalina이 arXiv에 게시한 'When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-19 13:15:01+0900","permalink":"/ai/review/2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs","tags":["Review","LLM Robustness","Prompt Sensitivity","In-Context Learning","Fine-Tuning","Batch Calibration","Template Ensembles","Distribution Shift"],"text":"링크: 논문 PDF로 바로 열기 저자: Mikhail Seleznyov, Mikhail Chaichuk, Gleb Ershov, Alexander Panchenko, Elena Tutubalina, Oleg Somov 핵심 연구 목표 본 연구는 LLM이 프롬프트 구문 및 형식의 미묘한 비의미적 변화에 매우 민감하게 반응하는 문제를 해결하고자 합니다. 기존의"},{"id":"2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models","title":"[논문리뷰] A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models","excerpt":"Zishang Jiang이 arXiv에 게시한 'A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models","tags":["Review","Self-Refinement","Language Models","Reinforcement Learning","Proactive AI","Generation Process","Markov Decision Process","Adaptive Learning","LLM Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zishang Jiang, Tingyun li, Haiquan Zhao, Xinyi Wang, Jinyi Han 핵심 연구 목표 대규모 언어 모델(LLM)이 고정된 반복 횟수와 사후(posthoc) 방식에 의존하는 기존 자기 개선(selfrefinement) 방법의 한계를 극복하고자 합니다. 본 연구는 LLM이 내부 "},{"id":"2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends","title":"[논문리뷰] Advances in Speech Separation: Techniques, Challenges, and Future Trends","excerpt":"Zhuo Chen이 arXiv에 게시한 'Advances in Speech Separation: Techniques, Challenges, and Future Trends' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends","tags":["Review","Speech Separation","Deep Neural Networks","Cocktail Party Problem","Transformer Architecture","Unsupervised Learning","Supervised Learning","Evaluation Metrics","Datasets"],"text":"링크: 논문 PDF로 바로 열기 Advances in Speech Separation: Techniques, Challenges, and Future Trends Kai Li, Guo Chen, Wendi Sang, Yi Luo, Zhuo Chen, Shuai Wang, Shulin He, ZhongQiu Wang, Andong Li, Zhiyong Wu,"},{"id":"2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding","title":"[논문리뷰] Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding","excerpt":"Alina Landowska이 arXiv에 게시한 'Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding","tags":["Review","Large Language Models","Moral Reasoning","Bayesian Evaluation","Uncertainty Quantification","Natural Language Processing","Soft Labels"],"text":"링크: 논문 PDF로 바로 열기 저자: Maciej Skorski, Alina Landowska 핵심 연구 목표 본 연구는 대규모 언어 모델(LLMs)이 인간과 비교하여 도덕적 차원을 어떻게 이해하는지 평가하는 것을 목표로 합니다. 특히, 기존의 확정론적 정답(groundtruth) 가정에서 벗어나 어노테이터 불일치를 베이지안 방식으로 모델링 하여 인간의 "},{"id":"2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing","title":"[논문리뷰] CAMAR: Continuous Actions Multi-Agent Routing","excerpt":"Alexey Skrynnik이 arXiv에 게시한 'CAMAR: Continuous Actions Multi-Agent Routing' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing","tags":["Review","Multi-Agent Reinforcement Learning","Continuous Control","Pathfinding","MARL Benchmark","GPU Acceleration","Robotics Simulation","Scalability","Heterogeneous Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Artem Pshenitsyn, Aleksandr Panov, Alexey Skrynnik 핵심 연구 목표 이 논문은 기존 다중 에이전트 강화 학습(MARL) 벤치마크가 연속적인 상태 및 행동 공간, 복잡한 조정 및 계획 작업을 충분히 지원하지 못하는 한계를 해결하고자 합니다. 로봇 공학의 실제 응용 프로그램에 더 적"},{"id":"2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL","title":"[논문리뷰] Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL","excerpt":"Liam-Liu이 arXiv에 게시한 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL","tags":["Review","Chain-of-Agents","Agent Foundation Models","Multi-Agent Systems","Tool-Integrated Reasoning","Multi-agent Distillation","Agentic Reinforcement Learning","LLMs","End-to-End Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: LiamLiu, hugteste, kangz, wanwan1212, tianyue818 핵심 연구 목표 본 논문은 기존의 다중 에이전트 시스템(MAS)과 도구 통합 추론(TIR) 패러다임이 가진 한계를 극복하고, 단일 LLM(Large Language Model) 내에서 다중 에이전트 협업 능력을 내재화하여 복잡한 문"},{"id":"2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends","title":"[논문리뷰] Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends","excerpt":"Xixiang Zhao이 arXiv에 게시한 'Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends","tags":["Review","LLM Copyright Protection","Model Fingerprinting","Text Watermarking","Invasive Fingerprinting","Intrinsic Fingerprinting","Intellectual Property","Digital Rights Management","Backdoor Watermarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, Xixiang Zhao, Jingxuan Zhang, Wenjun Zeng, Wenpeng Xing, Dezhang Kong, Changting Lin, Meng Han 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)의 높은 개발"},{"id":"2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection","title":"[논문리뷰] CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection","excerpt":"Adriano Koshiyama이 arXiv에 게시한 'CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection","tags":["Review","Sparse Autoencoders","LLM Steering","Feature Selection","Correlation Analysis","AI Safety","Bias Mitigation","Mechanistic Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Seonglae Cho, Zekun Wu, Adriano Koshiyama 핵심 연구 목표 본 논문은 기존의 Sparse Autoencoder (SAE) 기반 LLM 조향 방식이 요구하는 대규모 대조 데이터셋 또는 방대한 활성화 저장 공간 의 한계를 해결하고자 합니다. 생성된 토큰의 SAE 활성화와 태스크 결과 간의 "},{"id":"2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","title":"[논문리뷰] Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations","excerpt":"Mounia Lalmas이 arXiv에 게시한 'Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","tags":["Review","Multimodal Large Language Models","Video Recommendation","Zero-Shot Learning","Content-Based Filtering","Natural Language Processing","Foundation Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Marco De Nadai, Andreas Damianou, Mounia Lalmas 핵심 연구 목표 기존 비디오 추천 시스템의 한계인 저수준 시각/음성 특징 및 메타데이터의 의미론적 깊이 부족 문제를 해결하는 것이 목표입니다. 사용자의 의도, 유머, 세계 지식과 같은 고수준의 의미를 포착하여 비디오 클립이 시청자에게"},{"id":"2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation","title":"[논문리뷰] Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation","excerpt":"Fei Ni이 arXiv에 게시한 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation","tags":["Review","Embodied AI","Robotic Manipulation","Reinforcement Learning","Vision-Language Model","Pointing","Zero-shot Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Pengyi Li, Yan Zheng, Jianye Hao, Zibin Dong 핵심 연구 목표 본 논문은 로봇 조작에서 \"seeingtodoing gap\"을 해소하고 일반화 능력을 향상시키는 것을 목표로 합니다. "},{"id":"2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge","title":"[논문리뷰] Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge","excerpt":"Alice Wang이 arXiv에 게시한 'Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge","tags":["Review","Podcast Recommendation","LLM-as-a-Judge","Offline Evaluation","User Profiling","Recommender Systems","Natural Language Processing"],"text":"링크: 논문 PDF로 바로 열기 저자: Francesco Fabbri, Gustavo Penha, Edoardo D'Amico, Alice Wang, Marco De Nadai, Jackie Doremus, Paul Gigioli, Andreas Damianou, Oskar Stål, Mounia Lalmas 핵심 연구 목표 본 논문은 팟캐스트와 같은 롱폼"},{"id":"2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery","title":"[논문리뷰] Leveraging Large Language Models for Predictive Analysis of Human Misery","excerpt":"Abhilash Nandy이 arXiv에 게시한 'Leveraging Large Language Models for Predictive Analysis of Human Misery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery","tags":["Review","Large Language Models (LLMs)","Affective Computing","Misery Score Prediction","Prompt Engineering","Few-shot Learning","Gamified Evaluation","Feedback-driven Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Bishanka Seal, Rahul Seetharaman, Aman Bansal, Abhilash Nandy 핵심 연구 목표 본 연구는 자연어 시나리오 설명으로부터 인간이 인지하는 불행 점수를 예측하는 것을 목표로 합니다. 이는 0에서 100까지의 척도 를 사용하는 회귀 문제로, 대규모 언어 모델(LLM)의 주관적인"},{"id":"2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos","title":"[논문리뷰] LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos","excerpt":"Yen-Yu Lin이 arXiv에 게시한 'LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos","tags":["Review","Novel View Synthesis","3D Gaussian Splatting","Unposed Reconstruction","Camera Pose Estimation","Incremental Optimization","Octree","Long Videos"],"text":"링크: 논문 PDF로 바로 열기 저자: ChinYang Lin, Cheng Sun, MinHung Chen, YenYu Lin, FuEn Yang, YuLun Liu 핵심 연구 목표 본 논문은 불규칙한 카메라 움직임, 알 수 없는 카메라 자세, 방대한 장면 크기 등 일반적인 긴 비디오에서 발생하는 Novel View Synthesis (NVS)의 핵심 문제"},{"id":"2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents","title":"[논문리뷰] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents","excerpt":"Jun Dong이 arXiv에 게시한 'MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents","tags":["Review","Multimodal Browsing","AI Agents","Benchmark","Vision-Language Models","Reasoning","Tool Use","Deep Search"],"text":"링크: 논문 PDF로 바로 열기 저자: Jun Dong, Jiaheng Liu, Wenjie Wang, Xingyuan Bu, Shilong Li 핵심 연구 목표 기존 웹 브라우징 벤치마크가 주로 텍스트 정보에만 초점을 맞춰 멀티모달 콘텐츠의 중요성을 간과하는 문제를 해결하고자 합니다. 이 연구는 AI 에이전트의 멀티모달 검색 및 추론 능력 을 평가하기 위"},{"id":"2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence","title":"[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence","excerpt":"Fernando López이 arXiv에 게시한 'MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence","tags":["Review","Audio Intelligence","Multimodal AI","Benchmark","Audio-Language Models","Holistic Evaluation","Reasoning","Long-Form Audio","Multicultural Music"],"text":"링크: 논문 PDF로 바로 열기 저자: Sonal Kumar, Šimon Sedláček, Vaibhavi Lokegaonkar, Fernando López, Wenyi Yu, Nishit Anand, Hyeonggon Ryu, Lichang Chen, Maxim Plička, Miroslav Hlaváček, William Fineas Ellingwood"},{"id":"2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation","title":"[논문리뷰] MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation","excerpt":"Jonas Geiping이 arXiv에 게시한 'MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation","tags":["Review","Medical Image Segmentation","Model Merging","Training-Free","SAM","Generalization","Zero-Order Optimization","Bayesian Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanwu Yang, Guinan Su, Jiesi Hu, Francesco Sammarco, Jonas Geiping, Thomas Wolfers 핵심 연구 목표 의료 영상 분할 분야에서 SAM(Segment Anything Model) 기반의 미세 조정된 모델들이 특정 작업에서 불균형한 성능과 제한된 일반화 능력을"},{"id":"2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation","title":"[논문리뷰] Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation","excerpt":"Xinyi Wang이 arXiv에 게시한 'Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation","tags":["Review","LLMs","Confidence Estimation","Fine-Grained","Generation Process","Calibration","Monte Carlo Sampling","Backward Confidence Integration"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinyi Han, Tingyun li, Shisong Chen, Jie shi, Xinyi Wang, Guanglei Yue, Jiaqing Liang, Xin Lin, Liqian Wen, Zulong Chen, Yanghua Xiao 핵심 연구 목표 대규모 언어 모델(LLM)이 답변 생성 과정에서 겪는 과신(ov"},{"id":"2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence","title":"[논문리뷰] Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence","excerpt":"Xin Chen이 arXiv에 게시한 'Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence","tags":["Review","Motion Transfer","Cross-topology","Sparse Correspondence","Motion Matching","Animation","Training-free","Few-shot Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: LINGHAO CHEN, YUHONG ZHANG, ZIXIN YIN, ZHIYANG DOU, XIN CHEN, JINGBO WANG, TAKU KOMURA, LEI ZHANG 핵심 연구 목표 이 논문은 골격 토폴로지가 크게 다른 캐릭터 간의 애니메이션 전이 문제를 해결하는 것을 목표로 합니다. 기존 방법론들이 내재된 "},{"id":"2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References","title":"[논문리뷰] MultiRef: Controllable Image Generation with Multiple Visual References","excerpt":"Shiyun Lang이 arXiv에 게시한 'MultiRef: Controllable Image Generation with Multiple Visual References' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References","tags":["Review","Controllable Image Generation","Multi-modal Generation","Visual References","Image-to-Image","Benchmark","Dataset","MLLM-as-a-Judge"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruoxi Chen, Dongping Chen, Siyuan Wu, Sinan Wang, Shiyun Lang, Peter Sushko, Gaoyang Jiang, Yao Wan, Ranjay Krishna 핵심 연구 목표 이 연구는 텍스트 프롬프트나 단일 이미지 참조에 의존하는 기존 이미지 생성 모델의 한계를 극복하"},{"id":"2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks","title":"[논문리뷰] OmniTry: Virtual Try-On Anything without Masks","excerpt":"Xiaoduan Feng이 arXiv에 게시한 'OmniTry: Virtual Try-On Anything without Masks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks","tags":["Review","Virtual Try-On","Diffusion Model","Mask-Free","Image Inpainting","ID Consistency","Wearable Objects","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoduan Feng, Linlin Zhang, Hengyuan Cao, Yiming Chen, Jian Cao, Yuxiong Wu, Bin Wang 핵심 연구 목표 이 논문은 기존 가상 착용(VTON) 기술이 의류에 국한되고 입력 마스크를 필요로 하는 한계를 극복하고자 합니다. 마스크 없이도 주얼리, 액세서리 "},{"id":"2025-8-20-Prompt-Orchestration-Markup-Language","title":"[논문리뷰] Prompt Orchestration Markup Language","excerpt":"Yuqing Yang이 arXiv에 게시한 'Prompt Orchestration Markup Language' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language","tags":["Review","Prompt Engineering","Large Language Models","Markup Language","Structured Prompting","IDE Support","Multimodal Data","Styling System","Development Toolkit"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuge Zhang, Nan Chen, Jiahang Xu, Yuqing Yang 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM) 프롬프트의 구조화, 데이터 통합, 형식 민감성 및 개발 도구의 부족이라는 현재의 과제를 해결하고자 합니다. 이를 위해 POML(Prompt Orchestration Markup Lan"},{"id":"2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research","title":"[논문리뷰] Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research","excerpt":"Susanne Schmidt이 arXiv에 게시한 'Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research","tags":["Review","Radiance Fields","XR","NeRF","3D Gaussian Splatting","View Synthesis","Systematic Review","Immersive Technology"],"text":"링크: 논문 PDF로 바로 열기 저자: Ke Li, Mana Masuda, Susanne Schmidt, Shohei Mori 핵심 연구 목표 이 논문은 NeRF 및 3DGS 와 같은 Radiance Field (RF) 기술이 확장 현실(XR) 분야에서 어떻게 구상되고(envisioned) 실제로 구현되었는지(addressed) 사이의 연구 격차를 체계적으"},{"id":"2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation","title":"[논문리뷰] Semantic IDs for Joint Generative Search and Recommendation","excerpt":"Enrico Palumbo이 arXiv에 게시한 'Semantic IDs for Joint Generative Search and Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation","tags":["Review","Generative Models","Search and Recommendation","Semantic IDs","Bi-Encoder","Quantization","Multi-Task Learning","Retrieval Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Enrico Palumbo, Edoardo D'Amico, Gustavo Penha, Francesco Fabbri, Marco De Nadai, Timothy Heath, Alex Tamborrino, Ali Vardasbi, Max LeFarov, Shawn Lin, Hugues Bouchard 핵심 연구 목표 본"},{"id":"2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models","title":"[논문리뷰] TempFlow-GRPO: When Timing Matters for GRPO in Flow Models","excerpt":"Jian Yang이 arXiv에 게시한 'TempFlow-GRPO: When Timing Matters for GRPO in Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models","tags":["Review","Flow Matching","Reinforcement Learning","Human Preference Alignment","GRPO","Temporal Credit Assignment","Generative AI","Text-to-Image"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, Bo Zhang 핵심 연구 목표 텍스트투이미지 플로우 매칭 모델의 GRPO(Generalized Policy Rejection Optimization) 훈련이 시간적 균일성"},{"id":"2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer","title":"[논문리뷰] Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer","excerpt":"Deyu Zhou이 arXiv에 게시한 'Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer","tags":["Review","Text-Guided Editing","Color Editing","Diffusion Transformers","Training-Free","Multi-Modal AI","Attention Control","Image Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: ZIXIN YIN, XILI DAI, LINGHAO CHEN, DEYU ZHOU, JIANAN WANG, DUOMIN WANG, GANG YU, LIONEL M. NI, LEI ZHANG, HEUNGYEUNG SHUM 핵심 연구 목표 본 논문은 텍스트 지시 기반의 이미지 및 비디오 색상 편집에서 물리적 일관성 을 유지"},{"id":"2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents","title":"[논문리뷰] ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents","excerpt":"Flora D. Salim이 arXiv에 게시한 'ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-20 13:26:54+0900","permalink":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents","tags":["Review","Zero-shot HAR","LLM Agents","Time-Series Analysis","Knowledge Base","Retrieval-Augmented Generation","Multi-sensor Fusion","Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Zechen Li, Baiyu Chen, Hao Xue, Flora D. Salim 핵심 연구 목표 본 논문은 기존 HAR(Human Activity Recognition) 시스템의 낮은 일반화 능력 , 제한적인 제로샷 기능 , 해석 불가능성 이라는 세 가지 주요 한계를 해결하고자 합니다. 특히, 원시 모션 시계열 데"},{"id":"2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization","title":"[논문리뷰] DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization","excerpt":"Yu Lu이 arXiv에 게시한 'DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization","tags":["Review","LLM Optimization","Self-Verification","Dual Learning","Preference Optimization","Self-Supervised Learning","Mathematical Reasoning","Multilingual Translation","RLHF"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuaijie She♠, Yu Bao♠, Yu Lu, Lu Xu♠, Tao Li, Wenhao Zhu, Shujian Huang(✉), Shanbo Cheng♠(✉), Lu Lu♠, Yuxuan Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 자기 검증 신뢰성을 높여 비용이 많이 드는 사람의 주석이"},{"id":"2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery","title":"[논문리뷰] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery","excerpt":"zijieqiu이 arXiv에 게시한 'From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery","tags":["Review","Agentic AI","Autonomous Scientific Discovery","AI for Science","Large Language Models","Multi-agent Systems","Scientific Workflow Automation","Natural Sciences"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, Zijie Qiu, Xuming He, Qiang Zhang, Chenyu "},{"id":"2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models","title":"[논문리뷰] From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models","excerpt":"Ziyan Kuang이 arXiv에 게시한 'From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models","tags":["Review","Financial LLMs","Cognitive Diagnosis Model","LLM Evaluation","Knowledge Assessment","Matrix Factorization","CPA-QKA","Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyan Kuang, Feiyu Zhu, Maowei Jiang, Qianqian Xie, et al. 핵심 연구 목표 기존 금융 LLM 벤치마크의 단일 점수 평가 방식(score flattening) 과 불균형한 개념 커버리지(coverage imbalance) 로 인해 모델의 실제 지식 수준과 한계를 파악하기 어"},{"id":"2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction","title":"[논문리뷰] FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction","excerpt":"tianlecai이 arXiv에 게시한 'FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction","tags":["Review","LLM Agents","Future Prediction","Live Benchmark","Dynamic Evaluation","Data Contamination","Tool Use","Web Search","Financial Forecasting","Misinformation"],"text":"링크: 논문 PDF로 바로 열기 저자: tianlecai, Nuori, YinLingyue, TianciHe, liujiashuo77 핵심 연구 목표 본 논문은 LLM 에이전트의 미래 예측 능력 평가를 위한 대규모 벤치마크 부재 문제를 해결하고자 합니다. 실시간 데이터 업데이트 및 데이터 오염 방지의 어려움 때문에 기존 벤치마크는 한계가 있었으며, Futu"},{"id":"2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell","title":"[논문리뷰] Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell","excerpt":"Ingrid Verbauwhede이 arXiv에 게시한 'Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell","tags":["Review","Fully Homomorphic Encryption (FHE)","TFHE","Levenshtein Distance","Programmable Bootstrapping (PBS)","Privacy-Preserving Computation","String Similarity"],"text":"링크: 논문 PDF로 바로 열기 저자: Wouter Legiest, JanPieter D'Anvers, Bojan Spasic, NamLuc Tran, Ingrid Verbauwhede 핵심 연구 목표 본 논문은 완전 동형 암호(FHE) 프레임워크, 특히 TFHE 와 같은 3세대 스킴에서 Levenshtein(편집) 거리 계산의 높은 연산 비용 을 획기적으"},{"id":"2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer","title":"[논문리뷰] Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer","excerpt":"Jeremiah Jiang이 arXiv에 게시한 'Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer","tags":["Review","Scale Equivariance","Deep Equilibrium Models","Canonicalization","Computer Vision","Image Classification","Semantic Segmentation","Latent Representation","Monotone Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Md Ashiqur Rahman, ChiaoAn Yang, Michael N. Cheng, Lim Jun Hao, Jeremiah Jiang, TeckYian Lim, Raymond A. Yeh 핵심 연구 목표 본 논문은 컴퓨터 비전에서 발생하는 객체의 지역적 스케일 변화 문제를 해결하고, 모델의 지역적 스케일 일관성"},{"id":"2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers","title":"[논문리뷰] MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers","excerpt":"Prathyusha Jwalapuram이 arXiv에 게시한 'MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers","tags":["Review","Large Language Models","Benchmarking","Model Context Protocol","Tool Use","Real-World Applications","Agent Evaluation","Long Context","Unknown Tools"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li 핵심 연구 목표 본 논문은 Model Context Protocol"},{"id":"2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds","title":"[논문리뷰] MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds","excerpt":"Jiangmiao이 arXiv에 게시한 'MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds","tags":["Review","LLM","Point Clouds","3D Reconstruction","Structured Mesh","Blender Python","Shape Editing","Part-based Representation","Large Language Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Bingquan Dai, Li Ray Luo, Qihong Tang, et al. 핵심 연구 목표 본 논문은 3D 포인트 클라우드로부터 편집 가능한 Blender Python 스크립트 형태의 구조화된 메시 코드를 생성하는 새로운 프레임워크인 MeshCoder 를 제안합니다. 기존 방법론의 제한적인 DSL(DomainS"},{"id":"2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model","title":"[논문리뷰] NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model","excerpt":"abercovich이 arXiv에 게시한 'NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model","tags":["Review","Hybrid Architecture","Mamba-Transformer","Reasoning LLM","Model Compression","Knowledge Distillation","Long Context","High Throughput","FP8 Training","Instruction Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Akhiad Bercovich, Aditya Malte, Adi Renduchintala, Abhijit Paithankar 핵심 연구 목표 논문은 Nemotron Nano 2 라는 하이브리드 MambaTransformer 언어 모델 을 소개하며, 유사 규모 모델 대비 추론 워크로드 처리량 을 최대 6배 향상 시키면서"},{"id":"2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting","title":"[논문리뷰] On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting","excerpt":"Guoyin Wang이 arXiv에 게시한 'On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting","tags":["Review","Large Language Models","Reinforcement Learning","Supervised Fine-Tuning","On-Policy RL","Off-Policy Experts","Dynamic Weighting","LLM Alignment","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 사후 튜닝에서 Supervised FineTuning (SFT) 과 Reinfo"},{"id":"2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs","title":"[논문리뷰] Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs","excerpt":"Haobo Xu이 arXiv에 게시한 'Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs","tags":["Review","Diffusion LLMs","Post-training Quantization (PTQ)","Model Compression","Activation Outliers","Quantization Methods","Efficient Deployment","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun 핵심 연구 목표 본 연구는 확산 기반 대규모 언어 모델(dLLM) 의 효율적인 배포를 저해하는 막대한 파라미터 규모 및 높은 자원 요"},{"id":"2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation","title":"[논문리뷰] Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation","excerpt":"Shiqing Wu이 arXiv에 게시한 'Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation","tags":["Review","Multi-modal Recommendation","Contrastive Learning","Graph Neural Network","Homography Relations","Meta-network","Orthogonal Constraint","Data Sparsity"],"text":"링크: 논문 PDF로 바로 열기 저자: Shouxing Ma, Yawen Zeng, Shiqing Wu, Guandong Xu 핵심 연구 목표 본 논문은 멀티모달 추천 시스템의 주요 문제점인 데이터 희소성을 해결하고, 기존 대조 학습(Contrastive Learning) 방법의 두 가지 한계를 극복하는 것을 목표로 합니다. 구체적으로, 노이즈가 많은 모달"},{"id":"2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World","title":"[논문리뷰] RynnEC: Bringing MLLMs into Embodied World","excerpt":"jiangpinliu이 arXiv에 게시한 'RynnEC: Bringing MLLMs into Embodied World' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World","tags":["Review","Multi-modal Large Language Models","Embodied AI","Embodied Cognition","Video Understanding","Instance Segmentation","Spatial Reasoning","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiangpin Liu, Zhikai Wang, Lin Xi, Deli Zhao, Dalmo Academy, Alibaba Group, Hupan Lab Zhejiang University. 핵심 연구 목표 본 논문의 핵심 목표는 기존 Multimodal Large Language Models ( MLLM )이 실제 "},{"id":"2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization","title":"[논문리뷰] Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization","excerpt":"Hao Chen이 arXiv에 게시한 'Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization","tags":["Review","3D Editing","Multi-View Consistency","Diffusion Models","Sparse Input","Zero-Shot Learning","Scene Completion","Gaussian Splatting"],"text":"링크: 논문 PDF로 바로 열기 저자: Canyu Zhao, Xiaoman Li, Tianjian Feng, Zhiyue Zhao, Hao Chen, Chunhua Shen 핵심 연구 목표 본 논문은 기존 3D 편집 방식의 주요 한계인 방대한 장면별 최적화(perscene optimization) 필요성 을 제거하고, 단일 또는 소수의 입력 이미지로부터 멀"},{"id":"2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions","title":"[논문리뷰] ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?","excerpt":"Daeyoung Kim이 arXiv에 게시한 'ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions","tags":["Review","Vision Language Models","Multimodal AI","Vietnamese Language","Educational Assessment","Low-Resource Languages","Cross-Lingual Reasoning","ViExam","Human-in-the-Loop"],"text":"링크: 논문 PDF로 바로 열기 저자: Vy Tuong Dang, An Vo, Quang Tau, Duc Dm, Daeyoung Kim 핵심 연구 목표 본 논문은 베트남어 다중 양식 시험 문제에 대한 Vision Language Models (VLMs) 의 성능을 평가하는 것을 목표로 합니다. 주로 영어 데이터로 훈련된 VLMs가 저자원 언어인 베트남어 환"},{"id":"2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning","title":"[논문리뷰] mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning","excerpt":"anoperson이 arXiv에 게시한 'mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-21 13:15:28+0900","permalink":"/ai/review/2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning","tags":["Review","Multilingual Benchmark","Commonsense Reasoning","LLM Evaluation","Reasoning Taxonomy","Benchmark Scaling","Data Synthesis","Cultural Nuances"],"text":"링크: 논문 PDF로 바로 열기 저자: Nghia Trung Ngo, Franck Dernoncourt, Thien Huu Nguyen 핵심 연구 목표 본 논문은 기존 상식 추론 벤치마크들이 다국어 및 다문화 환경에서 LLM의 인간 추론 능력 활용 방식을 체계적으로 평가하고, 태스크 난이도를 조절하는 데 한계가 있음을 지적합니다. 이를 해결하기 위해 LLM"},{"id":"2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks","title":"[논문리뷰] A Survey on Large Language Model Benchmarks","excerpt":"Siyi Li이 arXiv에 게시한 'A Survey on Large Language Model Benchmarks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks","tags":["Review","LLM Benchmarks","Evaluation","Systematic Review","General Capabilities","Domain-Specific Benchmarks","Target-Specific Benchmarks","Data Contamination","AI Ethics"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyi Li, Xuanang Chen, Shuaimin Li, Guhong Chen, Shiwen Ni 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 평가 벤치마크의 현재 상태와 발전 과정을 체계적으로 검토하고, 기존 벤치마크의 한계를 분석하며, 향후 벤치마크 혁신을 위한 설계 패러다임을 제시하는 것을 목표로"},{"id":"2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling","title":"[논문리뷰] ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling","excerpt":"Shunsuke Saito이 arXiv에 게시한 'ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling","tags":["Review","Parametric Human Model","3D Human Modeling","Shape-Skeleton Decoupling","Pose Correctives","Single Image Mesh Fitting","Expressive Modeling","Goliath Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Shunsuke Saito, Javier Romero, Jinhyung Park, Rawal Khirodkar, Takaaki Shiratori 핵심 연구 목표 기존 파라메트릭 인체 모델(예: SMPLX)이 겪는 골격 및 표면 간의 원치 않는 상관관계, 제한된 표현력, 그리고 미세한 속성 제어의 어려움을 해결하는 것을"},{"id":"2025-8-22-Deep-Think-with-Confidence","title":"[논문리뷰] Deep Think with Confidence","excerpt":"Xuewei Wang이 arXiv에 게시한 'Deep Think with Confidence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-Deep-Think-with-Confidence","tags":["Review","LLM Reasoning","Confidence Filtering","Self-Consistency","Test-Time Optimization","Computational Efficiency","Adaptive Sampling","Early Stopping","Majority Voting"],"text":"링크: 논문 PDF로 바로 열기 저자: Yichao Fu, Xuewei Wang, Yuandong Tian, Jiawei Zhao 핵심 연구 목표 본 논문은 LLM의 추론 태스크에서 selfconsistency (다수결 투표) 방식의 한계점인 정확도 저하 및 높은 연산 오버헤드를 해결하는 것을 목표로 합니다. 특히, 추론 과정의 효율성과 성능을 동시에 향상"},{"id":"2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries","title":"[논문리뷰] 'Does the cafe entrance look accessible? Where is the door?' Towards Geospatial AI Agents for Visual Inquiries","excerpt":"Xia Su이 arXiv에 게시한 'Does the cafe entrance look accessible? Where is the door? Towards Geospatial AI Agents for Visual Inquiries' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries","tags":["Review","Geospatial AI","Multimodal AI Agents","Visual Question Answering","Accessibility","Street View Imagery","Spatial Reasoning","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane 핵심 연구 목표 본 논문은 기존 지도 시스템이 구조화된 GIS 데이터에 의존하"},{"id":"2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models","title":"[논문리뷰] Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models","excerpt":"Lifan Guo이 arXiv에 게시한 'Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models","tags":["Review","Large Language Models","Process Reward Models","Financial Reasoning","Domain Specialization","RLHF","Best-of-N Selection","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang 핵심 연구 목표 본 논문은 기존 일반 목적 Process Reward Models (PRMs)이 금융과 같은 도메인 특화 태스크에서 요구되는 정밀성, 사실성, 논리적 일관성을 충"},{"id":"2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior","title":"[논문리뷰] INTIMA: A Benchmark for Human-AI Companionship Behavior","excerpt":"Yacine Jernite이 arXiv에 게시한 'INTIMA: A Benchmark for Human-AI Companionship Behavior' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior","tags":["Review","AI Companionship","Benchmark","Language Models (LLMs)","Human-AI Interaction","Emotional AI","Boundary Setting","Psychological Frameworks","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: LucieAimée Kaffee, Giada Pistilli, Yacine Jernite 핵심 연구 목표 이 논문은 사용자들이 AI 시스템과 감정적 유대감을 형성하는 AI 동반자 관계(AI companionship)의 증가에 주목합니다. 기존 평가 방법론이 주로 작업 성능, 사실 정확도, 안전성에 집중하여 사회적, 감"},{"id":"2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model","title":"[논문리뷰] Intern-S1: A Scientific Multimodal Foundation Model","excerpt":"xuhuang87이 arXiv에 게시한 'Intern-S1: A Scientific Multimodal Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model","tags":["Review","Multimodal Foundation Model","Scientific AI","Reinforcement Learning","Mixture-of-Experts (MoE)","Dynamic Tokenizer","Data Curation","Low-Resource Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: xuhuang87, ZhouqiHUA, Jerryhyl, guox18, gaoyang07 핵심 연구 목표 본 논문은 과학 분야에서 오픈 소스 파운데이션 모델과 클로즈드 소스 모델 간의 성능 격차를 줄이고자 합니다. 특히, 일반 파운데이션 모델의 발전이 더딘 저자원 과학 전문 분야 에서 멀티모달 대규모 추론 모델(mul"},{"id":"2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries","title":"[논문리뷰] LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries","excerpt":"huuuyeah이 arXiv에 게시한 'LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries","tags":["Review","AI Agents","Tool Use","Model Context Protocol (MCP)","Benchmarking","Large Language Models (LLMs)","Real-world Tasks","Evaluation","Error Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song "},{"id":"2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation","title":"[논문리뷰] Mobile-Agent-v3: Foundamental Agents for GUI Automation","excerpt":"Haowei Liu이 arXiv에 게시한 'Mobile-Agent-v3: Foundamental Agents for GUI Automation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation","tags":["Review","GUI Automation","Multimodal Agents","Foundational Models","Reinforcement Learning","Large Language Models","Cross-Platform","Self-Supervised Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiabo Ye, Xi Zhang, Haiyang Xu, Ziwei Zheng, Feiyu Gao, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Junjie Cao, Zhengxi Lu, Ming Yan, Qi Zheng, Fei Huang, Jingren Zhou (Tongyi Lab, A"},{"id":"2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass","title":"[논문리뷰] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass","excerpt":"Ya Zhang이 arXiv에 게시한 'SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass","tags":["Review","3D Scene Generation","Single-Image Input","Feedforward Networks","Diffusion Models","Geometric Modeling","Texture Synthesis","Transformer","Feature Aggregation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie† 핵심 연구 목표 본 논문의 핵심 목표는 단일 장면 이미지와 객체 마스크를 입력으로 받아, 최적화나 에셋 검색 과정 없이 하나의 피드포워드 패스 만으로 다수의 3D 에셋(기하학적 구조, 텍스처, 공간 배치 포함)을 동시에 효율적으로 생성하는"},{"id":"2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds","title":"[논문리뷰] Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds","excerpt":"Chuiyun Wu이 arXiv에 게시한 'Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds","tags":["Review","3D Human Reconstruction","Gaussian Splatting","Sparse View","Two-Image Input","Real-time Inference","Point Cloud Prediction","Feed-forward Network"],"text":"링크: 논문 PDF로 바로 열기 저자: Jia Lu, Taoran Yi, Jiemin Fang, Chen Yang, Chuiyun Wu, Wei Shen, Wenyu Liu, Qi Tian, Xinggang Wang 핵심 연구 목표 본 연구는 극도로 희소한 입력(전면 및 후면 이미지 단 두 장)만으로 3D 인체 가우시안을 재구성하는 도전적인 문제를 해결하고"},{"id":"2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation","title":"[논문리뷰] Waver: Wave Your Way to Lifelike Video Generation","excerpt":"Yifu Zhang이 arXiv에 게시한 'Waver: Wave Your Way to Lifelike Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation","tags":["Review","Video Generation","Foundation Model","Diffusion Model","Transformer","Text-to-Video","Image-to-Video","Super-Resolution","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Bytedance Waver Team 핵심 연구 목표 본 논문은 통합된 이미지 및 비디오 생성을 위한 고성능 파운데이션 모델인 Waver 를 제시하며, 특히 720p 원본 해상도에서 510초 길이의 비디오를 생성하고 1080p로 업스케일링하는 것을 목표로 합니다. 기존 비디오 생성 모델의 한계점인 복잡한 모션 시나리오"},{"id":"2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding","title":"[논문리뷰] When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding","excerpt":"Rui Guo이 arXiv에 게시한 'When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding","tags":["Review","Video-LLM","Diffusion Model","Temporal Grounding","Object Segmentation","Long Video Understanding","Multimodal AI","Video Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengcheng Fang, Yuxia Chen, Rui Guo 핵심 연구 목표 본 논문은 기존 VideoLLM의 한계인 불명확한 시간 인코딩, 프레임 수준의 낮은 연속성, 그리고 관심 엔티티에 대한 언어비전 정렬 불일치를 극복하는 것을 목표로 합니다. 특히 긴 비디오에서 발생하는 이벤트의 정밀한 시간적 위치 파악과 "},{"id":"2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists","title":"[논문리뷰] aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists","excerpt":"Heng Zhang이 arXiv에 게시한 'aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-22 13:10:52+0900","permalink":"/ai/review/2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists","tags":["Review","AI Agents","Open Access","Scientific Discovery","Peer Review","LLMs","Multi-agent Systems","Prompt Injection","Iterative Refinement"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang 핵심 연구 목표 AI가 생성한 과학 연구 콘텐츠가 파편화된 출판 생태계와 확장성 없는 인간 중심의 동료 검토 시스템으로 인해 확산에 어려움을 겪는 문제를 해결하는 것이 목표입니다. aiXiv 라는 차세대 오픈 액"},{"id":"2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions","title":"[논문리뷰] AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions","excerpt":"Yidi Du이 arXiv에 게시한 'AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions","tags":["Review","Competitive Programming","LLM Evaluation","Code Reasoning","Benchmark","Test Case Generation","Programming Competitions","Algorithmic Problems"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihan Wang, Jiaze Chen, Zhicheng Liu, Markus Mak, Yidi Du 핵심 연구 목표 현재 대규모 언어 모델(LLM)의 코드 추론 능력 평가 벤치마크들이 모델의 실제 역량을 과대평가하며, 엘리트 인간 프로그래머와의 격차를 숨기고 있다는 문제 의식에서 출발합니다. 본 논문은 기존 벤치마"},{"id":"2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications","title":"[논문리뷰] AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications","excerpt":"Liuyi Yao이 arXiv에 게시한 'AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications","tags":["Review","LLM Agents","Agentic Applications","ReAct Paradigm","Framework","Tool Use","Multi-Agent Systems","Developer Experience","Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Liuyi Yao, Weirui Kuang, Yuexiang Xie, Zitao Li, Dawei Gao, et al. 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트 애플리케이션 구축 시 발생하는 유연하고 효율적인 도구 기반 에이전트환경 상호작용의 어려움을 해결하고자 합니다. 이를 위해 Agent"},{"id":"2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR","title":"[논문리뷰] Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR","excerpt":"Ying Nian Wu이 arXiv에 게시한 'Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR","tags":["Review","Reinforcement Learning","Large Language Models","Self-Play","Variational Problem Synthesis","Policy Entropy","Pass@k","Reasoning Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, Weizhu Chen 핵심 연구 목표 본 논문은 Verifiable Rewards (RLVR) 기반 Large Language Models (LLMs) 학습 시 발생하는 Pass@"},{"id":"2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning","title":"[논문리뷰] CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning","excerpt":"Yulun Zhang이 arXiv에 게시한 'CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning","tags":["Review","LLM Reasoning","Contrastive Learning","Reinforcement Learning","Fine-tuning","Chain-of-Thought (CoT)","Annotated Data","Model Stability"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang 핵심 연구 목표 본 논문은 LLM의 추론 능력 향상을 목표로, 기존 SFT(Supervised FineTuning) 방식의 제한된 일반화 능력과 RL(Reinforcement Learning) 기반 방식"},{"id":"2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders","title":"[논문리뷰] CRISP: Persistent Concept Unlearning via Sparse Autoencoders","excerpt":"Yonatan Belinkov이 arXiv에 게시한 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders","tags":["Review","Concept Unlearning","Sparse Autoencoders (SAEs)","LLMs","Parameter-Efficient Fine-Tuning","Model Interpretability","Safety-Critical AI","Feature Suppression","WMDP Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)에서 불필요하거나 유해한 지식을 영구적으로 제거(Persistent Concept Unlearning) 하면서도 모델의 일반적인 유용성과"},{"id":"2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible","title":"[논문리뷰] Do What? Teaching Vision-Language-Action Models to Reject the Impossible","excerpt":"Roei Herzig이 arXiv에 게시한 'Do What? Teaching Vision-Language-Action Models to Reject the Impossible' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible","tags":["Review","Vision-Language-Action Models","Robotics","False Premise Detection","Instruction Following","Human-Robot Interaction","Clarification","Instruction Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: WenHan Hsieh, Elvis Hsieh, Dantong Niu, Trevor Darrell, Roei Herzig, David M. Chan 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA) 모델이 존재하지 않는 객체나 조건(\"falsepremise instructions\")을 참조하는"},{"id":"2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person","title":"[논문리뷰] EgoTwin: Dreaming Body and View in First Person","excerpt":"Wentao Wang이 arXiv에 게시한 'EgoTwin: Dreaming Body and View in First Person' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person","tags":["Review","Egocentric Video Generation","Human Motion Synthesis","Diffusion Transformers","Multimodal Generation","Viewpoint Alignment","Causal Interplay","First-Person Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingqiao Xiu, Fangzhou Hong, Yicong Li, Mengze Li, Wentao Wang 핵심 연구 목표 본 논문은 egocentric video 생성 분야의 미개척 영역을 탐구하며, 특히 카메라 착용자의 모션과 시점이 일관되고 인과적으로 연결된 방식으로 egocentric video와 인간 모"},{"id":"2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning","title":"[논문리뷰] End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning","excerpt":"Pengcheng Qiu이 arXiv에 게시한 'End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning","tags":["Review","Agentic RAG","Medical Diagnosis","Reinforcement Learning","Traceable AI","Large Language Models","Clinical Decision Support","Out-of-Distribution Generalization","Reward Design"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Pengcheng Qiu, Weidi Xie 핵심 연구 목표 본 논문은 기존 RAG(RetrievalAugmented Gener"},{"id":"2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles","title":"[논문리뷰] InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles","excerpt":"Diping Song이 arXiv에 게시한 'InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles","tags":["Review","LLM Evaluation","Human Reasoning Styles","Social Deduction Games","Theory of Mind","Adaptive Reasoning","Avalon Game","Cognitive Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Zizhen Li, Chuanhao Li, Yibin Wang, Qi Chen, Diping Song, Yukang Feng, Jianwen Sun, Jiaxin Ai, Fanrui Zhang, Mingzhu Sun, Kaipeng Zhang 핵심 연구 목표 본 연구는 LLM이 인간의 개별적인 추론 스타일, 특히 사회"},{"id":"2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts","title":"[논문리뷰] Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts","excerpt":"Liming Fang이 arXiv에 게시한 'Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts","tags":["Review","LLM Jailbreaking","Red Teaming","Malicious Content Detection","Developer Messages","D-Attack","DH-CoT","Adversarial Attacks","Dataset Cleaning"],"text":"링크: 논문 PDF로 바로 열기 저자: Chiyu Zhang, Lu Zhou, Xiaogang Xu, Jiafei Wu, Liming Fang, Zhe Liu 핵심 연구 목표 본 논문은 상업용 블랙박스 LLM에 대한 효과적인 탈옥(jailbreak) 공격 방법론을 개발하고, 기존 레드팀 데이터셋의 부적절한 프롬프트(Benign, Nonobvious Harm"},{"id":"2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics","title":"[논문리뷰] Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics","excerpt":"Xiao Sun이 arXiv에 게시한 'Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics","tags":["Review","Inverse Kinematics","Human Pose Estimation","SMPL Model","Neural Networks","Optimization-Free","Residual Learning","Data-Driven"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchen Yang, Linfeng Dong, Wei Wang, Zhihang Zhong, Xiao Sun 핵심 연구 목표 본 논문은 3D 인체 포즈 및 형태 추정에서 널리 사용되지만 계산 비용이 높은 SMPLify 의 반복적 최적화 과정을 데이터 기반 신경망 으로 대체하여, 최적화 없이 빠른 시간 내에 인버스 키네"},{"id":"2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding","title":"[논문리뷰] Selective Contrastive Learning for Weakly Supervised Affordance Grounding","excerpt":"Jae-Pil Heo이 arXiv에 게시한 'Selective Contrastive Learning for Weakly Supervised Affordance Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding","tags":["Review","Weakly Supervised Learning","Affordance Grounding","Contrastive Learning","CLIP","Part Discovery","Object Localization","DINO","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: WonJun Moon, Hyun Seok Seong, JaePil Heo 핵심 연구 목표 본 논문은 약지도 어포던스 그라운딩(Weakly Supervised Affordance Grounding, WSAG) 에서 모델이 어포던스 관련 부위 대신 일반적인 클래스 패턴에 집중하는 한계를 극복하고자 합니다. 픽셀 수준의 어"},{"id":"2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference","title":"[논문리뷰] TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference","excerpt":"Di Yin이 arXiv에 게시한 'TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-25 13:13:07+0900","permalink":"/ai/review/2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference","tags":["Review","LLM Inference","Tensor Parallelism","KV Cache Optimization","Latent Attention","Memory Efficiency","Decoding Speedup","Prefill/Decode Separation","Reparameterization"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang 핵심 연구 목표 본 논문은 DeepSeekV2 에서 도입된 MultiHead Latent Attention (MLA) 이 Tensor Parallelism (TP) 환경"},{"id":"2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling","title":"[논문리뷰] Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling","excerpt":"Daniil Orel이 arXiv에 게시한 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling","tags":["Review","Reasoning Depth","Cellular Automata","Transformer Architectures","Recurrence","Adaptive Computation Time","Chain-of-Thought","Reinforcement Learning","Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Ivan Rodkin, Daniil Orel, Konstantin Smirnov, Arman Bolatov, Bilal Elbouardi, Besher Hassan, Yuri Kuratov, Aydar Bulatov, Preslav Nakov, Timothy Baldwin, Artem Shelmanov, Mikhail"},{"id":"2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning","title":"[논문리뷰] Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning","excerpt":"Jiale Zhao이 arXiv에 게시한 'Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning","tags":["Review","Reinforcement Learning","Large Language Models","Exploration Bottleneck","Instructional Scaffolding","Rubric-based Rewards","General Reasoning","RL with Verifiable Rewards","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, et al. 핵심 연구 목표 대규모 언어 모델(LLM)의 일반 추론 능력 향상에 있어 강화 학습(RL) 의 고질적인 탐색 병목 현상 을 해결하는 것입니다. 고품질 샘플 학습의 필요성과 LLM의 제한된 탐색 능력 사"},{"id":"2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning","title":"[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning","excerpt":"Xin Zheng이 arXiv에 게시한 'Explain Before You Answer: A Survey on Compositional Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning","tags":["Review","Compositional Visual Reasoning","Multimodal AI","Vision-Language Models","Large Language Models","Chain-of-Thought","Tool Learning","Agentic AI","Survey"],"text":"링크: 논문 PDF로 바로 열기 저자: Fucai Ke, Joy Hsu, Zhixi Cai, Zixian Ma, Xin Zheng, et al. 핵심 연구 목표 본 설문조사는 복잡한 시각적 장면을 분해하고, 중간 개념을 이해하며, 다단계 논리적 추론을 수행하는 인간과 같은 능력을 기계에 부여하는 것을 목표로 하는 Compositional Visual Rea"},{"id":"2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German","title":"[논문리뷰] German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German","excerpt":"Cristian-George Craciun이 arXiv에 게시한 'German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German","tags":["Review","Text Simplification","Paraphrasing","Readability Control","German NLP","Dataset Generation","LLM Distillation","Multi-level Text Generation","Accessibility"],"text":"링크: 논문 PDF로 바로 열기 저자: Miriam Anschütz, Thanh Mai Pham, Eslam Nasrallah, Maximilian Müller, CristianGeorge Craciun, Georg Groh 핵심 연구 목표 이 논문은 독일어 텍스트를 다양한 독해 수준에 맞춰 재작성하는 분야의 중요한 격차를 해소하고자 합니다. 기존 독일어 "},{"id":"2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency","title":"[논문리뷰] InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency","excerpt":"jinglinglin이 arXiv에 게시한 'InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency","tags":["Review","Multimodal Large Language Models","Reinforcement Learning","Inference Efficiency","Vision-Language Models","Open-Source","Versatility","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, et al. 핵심 연구 목표 본 연구는 오픈소스 멀티모달 모델인 InternVL 시리즈를 다용성, 추론 능력, 그리고 추론 효율성 측면에"},{"id":"2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism","title":"[논문리뷰] Limitations of Normalization in Attention Mechanism","excerpt":"Radu State이 arXiv에 게시한 'Limitations of Normalization in Attention Mechanism' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism","tags":["Review","Attention Mechanism","Normalization","Softmax","Transformer Models","Gradient Sensitivity","Token Separability","Context Length","GPT-2"],"text":"링크: 논문 PDF로 바로 열기 저자: Timur Mudarisov, Mikhail Burtsev, Tatiana Petrova, Radu State 핵심 연구 목표 본 연구는 어텐션 메커니즘에서 사용되는 정규화, 특히 소프트맥스(softmax) 의 근본적인 한계를 밝히는 것을 목표로 합니다. 콘텍스트 길이 L 이 증가함에 따라 어텐션 가중치가 1/L 로 "},{"id":"2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment","title":"[논문리뷰] MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment","excerpt":"Doratossadat Dastgheib이 arXiv에 게시한 'MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment","tags":["Review","Multimodal Language Models","Multilingual Benchmarking","Persian Language","Educational Assessment","Vision-Language Models","Cultural Nuance","Reasoning Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Doratossadat Dastgheib, Seyed Mohammad Hadi Hosseini, Marzia Nouri, Arshia Hemmat, Omid Ghahroodi, Mohammad Vali Saniano, Alireza Sahebi, Reihaneh Zohrabi, Mohammad Hossein Rohba"},{"id":"2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion","title":"[논문리뷰] MV-RAG: Retrieval Augmented Multiview Diffusion","excerpt":"sagiebenaim이 arXiv에 게시한 'MV-RAG: Retrieval Augmented Multiview Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion","tags":["Review","Retrieval Augmented Generation","Multiview Diffusion","Text-to-3D Generation","Out-of-Domain","Image Retrieval","3D Consistency","Diffusion Models","Hybrid Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Yosef Dayani, Omer Benishu, Sagie Benaim 핵심 연구 목표 본 논문은 기존 Textto3D 생성 모델이 OutofDomain (OOD) 또는 희귀 개념을 처리할 때 겪는 기하학적 불일치, 부정확한 결과 및 현실성 부족 문제를 해결하고자 합니다. 텍스트 프롬프트만으로는 생성하기 어려운 새로"},{"id":"2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting","title":"[논문리뷰] MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting","excerpt":"Yanzhe Liang이 arXiv에 게시한 'MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting","tags":["Review","Sparse-View","Surface Reconstruction","Gaussian Splatting","2DGS","Novel View Synthesis","Generalizable","Mesh Extraction","3D Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanzhi Chang, Ruijie Zhu, Wenjie Chang, Mulin Yu, Yanzhe Liang, Jiahao Lu, Zhuoyuan Li, Tianzhu Zhang 핵심 연구 목표 본 논문은 극도로 희소한(sparseview) 이미지 로부터 정확한 3D 장면의 표면을 재구성하는 문제를 해결하고자 합니"},{"id":"2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges","title":"[논문리뷰] Neither Valid nor Reliable? Investigating the Use of LLMs as Judges","excerpt":"Golnoosh Farnadi이 arXiv에 게시한 'Neither Valid nor Reliable? Investigating the Use of LLMs as Judges' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges","tags":["Review","LLMs as Judges","NLG Evaluation","Measurement Theory","Validity","Reliability","Evaluation Bias","Scalability","Responsible AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, Golnoosh Farnadi 핵심 연구 목표 본 논문은 NLG(Natural Language Generation) 시스템 평가에서 LLM(Large Language Model)을 심사관(LLJ) 으로 활용하는 방"},{"id":"2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs","title":"[논문리뷰] PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs","excerpt":"Chenyu You이 arXiv에 게시한 'PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs","tags":["Review","Multi-Agent LLMs","Academic Poster Generation","Aesthetic Design","Layout Optimization","Typography","Color Palette","VLM-as-Judge","Content Fidelity"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhilin Zhang, Xiang Zhang, Jiaqi Wei, Yiwei Xu, Chenyu You 핵심 연구 목표 기존 학술 포스터 자동 생성 방식은 미학적 원칙을 간과하여 수동 수정이 많이 필요하다는 문제에 직면합니다. 본 논문은 전문 디자이너의 워크플로우를 모방하는 PosterGen 멀티 에이전트 LLM 프"},{"id":"2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering","title":"[논문리뷰] ST-Raptor: LLM-Powered Semi-Structured Table Question Answering","excerpt":"Wei Zhou이 arXiv에 게시한 'ST-Raptor: LLM-Powered Semi-Structured Table Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering","tags":["Review","Semi-structured Tables","Question Answering","LLMs","Hierarchical Orthogonal Tree","Table Layout Understanding","Pipeline Generation","Verification Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Zirui Tang, Boxiu Li, Guoliang Li, Boyu Niu, Wei Zhou, Xinyi Zhang, Xuanhe Zhou, Jiannan Wang, Fan Wu 핵심 연구 목표 본 논문은 금융 보고서나 의료 기록과 같이 유연하고 복잡한 레이아웃(계층적 헤더, 병합된 셀 등)을 가진 반정형 테이블("},{"id":"2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods","title":"[논문리뷰] SpotEdit: Evaluating Visually-Guided Image Editing Methods","excerpt":"Ersin Yumer이 arXiv에 게시한 'SpotEdit: Evaluating Visually-Guided Image Editing Methods' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods","tags":["Review","Visually-Guided Image Editing","Multimodal Models","Benchmark","Hallucination","Diffusion Models","Autoregressive Models","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Sara Ghazanfari, WeiAn Lin, Haitong Tian, Ersin Yumer 핵심 연구 목표 이 논문은 기존 벤치마크의 단순성과 실제 편집 과제에 대한 낮은 대표성이라는 한계를 극복하기 위해, 시각적으로 안내되는 이미지 편집(VisuallyGuided Image Editing) 모델을 체계적이고 세"},{"id":"2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation","title":"[논문리뷰] T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation","excerpt":"Xihui Liu이 arXiv에 게시한 'T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Reasoning Benchmark","Idiom Interpretation","Textual Image Design","Entity Reasoning","Scientific Reasoning","Multimodal LLM Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian Liu, Xihui Liu 핵심 연구 목표 본 논문은 기존 TexttoImage (T2I) 모델들이 리터럴한 프롬프트 해석을 넘어 내포된 의미(implicit meaning) 와 맥락적 뉘앙스(contextual nuances) 를 이해하"},{"id":"2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling","title":"[논문리뷰] TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling","excerpt":"Jiaqi Li이 arXiv에 게시한 'TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling","tags":["Review","Speech Tokenizer","Diffusion Model","Text-to-Speech","Speech Language Modeling","Low Bitrate Codec","End-to-End Training","Binary Spherical Quantization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuancheng Wang, Dekun Chen, Xueyao Zhang, Junan Zhang, Jiaqi Li, Zhizheng Wu 핵심 연구 목표 본 논문은 기존 스피치 토크나이저의 한계점, 즉 다층 RVQ 구조 또는 높은 프레임 레이트 에 대한 의존성, 보조 사전 학습 모델 을 통한 의미론적 증류의 필요성, "},{"id":"2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions","title":"[논문리뷰] UQ: Assessing Language Models on Unsolved Questions","excerpt":"Wei Liu이 arXiv에 게시한 'UQ: Assessing Language Models on Unsolved Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions","tags":["Review","LLM Evaluation","Unsolved Questions","AI Benchmark","Oracle-Free Validation","Generator-Validator Gap","Community Evaluation","Stack Exchange"],"text":"링크: 논문 PDF로 바로 열기 저자: Fan Nie, Ken Ziyu Liu, Wei Liu, Rui Sun, Zihao Wang 핵심 연구 목표 AI 연구의 진전을 이끄는 벤치마크가 난이도와 현실성 을 동시에 갖추지 못하는 문제점을 해결하고자 합니다. 특히, 기존 벤치마크의 한계(시험 기반의 인위적 난이도, 사용자 상호작용 기반의 쉬운 문제)를 극복하고"},{"id":"2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation","title":"[논문리뷰] Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation","excerpt":"Haoxiang Shi이 arXiv에 게시한 'Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-26 13:21:57+0900","permalink":"/ai/review/2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Reinforcement Learning","Chain of Thought","Multimodal LLMs","Stage-Aware Rewards","Semantic Reasoning","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaqi Li, Peng Chen, Mingyang Han, Bu Pi, Haoxiang Shi, Runzhou Zhao, Yang Yao, Xuan Zhang, Jun Song † 핵심 연구 목표 본 연구는 텍스트이미지(T2I) 생성 시 다중 속성 및 모호한 프롬프트 처리 능력의 한계 를 극복하고자 합니다. 기존 강"},{"id":"2025-8-27-Autoregressive-Universal-Video-Segmentation-Model","title":"[논문리뷰] Autoregressive Universal Video Segmentation Model","excerpt":"Albert Gu이 arXiv에 게시한 'Autoregressive Universal Video Segmentation Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Autoregressive-Universal-Video-Segmentation-Model","tags":["Review","Video Segmentation","Autoregressive Model","Universal Model","State Space Models","Mamba","Parallel Training","Streaming Video","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Albert Gu, YuChiang Frank Wang, Sukjun Hwang, Miran Heo, cmhungsteve 핵심 연구 목표 현재 단편화된 비디오 분할 태스크들을 단일 아키텍처 로 통합하고, 프롬프트 기반(prompted) 및 비프롬프트 기반(unprompted) 비디오 분할을 아우르는 범용 모델을 개발"},{"id":"2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics","title":"[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics","excerpt":"Dongchen Huang이 arXiv에 게시한 'CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics","tags":["Review","Large Language Models","Condensed Matter Physics","Benchmark","Scientific Reasoning","Evaluation Metric","Expression Edit Distance","Problem Solving"],"text":"링크: 논문 PDF로 바로 열기 저자: Weida Wang, Dongchen Huang, Jiatong Li, Tengchao Yang, Ziyang Zheng, et al. 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 복잡한 과학 도메인, 특히 응집 물질 물리학(Condensed Matter Physics, CMP) 문제 해결에 얼마나 능숙한"},{"id":"2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","title":"[논문리뷰] CineScale: Free Lunch in High-Resolution Cinematic Visual Generation","excerpt":"Ziwei Liu이 arXiv에 게시한 'CineScale: Free Lunch in High-Resolution Cinematic Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","tags":["Review","Diffusion Models","High-Resolution Generation","Image Generation","Video Generation","UNet Architecture","DiT Architecture","Scale Fusion","LoRA Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu 핵심 연구 목표 기존 확산 모델이 낮은 해상도 데이터로 훈련되어 고해상도 시각 콘텐츠 생성 시 반복적인 패턴이나 흐릿함, 품질 저하 문제를 겪는 한계를 해결합니다. 논문은 UNet 및 DiT 기반 확산 모델 모두에서 "},{"id":"2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation","title":"[논문리뷰] ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation","excerpt":"Kun Kuang이 arXiv에 게시한 'ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation","tags":["Review","Legal AI","Natural Language Processing","Claim Generation","Chinese Legal Dataset","Factuality","Clarity","Large Language Models","Zero-shot Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Siying Zhou, Yiquan Wu, Hui Chen, Xavier Hu, Kun Kuang, Adam Jatowt, Ming Hu, Chunyan Zheng, Fei Wu 핵심 연구 목표 본 논문은 법률 전문가가 아닌 일반인(예: 원고)을 위한 법률 청구 생성(Legal Claim Generation) 문제에 "},{"id":"2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning","title":"[논문리뷰] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning","excerpt":"Arman Cohan이 arXiv에 게시한 'Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning","tags":["Review","Large Language Models","Scientific Reasoning","Knowledge Retrieval","Reasoning Probing","Benchmarks","Chain-of-Thought","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan 핵심 연구 목표 본 논문은 LLM의 과학 문제 해결 능력에 있어 깊은 도메인 지식 과 복잡한 추론 능력 의 필요성을 강조하며, 이를 종합적으로 평가할 수 있는 통일된 벤치마크의 부재와 지식 및 추론의 역할을 체계적"},{"id":"2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling","title":"[논문리뷰] FastMesh:Efficient Artistic Mesh Generation via Component Decoupling","excerpt":"Xingang Pan이 arXiv에 게시한 'FastMesh:Efficient Artistic Mesh Generation via Component Decoupling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling","tags":["Review","3D Mesh Generation","Component Decoupling","Autoregressive Models","Bidirectional Transformer","Fidelity Enhancement","Prediction Filtering","Token Efficiency","Artistic Meshes"],"text":"링크: 논문 PDF로 바로 열기 저자: Jeonghwan Kim, Yushi Lan, Armando Fortes, Yongwei Chen, Xingang Pan 핵심 연구 목표 기존 메시 생성 방식이 토큰 시퀀스 내의 정점(vertex) 중복 사용으로 인해 발생하는 비효율성(과도한 토큰 길이, 느린 생성 프로세스)을 해결하고, 정점과 면(face)을 분리하"},{"id":"2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies","title":"[논문리뷰] MovieCORE: COgnitive REasoning in Movies","excerpt":"Hung-Ting Su이 arXiv에 게시한 'MovieCORE: COgnitive REasoning in Movies' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies","tags":["Review","Video Question Answering (VQA)","Cognitive Reasoning","System-2 Thinking","Multi-agent LLMs","Dataset Creation","Movie Understanding","Cinematic Content","Agentic Enhancement"],"text":"링크: 논문 PDF로 바로 열기 저자: Gueter Josmy Faure, MinHung Chen, JiaFong Yeh, Ying Cheng, HungTing Su, YungHao Tang, ShangHong Lai, Winston H. Hsu 핵심 연구 목표 본 논문은 기존의 비디오 질의응답(VQA) 데이터셋이 표면적인 이해에 머무는 한계를 극복하고, "},{"id":"2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models","title":"[논문리뷰] ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models","excerpt":"Beiqi Chen이 arXiv에 게시한 'ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models","tags":["Review","3D Inpainting","Multi-view Consistency","Video Diffusion Models","3D Object Completion","Generative Models","LoRA","3D Gaussian Splatting"],"text":"링크: 논문 PDF로 바로 열기 저자: Haitang Feng, Jie Liu, Jie Tang, Gangshan Wu, Beiqi Chen, Jianhuang Lai, Guangcong Wang 핵심 연구 목표 기존 3D 인페인팅 방법론들이 다중 뷰 2D 이미지 인페인팅에 의존하여 발생하는 뷰 간 불일치, 흐릿한 텍스처, 공간 불연속성 문제를 해결하고자 "},{"id":"2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation","title":"[논문리뷰] OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation","excerpt":"Jiaqi Yang이 arXiv에 게시한 'OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation","tags":["Review","Video Avatar Generation","Cognitive Simulation","Multimodal Large Language Models (MLLMs)","Diffusion Transformers (DiT)","Multimodal Fusion","Human Motion Synthesis","Contextual Animation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianwen Jiang, Chao Liang Wang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Liao, Han Liang, Yuan Zhang, Mingyuan Gao 핵심 연구 목표 기존 비디오 아바타 모델이 오디오 리듬에 국한된 물리적 애니메이션만 생성하는 한계를 넘어, 감정, "},{"id":"2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks","title":"[논문리뷰] Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks","excerpt":"Daisuke Nohara이 arXiv에 게시한 'Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks","tags":["Review","Mixture-of-Experts (MoE)","Sparsity","Scaling Laws","Reasoning Tasks","Memorization","Large Language Models","Generalization Gap","Top-k Routing"],"text":"링크: 논문 PDF로 바로 열기 저자: Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, TaishiN324 핵심 연구 목표 본 논문은 MoE(MixtureofExperts) 언어 모델에서 스파시티(sparsity)가 기억(memorization) 능력과 추론"},{"id":"2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels","title":"[논문리뷰] Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels","excerpt":"Dinesh Jayaraman이 arXiv에 게시한 'Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels","tags":["Review","3D Physics Prediction","Supervised Learning","CLIP Features","Neural Radiance Fields","Material Point Method","PIXIEVERSE Dataset","Zero-Shot Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Long Le, Ryan Lucas, Chen Wang, Chuhao Chen, Dinesh Jayaraman, Eric Eaton, Lingjie Liu 핵심 연구 목표 이 논문은 기존 3D 장면 재구성 모델(예: NeRF, Gaussian Splatting)이 시각적 외형에만 집중하고 물리적 속성 예측에는 한계가 "},{"id":"2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting","title":"[논문리뷰] QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting","excerpt":"Manuela Veloso이 arXiv에 게시한 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting","tags":["Review","Hallucination Mitigation","Large Language Models","Contextual Bandits","Query Rewriting","Semantic Features","No-Regret Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Nicole Cho, William Watson, Alec Koppel, Sumitra Ganesh, Manuela Veloso 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 환각 발생률 증가 문제를 해결하고자 합니다. 기존의 사후 필터링 방식 대신, 입력 쿼리의 17가지 언어학적 특징 을 활용하는 밴딧 프레"},{"id":"2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks","title":"[논문리뷰] ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks","excerpt":"Kai Jia이 arXiv에 게시한 'ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks","tags":["Review","Deep Research Agents","LLM Evaluation","Academic Survey","Factual Accuracy","Citation Verification","Report Generation","Benchmark","Hallucination"],"text":"링크: 논문 PDF로 바로 열기 저자: Minghao Li, Ying Zeng, Zhihao Cheng, Cong Ma, Kai Jia 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반의 심층 연구(Deep Research) 에이전트가 생성하는 연구 보고서의 내용 품질을 체계적으로 평가하기 위한 벤치마크인 ReportBench 를 제안합니다. 특히"},{"id":"2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration","title":"[논문리뷰] Spacer: Towards Engineered Scientific Inspiration","excerpt":"zerojun48이 arXiv에 게시한 'Spacer: Towards Engineered Scientific Inspiration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration","tags":["Review","Scientific Discovery","Large Language Models (LLMs)","Decontextualization","Keyword Graph","Multi-Agent System","Scientific Ideation","Research Automation","Inspiration Engine"],"text":"링크: 논문 PDF로 바로 열기 저자: zerojun48, kohandy, rallyduck1005, MoonRainy21, mhlee1022 핵심 연구 목표 Spacer는 기존 LLM의 한계인 제한된 창의성과 문맥 의존성을 극복하여 외부 개입 없이 창의적이고 사실에 기반한 과학적 개념을 생성하는 것을 목표로 합니다. 특히 \"의도적인 비문맥화(deliber"},{"id":"2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models","title":"[논문리뷰] ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models","excerpt":"Jiangjie Chen이 arXiv에 게시한 'ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models","tags":["Review","LLMs","Controllable Reasoning","Computational Efficiency","Reinforcement Learning","Supervised Fine-tuning","Reasoning Compression","Budget-Aware Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Qianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, Jiangjie Chen 핵심 연구 목표 대규모 언어 모델(LLMs)의 CoT(ChainofThought) 추론 능력은 뛰어나지만, 실제 배포 시 연산 비용을 효율적으로 제어하는 것이 어렵습니다. 이 연구는 OpenAI의 gptos"},{"id":"2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo","title":"[논문리뷰] Training Language Model Agents to Find Vulnerabilities with CTF-Dojo","excerpt":"Zijian Wang이 arXiv에 게시한 'Training Language Model Agents to Find Vulnerabilities with CTF-Dojo' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo","tags":["Review","LLM Agents","Cybersecurity","CTF Challenges","Vulnerability Detection","Execution Environments","Docker","Automated Training","Verifiable Feedback"],"text":"링크: 논문 PDF로 바로 열기 저자: Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, Zijian Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트를 활용하여 사이버 보안 취약점을 자동으로 탐지하고 악용하는 것을 목표로 합니다. 특히, LLM 에이전트 훈련을 위한 확장 가능하고 검"},{"id":"2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling","title":"[논문리뷰] TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling","excerpt":"Zhoufutu Wen이 arXiv에 게시한 'TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling","tags":["Review","Reinforcement Learning","Policy Optimization","Large Language Models","Inference Efficiency","Tree Search","Segment-level Decoding","Advantage Estimation","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhoufutu Wen, Qingshui Gu, zhangysk, aaabiao, yizhilll 핵심 연구 목표 대규모 언어 모델(LLMs)을 강화 학습(RL)으로 정렬하는 과정에서 발생하는 높은 온정책 롤아웃 비용 과 다양한 추론 경로 탐색의 한계 를 해결하고자 합니다. 본 논문은 시퀀스 생성을 트리 구조 검색 과"},{"id":"2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning","title":"[논문리뷰] UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning","excerpt":"Ran Guo이 arXiv에 게시한 'UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning","tags":["Review","Memory Networks","Mixture of Experts (MoE)","Long-Context Learning","Sparse Models","Transformer Architecture","LLMs","Efficient Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, Siyuan Qiao 핵심 연구 목표 본 논문은 Mixture of Experts (MoE) 모델이 겪는 높은 메모리 "},{"id":"2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities","title":"[논문리뷰] Unraveling the cognitive patterns of Large Language Models through module communities","excerpt":"Jianxi Gao이 arXiv에 게시한 'Unraveling the cognitive patterns of Large Language Models through module communities' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities","tags":["Review","Large Language Models","Network Community Structure","Cognitive Skills","AI Interpretability","Module Communities","Fine-tuning","Neural Plasticity"],"text":"링크: 논문 PDF로 바로 열기 저자: Kushal Raj Bhandari, PinYu Chen, Jianxi Gao 핵심 연구 목표 본 논문은 LLM의 내부 아키텍처와 인지 과정을 이해하기 어려운 ‘블랙박스’ 문제를 해결하고자 합니다. 특히 기존 연구에서 부족했던 스킬 간의 관계, 동적 적응성, 교차 도메인 일반화 및 메커니즘의 상세한 해석 가능성 탐색에"},{"id":"2025-8-27-VibeVoice-Technical-Report","title":"[논문리뷰] VibeVoice Technical Report","excerpt":"Yaoyao Chang이 arXiv에 게시한 'VibeVoice Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-VibeVoice-Technical-Report","tags":["Review","Speech Synthesis","Long-form Audio","Multi-speaker","Next-token Diffusion","Speech Tokenizer","Large Language Model","Variational Autoencoder","Audio Compression"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, Shaohan Huang, Yan Xia, Furu Wei 핵심 연구 목표 본 논문은 기존 시스템의 한계로"},{"id":"2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space","title":"[논문리뷰] VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space","excerpt":"Rui Chen이 arXiv에 게시한 'VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space","tags":["Review","3D Editing","Training-Free","Diffusion Models","Latent Space","3D Inversion","Contextual Feature Replacement","3D Consistency","Edit3D-Bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng 핵심 연구 목표 본 논문은 기존 2D 이미지 기반의 3D 편집 방법론이 겪는 비일관성 및 비정밀성의 한계를 극복하고, 네이티브 3D 잠재 공간 에서 훈련 없이(tra"},{"id":"2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation","title":"[논문리뷰] Wan-S2V: Audio-Driven Cinematic Video Generation","excerpt":"Chaonan Ji이 arXiv에 게시한 'Wan-S2V: Audio-Driven Cinematic Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-27 13:22:18+0900","permalink":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation","tags":["Review","Audio-Driven Video Generation","Cinematic Video","Diffusion Models","Transformer Architecture","Long Video Consistency","Human Animation","Multimodal Control","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: HumanAIGC Team, Tongyi Lab, Alibaba 핵심 연구 목표 본 연구는 기존 오디오 기반 캐릭터 애니메이션 모델이 복잡한 영화 및 TV 프로덕션 시나리오(미묘한 상호작용, 현실적인 신체 움직임, 다이내믹한 카메라 워크)에서 한계를 보이는 문제를 해결합니다. WanS2V 모델을 통해 오디오 입력을 기"},{"id":"2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models","title":"[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models","excerpt":"Yixiao Ge이 arXiv에 게시한 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models","tags":["Review","Text-to-Audio","Long-Form Audio Generation","Large Language Models","Narrative Reasoning","Diffusion Models","Multimodal AI","Progressive Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan 핵심 연구 목표 본 논문은 기존 TexttoAudio (TTA) 모델들이 단편적인 오디오 클립 생성에는 뛰어나지만, 시간적 일관성 과 구성적 추론 능력 이 요구되는 장문 서술형 오디오(l"},{"id":"2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR","title":"[논문리뷰] Beyond Transcription: Mechanistic Interpretability in ASR","excerpt":"Aviv Shamsian이 arXiv에 게시한 'Beyond Transcription: Mechanistic Interpretability in ASR' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR","tags":["Review","ASR","Mechanistic Interpretability","Logit Lens","Linear Probing","Activation Patching","Hallucinations","Repetitions","Encoder-Decoder"],"text":"링크: 논문 PDF로 바로 열기 저자: Neta Glazer, Yael SegalFeldman, Hilit Segev, Aviv Shamsian, Asaf Buchnick, Gill Hetz, Ethan Fetaya, Joseph Keshet, Aviv Navon 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)에서 성공적으로 적용된 메커니즘 해석 가능"},{"id":"2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning","title":"[논문리뷰] CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning","excerpt":"Jianze Liang이 arXiv에 게시한 'CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning","tags":["Review","GUI Agents","Reinforcement Learning","Planner-Executor Architecture","Decoupled Training","Large Vision-Language Models","Specialization","Generalization","Computer Use Agent"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang 핵심 연구 목표 GUI(Graphical User Interface) 기반 자율 에"},{"id":"2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis","title":"[논문리뷰] DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis","excerpt":"Ion Stoica이 arXiv에 게시한 'DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis","tags":["Review","Generative Research Synthesis","Live Benchmark","Automated Evaluation","LLM-as-a-judge","Related Work Generation","Retrieval-Augmented Generation","Verifiability"],"text":"링크: 논문 PDF로 바로 열기 저자: Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin 핵심 연구 목표 본 연구는 기존 질의응답 벤치마크나 수동 큐레이션 데이터셋의 한계를 극복하고, 생성형 연구 합성(Generative R"},{"id":"2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding","title":"[논문리뷰] Diffusion Language Models Know the Answer Before Decoding","excerpt":"Shilin Yan이 arXiv에 게시한 'Diffusion Language Models Know the Answer Before Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding","tags":["Review","Diffusion Language Models","DLM Acceleration","Early Answer Convergence","Early Commit Decoding","Confidence Gap","Inference Speedup","Training-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu 핵심 연구 목표 본 논문은 확산 언어 모델(DLM)의 주요 단점인 느린 추론 속도를 해결하는 것을 목표로 합니다. 특히, 기"},{"id":"2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies","title":"[논문리뷰] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies","excerpt":"Sitong Mao이 arXiv에 게시한 'Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies","tags":["Review","Vision-Language-Action (VLA)","Discrete Diffusion","Action Decoding","Transformer","Robot Control","Masked Modeling","Adaptive Decoding","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Jiangmiao Pang, Yao Mu, Ping Luo 핵심 연구 목표 본 논문은 기존 VisionLanguageAction (VLA) 모델 디코더의 한계(고정된 순서의 autoregressive "},{"id":"2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation","title":"[논문리뷰] Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation","excerpt":"Anton Ivaschenko이 arXiv에 게시한 'Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation","tags":["Review","rPPG","Multi-View Video Dataset","Health Biomarkers","Physiological Monitoring","Deep Learning","Telemedicine","Biosignals"],"text":"링크: 논문 PDF로 바로 열기 저자: Konstantin Egorov, Stepan Botman, Pavel Blinov, Galina Zubkova, Anton Ivaschenko, Andrey Savchenko, Alexander Kolsanov 핵심 연구 목표 기존 rPPG(remote PhotoPlethysmoGraphy) 데이터셋의 한계 (작은 "},{"id":"2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation","title":"[논문리뷰] MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation","excerpt":"Yan Zhou이 arXiv에 게시한 'MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation","tags":["Review","Multimodal Generation","Digital Human Synthesis","Real-time Video Generation","Autoregressive LLM","Diffusion Models","Deep Compression Autoencoder","Exposure Bias Mitigation","Streaming Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Chen, Liyuan Cui, Wenyuan Zhang, Yan Zhou, Xiaohan Li, Xiaoqiang Liu, Pengfei Wan 핵심 연구 목표 본 논문은 다양한 입력 신호에 실시간으로 반응하며, 낮은 지연 시간과 높은 시각적 일관성을 유지하는 대화형 디지털 휴먼 비디오 생성 시스템 을 구축"},{"id":"2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents","title":"[논문리뷰] Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents","excerpt":"Yue Yao이 arXiv에 게시한 'Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents","tags":["Review","Multimodal LLMs (MLLMs)","Smartphone Agents","Privacy Awareness","Benchmarking","Sensitive Data Detection","Risk Assessment","UI Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhixin Lin, Jungang Li, Shidong Pan, Yibo Shi, Yue Yao, Dongliang Xu 핵심 연구 목표 본 논문은 MLLM 기반 스마트폰 에이전트 의 개인정보 보호 인식(Privacy Awareness) 능력을 체계적으로 평가하기 위한 최초의 대규모 벤치마크를 구축하고, 에이전트들이"},{"id":"2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment","title":"[논문리뷰] MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment","excerpt":"An-An Liu이 arXiv에 게시한 'MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment","tags":["Review","Text-Guided Motion Generation","Rectified Flow Matching","Preference Alignment","Human Motion Synthesis","Real-time AI","Transformer Architecture","Self-supervised Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiting Gao, Dan Song, Diqiong Jiang, Chao Xue, AnAn Liu 핵심 연구 목표 본 논문은 기존 텍스트 기반 모션 생성 방법론이 겪는 언어적 설명과 모션 의미 간의 부정확한 정렬 및 느리고 비효율적인 다단계 추론 과정 의 문제를 해결하고자 합니다. 궁극적으로 강력한 의미론적 정렬,"},{"id":"2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling","title":"[논문리뷰] Predicting the Order of Upcoming Tokens Improves Language Modeling","excerpt":"Alham Fikri Aji이 arXiv에 게시한 'Predicting the Order of Upcoming Tokens Improves Language Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling","tags":["Review","Language Modeling","Next-Token Prediction","Multi-Token Prediction","Token Order Prediction","Auxiliary Objective","Learning-to-Rank","Transformer","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zayd M. K. Zuhri, Erland Hilman Fuadi & Alham Fikri Aji 핵심 연구 목표 기존 MultiToken Prediction (MTP) 이 정확한 미래 토큰 예측의 어려움으로 인해 보조 목표로서 불일치한 성능을 보이는 문제를 해결하고자 합니다. 본 논문은 NTP (NextToken "},{"id":"2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition","title":"[논문리뷰] Self-Rewarding Vision-Language Model via Reasoning Decomposition","excerpt":"Zhenwen Liang이 arXiv에 게시한 'Self-Rewarding Vision-Language Model via Reasoning Decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition","tags":["Review","Vision-Language Models","Reinforcement Learning","Self-Rewarding","Reasoning Decomposition","Visual Perception","Language Reasoning","Hallucinations","Language Shortcuts"],"text":"링크: 논문 PDF로 바로 열기 저자: Zongxia Li, Wenhao Yu, Chengsong Huang, Zhenwen Liang, Rui Liu, et al. 핵심 연구 목표 VisionLanguage Model (VLM)이 겪는 시각적 환각 및 언어적 지름길 문제를 해결하는 것을 목표로 합니다. 기존 VLM 훈련 방식이 외부 시각적 감독 부족으로 "},{"id":"2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning","title":"[논문리뷰] StepWiser: Stepwise Generative Judges for Wiser Reasoning","excerpt":"Olga Golovneva이 arXiv에 게시한 'StepWiser: Stepwise Generative Judges for Wiser Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning","tags":["Review","LLM Reasoning","Process Reward Models","Reinforcement Learning","Generative Judges","Stepwise Feedback","Chain-of-Thought","Meta-Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 복잡한 문제 해결을 위해 사용하는 다단계 추론(ChainofThought) 전략에서"},{"id":"2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference","title":"[논문리뷰] Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference","excerpt":"Chunlei Han이 arXiv에 게시한 'Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-28 13:10:39+0900","permalink":"/ai/review/2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference","tags":["Review","LLM Inference","Autoscaling","Disaggregated Architecture","Heterogeneous Hardware","Resource Management","Topology-aware Scheduling","GPU Utilization"],"text":"링크: 논문 PDF로 바로 열기 저자: Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu 핵심 연구 목표 전통적인 자동 스케일러가 PrefillDecode (P/D) 분리형 "},{"id":"2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI","title":"[논문리뷰] AWorld: Orchestrating the Training Recipe for Agentic AI","excerpt":"Qintong Wu이 arXiv에 게시한 'AWorld: Orchestrating the Training Recipe for Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI","tags":["Review","Agentic AI","Reinforcement Learning","Distributed Systems","Experience Generation","LLM Fine-tuning","GAIA Benchmark","Scalability","AWORLD Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengyue Yu, Siyuan Lu, Chenyi Zhuang, Dong Wang, Qintong Wu, Zongyue Li, Runsheng Gan, Chunfeng Wang, Siqi Hou, Gaochi Huang, Wenlong Yan, Lifeng Hong, Aohui Xue, Yanfeng Wang, "},{"id":"2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification","title":"[논문리뷰] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification","excerpt":"Liqiang Nie이 arXiv에 게시한 'CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification","tags":["Review","Vision-Language-Action Model","Sparsification","Instruction-Driven Routing","Cognition-Aligned AI","Robotics","Computational Efficiency","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie 핵심 연구 목표 본 논문은 기존 VisionLanguageAction (VLA) 모델의 높은 계산 오버헤드 와 모달리티 간의 의미론적 불일치(semantic fragmentation) 문제를 해결하여, VLA 모델의 확장성과"},{"id":"2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation","title":"[논문리뷰] Collaborative Multi-Modal Coding for High-Quality 3D Generation","excerpt":"Ziwei Liu이 arXiv에 게시한 'Collaborative Multi-Modal Coding for High-Quality 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation","tags":["Review","3D Generation","Multi-modal Learning","Diffusion Models","Triplane Representation","Collaborative Coding","Image-to-3D","Latent Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziang Cao, Zhaoxi Chen, Liang Pan, Ziwei Liu 핵심 연구 목표 본 논문은 기존 3D 생성 모델들이 단일 모달리티(예: RGB 이미지)에 의존하여 훈련 데이터의 범위가 제한되고 멀티모달 데이터의 상호 보완적 이점을 간과하는 문제를 해결하고자 합니다. RGB 이미지의 기하학적 모호성과 포"},{"id":"2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview","title":"[논문리뷰] Dress&Dance: Dress up and Dance as You Like It - Technical Preview","excerpt":"Yu-Xiong Wang이 arXiv에 게시한 'Dress&Dance: Dress up and Dance as You Like It - Technical Preview' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview","tags":["Review","Virtual Try-On","Video Diffusion","Multi-modal Conditioning","Garment Transfer","Pose Animation","Generative AI","Fashion Tech","CondNet"],"text":"링크: 논문 PDF로 바로 열기 저자: JunKun Chen, Aayush Bansal, Minh Phuoc Vo, YuXiong Wang 핵심 연구 목표 본 논문은 정적인 2D 이미지 기반의 가상 착용(virtual tryon) 방식과 기존 비디오 생성 모델의 한계를 극복하여, 사용자가 원하는 옷을 입고 특정 동작(춤)을 수행하는 고품질의 5초 길이, 1"},{"id":"2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes","title":"[논문리뷰] FakeParts: a New Family of AI-Generated DeepFakes","excerpt":"Xi Wang이 arXiv에 게시한 'FakeParts: a New Family of AI-Generated DeepFakes' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes","tags":["Review","Deepfake Detection","Partial Deepfakes","AI-Generated Video","Benchmark Dataset","Video Forensics","Generative Models","Manipulation Detection","Human Perception"],"text":"링크: 논문 PDF로 바로 열기 저자: Gaëtan Brison, Soobash Daiboo, Samy Aïmeur, Awais Hussain Sani, Xi Wang, Gianni Franchi, Vicky Kalogeiton 핵심 연구 목표 본 연구는 미묘하고 국소적인 조작이 가해져 탐지하기 어려운 새로운 형태의 딥페이크인 FakeParts 를 정의하고"},{"id":"2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers","title":"[논문리뷰] MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers","excerpt":"Shashank Biju이 arXiv에 게시한 'MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers","tags":["Review","LLM Agents","Tool Use","Benchmarking","Model Context Protocol (MCP)","Cross-Domain Orchestration","Fuzzy Instructions","Multi-Step Tasks","Real-World Scenarios"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, ChengEn Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, Eugene Siow 핵심 연구 목표 이 논문은 기존 도구 사용 벤치마크의 한계를 극복"},{"id":"2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation","title":"[논문리뷰] Mixture of Contexts for Long Video Generation","excerpt":"Junfei Xiao이 arXiv에 게시한 'Mixture of Contexts for Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation","tags":["Review","Long Video Generation","Diffusion Transformers (DiT)","Sparse Attention","Context Routing","Memory Management","Generative Models","Video Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, Maneesh Agrawala, Lu Jiang, Gordon Wetzstein 핵"},{"id":"2025-8-29-Multi-View-3D-Point-Tracking","title":"[논문리뷰] Multi-View 3D Point Tracking","excerpt":"Irem Demir이 arXiv에 게시한 'Multi-View 3D Point Tracking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-Multi-View-3D-Point-Tracking","tags":["Review","3D Point Tracking","Multi-View","Transformer","kNN Correlation","Depth Estimation","Dynamic Scenes","Occlusion Handling","Feature Fusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Frano Rajič, Haofei Xu, Marko Mihajlovic, Siyuan Li, Irem Demir, Emircan Gündoğdu, Lei Ke, Sergey Prokudin, Marc Pollefeys, Siyu Tang 핵심 연구 목표 본 논문은 기존 단안 카메라 트래커의 깊이 모호성 및 가림(oc"},{"id":"2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models","title":"[논문리뷰] OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models","excerpt":"Alex Endert이 arXiv에 게시한 'OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models","tags":["Review","Large Language Models (LLMs)","Human-Computer Interaction (HCI)","Conversational AI","Goal Tracking","Visualization","Multi-Turn Dialogue","User Interface Design","Sensemaking"],"text":"링크: 논문 PDF로 바로 열기 저자: Adam J Coscia, Shunan Guo, Eunyee Koh, Alex Endert 핵심 연구 목표 다중 턴 대화에서 대규모 언어 모델(LLM) 과의 상호작용이 길고 복잡해짐에 따라, 사용자가 대화 목표 진행 상황 을 효과적으로 평가하고 검토하는 데 겪는 어려움을 해결하는 것이 핵심 연구 목표입니다. 특히, 사"},{"id":"2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning","title":"[논문리뷰] OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning","excerpt":"Yitong Wang이 arXiv에 게시한 'OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning","tags":["Review","Image Generation","Mask-Guided Editing","Reinforcement Learning","Human Preference Learning","Vision-Language Models","Multi-Task Learning","Flow Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuan Gong, Xionghui Wang†, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu 핵심 연구 목표 논문은 마스크 기반 이미지 편집(Image Fill, Extend, Object Removal, Text Rendering)의 다양한 하위 태스크에서 기존 모델들의 제한적인"},{"id":"2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD","title":"[논문리뷰] Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD","excerpt":"Roy Ka-Wei Lee이 arXiv에 게시한 'Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD","tags":["Review","Persuasion Dynamics","Large Language Models (LLMs)","Robustness","Gullibility","Receptiveness","Direct Preference Optimization (DPO)","Safety Alignment","Multi-turn Dialogue"],"text":"링크: 논문 PDF로 바로 열기 저자: Bryan Chen Zhengyu Tan, Daniel Wai Kit Chin, Zhengyuan Liu, Nancy F. Chen, Roy KaWei Lee 핵심 연구 목표 본 연구는 LLM이 다중 턴 대화에서 잘못된 정보에 대한 설득 저항성(robustness) 과 유효한 수정 사항에 대한 수용성(receptive"},{"id":"2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning","title":"[논문리뷰] Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning","excerpt":"Jiazi Bu이 arXiv에 게시한 'Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Text-to-Image Generation","GRPO","Reward Hacking","Pairwise Preference","Reward Model","Stable Optimization","UniGenBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang 핵심 연구 목표 본 논문은 텍스트투이미지(T2I) 생성에서 기존 GRPO(Group Relative Policy Optimizati"},{"id":"2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models","title":"[논문리뷰] Provable Benefits of In-Tool Learning for Large Language Models","excerpt":"Vivien Cabannes이 arXiv에 게시한 'Provable Benefits of In-Tool Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models","tags":["Review","Large Language Models","In-Tool Learning","In-Weight Learning","Factual Recall","Retrieval-Augmented Generation","Scaling Laws","Parameter Efficiency","Catastrophic Forgetting"],"text":"링크: 논문 PDF로 바로 열기 저자: Sam Houliston, Ambroise Odonnat, Charles Arnal, Vivien Cabannes 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)에서 도구 사용 학습(intool learning) 방식이 내부 가중치 학습(inweight learning) 방식보다 사실 정보 기억 및 회상에 있어 "},{"id":"2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos","title":"[논문리뷰] ROSE: Remove Objects with Side Effects in Videos","excerpt":"Hantang Liu이 arXiv에 게시한 'ROSE: Remove Objects with Side Effects in Videos' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos","tags":["Review","Video Object Removal","Side Effects","3D Rendering","Diffusion Transformer","Video Inpainting","Synthetic Data","Difference Mask"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenxuan Miao, Yutong Feng, Jianshu Zeng, Zixiang Gao, Hantang Liu, Yunfeng Yan, Donglian Qi, Xi Chen, Bin Wang, Hengshuang Zhao 핵심 연구 목표 기존 비디오 객체 제거 모델들이 객체의 그림자, 반사, 조명 변화 등 \""},{"id":"2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning","title":"[논문리뷰] TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning","excerpt":"Simin Ma이 arXiv에 게시한 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning","tags":["Review","Instruction Augmentation","Fine-tuning","Large Language Models","Task-Centric","Data Diversity","Task Alignment","Breadth-First Search","Constraint Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Simin Ma, Shujian Liu, Jun Tan, Yebowen Hu, Song Wang, Sathish Reddy Indurthi, Sanqiang Zhao, Liwei Wu, Jianbing Han, Kaiqiang Song 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 효율적인 인스트럭션 튜닝을 "},{"id":"2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection","title":"[논문리뷰] Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection","excerpt":"Bernard Ghanem이 arXiv에 게시한 'Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection","tags":["Review","LLM Safety","Alignment Amplification","Rank-One Update","Mechanistic Interpretability","Weight Steering","Jailbreak Robustness","Fine-tuning-free","Safety Injection"],"text":"링크: 논문 PDF로 바로 열기 저자: Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 안전 정렬(safety alignment)이 특정 내부 표현 방향에 의해 매개되며 우회될 수 있다는 기존 연구를 "},{"id":"2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning","title":"[논문리뷰] USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning","excerpt":"Jiahe Tian이 arXiv에 게시한 'USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning","tags":["Review","Style-Driven Generation","Subject-Driven Generation","Disentangled Representation","Reward Learning","Cross-Task Learning","Diffusion Models","Image Customization","Unified Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaojin Wu, Mengqi Huang, Yufeng Cheng, Wenxu Wu, Jiahe Tian, Yiming Luo, Fei Ding, Qian He 핵심 연구 목표 본 논문은 스타일 기반 생성(styledriven generation)과 주제 기반 생성(subjectdriven generation)이 "},{"id":"2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report","title":"[논문리뷰] rStar2-Agent: Agentic Reasoning Technical Report","excerpt":"Weijiang Xu이 arXiv에 게시한 'rStar2-Agent: Agentic Reasoning Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-29 13:14:44+0900","permalink":"/ai/review/2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report","tags":["Review","Agentic Reinforcement Learning","Math Reasoning","Code Interpreter","Tool Use","GRPO-RoC","LLM Training Efficiency","Self-Reflection"],"text":"링크: 논문 PDF로 바로 열기 저자: Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, Mao Yan"},{"id":"2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture","title":"[논문리뷰] AgroBench: Vision-Language Model Benchmark in Agriculture","excerpt":"Yoshitaka Ushiku이 arXiv에 게시한 'AgroBench: Vision-Language Model Benchmark in Agriculture' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture","tags":["Review","Vision-Language Models","Agriculture","Benchmarking","Disease Identification","Pest Management","Crop Management","Agronomy"],"text":"링크: 논문 PDF로 바로 열기 AgroBench: VisionLanguage Model Benchmark in Agriculture 저자: Risa Shinoda, Nakamasa Inoue, Hirokatsu Kataoka, Masaki Onishi, Yoshitaka Ushiku 키워드: , , , , , , 핵심 연구 목표 본 논문은 농업 분야에서 "},{"id":"2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification","title":"[논문리뷰] Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification","excerpt":"Abdelmalik Taleb-Ahmed이 arXiv에 게시한 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification","tags":["Review","Kolmogorov-Arnold Networks","Knowledge Distillation","Art Style Classification","Self-Supervised Learning","Spline-Based Activation","Dual-Teacher","Gram Matrix"],"text":"링크: 논문 PDF로 바로 열기 Beyond Linear Bottlenecks: SplineBased Knowledge Distillation for Culturally Diverse Art Style Classification 저자: Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Cosimo Distante, Ab"},{"id":"2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations","title":"[논문리뷰] C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations","excerpt":"Yiwen Guo이 arXiv에 게시한 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations","tags":["Review","Spoken Dialogue Models","Bilingual Benchmark","Complex Conversations","Ambiguity Resolution","Context Understanding","LLM Evaluation","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengqian Ma, Wei Tao, Yiwen Guo 키워드: , , , , , , 핵심 연구 목표 본 연구는 현존하는 음성 대화 모델(SDM)들이 인간의 복잡한 대화, 특히 음운론적/의미론적 모호성 과 맥락 의존성 (생략, 공참조, 다중 턴 상호작용)을 얼마나 효과적으로 이해하고 모방하는지에 대한 종합적인 벤치"},{"id":"2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation","title":"[논문리뷰] Efficient Machine Unlearning via Influence Approximation","excerpt":"Enhong Chen이 arXiv에 게시한 'Efficient Machine Unlearning via Influence Approximation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation","tags":["Review","Machine Unlearning","Influence Function","Incremental Learning","Privacy Protection","Gradient Optimization","Model Editing","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawei Liu, Chenwang Wu, Defu Lian, and Enhong Chen 키워드: , , , , , , 핵심 연구 목표 본 논문은 대규모 데이터셋과 빈번한 삭제 요청이 발생하는 환경에서 기존 영향 함수 기반 언러닝(unlearning) 방식의 높은 계산 비용과 메모리 오버헤드 문제를 해결하고자 합니"},{"id":"2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring","title":"[논문리뷰] Enhanced Arabic Text Retrieval with Attentive Relevance Scoring","excerpt":"Abdenour Hadid이 arXiv에 게시한 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring","tags":["Review","Arabic NLP","Dense Passage Retrieval","Attentive Relevance Scoring","Information Retrieval","Question Answering","Transformer Models","Semantic Matching"],"text":"링크: 논문 PDF로 바로 열기 제목: Enhanced Arabic Text Retrieval with Attentive Relevance Scoring 저자: Salah Eddine Bekhouche, Azeddine Benlamoudi, Yazid Bounab, Fadi Dornaika, Abdenour Hadid 키워드: , , , , , , 핵심 연"},{"id":"2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks","title":"[논문리뷰] Flow Equivariant Recurrent Neural Networks","excerpt":"T. Anderson Keller이 arXiv에 게시한 'Flow Equivariant Recurrent Neural Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks","tags":["Review","Flow Equivariance","Recurrent Neural Networks","Sequence Models","Group Equivariance","Lie Subgroups","Generalization","Time-Parameterized Symmetries"],"text":"링크: 논문 PDF로 바로 열기 저자: T. Anderson Keller 키워드: , , , , , , 핵심 연구 목표 본 논문은 기존 정적 변환 및 피드포워드 네트워크 에 국한된 equivariance 이론을 확장하여, 시각적 움직임과 같은 시간 매개변수화된 을 포착하는 순환 신경망(RNN) 에 적용하는 것을 목표로 합니다. 이를 통해 데이터의 시간 의존"},{"id":"2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting","title":"[논문리뷰] NeRF Is a Valuable Assistant for 3D Gaussian Splatting","excerpt":"ZeSheng Wang이 arXiv에 게시한 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting","tags":["Review","NeRF","3D Gaussian Splatting","Hybrid Model","Joint Optimization","Scene Representation","Neural Rendering","Residual Learning","Sparse View"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuangkang Fang, IChao Shen, Takeo Igarashi, Yufeng Wang, ZeSheng Wang, Yi Yang, Wenrui Ding, Shuchang Zhou 키워드: , , , , , , , 핵심 연구 목표 본 논문은 3D Gaussian Splatting (3DGS) 의 고유한 한"},{"id":"2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective","title":"[논문리뷰] On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective","excerpt":"Eric C. Larson이 arXiv에 게시한 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective","tags":["Review","Softmax Attention","Linear Attention","Recurrent Neural Networks (RNNs)","Taylor Series Expansion","Attention Mechanisms","Expressiveness","Transformer Architectures"],"text":"링크: 논문 PDF로 바로 열기 ON THE EXPRESSIVENESS OF SOFTMAX ATTENTION: A RECURRENT NEURAL NETWORK PERSPECTIVE 저자: Gabriel Mongaras, Eric C. Larson 키워드: , , , , , , 핵심 연구 목표 이 논문은 Softmax Attention 이 선형 Attenti"},{"id":"2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models","title":"[논문리뷰] Persona Vectors: Monitoring and Controlling Character Traits in Language Models","excerpt":"Jack Lindsey이 arXiv에 게시한 'Persona Vectors: Monitoring and Controlling Character Traits in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models","tags":["Review","Large Language Models (LLMs)","Persona Control","Activation Steering","Finetuning","Behavioral Shift Detection","Interpretability","Data Filtering"],"text":"링크: 논문 PDF로 바로 열기 논문 제목: Persona Vectors: Monitoring and Controlling Character Traits in Language Models 저자: Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey 키워드: , , , , , , 핵심 연구 목"},{"id":"2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding","title":"[논문리뷰] Phi-Ground Tech Report: Advancing Perception in GUI Grounding","excerpt":"Kai Qiu이 arXiv에 게시한 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding","tags":["Review","GUI grounding","AI agent","Large Multi-modal Model","Perception","Data Augmentation","Direct Preference Optimization","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, Baining Guo (Microsoft) 키워드: , , , , , , 핵심 연구 목표 본 논문은 "},{"id":"2025-8-3-RecGPT-Technical-Report","title":"[논문리뷰] RecGPT Technical Report","excerpt":"Jian Wu이 arXiv에 게시한 'RecGPT Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-RecGPT-Technical-Report","tags":["Review","Recommender Systems","Large Language Models (LLMs)","User Intent Modeling","Multi-Stage Training","Human-in-the-Loop","E-commerce","Filter Bubble Mitigation","Matthew Effect"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Wu, Jiakai Tang, Gaoyang Guo, Dian Chen, Chao Yi 키워드: , , , , , , , 핵심 연구 목표 기존 추천 시스템의 로그 기반(logfitting) 접근 방식 이 야기하는 과적합, 필터 버블, 롱테일 문제의 한계를 극복하고, 사용자 의도 를 중심으로 하는 차세대 추천 "},{"id":"2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents","title":"[논문리뷰] Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents","excerpt":"Anji Liu이 arXiv에 게시한 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents","tags":["Review","Reinforcement Learning","Multi-Task Learning","Visuomotor Agents","Spatial Reasoning","Generalization","Minecraft","Cross-View Goal Specification","Automated Task Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang 키워드: , , , , , , , 핵심 연구 목표 본 논문은 강화 학습(RL) 모델의 과적합 문제를 해결하여, visuomotor 에이전트가 다양한 환경에서 일반화 가능한 행동을 습득하지 못"},{"id":"2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving","title":"[논문리뷰] Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving","excerpt":"Zhicheng Jiang이 arXiv에 게시한 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving","tags":["Review","Automated Theorem Proving","Large Language Models","Formal Verification","Reinforcement Learning","Lean","Geometry Reasoning","Chain-of-Thought","Lemma-Style Proving"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhicheng Jiang, Wenhao Huang, Liankai Huang, Jinming Gu, Luoxin Chen 키워드: , , , , , , , 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 자연어 기반 정리 증명에서 명확한 감독 신호 부족으로 겪는 어려움을 해결하고자 합니다. Lean과 같은 형"},{"id":"2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs","title":"[논문리뷰] TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs","excerpt":"Jiasheng Tang이 arXiv에 게시한 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs","tags":["Review","MLLMs","Hallucination Reduction","Preference Optimization","Min-Max Optimization","Token-Adaptive Strategy","Spectral Regularization","Visual Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Kejia Zhang, Keda Tao, Zhiming Luo, Chang Liu, Jiasheng Tang, Huan Wang 키워드: , , , , , , 핵심 연구 목표 멀티모달 대규모 언어 모델(MLLMs)에서 발생하는 환각(hallucination) 문제를 해결하고 신뢰성을 향상하는 것이 목표입니다. 기존 직"},{"id":"2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model","title":"[논문리뷰] iLRM: An Iterative Large 3D Reconstruction Model","excerpt":"Abdelrahman Mohamed이 arXiv에 게시한 'iLRM: An Iterative Large 3D Reconstruction Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model","tags":["Review","3D Reconstruction","Gaussian Splatting","Iterative Refinement","Transformer Architecture","Multi-view Learning","Scalability","Feed-forward Models"],"text":"링크: 논문 PDF로 바로 열기 iLRM: An Iterative Large 3D Reconstruction Model 저자: Gyeongjin Kang, Seungtae Nam, Xiangyu Sun, Abdelrahman Mohamed, Sameh Khamis, Eunbyung Park 키워드: , , , , , , 핵심 연구 목표 본 논문은 일반화 가"},{"id":"2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models","title":"[논문리뷰] villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models","excerpt":"Kaixin Wang이 arXiv에 게시한 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-03 07:35:17+0900","permalink":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models","tags":["Review","Vision-Language-Action Models","Latent Actions","Robot Manipulation","Pre-training","Diffusion Models","Proprioceptive Feedback","Foundation Models"],"text":"링크: 논문 PDF로 바로 열기 villaX: Enhancing Latent Action Modeling in VisionLanguageAction Models 저자: Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang"},{"id":"2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding","title":"[논문리뷰] 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding","excerpt":"Hao Tang이 arXiv에 게시한 '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding","tags":["Review","3D Vision-Language Models","Reasoning","Scene Understanding","Reinforcement Learning","Chain-of-Thought","Dynamic View Selection","Multi-task Learning"],"text":"링크: 논문 PDF로 바로 열기 3DR1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding 저자: Ting Huang, Zeyu Zhang, Hao Tang 키워드: , , , , , , 핵심 연구 목표 본 논문은 기존 3D VisionLanguage Models (VLMs)이 복잡한 공간 관"},{"id":"2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models","title":"[논문리뷰] Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models","excerpt":"Jiaqi Wang이 arXiv에 게시한 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models","tags":["Review","Diffusion Large Language Models","Variable-Length Generation","Dynamic Length Adaptation","Denoising Strategy","Inference Optimization","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 Beyond Fixed: VariableLength Denoising for Diffusion Large Language Models 저자: Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin 키워드: , , , , , 핵심 연구 목표 Diffusi"},{"id":"2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation","title":"[논문리뷰] IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation","excerpt":"Jianjiang Feng이 arXiv에 게시한 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation","tags":["Review","Image-goal Navigation","3D Gaussian Splatting (3DGS)","Incremental Scene Representation","Coarse-to-fine Localization","Embodied AI","Robotics","Differentiable Rendering"],"text":"링크: 논문 PDF로 바로 열기 IGLNav: Incremental 3D Gaussian Localization for Imagegoal Navigation 저자: Wenxuan Guo, Xiuwei Xu, Hang Yin, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu 키워드: , , , , , , 핵심 연구 목표 본"},{"id":"2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages","title":"[논문리뷰] Investigating Hallucination in Conversations for Low Resource Languages","excerpt":"Fatemeh Jamshidi이 arXiv에 게시한 'Investigating Hallucination in Conversations for Low Resource Languages' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages","tags":["Review","LLM Hallucination","Low-resource Languages","Conversational AI","ROUGE Score","Cross-lingual Evaluation","Factual Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Amit Das, Md. Najib Hasan, Souvika Sarkar, Zheng Zhang, Fatemeh Jamshidi, Tathagata Bhattacharya, Nilanjana Raychawdhury, Dongji Feng, Vinija Jain, Aman Chadha 키워드: , , , , , 핵심 "},{"id":"2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges","title":"[논문리뷰] Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges","excerpt":"Chengfei Lv이 arXiv에 게시한 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges","tags":["Review","Multi-Turn Dialogue Evaluation","LLM-as-a-Judge","Multi-Judge Aggregation","Preference Learning","Dialogue Quality Assessment","Maximum Likelihood Estimation","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding 키워드: , , , , , , 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM) 기반의 대화 평가에서 현재 \"LLMasajudge\" 패러다임이 "},{"id":"2025-8-4-Multimodal-Referring-Segmentation-A-Survey","title":"[논문리뷰] Multimodal Referring Segmentation: A Survey","excerpt":"Zuxuan Wu이 arXiv에 게시한 'Multimodal Referring Segmentation: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-Multimodal-Referring-Segmentation-A-Survey","tags":["Review","Multimodal Learning","Referring Segmentation","Vision-Language Models","Image Segmentation","Video Segmentation","3D Vision","Survey"],"text":"링크: 논문 PDF로 바로 열기 Multimodal Referring Segmentation: A Survey 저자: Henghui Ding, Song Tang, Shuting He, Chang Liu, Zuxuan Wu, YuGang Jiang 키워드: , , , , , , 핵심 연구 목표 이 논문은 이미지, 비디오, 3D 장면과 같은 다양한 시각적 맥락"},{"id":"2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion","title":"[논문리뷰] PixNerd: Pixel Neural Field Diffusion","excerpt":"Limin Wang이 arXiv에 게시한 'PixNerd: Pixel Neural Field Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion","tags":["Review","Diffusion Models","Neural Fields","Pixel Space","Generative Models","Image Synthesis","Transformer Architecture","End-to-End Learning"],"text":"링크: 논문 PDF로 바로 열기 PixNerd: Pixel Neural Field Diffusion 저자: Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, Limin Wang 키워드: , , , , , , 핵심 연구 목표 이 논문은 Variational Autoencoder (VAE) 기반의 기존 확산 모델이 야기"},{"id":"2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution","title":"[논문리뷰] SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution","excerpt":"Heng Lian이 arXiv에 게시한 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution","tags":["Review","Multi-Agent System","Software Engineering","Fault Localization","Issue Resolution","Large Language Models","Competitive Debate","Graph Traversal"],"text":"링크: 논문 PDF로 바로 열기 저자: Heng Lian, Xiaodong Gu, Shaoxin Lin, Yuling Shi, Han Li 키워드: , , , , , , 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 소프트웨어 이슈 해결 시스템의 '제한된 관찰 범위(limited observation scope)' 문제를 해결하고자 합니다. 특"},{"id":"2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution","title":"[논문리뷰] SWE-Exp: Experience-Driven Software Issue Resolution","excerpt":"Heng Lian이 arXiv에 게시한 'SWE-Exp: Experience-Driven Software Issue Resolution' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution","tags":["Review","Software Issue Resolution","LLM Agents","Experience-Driven Learning","Automated Program Repair","Multi-Agent Systems","Knowledge Management","Continuous Learning"],"text":"링크: 논문 PDF로 바로 열기 SWEExp: ExperienceDriven Software Issue Resolution 저자: Silin Chen, Yuling Shi, Dong Chen, Shaoxin Lin, Heng Lian, Weiguo Sun, Qianxiang Wang, Xiaodong Gu, Longfei Yun, Lin Cao 키워드: ,"},{"id":"2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation","title":"[논문리뷰] SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation","excerpt":"Long Chen이 arXiv에 게시한 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-04 12:17:01+0900","permalink":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation","tags":["Review","Audio-driven Video Generation","Spatial Auditory Cues","Video Scene Layout","MLLM","Diffusion Models","Training-free"],"text":"링크: 논문 PDF로 바로 열기 저자: Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, Long Chen 키워드: , , , , , 핵심 연구 목표 본 논문은 기존 오디오 기반 비디오 생성 모델들이 주로 시맨틱 정보에만 초점을 맞춰 공간적 일관성이 부족하다는 한계를 지적합니다. 이에, 인간이 소리에서 위치 및 "},{"id":"2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models","title":"[논문리뷰] A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models","excerpt":"Zuxuan Wu이 arXiv에 게시한 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models","tags":["Review","Large Vision-Language Models (LVLMs)","Visual Token Pruning","Dynamic Compression","GlimpsePrune","Computational Efficiency","VQA","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 A Glimpse to Compress: Dynamic Visual Token Pruning for Large VisionLanguage Models 저자: QuanSheng Zeng, Yunheng Li, Qilong Wang, PengTao Jiang, Zuxuan Wu, MingMing Cheng, Qibin Hou 핵"},{"id":"2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks","title":"[논문리뷰] AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks","excerpt":"Zhiwei Zhang이 arXiv에 게시한 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks","tags":["Review","Large Language Models","LLM Agents","Test-time Scaling","Compute Optimization","Multi-stage Tasks","Resource Allocation","Search Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Fali Wang, Hui Liu, Zhenwei Dai, Jingying Zeng, Zhiwei Zhang, Zongyu Wu, Chen Luo, Zhen Li, Xianfeng Tang, Qi He, Suhang Wang 핵심 연구 목표 본 논문은 기존 연구가 주로 단일 단계 태스크에 집중했던 것과 달리, 다단계 "},{"id":"2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following","title":"[논문리뷰] Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following","excerpt":"Jiaqing Liang이 arXiv에 게시한 'Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following","tags":["Review","Self-Supervised RL","Instruction Following","Reasoning Models","Large Language Models","Reward Modeling","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 Beyond the Tradeoff: SelfSupervised Reinforcement Learning for Reasoning Models' Instruction Following 저자: Jiaqing Liang, Jie Zeng, Bowei Zhang, Qianyu He, Qingyu Ren 핵심 연구 목표 본 논문은 "},{"id":"2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models","title":"[논문리뷰] CellForge: Agentic Design of Virtual Cell Models","excerpt":"Daniel Shao이 arXiv에 게시한 'CellForge: Agentic Design of Virtual Cell Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models","tags":["Review","AI Scientist","Multi-Agent System","Virtual Cell Modeling","Single-Cell Perturbation Prediction","Deep Learning","Automated Model Design","Code Generation","Retrieval-Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Daniel Shao, Yan Cui, Jiapeng Chen, Zhuoyun Yu, Xiangru Tang 핵심 연구 목표 본 논문은 복잡한 생물학적 시스템, 이질적인 데이터 양식, 그리고 다학제적 전문 지식의 필요성으로 인해 어려움을 겪는 가상 세포 모델의 자율적인 구축 문제를 해결하고자 합니다. 주어진 생물학적 "},{"id":"2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime","title":"[논문리뷰] Cyber-Zero: Training Cybersecurity Agents without Runtime","excerpt":"Zijian Wang이 arXiv에 게시한 'Cyber-Zero: Training Cybersecurity Agents without Runtime' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime","tags":["Review","Cybersecurity Agents","LLM Training","Trajectory Synthesis","Runtime-Free Training","CTF Challenges","LLM Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, Zijian Wang 핵심 연구 목표 기존 대규모 언어 모델(LLM) 기반 소프트웨어 엔지니어링 에이전트들이 실행 환경을 통해 학습하지만, 사이버 보안 도메인에서는 이러한 실행 환경이 부족하여 고급 훈련 데이터 확보가"},{"id":"2025-8-5-Exploitation-Is-All-You-Need-for-Exploration","title":"[논문리뷰] Exploitation Is All You Need... for Exploration","excerpt":"Jesse Roberts이 arXiv에 게시한 'Exploitation Is All You Need... for Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-Exploitation-Is-All-You-Need-for-Exploration","tags":["Review","Reinforcement Learning","Exploration-Exploitation","Meta-RL","Transformer Architecture","Emergent Behavior","Multi-Armed Bandits","Gridworlds","Pseudo-Thompson Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Micah Rentschler, Jesse Roberts 핵심 연구 목표 본 논문은 기존 RL에서 탐색을 위해 명시적인 인센티브를 부여하는 방식과 달리, 순수한 탐욕적인(exploitationonly) 목적 만으로도 탐색적 행동이 자연스럽게 나타날 수 있는지 검증하는 것을 목표로 합니다. 이를 위해 반복적인 환경 구조"},{"id":"2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation","title":"[논문리뷰] InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation","excerpt":"Yang Tian이 arXiv에 게시한 'InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation","tags":["Review","Vision-Language-Action (VLA)","Instruction Tuning","Multimodal Reasoning","Robotic Manipulation","Catastrophic Forgetting","Mixture-of-Experts (MoE)","Flow Matching"],"text":"링크: 논문 PDF로 바로 열기 InstructVLA: VisionLanguageAction Instruction Tuning: From Understanding to Manipulation 저자: Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Y"},{"id":"2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report","title":"[논문리뷰] Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report","excerpt":"Anu Vellore이 arXiv에 게시한 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report","tags":["Review","Large Language Model","Cybersecurity","Instruction Tuning","Direct Preference Optimization","Cyber Threat Intelligence","Foundation Model","Chatbot"],"text":"링크: 논문 PDF로 바로 열기 저자: Anu Vellore, Baturay Saglam, Blaine Nelson, Paul Kassianik, Sajana Weerawardhena 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 사이버 보안 분야 통합이 데이터 부족, 복잡한 표현, 안전 및 규제 문제로 인해 제한적이라는 문제를 해결하고자 합니다."},{"id":"2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models","title":"[논문리뷰] Personalized Safety Alignment for Text-to-Image Diffusion Models","excerpt":"Kaidong Yu이 arXiv에 게시한 'Personalized Safety Alignment for Text-to-Image Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models","tags":["Review","Personalized Safety Alignment","Text-to-Image Diffusion Models","DPO","User Preferences","Content Moderation","Generative AI","Cross-Attention","Safety Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Lei, Jinbin Bai, Qingyu Shi, Aosong Feng, Kaidong Yu 핵심 연구 목표 현재 텍스트투이미지(T2I) 확산 모델의 안전 메커니즘이 사용자의 다양한 연령, 정신 건강, 개인 신념 등의 선호도를 고려하지 않고 일률적인 기준을 적용하여 발생하는 한계를 해결하고자 합니다. 본 연구는"},{"id":"2025-8-5-Qwen-Image-Technical-Report","title":"[논문리뷰] Qwen-Image Technical Report","excerpt":"Kaiyuan Gao이 arXiv에 게시한 'Qwen-Image Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-Qwen-Image-Technical-Report","tags":["Review","Image Generation","Text-to-Image","Image Editing","Text Rendering","Multimodal Diffusion Transformer","Curriculum Learning","Reinforcement Learning","Foundation Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiyuan Gao, Junyang Lin, Jingren Zhou, Jiahao Li, Chenfei Wu 핵심 연구 목표 본 논문은 복잡한 텍스트 렌더링 및 정밀한 이미지 편집 분야에서 기존 텍스트이미지(T2I) 모델의 한계를 해결하는 것을 목표로 합니다. 특히, 다중 줄 레이아웃, 비알파벳 언어(예: 중국어) "},{"id":"2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems","title":"[논문리뷰] RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems","excerpt":"Junkun Hong이 arXiv에 게시한 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems","tags":["Review","Brain-inspired AI","Lifelong Learning","Embodied AI","Multi-memory Systems","Knowledge Graph","Robotics","Closed-Loop Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingcong Lei, Honghao Cai, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, et al. 핵심 연구 목표 이 논문은 물리적 환경에 배치된 로봇 에이전트의 평생 학습(Lifelong Learning) 및 장기 계획(Longterm Planning) 을 위한 뇌"},{"id":"2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension","title":"[논문리뷰] SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension","excerpt":"Liyan Xu이 arXiv에 게시한 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension","tags":["Review","Dense Retrieval","Context-Aware Embedding","RAG","Long Document Comprehension","Residual Learning","Semantic Association","Text Embedding"],"text":"링크: 논문 PDF로 바로 열기 저자: Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, DitYan Yeung, Jie Zhou, Mo Yu 핵심 연구 목표 본 논문은 장문 문서에 대한 RAG(RetrievalAugmented Generation) 시스템에서 기존 임베딩 모델의 한계를 극"},{"id":"2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo","title":"[논문리뷰] VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo","excerpt":"Bin Jia이 arXiv에 게시한 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-05 11:40:52+0900","permalink":"/ai/review/2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo","tags":["Review","Omni-modal LLMs","Distributed Training","Model-centric","Parallelism","FSDP","Sequence Parallelism","Expert Parallelism","Mixture-of-Experts"],"text":"링크: 논문 PDF로 바로 열기 VeOmni: Scaling Any Modality Model Training with ModelCentric Distributed Recipe Zoo 저자: Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie L"},{"id":"2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization","title":"[논문리뷰] AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization","excerpt":"Aman Chadha이 arXiv에 게시한 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization","tags":["Review","Alignment Preservation","Fine-Tuning","LoRA","Fisher Information Matrix","Catastrophic Forgetting","LLM Safety","Riemannian Geometry","Parameter-Efficient Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Amitava Das, Abhilekh Borah, Vinija Jain, Aman Chadha 핵심 연구 목표 대규모 언어 모델(LLM)의 LoRA 미세 조정 과정에서 발생하는 정렬 드리프트(alignment drift) 문제를 해결하여, 안전 및 행동 제약을 유지하면서도 새로운 태스크에 대한 성능 저하를 방지하는 "},{"id":"2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search","title":"[논문리뷰] CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search","excerpt":"Jiwei Li이 arXiv에 게시한 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search","tags":["Review","Approximate Nearest Neighbor Search","Reinforcement Learning","Large Language Models","Code Optimization","HNSW","Retrieval-Augmented Generation","Contrastive Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li 핵심 연구 목표 논문은 ANNS(Approximate Nearest Neighbor Search) 알고리즘 최적화의 수작업적, 전문 지식 의존적 특성을 해결하는 것을 목표로 합니다. LLM을 강화 학습으로 증강하여 "},{"id":"2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning","title":"[논문리뷰] ChartCap: Mitigating Hallucination of Dense Chart Captioning","excerpt":"Gunhee Kim이 arXiv에 게시한 'ChartCap: Mitigating Hallucination of Dense Chart Captioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning","tags":["Review","Chart Captioning","Hallucination Mitigation","Dataset Generation","Visual Language Models","Cycle Consistency","Reference-Free Metric","Data Visualization"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyoung Lim, Jaewoo Ahn, Gunhee Kim 핵심 연구 목표 본 논문은 시각 언어 모델(VLMs)이 생성하는 차트 캡션의 환각 현상(hallucination)을 줄이고 정보의 정확성 및 밀도를 높이는 것 을 목표로 합니다. 기존 데이터셋의 외부 정보 포함 및 차트 유형별 핵심 정보 부족 문제를 해"},{"id":"2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward","title":"[논문리뷰] CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward","excerpt":"Songyang Gao이 arXiv에 게시한 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward","tags":["Review","LLM Evaluation","Answer Verification","Reward Model","Benchmarking","Data Augmentation","Reinforcement Learning","Formula Verification","Hallucination Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen 핵심 연구 목표 현재 대규모 언어 모델(LLM)의 답변 검증 방"},{"id":"2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction","title":"[논문리뷰] Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction","excerpt":"Jui-Hui Chung이 arXiv에 게시한 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction","tags":["Review","Automated Theorem Proving","Formal Verification","Language Models","Self-Correction","Data Synthesis","Reinforcement Learning","Model Averaging","Lean"],"text":"링크: 논문 PDF로 바로 열기 저자: Yong Lin, Shange Tang, Bohan Lyu, Ziran Yang, JuiHui Chung, Haoyu Zhao, Lai Jiang, Yihan Geng, Jiawei Ge, Jingruo Sun, Jiayun Wu, Jiri Gesi, Ximing Lu, David Acuna, Kaiyu Yang, H"},{"id":"2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer","title":"[논문리뷰] LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer","excerpt":"Shunyu Yao이 arXiv에 게시한 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer","tags":["Review","Multi-Image Composition","Layout Control","Diffusion Models","Transformer","Attention Mechanisms","Training-Free","Zero-Shot Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuzhuo Chen, Zehua Ma, Jianhua Wang, Kai Kang, Shunyu Yao, Weiming Zhang 핵심 연구 목표 본 논문은 여러 시각적 레퍼런스와 공간적 레이아웃 정보를 활용하여 일관되고 응집력 있는 이미지를 생성하는 것을 목표로 합니다. 특히, 기존 단일 레퍼런스 확산 모델을 훈련 "},{"id":"2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools","title":"[논문리뷰] LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?","excerpt":"Yaojie Lu이 arXiv에 게시한 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools","tags":["Review","LLM Agent","Tool-use","MCP","Benchmark","Large-scale","Real-world tasks","Automated Evaluation","Meta-tool-learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mo Guozhao, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun 핵심 연구 목표 본 논문은 기존 도구 사용 벤치마크가 시뮬레이션되거나 소규모의 MCP(Model Context Protocol) "},{"id":"2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation","title":"[논문리뷰] LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation","excerpt":"Chenyang Si이 arXiv에 게시한 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation","tags":["Review","Ultra-long Video Generation","Multimodal Guidance","Controllable Video Generation","Diffusion Models","Temporal Consistency","Visual Quality","Autoregressive Generation","Degradation-aware Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenyang Si, Jianfeng Feng, Xian Liu, Zhaoxi Chen, Jianxiong Gao, Yanwei Fu, Yu Qiao, Ziwei Liu 핵심 연구 목표 본 논문은 기존 비디오 생성 모델이 짧은 클립에는 효과적이지만, 시간적 불일치(temporal inconsistency) 와 시각적"},{"id":"2025-8-6-Multi-human-Interactive-Talking-Dataset","title":"[논문리뷰] Multi-human Interactive Talking Dataset","excerpt":"Mike Zheng Shou이 arXiv에 게시한 'Multi-human Interactive Talking Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset","tags":["Review","Multi-human Video Generation","Interactive Talking","Dataset","Audio-driven Animation","Pose Control","Speech Interaction","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyu Zhu, Weijia Wu, Mike Zheng Shou 핵심 연구 목표 기존 단일 화자 또는 얼굴 기반의 오디오구동 비디오 생성 모델의 한계를 극복하고, 다중 인간 상호작용 을 현실적으로 모델링하는 새로운 과제인 다중 인간 대화 비디오 생성(MultiHuman Talking Video Generation) "},{"id":"2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference","title":"[논문리뷰] Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference","excerpt":"Fan Xia이 arXiv에 게시한 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference","tags":["Review","Diffusion Models","Language Models","Code Generation","Non-Autoregressive Inference","High-Speed Inference","Discrete Diffusion","LLM Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Fan Xia, Pengyang Gao, Cheng Luo, Zheng Zhang, Yuxuan Song 핵심 연구 목표 본 논문은 이산 상태 확산 모델(discretestate diffusion models)의 고질적인 문제인 토큰순서 모델링의 유도 편향 과 추론 비효율성 을 해결하여, 코드 생성 대규모 언어 모델("},{"id":"2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation","title":"[논문리뷰] Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation","excerpt":"Tianyidan Xie이 arXiv에 게시한 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation","tags":["Review","Autoregressive Models","Multimodal AI","Image Generation","Image Editing","Visual Understanding","Unified Architecture","Parameter Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Peiyu Wang, Yi Peng, Yimeng Gan, Liang Hu, Eric Li, Xuchen Song, Tianyidan Xie, Xiaokun Wang, Yichen Wei, Chuanxin Tang, Bo Zhu, Changshi Li, Hongyang Wei, Yang Liu, Yahui Zhou 핵"},{"id":"2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs","title":"[논문리뷰] TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs","excerpt":"Aman Chadha이 arXiv에 게시한 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs","tags":["Review","LLM Alignment","Alignment Drift","Training Data Provenance","Belief Conflict Index (BCI)","Suffix Array","Safety Interventions","Reinforcement Learning from Human Feedback","Explainable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Amitava Das, Vinija Jain, Aman Chadha 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)이 왜 안전하지 않거나 정책을 위반하는 출력을 생성하는 '정렬 드리프트(alignment drift)'를 겪는지에 대한 근본적인 원인을 밝히는 것을 목표로 합니다. 단순히 행동적인 실패를 넘어서, 이"},{"id":"2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search","title":"[논문리뷰] Tool-integrated Reinforcement Learning for Repo Deep Search","excerpt":"Yanzhen Zou이 arXiv에 게시한 'Tool-integrated Reinforcement Learning for Repo Deep Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-06 13:46:36+0900","permalink":"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search","tags":["Review","Issue Localization","Large Language Models (LLMs)","Reinforcement Learning (RL)","Supervised Fine-tuning (SFT)","Tool-integrated Agents","Software Engineering","Code Search"],"text":"링크: 논문 PDF로 바로 열기 저자: Zexiong Ma, Chao Peng, Qunhong Zeng, Pengfei Gao, Yanzhen Zou, Bing Xie 핵심 연구 목표 소프트웨어 이슈 설명과 실제 결함 코드 사이의 의미론적 간극 및 다중 홉 추론 으로 인해 발생하는 이슈 로컬라이제이션(결함 코드 위치 식별)의 어려움을 해결하는 것이 목표입니"},{"id":"2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding","title":"[논문리뷰] A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding","excerpt":"Jianke Zhu이 arXiv에 게시한 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding","tags":["Review","3D Occupancy Grounding","Multi-modal Learning","Natural Language Understanding","Autonomous Driving","Voxel-based Prediction","Benchmark Dataset","Coarse-to-Fine"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu 핵심 연구 목표 논문은 기존 바운딩 박스 기반 시각 그라운딩의 한계를 극복하고, 자율주행 환경에서 자연어 설명을 기반으로 객체의 정확한 3D 점유(occupancy) 정보 를 파악하는 것을 목표로 합니다. 이를 위해 voxellevel 공간"},{"id":"2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning","title":"[논문리뷰] Agent Lightning: Train ANY AI Agents with Reinforcement Learning","excerpt":"Zilong Wang이 arXiv에 게시한 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Large Language Models","AI Agents","Framework","Markov Decision Process","Hierarchical RL","Training-Agent Disaggregation","Observability"],"text":"링크: 논문 PDF로 바로 열기 저자: Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, Yuqing Yang 핵심 연구 목표 본 논문은 기존 RL(강화 학습) 기반 LLM(대규모 언어 모델) 훈련 방법론들이 에이전트 설계와 밀접하게 결합되어 유연성이"},{"id":"2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor","title":"[논문리뷰] C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor","excerpt":"Jinbao Wang이 arXiv에 게시한 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor","tags":["Review","3D Anomaly Detection","Continual Learning","Kernel Attention","Learnable Advisor","Parameter Perturbation","Point Cloud","Industrial AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoquan Lu, Hanzhe Liang, Jie Zhang, Chenxi Hu, Jinbao Wang, Can Gao 핵심 연구 목표 본 연구는 3D 이상 감지(Anomaly Detection, AD)에서 기존 클래스특정 모델 의 한계를 극복하고, 새로운 객체 범주가 지속적으로 발생하는 실제 환경에 적응할 수 있"},{"id":"2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction","title":"[논문리뷰] CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction","excerpt":"Donghyeon Lee이 arXiv에 게시한 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction","tags":["Review","Toxicity Prediction","Large Language Model","Chain-of-Thought","Drug Development","Cheminformatics","Interpretable AI","IUPAC Nomenclature"],"text":"링크: 논문 PDF로 바로 열기 저자: Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang 핵심 연구 목표 기존 AI/ML 독성 예측 모델의 한계(데이터 의존성, 해석 불가능성)와 LLM 기반 접근법의 문제점(SMILES 이해 부족, 생물학적 맥락 "},{"id":"2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework","title":"[논문리뷰] DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework","excerpt":"Chao Liang이 arXiv에 게시한 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework","tags":["Review","Video Virtual Try-On","Diffusion Transformers","Stage-Wise Framework","Vision-Language Models","LoRA","Temporal Consistency","Garment Preservation"],"text":"링크: 논문 PDF로 바로 열기 저자: Tongchun Zuo, Zaiyu Huang, Shuliang Ning, Ente Lin, Chao Liang, Zerong Zheng, Jianwen Jiang, Yuan Zhang, Mingyuan Gao, Xin Dong 핵심 연구 목표 기존 비디오 가상 피팅(VVT) 기술의 한계, 즉 데이터 부족, 디테일 보"},{"id":"2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation","title":"[논문리뷰] EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation","excerpt":"Dong Chen이 arXiv에 게시한 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation","tags":["Review","C-to-Rust Conversion","Project-Level Translation","Large Language Models","Code Synthesis","Memory Safety","Software Migration","Hybrid Translation"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaofan Wang, Tingrui Yu, Jie Wang, Dong Chen, Wenrui Zhang, Yuling Shi, Xiaodong Gu, Beijun Shen 핵심 연구 목표 레거시 C 코드베이스를 Rust로 자동 변환할 때 발생하는 언어적 불일치(안전성, 관용성) 및 프로젝트 레벨의 모듈 간 종속성 "},{"id":"2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost","title":"[논문리뷰] Efficient Agents: Building Effective Agents While Reducing Cost","excerpt":"Yue Hou이 arXiv에 게시한 'Efficient Agents: Building Effective Agents While Reducing Cost' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost","tags":["Review","LLM Agents","Cost Efficiency","Performance-Cost Trade-off","Agent Frameworks","GAIA Benchmark","Optimization","Resource Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Yue Hou, He Zhu, Pai Liu, Xavier Hu, Ningning Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트 시스템의 확장성과 접근성을 위협하는 급증하는 비용 문제 를 해결하고자 합니다. 특히 현대 에이전트 시스템에서 효율성효과성 트레이드오프 에 대한 최초의 체계적인 "},{"id":"2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success","title":"[논문리뷰] Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success","excerpt":"Ruslan Rakhimov이 arXiv에 게시한 'Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success","tags":["Review","Reinforcement Learning","Vision-Language Models","Synthetic Worlds","Transfer Learning","PPO","Actor-Critic","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: George Bredis, Stanislav Dereka, Viacheslav Sinii, Ruslan Rakhimov, Daniil Gavrilov 핵심 연구 목표 본 논문은 대규모 시각언어 모델(VLM)이 다단계의 상호작용적 에이전트 태스크에서 직면하는 어려움을 해결하고, 특히 훈련 환경을 넘어 실세계 벤치마크로 "},{"id":"2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis","title":"[논문리뷰] Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis","excerpt":"Feng Zhao이 arXiv에 게시한 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis","tags":["Review","4D Generation","Video-to-3D Synthesis","Gaussian Splatting","Diffusion Models","Latent Space Modeling","Variational Autoencoder","Temporal Coherence"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, Baining Guo 핵심 연구 목표 본 논문은 단일 비디오 입력으로부터 고품질의 동적인 3D 콘텐츠(4D)를 생성하는 문제를 해결하고자 합니다. 특히, 기존 4D 확산 모델링의 주요 "},{"id":"2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score","title":"[논문리뷰] HPSv3: Towards Wide-Spectrum Human Preference Score","excerpt":"Hongsheng Li이 arXiv에 게시한 'HPSv3: Towards Wide-Spectrum Human Preference Score' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score","tags":["Review","Human Preference Score","Text-to-Image Generation","Image Evaluation","Vision-Language Models (VLMs)","Uncertainty-Aware Ranking Loss","Dataset","Iterative Refinement","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhang Ma, Xiaoshi Wu, Keqiang Sun, Hongsheng Li 핵심 연구 목표 본 논문은 기존 텍스트이미지 생성 모델 평가를 위한 인간 중심 지표들이 제한적인 데이터 커버리지 , 불완전한 특징 추출 , 비효율적인 손실 함수 로 인해 인간의 선호도와 충분히 정렬되지 못하는 문제를 해결하는 것을 "},{"id":"2025-8-7-IAUNet-Instance-Aware-U-Net","title":"[논문리뷰] IAUNet: Instance-Aware U-Net","excerpt":"Dmytro Fishman이 arXiv에 게시한 'IAUNet: Instance-Aware U-Net' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-IAUNet-Instance-Aware-U-Net","tags":["Review","Instance Segmentation","U-Net","Query-based Model","Transformer Decoder","Biomedical Imaging","Cell Segmentation","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman 핵심 연구 목표 본 논문은 생의학 이미징 분야에서 널리 사용되는 UNet 아키텍처와 인스턴스 분할 태스크 간의 격차를 해소하는 것을 목표로 합니다. 특히, 기존 쿼리 기반 모델이 단일 스케일 특징에 의존"},{"id":"2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards","title":"[논문리뷰] IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards","excerpt":"Ling-I Wu이 arXiv에 게시한 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards","tags":["Review","Instruction Following","Reinforcement Learning","Reward Hacking","LLMs","Curriculum Learning","Data Flywheel","Verifiable Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Xu Guo, Tianyi Liang, Tong Jian, Xiaogui Yang, LingI Wu, Chenhui Li, Zhihui Lu, Qipeng Guo, Kai Chen 핵심 연구 목표 본 논문은 LLM의 지시 따르기 능력을 향상시키는 Verifiable Rewards 기반 강화 학습(RLVR) 이 겪는 두"},{"id":"2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens","title":"[논문리뷰] Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens","excerpt":"Zhen Tan이 arXiv에 게시한 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens","tags":["Review","Chain-of-Thought","LLMs","OOD Generalization","Data Distribution Shift","Reasoning","Pattern Matching","DataAlchemy"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengshuai Zhao, Zhen Tan, Pingchuan Ma, Dawei Li, Bohan Jiang, Yancheng Wang, Yingzhen Yang, Huan Liu 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 ChainofThought (CoT) 추론 이 진정한 논리적 추론이 아닌, 훈련"},{"id":"2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought","title":"[논문리뷰] LaTCoder: Converting Webpage Design to Code with Layout-as-Thought","excerpt":"Tianpeng Lv이 arXiv에 게시한 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought","tags":["Review","Design-to-Code","Webpage Generation","Multimodal Large Language Models (MLLMs)","Layout Preservation","Chain-of-Thought (CoT)","UI Automation","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Gui, Zhen Li, Zhongyi Zhang, Guohao Wang, Tianpeng Lv, Gaoyang Jiang, Yi Liu, Dongping Chen, Yao Wan, Hongyu Zhang, Wenbin Jiang, Xuanhua Shi, Hai Jin 핵심 연구 목표 본 연구는 멀티모달 대규모 "},{"id":"2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding","title":"[논문리뷰] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding","excerpt":"Yuqing Yang이 arXiv에 게시한 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding","tags":["Review","LLM","KV Cache Optimization","Model Pruning","Efficient Decoding","Memory Optimization","Static Sparsity","Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu 핵심 연구 목표 대규모 언어 모델(LLMs)에서 증가하는 KeyValue(KV) 캐시 크기로 인한 GPU 메모리 사용량 증가와 느린 추론 속도 문"},{"id":"2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following","title":"[논문리뷰] Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following","excerpt":"Liang Xu이 arXiv에 게시한 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following","tags":["Review","LLMs","Instruction Following","Reasoning","Reinforcement Learning","Supervised Fine-tuning","Entropy Regularization","Self-Checking","Previewing"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenyang Wang, Liang Wen, Shousheng Jia, Xiangzheng Zhang, Liang Xu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 복잡한 지시를 따를 때 흔히 발생하는 \"게으른 추론\" 문제로 인한 일관성 부족을 해결하고자 합니다. 특히, 모델이 엄격한 지시 사항을 준수하"},{"id":"2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions","title":"[논문리뷰] MiDashengLM: Efficient Audio Understanding with General Audio Captions","excerpt":"Yadong Niu이 arXiv에 게시한 'MiDashengLM: Efficient Audio Understanding with General Audio Captions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions","tags":["Review","Audio-Language Model","General Audio Captions","Audio Understanding","Speech Recognition","Efficient Inference","Public Datasets","Multimodality","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yadong Niu, Jian Luan, Jizhong Liu, Gang Li, Heinrich Dinkel 핵심 연구 목표 본 논문은 기존 대규모 오디오 언어 모델(LALM)이 직면한 폐쇄형 데이터 의존성, 일반화 및 접근성 한계, 그리고 자동 음성 인식(ASR) 기반 사전 훈련의 비효율성을 해결하고자 합니다. 이를"},{"id":"2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets","title":"[논문리뷰] OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets","excerpt":"MaziyarPanahi이 arXiv에 게시한 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets","tags":["Review","Biomedical NER","Transformer","Domain Adaptation","LoRA","Open-Source","Named Entity Recognition","Healthcare AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Maziyar Panahi 핵심 연구 목표 의료 및 생명 과학 분야에서 비정형 텍스트로부터 구조화된 정보를 추출하는 데 필수적인 Named Entity Recognition (NER) 의 성능과 효율성을 개선하는 것을 목표로 합니다. 특히, 다양한 엔티티 유형에 걸쳐 최첨단(SOTA) 성능 을 달성하면서도 연산 효율성"},{"id":"2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference","title":"[논문리뷰] Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference","excerpt":"Jiaying Wu이 arXiv에 게시한 'Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference","tags":["Review","AI Conferences","Sustainability","Peer Review","Community Building","Environmental Impact","Mental Health","Centralized Model","Decentralized Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Nuo Chen, Moming Duan, Andre Huikai Lin, Qian Wang, Jiaying Wu, Bingsheng He 핵심 연구 목표 본 논문은 현재 AI 학술 대회의 중앙 집중식 모델 이 급격한 성장으로 인해 비정상적이고 지속 불가능한 상태 에 도달했음을 진단합니다. 과학적 지식 확산, 형평성, "},{"id":"2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","title":"[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization","excerpt":"Kechi Zhang이 arXiv에 게시한 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","tags":["Review","Large Language Models","Reinforcement Learning","Capability Collapse","Hybrid Policy Optimization","Multiple Importance Sampling","Exploration","Math Reasoning","Out-of-Distribution"],"text":"링크: 논문 PDF로 바로 열기 저자: Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, Zhi Jin, Fei Huang, Yongbin Li, Ge Li 핵심 연구 목표 본 논문은 LLM 의 강"},{"id":"2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks","title":"[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks","excerpt":"Haozhe Zhang이 arXiv에 게시한 'Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks","tags":["Review","Root Cause Analysis","Large Language Models","5G Wireless Networks","Supervised Fine-Tuning","Reinforcement Learning","Chain-of-Thought","TeleLogs Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohamed Sanat, Nicola Piovesan, Antonio De Domenico, Yibin Kang, Haozhe Zhang, Merouane Debbah, Fadhel Ayed 핵심 연구 목표 본 논문은 5G 모바일 네트워크에서 해석 가능성, 도메인 전문성, 인과적 추론이 필요한 루트 원인 분석(RCA"},{"id":"2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience","title":"[논문리뷰] SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience","excerpt":"Xiaoyi Dong이 arXiv에 게시한 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience","tags":["Review","Computer Use Agent","Self-Evolving","Reinforcement Learning","Curriculum Learning","Vision-Language Models","Experiential Learning","Specialist-to-Generalist"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, Jiaqi Wang 핵심 연구 목표 본 논문은 기존 컴퓨터 사용 에이전트(CUA)가 인간 주석 데이터에 크게 의존하고 새로운 또는 전문화된 소프트웨어 환경에서 어려움을 겪는 문제를"},{"id":"2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management","title":"[논문리뷰] Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management","excerpt":"Yunxin Liu이 arXiv에 게시한 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management","tags":["Review","Large Language Models","Active Context Management","Proactive Interference","Tool Augmentation","Working Memory","Context Curation","Long Context"],"text":"링크: 논문 PDF로 바로 열기 저자: Mo Li, L.H. Xu, Qitai Tan, Ting Cao, Yunxin Liu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 긴 컨텍스트를 처리할 때 발생하는 사전 간섭(proactive interference) 문제와 이로 인한 성능 저하를 해결하고자 합니다. 기존 컨텍스트 확장이나 외부 메모리 시"},{"id":"2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation","title":"[논문리뷰] Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation","excerpt":"Hao Huang이 arXiv에 게시한 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation","tags":["Review","Text-to-3D Generation","Prompt Engineering","Visual Analytics","Human-Computer Interaction","Multi-modal Large Language Models","3D Model Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Nan Xiang, Tianyi Liang, Haiwen Huang, Shiqi Jiang, Hao Huang, Yifei Huang, Liangyu Chen, Changbo Wang, and Chenhui Li 핵심 연구 목표 텍스트3D(T23D) 생성 과정에서 발생하는 '블라인드 시행착오' 프롬프트 문제와 그로 인"},{"id":"2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering","title":"[논문리뷰] SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering","excerpt":"Ambuj Mehrish이 arXiv에 게시한 'SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering","tags":["Review","Music Restoration","Audio Mastering","Generative Models","Flow Matching","Text-to-Audio","Audio Quality Enhancement","Multi-task Learning","Dataset Creation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jan Melechovsky, Ambuj Mehrish, Dorien Herremans 핵심 연구 목표 본 논문은 과도한 잔향, 왜곡, 클리핑, 음색 불균형 등 다양한 오디오 품질 문제를 해결하는 통합적이고 텍스트 제어 가능한 음악 복원 및 마스터링 모델 을 개발하는 것을 목표로 합니다. 기존의 개별적인 전문 도구를 "},{"id":"2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence","title":"[논문리뷰] Sotopia-RL: Reward Design for Social Intelligence","excerpt":"Keyang Xuan이 arXiv에 게시한 'Sotopia-RL: Reward Design for Social Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence","tags":["Review","Social Intelligence","Reinforcement Learning","Reward Design","Large Language Models","Utterance-level Rewards","Multi-dimensional Rewards","Partial Observability","SOTOPIA"],"text":"링크: 논문 PDF로 바로 열기 저자: Haofei Yu, Zhengyang Qi, Yining Zhao, Kolby Nottingham, Keyang Xuan, Bodhisattwa Prasad Majumder, Hao Zhu, Paul Pu Liang, Jiaxuan You 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)을 사회적으로 지능적인 에이"},{"id":"2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models","title":"[논문리뷰] The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models","excerpt":"Elisabetta Rocchetti이 arXiv에 게시한 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models","tags":["Review","Text-to-Image Generation","Diffusion Models","Cross-Attention Analysis","Content-Style Disentanglement","Artistic Style Transfer","Explainable AI","SDXL"],"text":"링크: 논문 PDF로 바로 열기 저자: Alfio Ferrara, Sergio Picascia, Elisabetta Rocchetti 핵심 연구 목표 텍스트투이미지(txt2img) 확산 모델이 학습 과정에서 명시적인 지침 없이도 회화에서 콘텐츠와 스타일 개념을 내부적으로 어떻게 인코딩하고 분리하는지 탐구하는 것입니다. 특히, 모델이 생성된 이미지에서 내용("},{"id":"2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning","title":"[논문리뷰] Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning","excerpt":"Maksim Nekrashevich이 arXiv에 게시한 'Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Large Language Models","Software Engineering","Multi-Turn Interaction","Long Context","DAPO","Autonomous Agents","SWE-BENCH"],"text":"링크: 논문 PDF로 바로 열기 저자: Maksim Nekrashevich, Ibragim Badertdinov, Sergei Polezhaev, Maria Trofimova, Alexander Golubev 핵심 연구 목표 본 논문은 실세계 소프트웨어 엔지니어링(SWE)과 같이 상태 저장 환경과의 풍부한 다중 턴 상호작용 을 요구하는 복잡한 문제에 강화 "},{"id":"2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents","title":"[논문리뷰] Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents","excerpt":"Xinyu Yang이 arXiv에 게시한 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-07 13:38:21+0900","permalink":"/ai/review/2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents","tags":["Review","Web Agent","Cognitive Reasoning","Knowledge-Induced","Large Multimodal Models (LMMs)","Bloom's Taxonomy","Chain-of-Thought (CoT)","Web-CogDataset","Web-CogBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, HaiMing Xu, Yong Da"},{"id":"2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts","title":"[논문리뷰] Are Today's LLMs Ready to Explain Well-Being Concepts?","excerpt":"Huan Liu이 arXiv에 게시한 'Are Today's LLMs Ready to Explain Well-Being Concepts?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts","tags":["Review","Large Language Models","Well-being Concepts","LLM Evaluation","Principle-Guided Evaluation","LLM-as-a-Judge","Supervised Fine-Tuning (SFT)","Direct Preference Optimization (DPO)","Explanation Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Bohan Jiang, Dawei Li, Zhen Tan, Chengshuai Zhao, Huan Liu 핵심 연구 목표 본 연구는 대규모 언어 모델(LLMs)이 웰빙 개념을 정확하고 다양한 잠재 고객(일반 대중 및 도메인 전문가)에게 적합하게 설명할 준비가 되어 있는지를 체계적으로 평가하는 것을 목표로 합니다. 특히"},{"id":"2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation","title":"[논문리뷰] Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?","excerpt":"Junjie Yang이 arXiv에 게시한 'Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation","tags":["Review","Retrieval-Augmented Generation","Multimodal LLMs","Benchmark Evaluation","Document Understanding","Multi-hop Reasoning","Information Retrieval","Evaluation Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin 핵심 연구 목표 이 논문은 현재 문서 검색 증강 생성(RAG) 시스템 의 평가 벤치마크가 실제 세계의 복잡성과 한계를 제대로 반영하지 못하는 문제점을 해결하고"},{"id":"2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability","title":"[논문리뷰] Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability","excerpt":"Yuan Wu이 arXiv에 게시한 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability","tags":["Review","Large Multimodal Models","Input Scrutiny","Error Detection","Faulty Inputs","Evaluation Framework","Modality Preference","Cross-Modal Inconsistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Haiqi Yang, Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu 핵심 연구 목표 본 논문은 대규모 멀티모달 모델(LMMs)이 결함 있는 입력을 수동적으로 수용하여 잘못된 추론을 유발하는 문제를 해결하고자 합니다. 특히, LMMs가 명시적인 지시 없이도 오류가 있는 입력을 능동적으로 감지하고"},{"id":"2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions","title":"[논문리뷰] CoAct-1: Computer-using Agents with Coding as Actions","excerpt":"Taiwei Shi이 arXiv에 게시한 'CoAct-1: Computer-using Agents with Coding as Actions' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions","tags":["Review","AI Agent","Multi-agent System","GUI Automation","Programmatic Control","Code Generation","OSWorld Benchmark","Hybrid AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, Ran Xu, Caiming Xiong 핵심 연구 목표 이 논문은 복잡하고 장기적인 컴퓨터 사용 "},{"id":"2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning","title":"[논문리뷰] DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning","excerpt":"Ziming Wang이 arXiv에 게시한 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning","tags":["Review","Vision Language Models (VLMs)","Agentic AI","Physical Reasoning","Benchmark","Simulation Environments","Action Planning","Interactive AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinrun Xu, Pi Bu, Ye Wang, Börje F. Karlsson, Ziming Wang 핵심 연구 목표 본 논문은 Vision Language Models(VLMs)이 복잡하고 동적인 물리 환경에서 정확한 행동 계획 및 공간/시간 추론 능력 에 한계를 보이는 문제를 해결하고자 합니다. 기존 벤치마크들이"},{"id":"2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models","title":"[논문리뷰] Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models","excerpt":"Fangzhou Yao이 arXiv에 게시한 'Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models","tags":["Review","Large Reasoning Models","Efficient Reasoning","Chain-of-Thought","Model Optimization","Model Collaboration","Overthinking Problem","LLM Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangzhou Yao, Weibo Gao, Yizhi Wang, Yichao Du, Linan Yue 핵심 연구 목표 본 설문 연구는 DeepSeek R1 과 같은 R1style Large Reasoning Models (LRMs) 에서 흔히 발생하는 '과잉 사고(overthinking)' 문제를 해결하고, 효율적인"},{"id":"2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation","title":"[논문리뷰] Evaluating, Synthesizing, and Enhancing for Customer Support Conversation","excerpt":"Feng Chen이 arXiv에 게시한 'Evaluating, Synthesizing, and Enhancing for Customer Support Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation","tags":["Review","Customer Support","Dialogue Generation","Large Language Models","Role-Playing","COPC Framework","Synthetic Data","Strategy Prediction","Empathetic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jie Zhu, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong 핵심 연구 목표 본 논문은 고객 지원 대화(Customer Support Conversation, CSC) 분야에서 전략적 지침과 고품질 데이터의 부족 문제를 해결하고자 합니다. 궁극"},{"id":"2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation","title":"[논문리뷰] Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation","excerpt":"Shengcong Chen이 arXiv에 게시한 'Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation","tags":["Review","Robotic Manipulation","World Model","Video Generation","Diffusion Model","Embodied AI","Foundation Model","Robotics Simulation","Policy Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengcong Chen, Donglin Yang, Siyuan Huang, Pengfei Zhou, Yue Liao 핵심 연구 목표 본 논문은 로봇 조작을 위한 통합된 세계 파운데이션 플랫폼 (Genie Envisioner) 을 제시하여, 정책 학습, 평가 및 시뮬레이션을 단일 비디오생성 프레임워크 내에서 통합하는"},{"id":"2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity","title":"[논문리뷰] Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity","excerpt":"Zhibing Li이 arXiv에 게시한 'Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity","tags":["Review","3D Generation Evaluation","Hierarchical Evaluation","Material Properties","Multi-Agent Annotation","Hybrid Scoring System","Video-based Evaluation","Part-level Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhan Zhang, Long Zhuo, Ziyang Chu, Tong Wu, Zhibing Li, Liang Pan, Dahua Lin, Ziwei Liu 핵심 연구 목표 본 논문은 3D 생성 모델의 품질 평가에 있어 기존 2D 이미지 기반 metrics의 한계와 평가의 거친 입자성(coarsegrained) 문제"},{"id":"2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis","title":"[논문리뷰] Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis","excerpt":"Reshmi Ghosh이 arXiv에 게시한 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis","tags":["Review","Multi-hop Question Answering","Large Language Models","Reasoning Errors","Error Taxonomy","Human Evaluation","Automated Evaluation","Overthinking"],"text":"링크: 논문 PDF로 바로 열기 저자: Reshmi Ghosh, Yashwanth Babu, Srujana Pillarichety, Isha Nalawade, Anushka Yadav 핵심 연구 목표 현재 대규모 언어 모델(LLM)이 다단계(multihop) 질문 답변 태스크에서 환각(hallucination)을 보이거나 추론에 실패하는 근본적인 원인을 진"},{"id":"2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations","title":"[논문리뷰] I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations","excerpt":"Chirag Shah이 arXiv에 게시한 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations","tags":["Review","LLM Bias","Hiring Evaluation","Linguistic Shibboleth","Hedging Language","Fairness","Benchmarking","Sociolinguistics"],"text":"링크: 논문 PDF로 바로 열기 저자: Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 채용 평가에서 언어적 시볼레트(linguistic shibboleths) , 특히 완곡어법(hedging language)을 기반으로 잠재적으로 인구통계학적 편향"},{"id":"2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking","title":"[논문리뷰] I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking","excerpt":"Chao Wang이 arXiv에 게시한 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking","tags":["Review","Multimodal Entity Linking","Large Language Models","Collaborative Reflection","Iterative Reasoning","Visual Information","Text-centric"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyan Liu, Junwen Li, Kaiwen Li, Tong Ruan, Chao Wang, Xinyan He, Zongyu Wang, Xuezhi Cao, Jingping Liu 핵심 연구 목표 본 논문은 기존 대규모 언어 모델(LLM) 기반의 다중모달 엔티티 연결(MEL) 방법론이 이미지 데이터를 불필요하게 "},{"id":"2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities","title":"[논문리뷰] InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities","excerpt":"Zhijie Sang이 arXiv에 게시한 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities","tags":["Review","LLM Alignment","Reasoning","Data Curation","Supervised Fine-tuning (SFT)","Direct Preference Optimization (DPO)","Sample Efficiency","Scalability","Multi-dimensional Filtering"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuo Cai, Su Lu, Qi Zhou, Kejing Yang, Zhijie Sang, Congkai Xie, Hongxia Yang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 능력을 향상시키기 위한 확장 가능 하고 샘플 효율적인 후속 학습 프레임워크인 InfiAlign 을 제안합니다. 특히, "},{"id":"2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes","title":"[논문리뷰] MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes","excerpt":"Xudong Jiang이 arXiv에 게시한 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes","tags":["Review","Video Object Segmentation","Dataset","Complex Scenes","Benchmark","Object Tracking","Computer Vision","Dataset Challenges"],"text":"링크: 논문 PDF로 바로 열기 저자: Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang 핵심 연구 목표 기존 VOS(Video Object Segmentation) 데이터셋들이 실제와 동떨어진 고립되고 눈에 띄는 객체에 치우쳐 있어 모델의 현실 적용성을 제한하는 문제를 해결하고자 합니다. 본"},{"id":"2025-8-8-Marco-Voice-Technical-Report","title":"[논문리뷰] Marco-Voice Technical Report","excerpt":"Qingjuan Li이 arXiv에 게시한 'Marco-Voice Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Marco-Voice-Technical-Report","tags":["Review","Speech Synthesis","Voice Cloning","Emotion Control","Text-to-Speech","Disentanglement","Contrastive Learning","Flow Matching","Emotional Speech Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang 핵심 연구 목표 본 논문은 음성 복제(voice cloning)와 감정"},{"id":"2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification","title":"[논문리뷰] On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification","excerpt":"Xinyu Ye이 arXiv에 게시한 'On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification","tags":["Review","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","Generalization","Reward Rectification","Dynamic Fine-Tuning (DFT)","LLM","Policy Gradient","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, MingHsuan Yang, Xu Yang 핵심 연구 목표 표준 Supervised FineTuning (SFT)이 Reinforcement Learn"},{"id":"2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction","title":"[논문리뷰] PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction","excerpt":"Prajit Das이 arXiv에 게시한 'PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction","tags":["Review","PII Redaction","Large Language Models","Instruction Tuning","Retrieval-Augmented Generation","Privacy Preservation","Model Evaluation","Cross-Domain Generalization","Open-Source LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Leon Garza, Anantaa Kotal, Aritran Piplai, Lavanya Elluri, Prajit Kumar Das, Aman Chadha 핵심 연구 목표 본 연구는 비정형 텍스트에서 개인 식별 정보(PII) 를 자동 제거하는 문제에 초점을 맞춥니다. 기존의 규칙 기반 시스템이나 도메인별 NER 모"},{"id":"2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data","title":"[논문리뷰] R-Zero: Self-Evolving Reasoning LLM from Zero Data","excerpt":"Zongxia Li이 arXiv에 게시한 'R-Zero: Self-Evolving Reasoning LLM from Zero Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data","tags":["Review","Self-Evolving LLM","Reinforcement Learning","Curriculum Learning","Reasoning","Large Language Models","Self-Play","Zero-Data Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, Dong Yu 핵심 연구 목표 본 연구는 기존 LLM의 자가 진화 방식이 방대한 인간 큐레이션 데이터 에 의존하는 한계를 극복하"},{"id":"2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation","title":"[논문리뷰] REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation","excerpt":"Xiao Yu이 arXiv에 게시한 'REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation","tags":["Review","Simultaneous Speech Translation","Adaptive Policy","Entropy-based Loss","Mutual Information","Latency-Quality Trade-off","Speech-to-Text Translation","REINA"],"text":"링크: 논문 PDF로 바로 열기 저자: Nameer Hirschkind, Joseph Liu, Xiao Yu, Mahesh Kumar Nandwana 핵심 연구 목표 동시 음성 번역(SimulST) 시스템에서 번역 품질과 지연 시간 간의 최적의 균형을 달성하는 것이 주요 과제입니다. 본 논문은 \"정보 획득 시에만 더 많은 입력을 기다린다\" 는 핵심 아이디어"},{"id":"2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation","title":"[논문리뷰] RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation","excerpt":"Jian Yang이 arXiv에 게시한 'RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation","tags":["Review","Robust PCA","Deep Unfolding","Sparse Segmentation","Interpretability","Image Decomposition","Computer Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Fengyi Wu, Yimian Dai, Tianfang Zhang, Yixuan Ding, Jian Yang, MingMing Cheng, Zhenming Peng 핵심 연구 목표 본 논문은 기존의 Robust PCA (RPCA) 모델이 가진 높은 계산 비용, 수동 튜닝에 따른 일반화 능력 부족, 그리고 경직된 사전"},{"id":"2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression","title":"[논문리뷰] Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression","excerpt":"Yifei Ji이 arXiv에 게시한 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression","tags":["Review","Image Compression","Diffusion Models","One-Step Decoding","Fidelity Guidance","Rate Annealing","VAE","Perceptual Quality"],"text":"링크: 논문 PDF로 바로 열기 저자: Zheng Chen, Mingde Zhou, Jinpei Guo, Jiale Yuan, Yifei Ji, Yulun Zhang 핵심 연구 목표 본 논문은 확산 기반 이미지 압축 모델의 주요 단점인 과도한 디코딩 지연 시간 과 낮은 충실도(fidelity) 문제를 해결하고자 합니다. 특히 낮은 비트레이트 환경에서 높은 "},{"id":"2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance","title":"[논문리뷰] StrandDesigner: Towards Practical Strand Generation with Sketch Guidance","excerpt":"Xiaobin Hu이 arXiv에 게시한 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance","tags":["Review","Strand Generation","Sketch Guidance","Diffusion Models","Multi-scale Learning","Adaptive Conditioning","3D Hair Modeling","Computer Graphics"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaobin Hu, Han Feng, Chengming Xu, Moran Li, Na Zhang 핵심 연구 목표 본 연구는 텍스트나 일반 이미지 프롬프트의 정밀도와 사용 편의성 부족 문제를 해결하기 위해, 스케치를 기반으로 하는 최초의 머리카락 스트랜드(strand) 생성 모델을 제안합니다. 복잡한 스트랜드 상호작용"},{"id":"2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling","title":"[논문리뷰] Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling","excerpt":"Ruolin Shen이 arXiv에 게시한 'Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-08-08 13:32:22+0900","permalink":"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling","tags":["Review","Visual Document Understanding","Visual Question Answering","Multi-Agent System","Test-Time Scaling","Self-Correction","Mixed Reward Modeling","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinlei Yu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Ruolin Shen, Jiangning Zhang, Xiaobin Hu, Yanwei Fu, Shuicheng Yan 핵심 연구 목표 본 연구는 기존 비전언어 모델(VLMs)이 매개변수 규모에 제약이 있고, 견고한 자가 수정"},{"id":"2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code","title":"[논문리뷰] A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code","excerpt":"Libo Chen이 arXiv에 게시한 'A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code","tags":["Review","AI-Generated Code Security","LLM Evaluation","Repository-Level Benchmark","Code Security","Vulnerability Detection","Static Analysis","Reproducibility","Context-Awareness"],"text":"링크: 논문 PDF로 바로 열기 저자: Keke Lian, Bing Wang, Lei Zhang, Libo Chen, Junjie Wang, Ziming Zhao, Yujiu Yang, Haotong Duan, Haoran Zhao, Shuang Liao, Mingda Guo, Jiazheng Quan, Yilu Zhong, Chenhao He, Zichu"},{"id":"2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers","title":"[논문리뷰] A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers","excerpt":"Jiamin Wu이 arXiv에 게시한 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers","tags":["Review","Scientific LLMs","AI for Science","Scientific Data","Agentic AI","Multimodal Integration","Knowledge Representation","Autonomous Discovery","Data Ecosystems"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiamin Wu, Wanghan Xu, Wei Li, Chenglong Ma, Ming Hu 핵심 연구 목표 이 논문은 과학 분야 대규모 언어 모델(SciLLMs)의 발전 과정을 데이터 기반과 에이전트 프론티어 관점에서 종합적으로 분석하는 것을 목표로 합니다. 특히, SciLLMs가 일반 자연어 처리(NLP) 데이터"},{"id":"2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models","title":"[논문리뷰] AHELM: A Holistic Evaluation of Audio-Language Models","excerpt":"Siwei Yang이 arXiv에 게시한 'AHELM: A Holistic Evaluation of Audio-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models","tags":["Review","Audio-Language Models","Holistic Evaluation","Benchmarking","Multimodality","Fairness","Robustness","Reasoning","Bias Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Tony Lee, Haoqin Tu, Chi Heem Wong, Yuyin Zhou, Zijun Wang, Siwei Yang, Yifan Mai, Cihang Xie, Percy Liang 핵심 연구 목표 오디오언어 모델(ALMs)의 표준화된 벤치마크 부족 문제를 해결하고, 기존 평가들이 제한된 기능에만 초점을 맞추"},{"id":"2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP","title":"[논문리뷰] CLIPSym: Delving into Symmetry Detection with CLIP","excerpt":"Raymond A. Yeh이 arXiv에 게시한 'CLIPSym: Delving into Symmetry Detection with CLIP' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP","tags":["Review","Symmetry Detection","Vision-Language Models","CLIP","Equivariant Networks","Prompt Engineering","Geometric Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tinghan Yang, Md Ashiqur Rahman, Raymond A. Yeh 핵심 연구 목표 본 논문은 기존 대규모 비전언어 모델(VisionLanguage Models, VLMs)인 CLIP 을 활용하여 이미지 내의 반사 및 회전 대칭을 더욱 정확하고 견고하게 탐지하는 것을 목표로 합니다. 특히, CLIP이"},{"id":"2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation","title":"[논문리뷰] Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation","excerpt":"Qi Jia이 arXiv에 게시한 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation","tags":["Review","3D Generation","Video Diffusion Models","Spatial Consistency","Semantic Knowledge","Multi-view Synthesis","Large-scale Dataset","Image-to-3D","Text-to-3D"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaochuan Li, Guoguang Du, Runze Zhang, Liang Jin, Qi Jia, Lihua Lu, Zhenhua Guo, Yaqian Zhao, Haiyang Liu, Tianqi Wang, Changsheng Li, Xiaoli Gong, Rengang Li, Baoyu Fan 핵심 연구 목"},{"id":"2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models","title":"[논문리뷰] Efficient Code Embeddings from Code Generation Models","excerpt":"Han Xiao이 arXiv에 게시한 'Efficient Code Embeddings from Code Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models","tags":["Review","Code Embeddings","Code Generation Models","Autoregressive Backbones","Last-Token Pooling","Instruction Tuning","Contrastive Learning","Retrieval-Augmented Generation","MTEB Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Daria Kryvosheieva, Saba Sturua, Michael Günther, Scott Martens, Han Xiao 핵심 연구 목표 본 논문은 기존 코드 임베딩 모델들이 겪는 지도 학습 데이터 부족 문제 와 대규모 비정렬 코드/자연어 데이터의 활용 미흡 을 해결하고자 합니다. 이를 위해 사전 훈련된 코"},{"id":"2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control","title":"[논문리뷰] EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control","excerpt":"Zhaoqing Chen이 arXiv에 게시한 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control","tags":["Review","Embodied AI","Robot Control","Vision-Language-Action Models","Multimodal Pretraining","Flow Matching","Foundation Models","Generalization","Real-world Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang 핵심 연구 목표 본 연구는 기존 VLA 모델들이 가진 제한된 도메인 및 유연성 문제를 해결하고, 개"},{"id":"2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation","title":"[논문리뷰] HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation","excerpt":"Tianhai Liang이 arXiv에 게시한 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation","tags":["Review","Dexterous Manipulation","Mobile Manipulation","Human-to-Robot Learning","Sim2Real","Reinforcement Learning","Depth Image","Visual Localization","Bimanual Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianhai Liang, Pu Hua, Langzhe Gu, Tianming Wei, Zhecheng Yuan 핵심 연구 목표 이 논문은 복잡한 다지(multifingered) 로봇 핸드를 활용한 모바일 양손 로봇 조작(mobile bimanual dexterous manipulation)에서 다양한 소스의 인간 동"},{"id":"2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery","title":"[논문리뷰] Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery","excerpt":"Wenjie Zhou이 arXiv에 게시한 'Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery","tags":["Review","Physics Formula Discovery","Multimodal AI","Vision-Language Models","Symbolic Regression","Causal Chain of Thought","Reinforcement Learning","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaqi Liu, Songning Lai, Pengze Li, Di Yu, Wenjie Zhou 핵심 연구 목표 본 논문은 기존의 단일 모달(symbolic regression 또는 LLM) 접근법이 물리학자들이 현상학적 시각적 표현을 활용하는 점을 간과하여 동적 현상 내재의 시공간 패턴을 해석하는 능력이 약하다는 "},{"id":"2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices","title":"[논문리뷰] Morae: Proactively Pausing UI Agents for User Choices","excerpt":"Amy Pavel이 arXiv에 게시한 'Morae: Proactively Pausing UI Agents for User Choices' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices","tags":["Review","UI Agents","Accessibility","Human-Agent Interaction","Mixed-Initiative AI","Large Multimodal Models","Proactive AI","User Choice","Blind and Low-Vision Users"],"text":"링크: 논문 PDF로 바로 열기 저자: YiHao Peng, Dingzeyu Li, Jeffrey P. Bigham, Amy Pavel 핵심 연구 목표 본 논문은 기존 UI 에이전트들이 맹인 및 저시력(BLV) 사용자들에게 중요한 의사결정 시 선택권을 주지 않고 자동으로 작업을 완료하여 사용자 주도성을 저해하는 문제를 해결하고자 합니다. Morae는 BLV"},{"id":"2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning","title":"[논문리뷰] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning","excerpt":"Han Hu이 arXiv에 게시한 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning","tags":["Review","Multimodal Large Language Models (MLLMs)","Auto-Thinking","Reinforcement Learning (RL)","Bi-mode Annealing","Bi-mode Policy Optimization (BPO)","General-Purpose AI","Reasoning","Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Han Hu, Shiming Xiang, Bolin Ni, Qi Yang, Jie Jiang 핵심 연구 목표 본 논문은 복잡한 추론 문제에서 뛰어난 성능을 보이는 기존 MLLM의 stepbystep 사고(thinking) 과정이 단순 문제에서는 불필요한 연산 오버헤드를 유발하는 비효율성을 해결하고자 합니다. 이를 위해"},{"id":"2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis","title":"[논문리뷰] TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis","excerpt":"Pengcheng Chen이 arXiv에 게시한 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis","tags":["Review","Audio-Driven Talking Head Synthesis","Large-Scale Dataset","Data Diversity","Data Curation","Evaluation Benchmark","Generalization Gap","Algorithmic Fairness"],"text":"링크: 논문 PDF로 바로 열기 저자: Shunian Chen, Hejin Huang, Yexin Liu, Zihan Ye, Pengcheng Chen, Chenghao Zhu, Michael Guan, Rongsheng Wang, Junying Chen, Guanbin Li, SerNam Lim, Harry Yang, Benyou Wang 핵심 연구 목표"},{"id":"2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models","title":"[논문리뷰] Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models","excerpt":"Yifan Lu이 arXiv에 게시한 'Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models","tags":["Review","Large Language Models","Reinforcement Learning","Game AI","Procedural Knowledge","Declarative Knowledge","Explainable AI","Strategic Decision-Making"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Liao, Yu Gu, Yuan Sui, Zining Zhu, Yifan Lu, Guohua Tang, Zhongqian Sun, Wei Yang 핵심 연구 목표 대규모 언어 모델(LLM)이 복잡한 추론 작업에는 능숙하지만, 인간 아이들이 쉽게 수행하는 간단한 상호작용 작업에서는 어려움을 겪는 문제를 해결하고자 "},{"id":"2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training","title":"[논문리뷰] TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training","excerpt":"Jiyao Deng이 arXiv에 게시한 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training","tags":["Review","Language Model Pre-training","Dynamic Data Mixing","Data Influence","Group Influence","Optimization","Regression Model","LLM Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Wang, Binbin Liu, Fengze Liu, Yuanfan Guo, Jiyao Deng, Xuecheng Wu, Weidong Zhou, Xiaohuan Zhou, Taifeng Wang 핵심 연구 목표 언어 모델 사전 훈련 과정에서 고정된 데이터 혼합 전략은 모델의 학습 선호도가 동적으로 변화함에"},{"id":"2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning","title":"[논문리뷰] UItron: Foundational GUI Agent with Advanced Perception and Planning","excerpt":"Yufeng Zhong이 arXiv에 게시한 'UItron: Foundational GUI Agent with Advanced Perception and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-01 13:14:34+0900","permalink":"/ai/review/2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning","tags":["Review","GUI Agent","Foundational Model","Multimodal LLM","Perception","Planning","Reinforcement Learning","Data Engineering","Chinese App Scenarios"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong 핵심 연구 목표 이 논문은 Mobile/PC 환경에서 복잡한 작업을 자동화하는 GUI 에이전트 의 핵심 역량을 강화하는 오픈소스 파운데이션 모델, Ultron 을 제시합니다. 기존 GUI 에이전트의 "},{"id":"2025-9-10-Causal-Attention-with-Lookahead-Keys","title":"[논문리뷰] Causal Attention with Lookahead Keys","excerpt":"Quanquan Gu이 arXiv에 게시한 'Causal Attention with Lookahead Keys' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Causal-Attention-with-Lookahead-Keys","tags":["Review","Causal Attention","Lookahead Keys","Autoregressive Modeling","Language Models","Transformer","Perplexity Reduction","Parallel Training","Efficient Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuoqing Song, Peng Sun, Huizhuo Yuan, Quanquan Gu 핵심 연구 목표 이 연구는 자기회귀(autoregressive) 언어 모델 의 핵심 구성 요소인 표준 인과적 어텐션(causal attention)이 이전 문맥에만 의존하여 전역적 문맥 파악과 자연어 이해 능력을 저해하는 문제를"},{"id":"2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology","title":"[논문리뷰] Curia: A Multi-Modal Foundation Model for Radiology","excerpt":"Elodie Ferreres이 arXiv에 게시한 'Curia: A Multi-Modal Foundation Model for Radiology' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology","tags":["Review","Foundation Model","Radiology","Computed Tomography (CT)","Magnetic Resonance Imaging (MRI)","Self-supervised Learning","Vision Transformer","Cross-Modality Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Corentin Dancette, Julien Khlaut, Antoine Saporta, Helene Philippe, Elodie Ferreres, Baptiste Callard, Théo Danielou, Léo Alberge, Léo Machado, Daniel Tordjman, Julie Dupuis, Kor"},{"id":"2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference","title":"[논문리뷰] Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference","excerpt":"Yingfang Zhang이 arXiv에 게시한 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference","tags":["Review","Diffusion Models","Reinforcement Learning","Human Preference","Text-to-Image Generation","Reward Hacking","Direct-Align","SRPO","Fine-Grained Control","Flow Matching Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, Yansong Tang 핵심 연구 목표 본 논문은 기존 온라인 강화 학습(OnlineRL) 기반 확산 모델 정렬 방식의 한계를 극"},{"id":"2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions","title":"[논문리뷰] F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions","excerpt":"Zherui Qiu이 arXiv에 게시한 'F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions","tags":["Review","Vision-Language-Action","Embodied AI","Visual Foresight","Predictive Inverse Dynamics","Mixture-of-Transformer","Robot Manipulation","Multi-stage Training","Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang 핵심 연구 목표 본 논문은 동적인 시각 환경에서 언어 조건부 태스크를 실행하는 로봇의 한계를 극복하고자 합니다. 기존"},{"id":"2025-9-10-Language-Self-Play-For-Data-Free-Training","title":"[논문리뷰] Language Self-Play For Data-Free Training","excerpt":"Vijai Mohan이 arXiv에 게시한 'Language Self-Play For Data-Free Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training","tags":["Review","Large Language Models","Reinforcement Learning","Self-Play","Data-Free Training","Instruction Following","Adversarial Training","Reward Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, Vijai Mohan 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM) 훈련의 핵심 병목인 고품질 훈련 데이터의 지속적인 필요성을 해결하는 것을 목표로 합니다. 데이터에 대한 의존성을 제거하고, 모델이 추가 데이터 "},{"id":"2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search","title":"[논문리뷰] Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search","excerpt":"Tianjian Li이 arXiv에 게시한 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search","tags":["Review","Visual Search","Multi-Turn Reasoning","Reinforcement Learning","Tool-Integrated Agents","Exploratory Reasoning","Data Augmentation","Over-turn Masking","Visual Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao 핵심 연구 목표 기존 오픈소스 VLM(VisionLanguage Model)의 단조로운 추론 패턴과 제한된 상호작용 턴 수로 인해 시행착오적 탐색 이 필요한 어려운 시각 검색 작업을 해결하지 못하는 문제"},{"id":"2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning","title":"[논문리뷰] Parallel-R1: Towards Parallel Thinking via Reinforcement Learning","excerpt":"Xinyu Yang이 arXiv에 게시한 'Parallel-R1: Towards Parallel Thinking via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning","tags":["Review","Large Language Models","Parallel Thinking","Reinforcement Learning","Mathematical Reasoning","Progressive Curriculum","Reward Design","Exploration Scaffold"],"text":"링크: 논문 PDF로 바로 열기 저자: Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, et al. 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 복잡한 추론 문제에서 병렬적 사고를 습득하도록 훈련하는 데 있어 기존 지도 학습(SFT) 방식의 한계를 극복하고자 합니다. 특히, S"},{"id":"2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling","title":"[논문리뷰] Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling","excerpt":"Diana Marculescu이 arXiv에 게시한 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling","tags":["Review","Diffusion Models","Quantization","Few-Step Generation","Model Compression","Noise Scheduling","Post-Training Quantization","Image Quality Metrics","Latent Consistency Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Natalia Frumkin, Diana Marculescu 핵심 연구 목표 본 논문은 계산 비용이 높은 텍스트이미지 확산 모델의 추론 효율성 을 개선하는 것을 목표로 합니다. 특히, 기존 소수 단계(fewstep) 확산 모델이 여전히 대규모 모델 백본에 의존하고 기존 후속 훈련 양자화(PTQ) 방법론이 완전 정밀도("},{"id":"2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models","title":"[논문리뷰] Reconstruction Alignment Improves Unified Multimodal Models","excerpt":"XuDong Wang이 arXiv에 게시한 'Reconstruction Alignment Improves Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models","tags":["Review","Unified Multimodal Models","Image Generation","Image Editing","Post-training","Self-supervised Learning","Reconstruction Alignment","Visual Embeddings"],"text":"링크: 논문 PDF로 바로 열기 저자: Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang 핵심 연구 목표 논문은 통합 멀티모달 모델(UMM)이 이미지텍스트 쌍으로 훈련될 때 캡션의 희소성으로 인해 미세한 시각적 디테일을 놓치고, 이해와 생성 간의 정렬이 불완전하다는 문제를 해결하고자 합니다. 이를 위해 적은 리"},{"id":"2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge","title":"[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge","excerpt":"Dipanjan Das이 arXiv에 게시한 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge","tags":["Review","LLM Factuality","Parametric Knowledge","Benchmark","Question Answering","Data Curation","Evaluation Metrics","Hallucination Mitigation","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Lukas Haas, Gal Yona, Giovanni D'Antonio, Sasha Goldshtein, Dipanjan Das 핵심 연구 목표 Large Language Model (LLM)의 내부 파라미터 기반 사실성(parametric factuality) 을 측정하는 데 있어 기존 벤치마크의 한계를 해결하는 "},{"id":"2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding","title":"[논문리뷰] Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding","excerpt":"Yongcheng Zeng이 arXiv에 게시한 'Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding","tags":["Review","RLVR","LLM Reasoning","Adaptive Learning","Hint Scaffolding","Item Response Theory","Exploration Efficiency","Problem Difficulty","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min, Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen, ZhiHong Deng 핵심 연구 목표 대규모 언어 모델(LLM)의 추론 능력 강화를 위한 기존 확인 가능한"},{"id":"2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward","title":"[논문리뷰] UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward","excerpt":"Fei Ding이 arXiv에 게시한 'UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward","tags":["Review","Image Customization","Multi-Identity Generation","Identity Consistency","Identity Confusion","Reinforcement Learning","Diffusion Models","Matching Reward","Global Assignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He 핵심 연구 목표 본 논문은 이미지 커스터마이징 모델에서 다중 정체성(multiidentity)을 생성할 때 발생하는 정체성 일관성 부족(identity consistency) 과 정체성 혼란(ide"},{"id":"2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models","title":"[논문리뷰] Visual Representation Alignment for Multimodal Large Language Models","excerpt":"Heeseong Shin이 arXiv에 게시한 'Visual Representation Alignment for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models","tags":["Review","Multimodal LLMs","Visual Representation Alignment","Foundation Models","Regularization","Fine-grained Visual Understanding","Spatial Reasoning","Object Counting","Vision-Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Chanho Eom, Sunghwan Hong, Seungryong Kim 핵심 연구 목표 본 논문은 시각적"},{"id":"2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR","title":"[논문리뷰] ΔL Normalization: Rethink Loss Aggregation in RLVR","excerpt":"Lili Qiu이 arXiv에 게시한 'ΔL Normalization: Rethink Loss Aggregation in RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-10 13:11:01+0900","permalink":"/ai/review/2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR","tags":["Review","Reinforcement Learning","LLMs","Gradient Variance","Loss Aggregation","Unbiased Estimator","RLVR","Policy Gradient","Normalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu 핵심 연구 목표 이 논문은 Verifiable Rewards를 사용하는 강화 학습 (RLVR) 환경에서 응답 길이의 동적 변화로 인해 발생하는 문제에 주목합니다. 특히 응답 길이의 큰 변동성으로 인한 높은 기울기 분"},{"id":"2025-9-11-3D-and-4D-World-Modeling-A-Survey","title":"[논문리뷰] 3D and 4D World Modeling: A Survey","excerpt":"Ao Liang이 arXiv에 게시한 '3D and 4D World Modeling: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey","tags":["Review","3D World Modeling","4D World Modeling","Generative Models","Predictive Models","LiDAR","Occupancy Grids","Video Generation","Autonomous Driving","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyua"},{"id":"2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models","title":"[논문리뷰] A Survey of Reinforcement Learning for Large Reasoning Models","excerpt":"Runze Liu이 arXiv에 게시한 'A Survey of Reinforcement Learning for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models","tags":["Review","Reinforcement Learning","Large Reasoning Models","LLMs","Reward Design","Policy Optimization","Verifiable Rewards","Agentic AI","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Hao"},{"id":"2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","title":"[논문리뷰] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning","excerpt":"Honglin Guo이 arXiv에 게시한 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","tags":["Review","LLM Agents","Reinforcement Learning","Multi-Turn Interaction","Long-Horizon Decision Making","Agent Framework","Exploration-Exploitation","Progressive Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhe"},{"id":"2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI","title":"[논문리뷰] EnvX: Agentize Everything with Agentic AI","excerpt":"Wenzheng Tom Tang이 arXiv에 게시한 'EnvX: Agentize Everything with Agentic AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI","tags":["Review","Agentic AI","Multi-Agent Systems","Code Repository","Agentization","Natural Language Interaction","Agent-to-Agent Protocol","LLM-based Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Linyao Chen, Zimian Peng, Yingxuan Yang, Yikun Wang, Wenzheng Tom Tang, Hiroki H. Kobayashi, Weinan Zhang 핵심 연구 목표 이 논문은 오픈소스 코드 저장소의 재활용 및 협업의 비효율성을 해결하기 위해, 저장소를 지능적인 자율 에이전트 로"},{"id":"2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants","title":"[논문리뷰] HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants","excerpt":"Jacy Reese Anthis이 arXiv에 게시한 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants","tags":["Review","Human Agency","AI Assistants","LLM Evaluation","Benchmark","Sociotechnical AI","AI Alignment","Scalable Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Benjamin Sturgeon, Daniel Samuelson, Jacob Haimes, Jacy Reese Anthis 핵심 연구 목표 AI에 대한 인간의 의존도가 높아짐에 따라 개인 및 집단적 통제력을 상실하는 '인간 에이전시 상실' 문제에 대응하고자 합니다. 본 논문은 AI 어시스턴트가 인간의 에이전시를 얼마나"},{"id":"2025-9-11-Hunyuan-MT-Technical-Report","title":"[논문리뷰] Hunyuan-MT Technical Report","excerpt":"Yang Du이 arXiv에 게시한 'Hunyuan-MT Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-Hunyuan-MT-Technical-Report","tags":["Review","Machine Translation","Large Language Model","Multilingual","Low-Resource Languages","Reinforcement Learning","Weak-to-Strong Learning","Slow Thinking"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Du, Mingyang Song, Bingxin Qu, Zheng Li, Mao Zheng 핵심 연구 목표 본 논문은 오픈소스 다국어 기계 번역 모델인 HunyuanMT7B 및 HunyuanMTChimera7B 를 소개하며, 33개 언어 에 대한 양방향 번역에서 최첨단 성능을 달성하고 특히 만다린어와 소수 민"},{"id":"2025-9-11-P3-SAM-Native-3D-Part-Segmentation","title":"[논문리뷰] P3-SAM: Native 3D Part Segmentation","excerpt":"Yunhan Yang이 arXiv에 게시한 'P3-SAM: Native 3D Part Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-P3-SAM-Native-3D-Part-Segmentation","tags":["Review","3D Part Segmentation","Point Cloud Segmentation","Prompt-based Segmentation","Deep Learning","Transformer","Interactive Segmentation","Automatic Segmentation","Native 3D"],"text":"링크: 논문 PDF로 바로 열기 저자: Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, Chunchao Guo 핵심 연구 목표 본 논문은 기존 3D 파트 분할 방법론의 한계, 특히 복잡한 객체에 대한 불충분한 견고"},{"id":"2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation","title":"[논문리뷰] RewardDance: Reward Scaling in Visual Generation","excerpt":"Liang Li이 arXiv에 게시한 'RewardDance: Reward Scaling in Visual Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation","tags":["Review","Reward Model","Visual Generation","RLHF","VLM","Reward Scaling","Reward Hacking","Generative Paradigm","Context Scaling","Text-to-Image","Text-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, Weilin Huang 핵심 연구 목표 시각 생성 모델의 RM(Reward Model) 스케일링 패러다임이 기존 CLIP"},{"id":"2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs","title":"[논문리뷰] <think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs","excerpt":"Alexander Panchenko이 arXiv에 게시한 '<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-11 13:02:36+0900","permalink":"/ai/review/2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs","tags":["Review","Toxic Text Generation","LLMs","Text Detoxification","Lexical Diversity","Synthetic Data","Human Annotation","Style Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Sergey Pletenev, Daniil Moskovskiy, Alexander Panchenko 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)이 생성한 독성 텍스트가 텍스트 정화(detoxification) 모델 훈련을 위한 인간 주석 데이터를 효과적으로 대체할 수 있는지 평가하는 것을 목표로 합니다. 특히"},{"id":"2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting","title":"[논문리뷰] 2D Gaussian Splatting with Semantic Alignment for Image Inpainting","excerpt":"Guangming Lu이 arXiv에 게시한 '2D Gaussian Splatting with Semantic Alignment for Image Inpainting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting","tags":["Review","Image Inpainting","2D Gaussian Splatting","Semantic Alignment","DINO Features","Patch-level Rasterization","Continuous Representation","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongyu Li, Chaofeng Chen, Xiaoming Li, Guangming Lu 핵심 연구 목표 본 논문은 기존 이미지 인페인팅 방법론의 이산적인 픽셀 처리 방식이 갖는 한계를 극복하고, 2D Gaussian Splatting(2DGS) 의 연속적인 특성을 활용하여 픽셀 수준의 일관성과 전역적인 의미론적 "},{"id":"2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist","title":"[논문리뷰] Can Understanding and Generation Truly Benefit Together -- or Just Coexist?","excerpt":"Hui Han이 arXiv에 게시한 'Can Understanding and Generation Truly Benefit Together -- or Just Coexist?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist","tags":["Review","Multimodal Understanding","Multimodal Generation","Unified Models","Auto-Encoder","Reinforcement Learning","Image-to-Text","Text-to-Image","Reconstruction Fidelity"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Yan, Kaiqing Lin, Zongjian Li, Junyan Ye, Hui Han, Zhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, Xinyan Xiao, Jingdong Wang, Haifeng Wang, Li Yuan 핵심 연구 목표 이 논문은 멀티모달 "},{"id":"2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs","title":"[논문리뷰] EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs","excerpt":"Kaiqi Kou이 arXiv에 게시한 'EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs","tags":["Review","Speech-to-Speech LLMs","Acoustic-Semantic Gap","Echo Training","Unit Language","Streaming Inference","Knowledge-based QA"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li 핵심 연구 목표 본 논문은 텍스트 기반 LLM에서 파생된 SLLM(SpeechtoSpeech Large Language Models)이 지식 및 추론 능력에서 저하를 "},{"id":"2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark","title":"[논문리뷰] FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark","excerpt":"Shuai Bai이 arXiv에 게시한 'FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark","tags":["Review","Text-to-Image Generation","Reasoning Dataset","Benchmark","Generation Chain-of-Thought","Vision-Language Model","Image Aesthetics","Prompt Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li 핵심 연구 목표 본 연구는 오픈소스 TexttoImage (T2I) 모델의 추론 능력 발전을 저해하는 "},{"id":"2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval","title":"[논문리뷰] Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval","excerpt":"Kaicheng Yang이 arXiv에 게시한 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval","tags":["Review","Text-based Person Retrieval","CLIP","MLLM","Data Curation","Dual-Masking","Gradient-Attention","WebPerson Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, Qichuan Ding 핵심 연구 목표 본 연구는 텍스트 기반 인물 검색(Textbased Person Retrieval)에서 CLIP 의 성능 저하를 야기하는 두 가지 주요 문제점을 해결하는 것을 목표"},{"id":"2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents","title":"[논문리뷰] Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents","excerpt":"Xintao Wang이 arXiv에 게시한 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents","tags":["Review","LLM Agents","Reinforcement Learning","Policy Gradients","Entropy Modulation","Credit Assignment","Uncertainty","Long-Horizon Tasks","Self-Calibrating Gradient Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, Ke Wang 핵심 연구 목표 본 논문은 장기 시퀀스(longhorizon) LLM 에이전트 태스크에서 희소한 보상(sparse reward"},{"id":"2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning","title":"[논문리뷰] HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning","excerpt":"Zhuowei Chen이 arXiv에 게시한 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning","tags":["Review","Human-Centric Video Generation","Multimodal Conditioning","Text-to-Video","Image-to-Video","Audio-to-Video","Diffusion Models","Subject Preservation","Audio-Visual Synchronization","Progressive Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen 핵심 연구 목표 본 논문은 사람 중심 비디오 생성(HCVG)에서 겪는 두 가지 주요 문제, 즉 다중 모드 조건(텍스트, 이미지, 오디오)의 희소한 학습 데이터 와 주제 보존 및 오디오시각 동기화 간의 "},{"id":"2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis","title":"[논문리뷰] Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis","excerpt":"Wentao Hu이 arXiv에 게시한 'Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis","tags":["Review","Avatar Animation","Multimodal Instructions","Long-Duration Video Generation","MLLM Director","Cascaded Framework","Lip Synchronization","Instruction Grounding","Video Diffusion Transformers"],"text":"링크: 논문 PDF로 바로 열기 저자: Yikang Ding, Jiwen Liu, Wenyuan Zhang, Zekun Wang, Wentao Hu 핵심 연구 목표 기존 아바타 애니메이션 방법론의 지시 불이행 및 장기적 일관성 부족 문제를 해결하고, 오디오, 이미지, 텍스트 등 다중 모드 지시 를 심층적으로 이해하여 표정, 동작, 립싱크 가 정교하고 사실적"},{"id":"2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering","title":"[논문리뷰] LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering","excerpt":"Jianguo Zhang이 arXiv에 게시한 'LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering","tags":["Review","Long-Context LLMs","Software Engineering","Code Evaluation","Benchmark","Multi-file Reasoning","Architectural Understanding","Context Length","Software Development Lifecycle","Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianguo Zhang, Rithesh Murthy, Zhiwei Liu, Zuxin Liu, Jielin Qiu 핵심 연구 목표 본 논문은 기존 코드 평가 벤치마크의 한계를 극복하고, 수백만 토큰으로 확장된 컨텍스트 윈도우 를 가진 LLM이 현실적이고 복잡한 소프트웨어 개발 시나리오에서 긴 컨텍스트를 얼마나 잘 이"},{"id":"2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation","title":"[논문리뷰] Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation","excerpt":"Dong-Ho Lee이 arXiv에 게시한 'Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation","tags":["Review","Multimodal Recommendation","Modality Alignment","Attention Mechanism","Dilated Convolution","Maximum Mean Discrepancy","Contrastive Learning","Dimensionality Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Kelin Ren, ChanYang Ju, DongHo Lee 핵심 연구 목표 본 논문은 기존 멀티모달 추천 시스템의 두 가지 주요 한계를 해결하고자 합니다: (1) 미세정교한 교차모달 연관성을 모델링하는 능력 부족으로 인한 최적 이하의 융합 품질, (2) 전역 분포 수준의 일관성 부족으로 발생하는 표현 편향. 이를 "},{"id":"2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning","title":"[논문리뷰] OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning","excerpt":"Yuzheng Zhuang이 arXiv에 게시한 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning","tags":["Review","Embodied AI","Multimodal LLMs","3D Grounding","Task-Adaptive Reasoning","Embodiment-Aware Planning","Robotics","Spatial Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuzheng Zhuang, Zhanguang Zhang, Shiguang Wu, Dafeng Chi, Yuecheng Liu 핵심 연구 목표 본 논문은 기존 MLLM 기반 Embodied 시스템의 Geometric Adaptability Gap (다양한 공간 요구사항에 대한 3D 정보 부족)과 Embodiment C"},{"id":"2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated","title":"[논문리뷰] Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated","excerpt":"Jamie Hayes이 arXiv에 게시한 'Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated","tags":["Review","LLM Security","Data Poisoning","Chain-of-Thought","Reasoning Models","Backdoor Attacks","CoT Unfaithfulness","Emergent Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanna Foerster, Ilia Shumailov, Jamie Hayes, Robert Mullins, Yiren Zhao, Harsh Chaudhari, Yarin Gal 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 단계별 추론(ChainofThought, CoT) 능력 이 새로운 유형의 데이터 포이"},{"id":"2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning","title":"[논문리뷰] SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning","excerpt":"Zhaohui Yang이 arXiv에 게시한 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning","tags":["Review","Reinforcement Learning (RL)","Vision-Language-Action (VLA) Models","Robotic Manipulation","Data Scarcity","Generalization","Sim-to-Real Transfer","Online RL","Long-Horizon Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaohui Yang, Yuhao Zhang, Jiale Yu, Yuxin Zuo, Haozhan Li, et al. 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA) 모델이 로봇 조작 태스크에서 겪는 데이터 희소성 과 일반화 능력 부족 이라는 두 가지 근본적인 문제를 해결하는 것을 목표로 "},{"id":"2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations","title":"[논문리뷰] SpatialVID: A Large-Scale Video Dataset with Spatial Annotations","excerpt":"Jian Gao이 arXiv에 게시한 'SpatialVID: A Large-Scale Video Dataset with Spatial Annotations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations","tags":["Review","Video Dataset","Spatial Annotation","Camera Pose Estimation","Depth Map","Structured Caption","Motion Instruction","3D Vision","World Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Gao, Youtian Lin, Rujie Zheng, Yufeng Yuan, Jiahao Wang 핵심 연구 목표 본 논문은 대규모의 실세계 동적 비디오 데이터셋에 부족한 명시적인 공간 정보 및 풍부한 의미론적 주석의 부재 문제를 해결하고자 합니다. 이는 3D 재구성, 세계 모델링, 그리고 동적 장면 합성과"},{"id":"2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward","title":"[논문리뷰] The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward","excerpt":"Xiaoyu Tan이 arXiv에 게시한 'The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward","tags":["Review","Reinforcement Learning","Large Language Models (LLMs)","Diversity Collapse","f-divergence","Forward-KL","JS-divergence","Pass@k","Catastrophic Forgetting"],"text":"링크: 논문 PDF로 바로 열기 저자: Long Li, Jiaran Hao, Jason Klein Liu, Zhijian Zhou, Xiaoyu Tan, et al. 핵심 연구 목표 본 논문은 RLVR (Reinforcement Learning with Verifiable Reward) 로 미세 조정된 대규모 언어 모델(LLM)에서 빈번하게 발생하는 Pas"},{"id":"2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model","title":"[논문리뷰] VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model","excerpt":"Zirui Ge이 arXiv에 게시한 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model","tags":["Review","Vision-Language-Action Models","Robotics","Multimodal Learning","Efficient AI","Model Adaptation","Bridge Attention","Low-resource Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Yihao Wang, Pengxiang Ding, Lingxiao Li, et al. 핵심 연구 목표 VLA(VisionLanguageAction) 모델이 대규모 VLM(VisionLanguage Model)과 광범위한 사전 훈련에 크게 의존하여 발생하는 높은 훈련 비용, 느린 미세 조정, 과도한 VRAM 사용 및 낮"},{"id":"2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding","title":"[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding","excerpt":"Ethan Chern이 arXiv에 게시한 'Visual Programmability: A Guide for Code-as-Thought in Chart Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-12 13:12:46+0900","permalink":"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding","tags":["Review","Visual Programmability","Code-as-Thought (CaT)","Chart Understanding","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Adaptive Reasoning","Dual-Reward System","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Bohao Tang, Yan Ma, Fei Zhang, Jiadi Su, Ethan Chern, Zhulin Hu, Zhixin Wang, Pengfei Liu, Ya Zhang 핵심 연구 목표 VisionLanguage Models (VLM)이 차트 이해 태스크에서 고정된 추론 전략(예: 외부 도구 의존 또는 단일 "},{"id":"2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China","title":"[논문리뷰] CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China","excerpt":"XU Han이 arXiv에 게시한 'CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China","tags":["Review","Headline Generation","Minority Languages","Low-Resource NLP","Dataset","Benchmark","Natural Language Generation","Chinese Minority Languages"],"text":"링크: 논문 PDF로 바로 열기 저자: Guixian Xu, Zeli Su, Ziyin Zhang, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong 핵심 연구 목표 중국 내 소수 언어(티베트어, 위구르어, 몽골어)의 헤드라인 생성 을 위한 공개 데이터셋 및 벤치마크 부재 문제를 해결하고자 합니다. 이들 언어는 고유한 문자"},{"id":"2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies","title":"[논문리뷰] FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies","excerpt":"Fabian Otto이 arXiv에 게시한 'FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies","tags":["Review","Generalist Robot Policies","Vision-Language-Action Models","Efficient AI","Imitation Learning","Diffusion Models","Intermediate Fusion","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Moritz Reuss, Hongyi Zhou, Marcel Rühle, Ömer Erdinç Yağmurlu, Fabian Otto, Rudolf Lioutikov 핵심 연구 목표 본 논문은 현재 VisionLanguageAction (VLA) 정책의 높은 계산 비용과 자원 요구사항 문제를 해결하고자 합니다. 특히,"},{"id":"2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering","title":"[논문리뷰] HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering","excerpt":"Zhehao Tan이 arXiv에 게시한 'HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering","tags":["Review","Retrieval-Augmented Generation","Multi-hop QA","Noise Resistance","LLM","Query Decomposition","Adaptive Retrieval","Heuristic Framework","Revelator"],"text":"링크: 논문 PDF로 바로 열기 저자: Duolin Sun, Dan Yang, Yue Shen, Yihan Jiao, Zhehao Tan 핵심 연구 목표 본 논문은 멀티홉 질문(multihop queries) 처리 시 기존 RAG(RetrievalAugmented Generation) 시스템이 겪는 비효율성(과도한 반복 검색), 비합리적인 쿼리(원래 쿼리에"},{"id":"2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis","title":"[논문리뷰] InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis","excerpt":"Song Guo이 arXiv에 게시한 'InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis","tags":["Review","Image Synthesis","Resolution-Agnostic","Diffusion Models","Latent Space","VAE Decoder","High-Resolution Image Generation","Generative AI","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Tao Han, Wanghan Xu, Junchao Gong, Xiaoyu Yue, Song Guo, Luping Zhou, Lei Bai 핵심 연구 목표 본 논문은 기존 확산 모델이 고해상도 이미지 생성 시 해상도에 따라 연산 요구량이 제곱으로 증가 하여 4K 이미지 생성에 100초 이상 이 소요되는 문제점을 해결하"},{"id":"2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models","title":"[논문리뷰] Inpainting-Guided Policy Optimization for Diffusion Large Language Models","excerpt":"Chenyu Wang이 arXiv에 게시한 'Inpainting-Guided Policy Optimization for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models","tags":["Review","Diffusion LLMs","Reinforcement Learning","Inpainting","Policy Optimization","Exploration","Mathematical Reasoning","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen 핵심 연구 목표 본 논문은 Diffusion Large Language Models"},{"id":"2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations","title":"[논문리뷰] IntrEx: A Dataset for Modeling Engagement in Educational Conversations","excerpt":"Gabriele Pergola이 arXiv에 게시한 'IntrEx: A Dataset for Modeling Engagement in Educational Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations","tags":["Review","Educational Dialogue","Engagement Modeling","Dataset Annotation","Second Language Learning","Human Feedback","LLM Alignment","Readability Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingwei Tan, Mahathi Parvatham, Chiara Gambi, Gabriele Pergola 핵심 연구 목표 본 논문은 제2언어 학습자를 위한 교육 대화에서 '흥미로움(interestingness)'과 '예상되는 흥미로움(expected interestingness)'을 모델링하기 위한 IntrEx"},{"id":"2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios","title":"[논문리뷰] LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios","excerpt":"Bing Su이 arXiv에 게시한 'LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios","tags":["Review","Long-tailed Learning","Semi-Supervised Learning","Parameter-Efficient Fine-Tuning","Foundation Models","Open-World Scenarios","OOD Detection","Confidence Calibration"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahao Chen, Zhiyuan Huang, Yurou Liu, Bing Su 핵심 연구 목표 본 논문은 LongTailed SemiSupervised Learning (LTSSL)에서 발생하는 기존 문제점들, 즉 모델의 과신(overconfidence) 과 저품질 의사 레이블(pseudolabels) 문제를 해"},{"id":"2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools","title":"[논문리뷰] MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools","excerpt":"Xiaorui Wang이 arXiv에 게시한 'MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools","tags":["Review","Language Agents","Tool Use","Benchmarks","Model Context Protocol (MCP)","LLM Evaluation","Agentic AI","Real-World Performance"],"text":"링크: 논문 PDF로 바로 열기 저자: Zikang Guo, Benfeng Xu, Chiwei Zhu, Wentao Hong, Xiaorui Wang, Zhendong Mao 핵심 연구 목표 본 논문은 을 통해 도구를 사용하는 언어 에이전트의 실제 성능을 정확하게 평가할 수 있는 표준화된 벤치마크의 부재 문제를 해결하고자 합니다. 기존 벤치마크가 패러다임 "},{"id":"2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading","title":"[논문리뷰] QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading","excerpt":"Chenyu You이 arXiv에 게시한 'QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading","tags":["Review","High-Frequency Trading","Multi-Agent Systems","Large Language Models","Technical Analysis","Algorithmic Trading","Financial Reasoning","Price-Driven Signals"],"text":"링크: 논문 PDF로 바로 열기 저자: Fei Xiong, Xiang Zhang, Aosong Feng, Siqi Sun, Chenyu You 핵심 연구 목표 기존 LLM 기반 금융 시스템이 텍스트 기반 입력에 주로 의존하여 고주파 매매(HFT)의 속도 및 정확성 요구사항에 부적합하다는 한계를 해결하고자 합니다. 본 연구는 순전히 가격 기반 신호 를 활용하"},{"id":"2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs","title":"[논문리뷰] The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs","excerpt":"Jonas Geiping이 arXiv에 게시한 'The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs","tags":["Review","Large Language Models","Long-Horizon Tasks","Execution Capability","Scaling Laws","Self-Conditioning","Thinking Models","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, Jonas Geiping 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 지속적인 스케일링이 한계 효용 체감(diminishing returns)으로 이어지는지에 대한 논쟁을 다루며, 특히 장기적인 태스크(l"},{"id":"2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions","title":"[논문리뷰] VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions","excerpt":"Dong Zhang이 arXiv에 게시한 'VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions","tags":["Review","Voice Style Adaptation","Spoken Language Models","Benchmark","LALM-as-a-Judge","Speech Generation","Multilingual","Evaluation Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Jun Zhan, Mingyang Han, Yuxuan Xie, Chen Wang, Dong Zhang, Kexin Huang, Haoxiang Shi, DongXiao Wang, Tengtao Song, Qinyuan Cheng, Shimin Li, Jun Song, Xipeng Qiu, Bo Zheng 핵심 연구 "},{"id":"2025-9-15-Virtual-Agent-Economies","title":"[논문리뷰] Virtual Agent Economies","excerpt":"William A. Cunningham이 arXiv에 게시한 'Virtual Agent Economies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-Virtual-Agent-Economies","tags":["Review","AI Agents","Virtual Economy","Multi-Agent Systems","Economic Mechanisms","Governance","Blockchain","Resource Allocation","Agent Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Nenad Tomašev, Matija Franklin, Joel Z. Leibo, Julian Jacobs, William A. Cunningham, Iason Gabriel, Simon Osindero 핵심 연구 목표 논문은 자율 AI 에이전트의 급속한 확산으로 인해 발생하는 새로운 경제적 레이어, 즉 \"가상 에이"},{"id":"2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition","title":"[논문리뷰] X-Part: high fidelity and structure coherent shape decomposition","excerpt":"Yunhan Yang이 arXiv에 게시한 'X-Part: high fidelity and structure coherent shape decomposition' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-15 13:12:08+0900","permalink":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition","tags":["Review","3D Shape Decomposition","Diffusion Models","Part-level Generation","Controllable Generation","Bounding Box Prompts","Semantic Features","Interactive Editing","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, Chunchao Guo 핵심 연구 목표 기존 파트 기반 3D 형태 생성 방식이 낮은 제어 가능성과 의"},{"id":"2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media","title":"[논문리뷰] CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media","excerpt":"Subasish Das이 arXiv에 게시한 'CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media","tags":["Review","Sentiment Analysis","Narrative Analysis","Decentralized Social Media","Bluesky","Transformer Models","Topic Modeling","Real-time Processing","Data Visualization"],"text":"링크: 논문 PDF로 바로 열기 저자: Gaurab Chhetri, Anandi Dutta, Ph.D., Subasish Das, Ph.D. 핵심 연구 목표 본 연구는 분산형 소셜 미디어 플랫폼인 Bluesky 에서 실시간으로 대규모 공개 담론을 분석하기 위한 확장 가능한 오픈 소스 프레임워크인 CognitiveSky 를 제안합니다. 기존 트위터(X) AP"},{"id":"2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding","title":"[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding","excerpt":"Li Zheng이 arXiv에 게시한 'Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding","tags":["Review","Video Hallucination","Large Video Models (LVMs)","Hierarchical Reasoning","Spatial-Temporal Grounding","Diagnostic Framework","Benchmark Dataset","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Li Zheng, Tianjie Ju, Liqiang Jing, Shengqiong Wu, Meng Luo 외 핵심 연구 목표 본 논문은 대규모 비디오 모델(LVM)이 입력 비디오와 불일치하는 내용을 생성하는 \"환각(hallucination)\" 문제를 해결하는 것을 목표로 합니다. 기존 환각 평가 벤치마크의 단편적인 "},{"id":"2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI","title":"[논문리뷰] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI","excerpt":"UVSKKR이 arXiv에 게시한 'EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI","tags":["Review","Ethical Reasoning","Mental Health AI","Benchmark Dataset","Large Language Models","AI Ethics","Clinical Decision Support","Human-in-the-loop"],"text":"링크: 논문 PDF로 바로 열기 저자: Sai Kartheek Reddy Kasu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 정신 건강과 같은 민감한 도메인에서 직면하는 윤리적 추론의 한계를 해결하고자 합니다. 기존 벤치마크들이 정신 건강 분야의 고유한 윤리적 딜레마(기밀 유지, 자율성, 선행, 편향 등)를 충분히 포착하지 못하는 문제를 인식하"},{"id":"2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings","title":"[논문리뷰] GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings","excerpt":"Yixuan Tang이 arXiv에 게시한 'GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings","tags":["Review","Model Pruning","Domain Adaptation","Embedding Models","Gradient Alignment","Fisher Information","Model Compression","LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Yixuan Tang, Yi Yang 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM) 기반 임베딩 모델의 배포 문제를 해결하기 위해, 기존 가지치기(pruning) 방법론이 일반적인 의미론적 표현과 도메인 특화 패턴을 구분하지 못하여 발생하는 비최적화된 가지치기 결정 의 한계를 극복하고자 합니다. 궁극적으로 도메"},{"id":"2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts","title":"[논문리뷰] InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts","excerpt":"Wenzhe Cai이 arXiv에 게시한 'InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts","tags":["Review","Embodied AI","3D Scene Dataset","Simulation Environment","Scene Generation","Point-Goal Navigation","Realistic Layouts","Object Interaction","Real-to-Sim"],"text":"링크: 논문 PDF로 바로 열기 저자: Weipeng Zhong, Peizhou Cao, Yichen Jin, Li Luo, Wenzhe Cai, Jingli Lin, Hanqing Wang, Zhaoyang Lyu, Tai Wang, Bo Dai, Xudong Xu, Jiangmiao Pang 핵심 연구 목표 본 연구는 Embodied AI 의 발전을 위"},{"id":"2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence","title":"[논문리뷰] LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence","excerpt":"Lionel M. Ni이 arXiv에 게시한 'LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence","tags":["Review","Image Editing","Diffusion Models","Multi-Modal Transformers","Drag-based Editing","Explicit Correspondence","Attention Control","Identity Preservation","Training-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Zixin Yin, Xili Dai, Duomin Wang, Xianfang Zeng, Lionel M. Ni, Gang Yu, HeungYeung Shum 핵심 연구 목표 본 논문은 드래그 기반 이미지 편집에서 MultiModal Diffusion Transformers (MMDiTs) 의 불안정성을 해결하고, 기존"},{"id":"2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting","title":"[논문리뷰] Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting","excerpt":"Changlong Yu이 arXiv에 게시한 'Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting","tags":["Review","Multi-objective Reinforcement Learning","LLM Alignment","Dynamic Reward Weighting","Pareto Front Optimization","Hypervolume Indicator","Gradient-based Optimization","Online RL"],"text":"링크: 논문 PDF로 바로 열기 저자: Yining Lu, Zilong Wang, Shiyang Li, Xin Liu, Changlong Yu, Qingyu Yin, Zhan Shi, Zixuan Zhang, Meng Jiang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 다중 목표 정렬(multiobjective alignment) 과정에서 고"},{"id":"2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics","title":"[논문리뷰] Locality in Image Diffusion Models Emerges from Data Statistics","excerpt":"Vincent Sitzmann이 arXiv에 게시한 'Locality in Image Diffusion Models Emerges from Data Statistics' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics","tags":["Review","Diffusion Models","Locality","Data Statistics","Optimal Denoiser","Wiener Filter","Sensitivity Fields","Generative Models","Inductive Bias"],"text":"링크: 논문 PDF로 바로 열기 저자: Artem Lukoianov, Justin Solomon, Chenyang Yuan, Vincent Sitzmann 핵심 연구 목표 본 연구는 확산 모델(Diffusion Models)의 학습된 지역성(locality)이 모델 아키텍처의 귀납적 편향(inductive bias)보다는 이미지 데이터셋의 통계적 속성 에서"},{"id":"2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models","title":"[논문리뷰] Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models","excerpt":"Shuo Ren이 arXiv에 게시한 'Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models","tags":["Review","Vision-Language Models","Visual Reasoning","Reflection","Reinforcement Learning","Visual Attention","Slow Thinking","Multimodal Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Pu Jian, Junhong Wu, Wei Sun, Chen Wang, Shuo Ren, Jiajun Zhang 핵심 연구 목표 논문은 기존 VisionLanguage Models (VLMs)이 복잡한 시각적 추론 과정에서 시각적 정보에 대한 의존도가 빠르게 감소하여 \"텍스트 환각\" 및 \"시각적 무시\"를 겪는 문제를"},{"id":"2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models","title":"[논문리뷰] Lost in Embeddings: Information Loss in Vision-Language Models","excerpt":"Ivan Vulić이 arXiv에 게시한 'Lost in Embeddings: Information Loss in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models","tags":["Review","Vision-Language Models","Information Loss","Embeddings","Connectors","k-NN Overlap Ratio","Embedding Reconstruction","Multimodal AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenyan Li, Raphael Tang, Chengzu Li, Caiqi Zhang, Ivan Vulić, Anders Søgaard 핵심 연구 목표 본 논문은 VisionLanguage Models (VLMs) 에서 시각적 정보를 언어 모델 임베딩 공간으로 투영하는 커넥터(connector) 모듈로 인해 발생하는"},{"id":"2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models","title":"[논문리뷰] Measuring Epistemic Humility in Multimodal Large Language Models","excerpt":"Kaiyang Zhou이 arXiv에 게시한 'Measuring Epistemic Humility in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models","tags":["Review","Multimodal Large Language Models","Hallucination","Epistemic Humility","Benchmark","False-Option Rejection","Visual Question Answering","Scene Graph"],"text":"링크: 논문 PDF로 바로 열기 저자: Bingkui Tong, Jiaer Xia, Sifeng Shang, Kaiyang Zhou 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLM)의 환각(hallucination) 문제를 해결하고, 특히 모델이 불확실한 상황에서 잘못된 정보를 확신하지 않고 \"모르는 것을 모른다고 인정하는\" 능력 , 즉 인식"},{"id":"2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling","title":"[논문리뷰] OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling","excerpt":"Yang Zhou이 arXiv에 게시한 'OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling","tags":["Review","4D World Modeling","Multi-Modal Dataset","Multi-Domain Data","Geometric Foundation Models","Video Generation","Spatio-Temporal Data","Dataset Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, Mingyu Liu, Dingning Liu, Jiange Yang, Zhoujie Fu, Junyi "},{"id":"2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits","title":"[논문리뷰] PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits","excerpt":"Zhenhao Chen이 arXiv에 게시한 'PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits","tags":["Review","Multimodal Dataset","LLM Inference","Behavioral Traits","Causal Representation Learning","Big Five","Multimodal AI","Causal Discovery","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Loka Li, Wong Yu Kang, Minghao Fu, Guangyi Chen, Zhenhao Chen, Gongxu Luo, Yuewen Sun, Salman Khan, Peter Spirtes, Kun Zhang 핵심 연구 목표 본 논문은 인간 행동 특성 분석을 위한 멀티모달 데이터셋의 부족 문제 를 해결하"},{"id":"2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation","title":"[논문리뷰] SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation","excerpt":"Heshaam Faili이 arXiv에 게시한 'SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation","tags":["Review","LLM","Instruction Tuning","Domain Adaptation","Retrieval-Augmented Generation","Dataset Creation","Model Editing","Supervised Fine-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Iman Barati, Mostafa Amiri, Heshaam Faili 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)의 특정 도메인 적응을 위한 고품질 SFT(Supervised FineTuning) 데이터셋 생성 의 어려움을 해결하는 것을 목표로 합니다. 특히, 기존 LLM의 내부 지식 부족 과 데이터 희"},{"id":"2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning","title":"[논문리뷰] UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning","excerpt":"Yongliang Shen이 arXiv에 게시한 'UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-16 13:16:41+0900","permalink":"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning","tags":["Review","GUI Automation","Reinforcement Learning","Semi-online RL","Offline RL","Online RL","Patch Module","Multi-turn Interaction","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengxi Lu, Jiabo Ye, Fei Tang, Yongliang Shen, Haiyang Xu, Ziwei Zheng, Weiming Lu, Ming Yan, Fei Huang, Jun Xiao, Yueting Zhuang 핵심 연구 목표 본 논문은 GUI(Graphical User Interface) 에이"},{"id":"2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model","title":"[논문리뷰] 3D Aware Region Prompted Vision Language Model","excerpt":"Xiaolong Li이 arXiv에 게시한 '3D Aware Region Prompted Vision Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model","tags":["Review","3D Vision","Vision-Language Models","Spatial Reasoning","Region Prompting","Multi-view Learning","Depth Estimation","Unified Representation","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: AnChieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li 핵심 연구 목표 본 논문은 단일 뷰 2D 이미지와 다중 뷰 3D 데이터를 공유된 시각 토큰 공간으로 연결하는 3Daware VisionLanguage Model (VLM) 인 SR3D 를 제안하여, 복잡한 3"},{"id":"2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving","title":"[논문리뷰] EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving","excerpt":"Shansan Gong이 arXiv에 게시한 'EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving","tags":["Review","Automated Theorem Proving","LLM","Test-Time Scaling","Chain-of-Thought","Reinforcement Learning","Efficiency Optimization","Token Cost","Sampling Cost","Dynamic CoT Switching"],"text":"링크: 논문 PDF로 바로 열기 저자: Mukai Li, Linfeng Song, Zhenwen Liang, Jiahao Xu, Shansan Gong, Qi Liu, Haitao Mi, Dong Yu 핵심 연구 목표 논문은 LLM 기반의 Automated Theorem Proving(ATP) 모델들이 ChainofThought (CoT) 추론 및 다중 샘"},{"id":"2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms","title":"[논문리뷰] Exact Coset Sampling for Quantum Lattice Algorithms","excerpt":"Yifan Zhang이 arXiv에 게시한 'Exact Coset Sampling for Quantum Lattice Algorithms' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms","tags":["Review","Quantum Algorithms","Lattice Problems","Coset Sampling","Quantum Fourier Transform (QFT)","Modular Arithmetic","Quantum Cryptography","Exact Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Zhang 핵심 연구 목표 본 논문은 최근 발표된 windowedQFT 양자 격자 알고리즘(Chen, 2024)의 논란이 있는 \"도메인 확장\" 단계(Step 9)에서 발생하는 주기성/지원 불일치 문제를 해결하는 것을 목표로 합니다. 알려지지 않은 오프셋을 정확히 상쇄하고, 정확하고 균일한 CRTcoset 상"},{"id":"2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation","title":"[논문리뷰] Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation","excerpt":"Lixin Xu이 arXiv에 게시한 'Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation","tags":["Review","3D Asset Generation","AI Pipeline","Generative AI","Game Development","Diffusion Models","Neural Modules","Retopology","UV Unwrapping"],"text":"링크: 논문 PDF로 바로 열기 저자: Lixin Xu, Shuhui Yang, Xinhai Liu, Yang Li, Biwen Lei 핵심 연구 목표 이 논문은 노동 집약적이고 전문화된 기존 3D 에셋 생성 워크플로우로 인한 게임 개발의 병목 현상을 해결하고자 합니다. 단일 이미지나 텍스트 설명으로부터 게임에 즉시 사용 가능한(gameready) 3D 에"},{"id":"2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge","title":"[논문리뷰] Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge","excerpt":"Wentao Zhang이 arXiv에 게시한 'Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge","tags":["Review","Multimodal Reasoning","Science AI","Caption-assisted Reasoning","SeePhys Challenge","Large Language Models","Visual Question Answering","Physics Problems","Cross-modal Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Wentao Zhang, Junbo Niu, Ruitao Wu, Hao Liang, zbhpku 핵심 연구 목표 본 논문은 인공지능 분야의 근본적인 도전 과제인 멀티모달 추론 의 한계를 극복하는 것을 목표로 합니다. 특히, 최첨단 GPT03 과 같은 모델도 시각 정보 통합에 어려움을 겪는 과학 분야의 멀티모달 시나리오"},{"id":"2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis","title":"[논문리뷰] Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis","excerpt":"Bo Liu이 arXiv에 게시한 'Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis","tags":["Review","Multiple Instance Learning","Hard Instance Mining","Computational Pathology","Whole Slide Images","Masked Learning","Siamese Network","Medical Image Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhao Tang, Sheng Huang, Heng Fang, Fengtao Zhou, Bo Liu, Qingshan Liu 핵심 연구 목표 기존 Multiple Instance Learning (MIL) 기반의 컴퓨터 병리학(CPath) 모델들이 기가픽셀 Whole Slide Images (WSIs)에서 쉽게 분"},{"id":"2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs","title":"[논문리뷰] Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs","excerpt":"Luca Benini이 arXiv에 게시한 'Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs","tags":["Review","LLM Compression","Quantization","Sparsification","Post-training Quantization","Hessian-based Optimization","Error Compensation","Low-bit LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Hang Guo, Yawei Li, Luca Benini 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 효율적인 배포를 위해 양자화(Quantization) 와 희소화(Sparsification) 를 동시에 적용하는 새로운 압축 방법을 제안합니다. 특히, 양자화가 요구하는 좁고 균일한 가중치 분포 와 희소화"},{"id":"2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization","title":"[논문리뷰] ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization","excerpt":"Litu Ou이 arXiv에 게시한 'ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization","tags":["Review","LLM Agents","Context Management","Summarization","ReAct","Reinforcement Learning","Web Search","Long-Horizon Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, et al. 핵심 연구 목표 이 논문은 기반 에이전트가 작업을 수행할 때 의 제한으로 인해 충분한 탐색이 불가능한 문제를 해결하고자 합니다. 패러다임에서 대화 기록이 빠르게 컨텍스트 한도를 초과하여 작업이 중단되는 것을 방지하고"},{"id":"2025-9-17-Scaling-Agents-via-Continual-Pre-training","title":"[논문리뷰] Scaling Agents via Continual Pre-training","excerpt":"Guangyu Li이 arXiv에 게시한 'Scaling Agents via Continual Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Scaling-Agents-via-Continual-Pre-training","tags":["Review","Agentic LLMs","Continual Pre-training","Deep Research Agents","Tool Use","Multi-step Reasoning","Data Synthesis","Scaling Laws"],"text":"링크: 논문 PDF로 바로 열기 저자: Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, et al. 핵심 연구 목표 본 논문은 기존의 에이전트 LLM 훈련 방법론(SFT, RL)이 복잡한 에이전트 태스크에서, 특히 오픈소스 구현체에서 저조한 성능을 보이"},{"id":"2025-9-17-Single-stream-Policy-Optimization","title":"[논문리뷰] Single-stream Policy Optimization","excerpt":"Zihan Ding이 arXiv에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Single-stream-Policy-Optimization","tags":["Review","Reinforcement Learning","LLM Optimization","Policy Gradient","Variance Reduction","Adaptive Sampling","Scalability","Agentic Systems","RLVR"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongwen Xu, Zihan Ding 핵심 연구 목표 본 논문은 LLM을 위한 기존 그룹 기반 정책 최적화 방식( GRPO 등)이 겪는 비효율성(퇴화 그룹으로 인한 학습 신호 손실)과 동기화 장벽으로 인한 확장성 문제를 해결하고자 합니다. 연구 목표는 이러한 한계를 극복하고 LLM 추론을 위한 안정적이고 효율적이"},{"id":"2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling","title":"[논문리뷰] Towards General Agentic Intelligence via Environment Scaling","excerpt":"Guangyu Li이 arXiv에 게시한 'Towards General Agentic Intelligence via Environment Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling","tags":["Review","Agentic AI","Environment Scaling","Function Calling","Tool Use","Large Language Models","Synthetic Data Generation","Supervised Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei"},{"id":"2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents","title":"[논문리뷰] WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents","excerpt":"Wenbiao Yin이 arXiv에 게시한 'WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents","tags":["Review","Agentic AI","Deep Research","Iterative Reasoning","Long-Horizon Tasks","Context Management","Data Synthesis","Tool-Augmented LLMs","Markov Decision Process"],"text":"링크: 논문 PDF로 바로 열기 저자: Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Hua"},{"id":"2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning","title":"[논문리뷰] WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning","excerpt":"Huifeng Yin이 arXiv에 게시한 'WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning","tags":["Review","Web Agents","Reinforcement Learning","Synthetic Data","Knowledge Graphs","LLMs","Supervised Fine-Tuning","Sim-to-Real Transfer","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Huifeng Yin, Zhongwang Zhang, Kuan Li, callanwu, xxwu 핵심 연구 목표 WebSailorV2는 오픈소스 웹 에이전트의 역량을 혁신적으로 향상시켜, 독점 시스템과의 성능 격차를 줄이는 것을 목표로 합니다. 특히 데이터 구성 및 확장 가능한 강화 학습(RL) 훈련의 두 가지 주요 "},{"id":"2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research","title":"[논문리뷰] WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research","excerpt":"Houquan Zhou이 arXiv에 게시한 'WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-17 13:16:01+0900","permalink":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research","tags":["Review","Open-Ended Deep Research","LLM Agents","Dynamic Outline","Evidence Acquisition","Hierarchical Writing","Memory Bank","State-of-the-Art","Supervised Fine-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, Jun Zhang, Jingren Zhou 핵심 연구 목표 본 논문은 AI 에이전트가 방대한 웹 스케일 정보를"},{"id":"2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam","title":"[논문리뷰] GenExam: A Multidisciplinary Text-to-Image Exam","excerpt":"Yu Qiao이 arXiv에 게시한 'GenExam: A Multidisciplinary Text-to-Image Exam' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam","tags":["Review","Text-to-Image Generation","Multidisciplinary","Benchmark","Evaluation","AGI","Reasoning","Scoring System","Visual Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaokai Wang, Penghao Yin, Xiangyu Zhao, Changyao Tian, Yu Qiao, Wenhai Wang, Jifeng Dai, Gen Luo 핵심 연구 목표 기존 텍스트투이미지(T2I) 벤치마크들이 일반적인 세계 지식이나 개념 설명에 치우쳐 엄격한 도면 시험 평가에 미흡하다는 문제점을"},{"id":"2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale","title":"[논문리뷰] Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale","excerpt":"Bernard Ghanem이 arXiv에 게시한 'Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale","tags":["Review","Arabic NLP","Instruction Tuning","Machine Translation","Large Language Models","FP8 Quantization","Data Bootstrapping","Model Merging","Language-Centric AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Hasan Abed Al Kader Hammoud, Mohammad Zbeeb, Bernard Ghanem 핵심 연구 목표 아랍어 고품질 명령어 데이터의 부족과 다국어 LLM에서 언어별 깊이의 불균형 문제를 해결하는 것을 목표로 합니다. 효율적인 번역튜닝 파이프라인 을 통해 아랍어 중심의 명령어 및 번역 모델(HALA"},{"id":"2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning","title":"[논문리뷰] Improving Context Fidelity via Native Retrieval-Augmented Reasoning","excerpt":"Xiangru Tang이 arXiv에 게시한 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning","tags":["Review","Context Fidelity","Retrieval-Augmented Generation (RAG)","Large Language Models (LLMs)","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Hallucination","Question Answering","In-context Retrieval","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Suyuchen Wang, Jinlin Wang, Xinyu Wang, Shiqi Li, Xiangru Tang, Sirui Hong, XiaoWen Chang, Chenglin Wu, Bang Liu 핵심 연구 목표 논문은 대규모 언어 모델(LLMs)이 제공된 컨텍스트에 대한 충실도(context fidelity)를"},{"id":"2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook","title":"[논문리뷰] MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook","excerpt":"Bowen Zhou이 arXiv에 게시한 'MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook","tags":["Review","Multimodal Reasoning","Large Language Models (LLMs)","Multimodal Large Language Models (MLLMs)","Visual Grounding","Visual Question Answering","Advertisement Video Analysis","Real-world Scenarios","Challenge Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou 핵심 연구 목표 논문은 MARS2 2025 Challenge 를 통해 멀티모달 기계 학습 및 LLM 분야의 발전을 촉진하는 것을 목표로 합니다. 특히, 기존 벤치마크의 한계를 넘어 실제 세계 시나리오와 도메"},{"id":"2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era","title":"[논문리뷰] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era","excerpt":"Zihao Dongfang이 arXiv에 게시한 'PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era","tags":["Review","Omnidirectional Vision","Embodied AI","Panoramic Perception","Multi-modal Learning","Dataset Development","Robot Navigation","Spatial Reasoning","System Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang 핵심 연구 목표 본 논문은 기존 핀홀(pinhole) 비전에 비해 연구가 뒤처진 옴니디렉셔널(omnidirectional) 비전의 잠재력을 발현하고, 데이터 병목 현상, 모델 역량 한계, 애플리케이션 공백과"},{"id":"2025-9-18-SAIL-VL2-Technical-Report","title":"[논문리뷰] SAIL-VL2 Technical Report","excerpt":"Zijian Kang이 arXiv에 게시한 'SAIL-VL2 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-SAIL-VL2-Technical-Report","tags":["Review","Vision-Language Model","Multimodal Understanding","Mixture-of-Experts","Progressive Training","Data Curation","Supervised Fine-tuning","Reinforcement Learning","SAIL-ViT"],"text":"링크: 논문 PDF로 바로 열기 저자: Zijian Kang, Yue Liao, Fangxun Shu, Yongjie Ye, Weijie Yin 핵심 연구 목표 본 논문은 포괄적인 멀티모달 이해 및 추론을 위한 개방형 비전언어 파운데이션 모델인 SAILVL2 를 소개합니다. 특히 2B 및 8B 파라미터 스케일에서 다양한 이미지 및 비디오 벤치마크에 걸쳐 최"},{"id":"2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning","title":"[논문리뷰] Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning","excerpt":"Zhou Yang이 arXiv에 게시한 'Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning","tags":["Review","Code Language Models","Machine Unlearning","Sensitive Memorization","Privacy","Gradient Ascent","Model Utility","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhou Yang, Di Wang, Zhikun Zhang, Yao Wan, Zhaoyang Chu 핵심 연구 목표 본 논문은 Code Language Models (CLMs) 에서 발생하는 민감한 훈련 데이터의 의도치 않은 기억(memorization) 문제를 해결하고자 합니다. 특히, 기존 데이터 중복 제거 및 차"},{"id":"2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs","title":"[논문리뷰] SteeringControl: Holistic Evaluation of Alignment Steering in LLMs","excerpt":"Zhun Wang이 arXiv에 게시한 'SteeringControl: Holistic Evaluation of Alignment Steering in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs","tags":["Review","LLM Alignment","Representation Steering","Benchmark","Behavioral Entanglement","Bias Mitigation","Harmful Generation","Hallucination Control","Modular Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang 핵심 연구 목표 대규모 언어 모델(LLM)의 정렬 조작(alignment steering) 방법론들을 총체적으로 평가 하는 것"},{"id":"2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning","title":"[논문리뷰] THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning","excerpt":"Yicheng Pan이 arXiv에 게시한 'THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning","tags":["Review","Mathematical Reasoning","Tool-Integrated Reasoning","Reinforcement Learning","Hierarchical Optimization","Self-Correction","Large Language Models","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yicheng Pan, Jiefeng Ma, Pengfei Hu, Zhenrong Zhang, Qikai Chang 핵심 연구 목표 대규모 언어 모델(LLM)이 수학적 추론, 특히 고정밀 수치 계산 및 형식적 기호 조작과 같은 작업에서 겪는 한계를 극복하는 것을 목표로 합니다. 기존 도구 통합 방법론이 가진 TIR 데"},{"id":"2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication","title":"[논문리뷰] Wan-Animate: Unified Character Animation and Replacement with Holistic Replication","excerpt":"Mingyang Huang이 arXiv에 게시한 'Wan-Animate: Unified Character Animation and Replacement with Holistic Replication' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-18 13:07:00+0900","permalink":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication","tags":["Review","Character Animation","Video Replacement","Diffusion Models","Transformer","DiT","Relighting LoRA","Holistic Replication","Open-Source"],"text":"링크: 논문 PDF로 바로 열기 저자: HumanAIGC Team, Tongyi Lab, Alibaba (Mingyang Huang, Siqi Hu, Li Hu, Xin Gao, Gang Cheng 등) 핵심 연구 목표 논문은 캐릭터 애니메이션과 교체를 위한 통합 프레임워크 를 제시하여, 동작, 표정, 환경 상호작용에 대한 총체적인 제어 를 고품질로 달성하"},{"id":"2025-9-19-AToken-A-Unified-Tokenizer-for-Vision","title":"[논문리뷰] AToken: A Unified Tokenizer for Vision","excerpt":"Mingze Xu이 arXiv에 게시한 'AToken: A Unified Tokenizer for Vision' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision","tags":["Review","Unified Visual Tokenizer","Multimodal AI","Transformer Architecture","4D Representation","Adversarial-free Training","Reconstruction","Semantic Understanding","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, Yinfei Yang 핵심 연구 목표 ATOKEN은 기존 시각 토크나이저들의 모달리티 및 태스크별 분절 문제를 해결하고, 이미지, 비디오, 3D 에셋 전"},{"id":"2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence","title":"[논문리뷰] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence","excerpt":"Qinghua Huang이 arXiv에 게시한 'EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence","tags":["Review","Vision-Language Models","Ultrasound Imaging","Medical Diagnosis","Mixture-of-Experts (MoE)","Instruction Tuning","Multimodal AI","Report Generation","VQA"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaoyin She, Ruifang Lu, Lida Chen, Wei Wang, Qinghua Huang 핵심 연구 목표 본 연구는 의사 전문성에 크게 의존하고 주관적이며 비효율적인 기존 초음파 진단의 한계를 극복하고, 일반적인 VLM(VisionLanguage Model) 의 초음파 의료 도메인 지식 부족 문제를 "},{"id":"2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation","title":"[논문리뷰] Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation","excerpt":"Kishan Panaganti이 arXiv에 게시한 'Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation","tags":["Review","Label-free Reinforcement Learning","LLMs","Self-improvement","Entropy Collapse","Novelty Reward","Test-Time RL","GRPO","Evolutionary Computing Principles"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu 핵심 연구 목표 논문은 LLM이 라벨이나 외부 평가 없이 스스로 개선하려는 라벨프리(lab"},{"id":"2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection","title":"[논문리뷰] FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection","excerpt":"Zhewei Zhang이 arXiv에 게시한 'FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection","tags":["Review","Change Detection","Remote Sensing","Frequency-Spatial Analysis","Wavelet Transform","Attention Mechanism","Gated Fusion","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongxiang Xie, Shuangxi Miao, Yuhan Jiang, Zhewei Zhang, Jing Yao, Member, IEEE, Xuecao Li, Jianxi Huang, Senior Member, IEEE, Pedram Ghamisi, Senior Member, IEEE 핵심 연구 목표 고해상도 "},{"id":"2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning","title":"[논문리뷰] FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning","excerpt":"Jiashuo Liu이 arXiv에 게시한 'FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning","tags":["Review","Financial LLMs","Agent Benchmarking","Open-domain Search","Financial Reasoning","Time-Sensitive Data","Multi-hop QA","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiashuo Liu, Jianpeng Jiao, Liang Hu, Wenhao Huang, zhangysk 핵심 연구 목표 본 연구는 LLM 기반 에이전트의 현실적인 금융 데이터 검색 및 추론 능력을 평가하기 위한 종단 간(endtoend) 벤치마크 의 부재를 해결하는 것을 목표로 합니다. 기존 벤치마크들이 검색 능"},{"id":"2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning","title":"[논문리뷰] FlowRL: Matching Reward Distributions for LLM Reasoning","excerpt":"Hengli Li이 arXiv에 게시한 'FlowRL: Matching Reward Distributions for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning","tags":["Review","Reinforcement Learning","Large Language Models","Reward Distribution Matching","GFlowNets","Mode Collapse","Diverse Reasoning","Flow-Balanced Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengli Li, Dinghuai Zhang, jayyoung0802, daixuancheng, xuekai 외 다수 핵심 연구 목표 대규모 언어 모델(LLM)의 강화 학습(RL) 추론에서 발생하는 모드 붕괴(mode collapse) 와 다양성 부족 문제를 해결하는 것을 목표로 합니다. 기존의 보상 최대화(rewa"},{"id":"2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs","title":"[논문리뷰] Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs","excerpt":"Katharina von der Wense이 arXiv에 게시한 'Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs","tags":["Review","LLM Evaluation","Multiple-Choice QA","Tokenization","Prompt Sensitivity","Accuracy","Calibration","Model Ranking"],"text":"링크: 논문 PDF로 바로 열기 저자: Mario SanzGuerrero, Minh Duc Bui, Katharina von der Wense 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 객관식 질문 답변(MCQA) 평가 시, 답변 레이블 직전의 공백 문자 토큰화 방식이 모델 성능에 미치는 영향을 규명하는 것을 목표로 합니다. 현재 표준화되지 않"},{"id":"2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks","title":"[논문리뷰] MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks","excerpt":"Xijun Gu이 arXiv에 게시한 'MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks","tags":["Review","Instruction-based Image Editing","Dataset","Multi-modal LLM","Image Generation","Style Transfer","Multi-task Learning","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingsong Li, Lin Liu, Hongjun Wang, Haoxing Chen, Xijun Gu, Shizhan Liu, Dong Gong, Junbo Zhao, Zhenzhong Lan, Jianguo Li 핵심 연구 목표 본 연구는 기존 지시 기반 이미지 편집(IBIE) 방법론의 한계, 특히 제한된 데이터"},{"id":"2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration","title":"[논문리뷰] Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration","excerpt":"Zhilin Wang이 arXiv에 게시한 'Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration","tags":["Review","LLMs","Specification Alignment","Test-Time Deliberation","Safety-Behavior Trade-off","ALIGN3","SPECBENCH","Prompt Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhilin Wang, Dongrui Liu, Xuyang Hu, Yafu Li, zzzhr97 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 시나리오별로 맞춤 설정된 동적 행동 및 안전 명세(spec)를 따르는 능력인 명세 정렬(Specification Alignment) 문제를 해결하는 것을 목표로 합니"},{"id":"2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems","title":"[논문리뷰] RecoWorld: Building Simulated Environments for Agentic Recommender Systems","excerpt":"Mingyuan Wu이 arXiv에 게시한 'RecoWorld: Building Simulated Environments for Agentic Recommender Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems","tags":["Review","Agentic Recommender Systems","Simulated Environments","LLM-driven Simulation","Multi-turn Interaction","Reinforcement Learning","User Retention","Instruction Following","Multi-agent Systems"],"text":"링크: 논문 PDF로 바로 열기 저자: Fei Liu, Xinyu Lin, Hanchao Yu, Mingyuan Wu, Jianyu Wang, Qiang Zhang, Zhuokai Zhao, Yinglong Xia, Yao Zhang, Weiwei Li, Mingze Gao, Qifan Wang, Lizhu Zhang, Benyu Zhang, Xiangju"},{"id":"2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation","title":"[논문리뷰] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation","excerpt":"SpaceProduct이 arXiv에 게시한 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation","tags":["Review","Vision-Language-Action (VLA) Model","Robot Manipulation","Human Demonstrations","Video Generative Pretraining","Ego-Centric Video","Trajectory Prediction","ActionVAE","Transformer"],"text":"링크: 논문 PDF로 바로 열기 Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, Xin Li 핵심 연구 목표 본 논문은 대규모 로봇 조작 데이터 부"},{"id":"2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data","title":"[논문리뷰] ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data","excerpt":"Zehao Li이 arXiv에 게시한 'ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data","tags":["Review","Computer Use Agents","Vision-Language Models","Cross-Platform Data","GUI Automation","Data Scaling","Open-Source","Task Completion","GUI Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoyang Liu, Jingjing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Xuan Dong, Yue Yu, Chenyu Lu, Y"},{"id":"2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation","title":"[논문리뷰] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation","excerpt":"Xihui Liu이 arXiv에 게시한 'Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation","tags":["Review","Autoregressive Models","Image Generation","Self-Supervised Learning","Visual Understanding","Masked Image Modeling","Contrastive Learning","Next-Token Prediction","LlamaGen"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaoyu Yue, Xihui Liu, Zidong Wang, Wanli Ouyang, Yuqing Wang, Lei Bai, Wenlong Zhang, Luping Zhou 핵심 연구 목표 본 논문은 자연어 처리에서 성공적인 자기회귀(Autoregressive, AR) 모델이 이미지 생성 시 고수준 시각적 의미 학"},{"id":"2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding","title":"[논문리뷰] Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding","excerpt":"Rynson W. H. Lau이 arXiv에 게시한 'Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding","tags":["Review","Spatio-Temporal Video Grounding","Multimodal Large Language Models","Zero-Shot Learning","Visual Grounding","Decomposed Spatio-Temporal Highlighting","Logit-Guided Re-attention","Temporal-Augmented Assembling"],"text":"링크: 논문 PDF로 바로 열기 저자: Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W.H. Lau 핵심 연구 목표 본 논문은 입력 텍스트 질의를 기반으로 비디오 내에서 대상의 시공간 튜브(spatiotemporal tube)를 찾아내는 시공간 비디오 그라운딩(STVG) 태스크에서, MLLM(Multimodal La"},{"id":"2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance","title":"[논문리뷰] WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance","excerpt":"Ruibo Li이 arXiv에 게시한 'WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-19 13:12:21+0900","permalink":"/ai/review/2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance","tags":["Review","Video Diffusion Models","3D/4D Generation","Training-Free Guidance","Camera Trajectory Control","Novel View Synthesis","Geometric Consistency","Inference-Time Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang 핵심 연구 목표 본 연구는 기존 비디오 확산 모델(VDM)이 3D/4D 작업에서 겪는 , , 의 한계를 해결하고자 합니다. 특히, VDM의 풍부한 사전 지식을 활용하여 를 가능하게 하면서도 및 을 유지하는 것을 목표"},{"id":"2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents","title":"[논문리뷰] From reactive to cognitive: brain-inspired spatial intelligence for embodied agents","excerpt":"Songming Liu이 arXiv에 게시한 'From reactive to cognitive: brain-inspired spatial intelligence for embodied agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","permalink":"/ai/review/2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents","tags":["Review","Spatial Cognition","Embodied Agents","Brain-inspired AI","Cognitive Map","Spatial Memory","MLLMs","Navigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shouwei Ruan, Liyuan Wang, Caixin Kang, Qihui Zhu, Songming Liu, Xingxing Wei, Hang Su 핵심 연구 목표 본 논문은 기존의 반응적(reactive) 접근 방식이 가진 공간 기억의 부재와 그로 인한 복잡한 실세계 환경에서의 일반화 및 적응성 부족 문제를 "},{"id":"2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench","title":"[논문리뷰] How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench","excerpt":"Jayanth Srinivasa이 arXiv에 게시한 'How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","permalink":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench","tags":["Review","LLM Agents","Tool Use","Function Calling","Input Reformulation","Dynamic Environments","τ-bench","Context Engineering","Multi-Agent Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Venkatesh Mishra, Jayanth Srinivasa, Amir Saeidi, Satyam Raj, Mutsumi Nakamura, Gaowen Liu, Ali Payani, Chitta Baral 핵심 연구 목표 본 논문은 복잡하고 동적인 다중 턴 환경(예: τbench )에서 대규모 언어 모델(LLM) "},{"id":"2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes","title":"[논문리뷰] No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes","excerpt":"Danijel Skočaj이 arXiv에 게시한 'No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","permalink":"/ai/review/2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes","tags":["Review","Surface Defect Detection","Anomaly Detection","Mixed Supervision","Deep Learning","Industrial Inspection","Unified Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Blaž Rolih, Matic Fučka, Danijel Skočaj 핵심 연구 목표 본 논문은 기존 표면 결함 감지 모델들이 특정 감독 시나리오에 제한되거나 다양한 데이터 주석 유형(비지도, 약지도, 혼합, 완전 지도)에 적응하기 어려운 문제를 해결하고자 합니다. 모든 감독 체제에서 사용 가능한 모든 데이터 주석을"},{"id":"2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning","title":"[논문리뷰] PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning","excerpt":"Yuewei Zhang이 arXiv에 게시한 'PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","permalink":"/ai/review/2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning","tags":["Review","Reinforcement Learning","Critic-Free RL","Agentic Reasoning","Policy Optimization","Large Language Models (LLMs)","Advantage Estimation","Group Sampling","Static Value Estimation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang, Hao Wang 핵심 연구 목표 본 연구는 에이전트 추론(agentic reasoning)을 위한 criticfree 강화 학습 방법론, 특히 그룹 정책(group policies)의 한계를 "},{"id":"2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables","title":"[논문리뷰] T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables","excerpt":"Yu Zhao이 arXiv에 게시한 'T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","permalink":"/ai/review/2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables","tags":["Review","Table-to-Report Generation","Large Language Models (LLMs)","Benchmark Dataset","Industrial Applications","Table Reasoning","Evaluation Metrics","Real-world Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Zhao, Sishi Xiong, Kaiwen Wei, Changzai Pan, Jie Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 테이블 추론 능력을 산업 애플리케이션에 적용하는 데 있어, 테이블 정보를 포괄적인 보고서로 변환하는 핵심 과제를 해결하고자 합니다. 특히, 복잡하고 다양한 테"},{"id":"2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat","title":"[논문리뷰] UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat","excerpt":"Omartificial-Intelligence-Space이 arXiv에 게시한 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-02 13:01:41+0900","permalink":"/ai/review/2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat","tags":["Review","Arabic LLM","UI-level Evaluation","ALLaM 34B","HUMAIN Chat","Dialectal Arabic","LLM as a Judge","Safety Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Omer Nacar 핵심 연구 목표 본 연구는 영어 중심 LLM들이 아랍어의 언어적, 문화적 뉘앙스를 포착하는 데 어려움을 겪는 문제를 해결하기 위해 개발된 ALLaM 34B 모델에 대한 포괄적인 UI레벨 평가를 수행하는 것을 목표로 합니다. HUMAIN Chat을 통해 실제 사용자 경험을 반영한 평가를 진행하여, A"},{"id":"2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning","title":"[논문리뷰] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning","excerpt":"Jiangmiao이 arXiv에 게시한 'A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning","tags":["Review","Robotics","Reinforcement Learning (RL)","Vision-Language-Action (VLA) Models","Reward Modeling","Human-in-the-Loop","Dense Rewards","Generalization","Autoregressive Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang, Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, Jiangmiao Pang 핵심 연구 목표 로봇의 실세계 강화 학습(RL)에서 희소하고 수작업으로 제작된 보상 및"},{"id":"2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue","title":"[논문리뷰] Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue","excerpt":"Hui Zhang이 arXiv에 게시한 'Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue","tags":["Review","Embodied AI","Human-Robot Interaction","Multi-turn Dialogue","Instruction Following","Vision-Language Models","Diffusion Models","Ambiguity Resolution","Low-level Actions"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingyao Lin, Xinghao Zhu, Tianyi Lu, Sicheng Xie, Hui Zhang, Xipeng Qiu, Zuxuan Wu, YuGang Jiang 핵심 연구 목표 현재 VLA(VisionLanguageAction) 기반 로봇 이 모호한 지시를 처리하지 못하고 수동적으로 명령을 실행하는 한계를"},{"id":"2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent","title":"[논문리뷰] BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent","excerpt":"Jiahui Yang이 arXiv에 게시한 'BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent","tags":["Review","GUI Agent","Human-GUI Interaction","Cognitive Modeling","Reinforcement Learning","Multimodal Large Language Models","Attention Mechanisms","Action Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaojie Zhang, Ruoceng Zhang, Pei Fu, Shaokang Wang, Jiahui Yang, Xin Du, Shiqi Cui, Bin Qin, Ying Huang, Zhenbo Luo, Jian Luan 핵심 연구 목표 AI 기반 GUI 에이전트의 상호작용 논리가 인간의 자연스러운 GUI 소통"},{"id":"2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model","title":"[논문리뷰] BaseReward: A Strong Baseline for Multimodal Reward Model","excerpt":"jianfeipan이 arXiv에 게시한 'BaseReward: A Strong Baseline for Multimodal Reward Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model","tags":["Review","Multimodal Reward Model","MLLM Alignment","RLHF","Reward Head Architecture","Data Curation","Ensemble Methods","BaseReward"],"text":"링크: 논문 PDF로 바로 열기 저자: YiFan Zhang, Haihua Yang, Huanyu Zhang, Yang Shi, Zezhou Chen, Haochen Tian, Chaoyou Fu, Kai Wu, Bo Cui, Xu Wang, Jianfei Pan, Haotian Wang, Zhang Zhang, Liang Wang 핵심 연구 목표 본 연구"},{"id":"2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems","title":"[논문리뷰] Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems","excerpt":"Hung-yi Lee이 arXiv에 게시한 'Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems","tags":["Review","Instruction-Guided TTS","Expressive Speech Synthesis","Human Perception","Subjective Evaluation","Controllability","Instruction Following","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: YiCheng Lin, HuangCheng Chou, TzuChieh Wei, KuanYu Chen, Hungyi Lee 핵심 연구 목표 이 논문은 ITTS (InstructionGuided TexttoSpeech) 시스템에서 사용자의 자연어 명령(natural language prompts)과 청취자의 음성 지각(l"},{"id":"2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification","title":"[논문리뷰] Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification","excerpt":"Wenyu Wang이 arXiv에 게시한 'Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification","tags":["Review","Generative Modeling","Representation Learning","Classification","Unified Framework","Latent Space","Flow Matching","Deep Learning","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zinan Lin, Junyi Zhu, Enshu Liu, Xuefei Ning, Wenyu Wang, Sergey Yekhanin 핵심 연구 목표 본 논문은 생성 모델링(Generative Modeling) , 표현 학습(Representation Learning) , 분류(Classification) 라는 세 가지"},{"id":"2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation","title":"[논문리뷰] Lynx: Towards High-Fidelity Personalized Video Generation","excerpt":"Linjie Luo이 arXiv에 게시한 'Lynx: Towards High-Fidelity Personalized Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation","tags":["Review","Personalized Video Generation","Diffusion Transformer","Identity Preservation","Video Synthesis","Adapter Networks","Facial Recognition","Cross-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Shen Sang, Tiancheng Zhi, Tianpei Gu, Jing Liu, Linjie Luo 핵심 연구 목표 본 논문은 단일 입력 이미지로부터 고품질의 개인화된 비디오를 합성 하는 모델인 Lynx를 제시하며, 특히 높은 신원 보존 을 목표로 합니다. 기존 비디오 생성 모델의 한계인 대상의 신원 불일치 문제"},{"id":"2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer","title":"[논문리뷰] MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer","excerpt":"jialingt이 arXiv에 게시한 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer","tags":["Review","Multimodal LLM","Hybrid Tokenizer","Text-to-Image Generation","Visual Question Answering","Autoregressive Model","Diffusion Decoder","Unified Architecture","Model Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Min"},{"id":"2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes","title":"[논문리뷰] RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes","excerpt":"Narendra Ahuja이 arXiv에 게시한 'RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes","tags":["Review","Camera Parameter Optimization","Dynamic Scenes","RGB-Only Supervision","Structure from Motion","Outlier Robustness","3D Gaussian Splatting","Two-stage Optimization","Point Tracking"],"text":"링크: 논문 PDF로 바로 열기 저자: Fang Li, Hao Zhang, Narendra Ahuja 핵심 연구 목표 본 연구는 동적 장면에서 카메라 파라미터(초점 거리, 회전, 번역)를 효율적이고 정확하게 최적화하는 것을 목표로 합니다. 기존 COLMAP 방법의 긴 런타임과 동적 장면에서의 GT(Ground Truth) 모션 마스크 의존성 한계를 극복하고"},{"id":"2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation","title":"[논문리뷰] RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation","excerpt":"Steven Liu이 arXiv에 게시한 'RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation","tags":["Review","Code Generation","LLMs","Repository Planning","Graph-based Representation","Software Engineering","Agent Frameworks","Scalable Codebase"],"text":"링크: 논문 PDF로 바로 열기 저자: Steven Liu, Xin Zhang, Kyleraha, Cipherxzc, Luo2003 핵심 연구 목표 대규모 언어 모델(LLMs)이 함수 및 파일 수준 코드 생성에는 뛰어나지만, 완전한 저장소(repository)를 처음부터 생성 하는 데는 한계가 있습니다. 이는 제안 및 구현 단계 전반에 걸친 일관되고 신뢰할"},{"id":"2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation","title":"[논문리뷰] SPATIALGEN: Layout-guided 3D Indoor Scene Generation","excerpt":"Yongsen Mao이 arXiv에 게시한 'SPATIALGEN: Layout-guided 3D Indoor Scene Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation","tags":["Review","3D Scene Generation","Layout Guidance","Diffusion Models","Multi-view Synthesis","Synthetic Dataset","Indoor Environments","Gaussian Splatting","Semantic Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan 핵심 연구 목표 고품질의 3D 실내 환경 모델을 생성하는 기존 방식의 시간 소모성 및 제한된 다양성 문제를 해결하고, 시각적 품질, 다양성, "},{"id":"2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents","title":"[논문리뷰] Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents","excerpt":"Chao Zhang이 arXiv에 게시한 'Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents","tags":["Review","Role-playing Agents (RPAs)","Multimodal AI","Video Understanding","Large Language Models (LLMs)","Dataset Creation","Dynamic Role Profiles","Adaptive Temporal Sampling","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xueqiao Zhang, Chao Zhang, Jingtao Xu, Yifan Zhu, Xin Shi, Yi Yang, Yawei Luo 핵심 연구 목표 기존 Roleplaying Agents (RPAs) 가 정적인 역할 프로필에만 의존하여 인간의 동적인 지각 능력을 포착하지 못하는 한계를 극복하는 것입니다. 비디오"},{"id":"2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers","title":"[논문리뷰] WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers","excerpt":"Karun Kumar이 arXiv에 게시한 'WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-22 13:11:29+0900","permalink":"/ai/review/2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers","tags":["Review","ASR","Domain Adaptation","Text-Only Training","Transformer","Variational Autoencoder","Deep Supervision","Whisper","Encoder-Decoder Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Akshat Pandey,¹ Karun Kumar,¹ Raphael Tang² 핵심 연구 목표 본 논문은 Whisper 와 같은 사전 훈련된 최신 ASR(Automatic Speech Recognition) 모델이 미지의 도메인 어휘와 발화를 처리할 때 발생하는 성능 저하 문제를 해결하고자 합니다. 특히, 목표 도메인"},{"id":"2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations","title":"[논문리뷰] ARE: Scaling Up Agent Environments and Evaluations","excerpt":"Matteo Bettini이 arXiv에 게시한 'ARE: Scaling Up Agent Environments and Evaluations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations","tags":["Review","Agent Environments","Agent Evaluation","LLM Agents","Asynchronous Systems","Reinforcement Learning","Tool Use","Multi-agent Collaboration","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Pierre Andrews, Amine Benhalloum, Matteo Bettini, Virginie Do, Romain Froger, et al. 핵심 연구 목표 논문은 AI 에이전트 개발 및 평가를 위한 확장 가능한 연구 플랫폼인 Meta Agents Research Environments (ARE) 를 소개하"},{"id":"2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels","title":"[논문리뷰] Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels","excerpt":"Qi Zhang이 arXiv에 게시한 'Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels","tags":["Review","Supervised Fine-Tuning (SFT)","Large Language Models (LLMs)","Model Knowledge","Closed-Book Question Answering (CBQA)","Parameter Restoration","Kullback-Leibler Divergence","Knowledge Forgetting"],"text":"링크: 논문 PDF로 바로 열기 저자: Junjie Ye, Yuming Yang, Yang Nan, Shuo Li, Qi Zhang, Tao Gui, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan 핵심 연구 목표 본 논문은 LLM에서 SFT가 모델의 지식에 미치는 영향 이 충분히 이해되지 않고 있다는 문제"},{"id":"2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing","title":"[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?","excerpt":"Jaeho Lee이 arXiv에 게시한 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing","tags":["Review","Auditory Knowledge","Large Language Models","Multimodal Reasoning","Benchmark","Chain-of-Thought","Auditory Imagination","Text-only Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyunjong Ok, Suho Yoo, Hyeonjun Kim, Jaeho Lee 핵심 연구 목표 언어 모델(LLMs)이 오디오 입력 없이 텍스트만으로 청각적 상식과 추론 능력을 이해하는 데 부족함을 해결하고자 합니다. 이 격차를 해소하기 위해 청각 지식을 평가하는 AuditoryBench++ 벤치마크를 제시하고, "},{"id":"2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces","title":"[논문리뷰] ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces","excerpt":"Jiafeng Xu이 arXiv에 게시한 'ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces","tags":["Review","Robotics","Parallel Manipulator","Robotic Wrist","Confined Space Manipulation","Kinematics","Anthropomorphic Robot","Robot Design"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawen Tian, Liqun Huang, Zhongren Cui, Jingchao Qiao, Jiafeng Xu, Xiao Ma, Zeyu Ren 핵심 연구 목표 이 논문은 기존 로봇 손목이 좁고 제한된 공간에서의 작업 시 겪는 유연성, 컴팩트함, 동적 응답성 한계를 해결하고자 합니다. 특히, 유연하고 인간과 유"},{"id":"2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects","title":"[논문리뷰] CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects","excerpt":"Hang Yu이 arXiv에 게시한 'CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects","tags":["Review","Code Review","LLMs","Benchmark","Python Projects","End-to-End Evaluation","Context-Awareness","Software Engineering","LLM-as-a-Judge"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanyang Guo, Xunjin Zheng, Zihan Liao, Hang Yu, Peng DI, Ziyin Zhang, HongNing Dai 핵심 연구 목표 기존 LLM 기반 코드 리뷰(CR) 벤치마크가 겪는 \"현실성 격차\"(reality gap) 문제를 해결하고자 합니다. 이는 , , 에서 기인하며, 논문은 "},{"id":"2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment","title":"[논문리뷰] ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment","excerpt":"Yue Ma이 arXiv에 게시한 'ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment","tags":["Review","Video Object Editing","Training-Free","Diffusion Transformers","Rectified Flow","Adaptive Context Enrichment","Guidance Responsiveness","Temporal Consistency","Image-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiyang Chen¹, Xuanhua He2,†, Xiujun Ma¹†, Yue Ma2,† 핵심 연구 목표 훈련 없이 비디오 객체 편집(삽입, 교체, 삭제)을 수행할 때 발생하는 정확한 인버전 실패와 부적절한 특성 대체로 인한 문맥적 충돌 문제를 해결하고, 특히 Diffusion Transformer (DiT) 기반"},{"id":"2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models","title":"[논문리뷰] Cross-Attention is Half Explanation in Speech-to-Text Models","excerpt":"Luisa Bentivogli이 arXiv에 게시한 'Cross-Attention is Half Explanation in Speech-to-Text Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models","tags":["Review","Cross-attention","Speech-to-Text (S2T)","Explainable AI (XAI)","Saliency Maps","Feature Attribution","Transformer","Context Mixing","Correlation"],"text":"링크: 논문 PDF로 바로 열기 저자: Sara Papi, Dennis Fucci, Marco Gaido, Matteo Negri, Luisa Bentivogli 핵심 연구 목표 본 논문은 S2T 모델에서 교차 어텐션(crossattention) 점수가 입력출력 의존성을 얼마나 잘 설명하는지 체계적으로 분석합니다. 특히, 교차 어텐션이 입력출력 정렬의 대리"},{"id":"2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context","title":"[논문리뷰] DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context","excerpt":"Maunendra Sankar Desarkar이 arXiv에 게시한 'DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context","tags":["Review","Cultural Adaptation","Large Language Models","Indian Culture","Dataset Creation","CSI","Human Evaluation","LLM Evaluation","Cultural Bias"],"text":"링크: 논문 PDF로 바로 열기 저자: Pramit Sahoo, Maharaj Brahma, Maunendra Sankar Desarkar 핵심 연구 목표 대규모 언어 모델(LLMs)이 서구 문화에 편향된 훈련 데이터로 인해 문화적 적합성과 지역적 다양성 측면에서 부족하다는 문제를 해결하고자 합니다. 특히 인도의 다양한 문화적 배경을 반영하는 문화 텍스트 "},{"id":"2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process","title":"[논문리뷰] DiffusionNFT: Online Diffusion Reinforcement with Forward Process","excerpt":"Qinsheng Zhang이 arXiv에 게시한 'DiffusionNFT: Online Diffusion Reinforcement with Forward Process' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process","tags":["Review","Diffusion Models","Reinforcement Learning","Online RL","Flow Matching","Forward Process","CFG-free","Image Generation","Negative-Aware FineTuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiwen Zheng, Huayu Chen, Haotian Ye, Haoxiang Wang, Qinsheng Zhang, Kai Jiang, Hang Su, Stefano Ermon, Jun Zhu, MingYu Liu 핵심 연구 목표 본 논문은 확산 모델의 온라인 강화 학습(RL) 적용 시 발생하는 고유한 문제점,"},{"id":"2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering","title":"[논문리뷰] EpiCache: Episodic KV Cache Management for Long Conversational Question Answering","excerpt":"Minsik Cho이 arXiv에 게시한 'EpiCache: Episodic KV Cache Management for Long Conversational Question Answering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering","tags":["Review","KV Cache Management","Long Conversational QA","LLMs","Memory Efficiency","Episodic Clustering","Block Prefill Eviction","Sensitivity-aware Allocation"],"text":"링크: 논문 PDF로 바로 열기 저자: Minsoo Kim, Arnav Kundu, HanByul Kim, Richa Dixit, Minsik Cho 핵심 연구 목표 대규모 언어 모델(LLM) 기반의 장기 대화형 질문 답변(LongConvQA) 시스템에서 KV 캐시의 메모리 사용량이 대화 길이에 따라 선형적으로 증가 하는 문제를 해결하는 것이 목표입니다. "},{"id":"2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions","title":"[논문리뷰] FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions","excerpt":"tengdai722이 arXiv에 게시한 'FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions","tags":["Review","Large Reasoning Models","LLM Evaluation","Multimodal AI","Reasoning Behaviors","Hallucination","Contamination-Free","AI Safety","Instruction Following"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Qin, Chen Yue, Teng Dai, JingShu Zheng, Miguel Hu Chen, Richeng Xuan, et al. 핵심 연구 목표 본 논문은 최신 대규모 추론 모델(LRMs) 을 자동으로 검증 가능한 텍스트 및 시각 질문 에 대해 오염 없는(contaminationfree) 방식으로 "},{"id":"2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem","title":"[논문리뷰] From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem","excerpt":"Ahmed E. Hassan이 arXiv에 게시한 'From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem","tags":["Review","Open-Source AI","License Compliance","License Drift","AI Supply Chain","Hugging Face","GitHub","LicenseRec","Legal Risk"],"text":"링크: 논문 PDF로 바로 열기 저자: James Jewitt, Hao Li, Bram Adams, Gopi Krishnan Rajbahadur, Ahmed E. Hassan 핵심 연구 목표 오픈 소스 AI 생태계 내에서 데이터셋, 모델, 그리고 이를 활용하는 소프트웨어 애플리케이션 전반에 걸쳐 발생하는 라이선스 충돌과 '라이선스 드리프트'의 정도를 정량적"},{"id":"2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature","title":"[논문리뷰] From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature","excerpt":"Bin Cui이 arXiv에 게시한 'From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature","tags":["Review","Reinforcement Learning","LLMs","Policy Optimization","Token Heterogeneity","Adaptive Sampling","Advantage Redistribution","Asymmetric Clipping","Entropy-based RL"],"text":"링크: 논문 PDF로 바로 열기 저자: Zheng Liu, Mengjie Liu, Siwei Wen, Mengzhang Cai, Bin Cui, Conghui He, Wentao Zhang 핵심 연구 목표 기존 RLHF (Reinforcement Learning from Human Feedback) 알고리즘이 LLM의 추론 과정에서 토큰의 다양한 역할을 무"},{"id":"2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning","title":"[논문리뷰] GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning","excerpt":"Hou Pong Chan이 arXiv에 게시한 'GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning","tags":["Review","Multimodal Large Language Models (MLLMs)","Geometric Reasoning","Visual Perception","Reinforcement Learning (RL)","Two-stage Training","GeoPQA Benchmark","Perceptual Bottleneck"],"text":"링크: 논문 PDF로 바로 열기 저자: Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Deli Zhao, Anh Tuan Luu, Yu Rong 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLM)이 기하학적 추론과 같은 시각 집중 태스크에서 자주 발생하는 환각 현상 과 부정확한 추론 문제를 해결하고자"},{"id":"2025-9-23-LIMI-Less-is-More-for-Agency","title":"[논문리뷰] LIMI: Less is More for Agency","excerpt":"happyZYM이 arXiv에 게시한 'LIMI: Less is More for Agency' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency","tags":["Review","AI Agency","Data Curation","Less Is More","Agentic Intelligence","Foundation Models","Evaluation Benchmark","Efficiency Principle","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: happyZYM, evanlin2570, weizhihao1, mhjiang0408, YangXiaonlp 핵심 연구 목표 현재 AI 에이전트 개발이 대규모 데이터가 더 나은 에이전시를 가져온다는 기존 스케일링 법칙을 따르는 한계를 극복하는 것을 목표로 합니다. LIMI (Less Is More for Intellig"},{"id":"2025-9-23-Mano-Report","title":"[논문리뷰] Mano Report","excerpt":"Minghui Wu이 arXiv에 게시한 'Mano Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Mano-Report","tags":["Review","GUI Agent","Multi-modal Foundation Model","Reinforcement Learning","Supervised Fine-tuning","Simulated Environment","Data Generation","Error Recovery","Web Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu 외 다수 핵심 연구 목표 본 논문은 시각적 복잡성, 동적 환경, 다단계 추론 요구사항으로 인해 어려운 GUI 상호작용 자동화 문제를 해결하는 것을 목표로 합니다. 기존 VisionLanguage Model (VL"},{"id":"2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction","title":"[논문리뷰] MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction","excerpt":"Xintao Chen이 arXiv에 게시한 'MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction","tags":["Review","Multimodal Retrieval","Late Interaction","Meta Tokens","Matryoshka Representation Learning","Test-Time Scaling","Vision-Language Models","Dense Retrieval","Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zilin Xiao, Qi Ma, Mengting Gu, Jason Chen, Xintao Chen, Vicente Ordonez, Vijai Mohan 핵심 연구 목표 기존 멀티모달 검색 방법론들이 단일 벡터 임베딩의 표현력 한계에 부딪히거나, 다수의 토큰으로 인한 다중 벡터 방식의 계산 비용 문제로 확장성에 제약을"},{"id":"2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","title":"[논문리뷰] OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models","excerpt":"Pengze Zhang이 arXiv에 게시한 'OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","tags":["Review","Video Insertion","Diffusion Models","Diffusion Transformers","Mask-Free","Data Augmentation","Progressive Training","Preference Optimization","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinshu Chen, Xinghui Li, Xu Bai, Tianxiang Ma, Pengze Zhang, Zhuowei Chen, Gen Li, Lijie Liu, Songtao Zhao, Bingchuan Li, Qian He 핵심 연구 목표 본 논문은 기존 비디오 삽입 모델의 복잡한 제어 신호(예: 마스크, 포"},{"id":"2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models","title":"[논문리뷰] QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models","excerpt":"Jae-Joon Kim이 arXiv에 게시한 'QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models","tags":["Review","LLM Fine-tuning","Quantization-Aware PEFT","Walsh-Hadamard Transform","Sparse Adaptation","Low-bit Quantization","Parameter-Efficient Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, JaeJoon Kim 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 효율적인 배포를 위해 양자화인식(QuantizationAware) PEFT (ParameterEfficient FineTuning) 방법을 개발하"},{"id":"2025-9-23-Qwen3-Omni-Technical-Report","title":"[논문리뷰] Qwen3-Omni Technical Report","excerpt":"Lhma-aslp이 arXiv에 게시한 'Qwen3-Omni Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Qwen3-Omni-Technical-Report","tags":["Review","Multimodal Model","Thinker-Talker Architecture","Mixture-of-Experts","Low-latency","Audio Understanding","Cross-modal Reasoning","State-of-the-Art","Real-time Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Qwen Team 핵심 연구 목표 본 논문은 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티 전반에 걸쳐 단일 멀티모달 모델(Qwen3Omni) 이 기존 단일 모달 모델과 비교하여 성능 저하 없이 최첨단 성능을 유지 하는 것을 목표로 합니다. 또한, 교차 모달 추론 능력 과 실시간 시청각 상호작용 을 향상시키는 "},{"id":"2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning","title":"[논문리뷰] Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning","excerpt":"Damien Sileo이 arXiv에 게시한 'Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning","tags":["Review","LLM Reasoning","Symbolic AI","Reinforcement Learning","Procedural Content Generation","Verifiable Rewards","Adaptive Curricula","First-Order Logic","PDDL Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Valentin Lacombe, Valentin Quesnel, and Damien Sileo 핵심 연구 목표 본 연구는 LLM의 기초적인 기호 추론 능력을 향상시키기 위한 확장 가능한 RLVR (Reinforcement Learning with Verifiable Rewards) 환경인 Reasoning Core 를"},{"id":"2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning","title":"[논문리뷰] SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning","excerpt":"Zhaopeng Tu이 arXiv에 게시한 'SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning","tags":["Review","Process Reward Models","Monte Carlo Annotation","Noise Denoising","Robust Learning","Self-Supervision","Mathematical Reasoning","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuyang Ding, Xinyu Shi, Juntao Li, Xiaobo Liang, Zhaopeng Tu, Min Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 추론 과정을 평가하는 Process Reward Models (PRMs) 개발의 핵심 난제인 높은 비용의 사람 주석 데이터 와 Mon"},{"id":"2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks","title":"[논문리뷰] SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?","excerpt":"Yannis Yiming He이 arXiv에 게시한 'SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks","tags":["Review","AI Agents","Software Engineering","LLMs","Code Generation","Benchmark","Contamination Resistance","Long-Horizon Tasks","Enterprise Software"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundur"},{"id":"2025-9-23-Synthetic-bootstrapped-pretraining","title":"[논문리뷰] Synthetic bootstrapped pretraining","excerpt":"Emmanuel Candès이 arXiv에 게시한 'Synthetic bootstrapped pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Synthetic-bootstrapped-pretraining","tags":["Review","Language Model Pretraining","Synthetic Data","Inter-document Correlation","Data Augmentation","Transformer","Bootstrapping","Concept Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zitong Yang, Aonan Zhang, Hong Liu, Tatsunori Hashimoto, Emmanuel Candès, Chong Wang, Ruoming Pang 핵심 연구 목표 본 논문은 대규모 언어 모델(LM) 사전 훈련 시 고품질 텍스트 데이터 고갈 문제를 해결하고, 표준 사전 훈련에서 간과되는 문"},{"id":"2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs","title":"[논문리뷰] TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs","excerpt":"Shaohui Jiao이 arXiv에 게시한 'TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs","tags":["Review","Video LLMs","Temporal Grounding","Reinforcement Learning","Off-policy Learning","Reward Shaping","Chain-of-Thought","Multimodal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, MingMing Cheng 핵심 연구 목표 이 논문은 비디오 시간적 접지(temporal grounding) 작업에서 멀티모달 대규모 언어 모델(MLLMs) 의 효율성을 개선하는 것"},{"id":"2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications","title":"[논문리뷰] Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications","excerpt":"Fatma Betül Terzioğlu이 arXiv에 게시한 'Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications","tags":["Review","Hallucination Detection","Retrieval Augmented Generation","Large Language Models","Turkish NLP","Token Classification","ModernBERT","Low-Resource Languages"],"text":"링크: 논문 PDF로 바로 열기 저자: Selva Taş, Mahmut El Huseyni, Özay Ezerceli, Reyhan Bayraktar, Fatma Betül Terzioğlu 핵심 연구 목표 대규모 언어 모델(LLMs)의 환각(hallucination) 문제를 해결하고, 특히 형태학적으로 복잡한 터키어 RAG(RetrievalAugmente"},{"id":"2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering","title":"[논문리뷰] Understanding Embedding Scaling in Collaborative Filtering","excerpt":"Yonghui Yang이 arXiv에 게시한 'Understanding Embedding Scaling in Collaborative Filtering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering","tags":["Review","Collaborative Filtering","Embedding Scaling","Noise Robustness","Recommender Systems","Graph Neural Networks","Self-supervised Learning","Performance Degradation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuangzhuang He, Kaiyu Zhou, Haoyue Bai, Fengbin Zhu, Yonghui Yang 핵심 연구 목표 협업 필터링 모델에서 임베딩 차원을 확장할 때 발생하는 성능 변화를 이해하고, 기존에 알려진 '단일 봉우리(singlepeak)' 현상을 넘어서는 새로운 스케일링 패턴을 발견하는 것이"},{"id":"2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery","title":"[논문리뷰] VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery","excerpt":"Shiya Huang이 arXiv에 게시한 'VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery","tags":["Review","Multimodal Large Language Models","Visual Question Answering","Reinforcement Learning","Cultural Heritage","Ancient Greek Pottery","Supervised Fine-Tuning","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinchao Ge, Tengfei Cheng, Biao Wu, Zeyu Zhang, Shiya Huang 핵심 연구 목표 본 연구는 고대 그리스 도자기에 대한 전문가 수준의 추론 능력을 갖춘 MLLM(Multimodal Large Language Models) 에이전트를 개발하는 것을 목표로 합니다. 일반적인 MLL"},{"id":"2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models","title":"[논문리뷰] VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models","excerpt":"Sunghyun Cho이 arXiv에 게시한 'VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models","tags":["Review","3D Scene Generation","Video Diffusion","Image Diffusion","Generative Models","Computer Graphics","Temporal Consistency","Sparse Anchor Views"],"text":"링크: 논문 PDF로 바로 열기 저자: Sunghyun Cho, Janghyeok Han, Geonung Kim 핵심 연구 목표 본 논문은 조잡한(coarse) 3D 지오메트리, 카메라 궤적, 그리고 참조 이미지를 사용하여 고품질 3D 장면 비디오를 생성하는 문제를 해결하고자 합니다. 기존 비디오 확산 모델이 복잡한 장면에서 시각적 품질, 움직임, 시간적 "},{"id":"2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs","title":"[논문리뷰] When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs","excerpt":"Anand Mishra이 arXiv에 게시한 'When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-23 13:36:03+0900","permalink":"/ai/review/2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs","tags":["Review","VQA","Small VLMs","Large VLMs","Knowledge Transfer","Pseudo-labeling","Label-Free Learning","Model Parity Alignment","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Abhirama Subramanyam Penamakuri, Navlika Singh, Piyush Arora, Anand Mishra 핵심 연구 목표 본 논문은 시각 질문 답변(VQA) 태스크에서 Small VisionLanguage Models (SVLMs) 의 성능을 향상시키는 것을 목표로 합니다. 이는 Large"},{"id":"2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR","title":"[논문리뷰] Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR","excerpt":"Zeina Aldallal이 arXiv에 게시한 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR","tags":["Review","Arabic OCR","Vision-Language Model","Fine-tuning","Document Understanding","Markdown Conversion","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Khalil Hennara, Muhammad Hreden, Mohamed Motasim Hamed, Ahmad Bastati, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan 핵심 연구 목표 본 논문은 필기체 스크립트, 다양한 글꼴, 발음 기호, 우좌향 텍스트 방향성으로 인해 어려운"},{"id":"2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching","title":"[논문리뷰] CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching","excerpt":"Rui Qian이 arXiv에 게시한 'CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching","tags":["Review","Flow Matching","Conditional Generative Models","Reparameterization","Mode Collapse","Image Generation","Latent Space Alignment","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Chen Chen, Pengsheng Guo, Liangchen Song, Jiasen Lu, Rui Qian, Xinze Wang, TsuJui Fu, Wei Liu, Yinfei Yang, Alex Schwing 핵심 연구 목표 조건부 생성 모델에서 속도 네트워크가 데이터 분포의 질량 이동(mass transpor"},{"id":"2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies","title":"[논문리뷰] Do You Need Proprioceptive States in Visuomotor Policies?","excerpt":"Yushen Liang이 arXiv에 게시한 'Do You Need Proprioceptive States in Visuomotor Policies?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies","tags":["Review","Visuomotor Policies","Spatial Generalization","Imitation Learning","Proprioception","State-free Policies","Robot Manipulation","End-Effector Control","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Juntu Zhao, Wenbo Lu, Di Zhang, Yufeng Liu, Yushen Liang 핵심 연구 목표 본 연구는 로봇의 시각운동 정책(visuomotor policies)에서 고유 수용성 상태(proprioceptive states)의 필요성을 재평가하고, 기존 상태 기반 정책이 학습 궤적에 과적합되어"},{"id":"2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction","title":"[논문리뷰] GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction","excerpt":"Jin Zheng이 arXiv에 게시한 'GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction","tags":["Review","Surface Reconstruction","Sparse Voxels","Geometric Accuracy","Neural Radiance Fields","3D Gaussian Splatting","Monocular Depth","Voxel Uncertainty"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahe Li, Jiawei Zhang, Youmin Zhang, Xiao Bai, Jin Zheng, Xiaohan Yu, Lin Gu 핵심 연구 목표 본 논문은 기존 3D Gaussian Splatting (3DGS) 기반 표면 재구성 방법론의 한계, 즉 초기화 시 점군(point clouds) 에 대한 의존성,"},{"id":"2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis","title":"[논문리뷰] HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis","excerpt":"Dan Xu이 arXiv에 게시한 'HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis","tags":["Review","Novel View Synthesis","3D Gaussian Splatting (3DGS)","Neural Radiance Fields (NeRF)","Memory Efficiency","High-Quality Rendering","Hybrid Representation","Real-time Rendering"],"text":"링크: 논문 PDF로 바로 열기 저자: Zipeng Wang, Dan Xu 핵심 연구 목표 3D Gaussian Splatting (3DGS) 의 실시간 고품질 렌더링 장점은 유지하면서, 뷰의존적 효과 및 이방성 모양 모델링으로 인한 막대한 메모리 오버헤드 를 해결하는 것을 목표로 합니다. 기존 Neural Field 기반 압축 방식이 고주파 공간 변화를 "},{"id":"2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation","title":"[논문리뷰] Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation","excerpt":"Jianbin Zheng이 arXiv에 게시한 'Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation","tags":["Review","Multimodal AI","Acceleration Framework","Speculative Decoding","Diffusion Distillation","Unified Models","Text-to-Image Generation","Image Editing","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanzuo Lu, Xin Xia, Manlin Zhang, Huafeng Kuang, Jianbin Zheng, Yuxi Ren, Xuefeng Xiao 핵심 연구 목표 통합 멀티모달 모델에서 확산 디노이징과 자기회귀 디코딩의 반복적인 프로세스로 발생하는 상당한 계산 오버헤드 를 해결하는 것이 주 목표입니다. Hyp"},{"id":"2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects","title":"[논문리뷰] Large Language Models Discriminate Against Speakers of German Dialects","excerpt":"Katharina von der Wense이 arXiv에 게시한 'Large Language Models Discriminate Against Speakers of German Dialects' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects","tags":["Review","Large Language Models","Bias","German Dialects","Sociolinguistics","Stereotypes","Implicit Association Test","Decision Making"],"text":"링크: 논문 PDF로 바로 열기 저자: Minh Duc Bui, Carolin Holtermann, Valentin Hofmann, Anne Lauscher, Katharina von der Wense 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 독일 방언 사용자에 대한 사회적 고정관념을 반영하고 강화하는지 탐구하는 것을 목표로 합니다. 특히,"},{"id":"2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation","title":"[논문리뷰] Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation","excerpt":"Yifeng Jiang이 arXiv에 게시한 'Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation","tags":["Review","Generative AI","3D Scene Reconstruction","Video Diffusion Models","Self-Distillation","3D Gaussian Splatting","Dynamic 4D Generation","Monocular Input"],"text":"링크: 논문 PDF로 바로 열기 저자: Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanc"},{"id":"2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization","title":"[논문리뷰] MAPO: Mixed Advantage Policy Optimization","excerpt":"Xuankun Rong이 arXiv에 게시한 'MAPO: Mixed Advantage Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization","tags":["Review","Reinforcement Learning","Foundation Models","Policy Optimization","Advantage Function","Trajectory Certainty","Multimodal Reasoning","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, et al. 핵심 연구 목표 본 연구는 파운데이션 모델의 추론 성능 향상을 위한 기존 강화 학습(RL) 방법론, 특히 Group Relative Policy Optimization (GRPO) 이 겪는 \""},{"id":"2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe","title":"[논문리뷰] MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe","excerpt":"Wenshuo Ma이 arXiv에 게시한 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe","tags":["Review","MLLM Efficiency","Multimodal Transformer","3D-Resampler","Document AI","Hybrid Reinforcement Learning","Video Understanding","Efficient Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, et al. 핵심 연구 목표 본 논문은 급속히 발전하는 Multimodal Large Language Models (MLLMs)의 고질적인 훈련 및 추론 효율성 문제를 해결하는 것을 목표로 합니다. 특히, 시"},{"id":"2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation","title":"[논문리뷰] OpenGVL - Benchmarking Visual Temporal Progress for Data Curation","excerpt":"Viktor Petrenko이 arXiv에 게시한 'OpenGVL - Benchmarking Visual Temporal Progress for Data Curation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation","tags":["Review","Robotics Data Curation","Visual Temporal Progress","Generative Value Learning (GVL)","Vision-Language Models (VLMs)","Benchmark","Task Progress Prediction","Value-Order Correlation (VOC)"],"text":"링크: 논문 PDF로 바로 열기 저자: Paweł Budzianowski, Emilia Wiśnios, Gracjan Góral, Igor Kulakov, Viktor Petrenko, Krzysztof Walas 핵심 연구 목표 로봇 공학 분야의 데이터 부족 문제를 해결하고, 대규모 로봇 데이터셋을 자동으로 주석 및 큐레이션할 수 있는 도구의 필요성을 강"},{"id":"2025-9-24-Reinforcement-Learning-on-Pre-Training-Data","title":"[논문리뷰] Reinforcement Learning on Pre-Training Data","excerpt":"Evander Yang이 arXiv에 게시한 'Reinforcement Learning on Pre-Training Data' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data","tags":["Review","Reinforcement Learning","Pre-training","Large Language Models","Self-supervised Learning","Scaling Laws","Next-segment Reasoning","Reward Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang 핵심 연구 목표 논문은 대규모 언어 모델(LLM)의 훈련 시 발생하는 컴퓨팅 자원의 기하급수적 증가와 고품질 텍스트 데이터의 유한한 성장 사이의 불균형 문제를 해결하고자 합니다. 인간의 어노테이션에 의존하지 않고"},{"id":"2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction","title":"[논문리뷰] VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction","excerpt":"So Fukuda이 arXiv에 게시한 'VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction","tags":["Review","Multimodal LLMs","Video Understanding","Geospatial Reasoning","Temporal Reasoning","Travel Itinerary Reconstruction","Benchmark","Agent System","VLOG"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, YiChia Chen, Hiroyuki Otomo, Hiroki Ouchi, D"},{"id":"2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction","title":"[논문리뷰] VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction","excerpt":"Haoxiao Wang이 arXiv에 게시한 'VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction","tags":["Review","3D Gaussian Splatting","Novel View Synthesis","Voxel-Aligned Prediction","Feed-Forward Reconstruction","Multi-View Consistency","Scene Representation","Computer Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang 핵심 연구 목표 기존 FeedForward 3D Gaussian Splatting (3"},{"id":"2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT","title":"[논문리뷰] What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT","excerpt":"Anthony Hartshorn이 arXiv에 게시한 'What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT","tags":["Review","Chain-of-Thought","Reasoning Effectiveness","Large Reasoning Models","Failed-Step Fraction","Test-time Scaling","Reasoning Graph","Model Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn 핵심 연구 목표 본 논문은 대규모 추론 모델(LRMs)에서 효과적인 CoT(ChainofThought) 추론의 특성을 규명하는 것을 목표로 합니다. 특히, 기존의 \"길수록 좋다\"는 CoT 길이 "},{"id":"2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications","title":"[논문리뷰] Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications","excerpt":"Genady Beryozkin이 arXiv에 게시한 'Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-24 13:14:19+0900","permalink":"/ai/review/2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications","tags":["Review","Remote Sensing","Zero-Shot Learning","Multimodal Models","Multi-spectral Imagery","Gemini 2.5","Prompt Engineering","Land Cover Classification","Pseudo-Image"],"text":"링크: 논문 PDF로 바로 열기 저자: Genady Beryozkin, Maxim Neumann, Dahun Kim, Yotam Gigi, Ganesh Mallya, Tomer Shekel, Anelia Angelova 핵심 연구 목표 본 논문은 RGB 전용 이미지로 훈련된 범용 대규모 멀티모달 모델(LMM) 이 원격 감지 분야에서 널리 사용되는 다중 스펙"},{"id":"2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO","title":"[논문리뷰] Advancing Speech Understanding in Speech-Aware Language Models with GRPO","excerpt":"Avihu이 arXiv에 게시한 'Advancing Speech Understanding in Speech-Aware Language Models with GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO","tags":["Review","Speech-Aware Language Models","SALLMs","GRPO","Reinforcement Learning","Speech Understanding","Spoken Question Answering","Automatic Speech Translation","BLEU Metric"],"text":"링크: 논문 PDF로 바로 열기 저자: Avishai Elmakies, Hagai Aronowitz, Nimrod Shabtay, Eli Schwartz, Ron Hoory, Avihu Dekel 핵심 연구 목표 본 논문은 GRPO (Group Relative Policy Optimization) 기반의 방법을 도입하여 SpeechAware Large La"},{"id":"2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning","title":"[논문리뷰] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning","excerpt":"Tianyu Wang이 arXiv에 게시한 'EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning","tags":["Review","Unified Multimodal Model","In-Context Learning","Image and Video Editing","Video Generation","Full Self-Attention","Rotary Positional Embedding","Cross-Modal Knowledge Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu 핵심 연구 목표 이 "},{"id":"2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations","title":"[논문리뷰] EmbeddingGemma: Powerful and Lightweight Text Representations","excerpt":"Marksherwood이 arXiv에 게시한 'EmbeddingGemma: Powerful and Lightweight Text Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations","tags":["Review","Text Embeddings","Lightweight Models","Encoder-Decoder","Knowledge Distillation","Model Souping","Quantization","Multilingual","Gemma"],"text":"링크: 논문 PDF로 바로 열기 저자: Marksherwood, osanseviero, ssmoot, SindhuRaghuram97, hschechter 및 EmbeddingGemma 팀 핵심 연구 목표 이 연구의 주요 목표는 강력하면서도 경량화된 오픈 소스 텍스트 임베딩 모델인 EmbeddingGemma 를 개발하는 것입니다. 특히, 500M 미만의 파라"},{"id":"2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines","title":"[논문리뷰] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines","excerpt":"Yanfang이 arXiv에 게시한 'LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines","tags":["Review","Large Language Models","Generative AI","Academic Disciplines","LLM Applications","Review","Cross-disciplinary Research","Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanfang (Fanny) Ye, Zheyuan Zhang, Tianyi Ma, et al. (교신저자: Yanfang (Fanny) Ye, Brett Savoie, Daniel Slate, Nitesh Chawla) 핵심 연구 목표 이 논문은 최첨단 거대 언어 모델(LLM) 과 이들이 다양한 학문 분야(인문학, 법"},{"id":"2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation","title":"[논문리뷰] Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation","excerpt":"Zhe Lin이 arXiv에 게시한 'Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation","tags":["Review","Multimodal AI","Masked Diffusion Models","Image Understanding","Image Generation","Image Editing","Object Grounding","ElasticMoT","Self-reflection"],"text":"링크: 논문 PDF로 바로 열기 저자: Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen 핵심 연구 목표 본 논문은 기존 멀티모달 Masked Diffusion Model (MDM)의 한계를 극복하고, 이미지 이해, 객체 접지, 이미지 편집, 고해상도(102"},{"id":"2025-9-25-Logics-Parsing-Technical-Report","title":"[논문리뷰] Logics-Parsing Technical Report","excerpt":"Fan Yang이 arXiv에 게시한 'Logics-Parsing Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-Logics-Parsing-Technical-Report","tags":["Review","Document Parsing","Large Vision-Language Models (LVLM)","Reinforcement Learning (RL)","Layout Analysis","Reading Order","Supervised Fine-Tuning (SFT)","HTML Annotation","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangyang Chen, Shuzhao Li, Xiuwen Zhu, Yongfan Chen, Fan Yang, Cheng Fang, Lin Qu, Xiaoxiao Xu, Hu Wei, Minggang Wu 핵심 연구 목표 본 논문은 기존 LVLM이 복잡한 문서 레이아웃 및 읽기 순서 처리에서 겪는 한계를 극복하고,"},{"id":"2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub","title":"[논문리뷰] On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub","excerpt":"Hajimu Iida이 arXiv에 게시한 'On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub","tags":["Review","Agentic Coding","AI Agents","Large Language Models","GitHub Pull Requests","Software Engineering","Empirical Study","Code Generation","Software Development"],"text":"링크: 논문 PDF로 바로 열기 저자: Miku Watanabe, Hao Li, Yutaro Kashiwa, Brittany Reid, Hajimu Iida, Ahmed E. Hassan 핵심 연구 목표 이 논문은 자율형 AI 에이전트(Claude Code) 가 생성한 GitHub Pull Request(PR)의 실질적인 유용성과 수용도 를 실증적으로 조사"},{"id":"2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","title":"[논문리뷰] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation","excerpt":"Yiming Huang이 arXiv에 게시한 'PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","tags":["Review","Video Generation","Physics-Grounded","Controllable Generation","Diffusion Models","Point Cloud Trajectories","Material Simulation","Generative Physics"],"text":"링크: 논문 PDF로 바로 열기 저자: Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou, Yuan Liu, Jiatao Gu, Lingjie Liu 핵심 연구 목표 기존 비디오 생성 모델들이 겪는 물리적 현실성 부족과 3D 제어의 한계를 극복하는 것을 목표로 합니다. 논문은 물리적 매개변수와 외부 힘을 명시적으로 제"},{"id":"2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought","title":"[논문리뷰] SIM-CoT: Supervised Implicit Chain-of-Thought","excerpt":"Yuhang Cao이 arXiv에 게시한 'SIM-CoT: Supervised Implicit Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought","tags":["Review","Implicit Reasoning","Chain-of-Thought","LLM","Latent Space","Supervised Learning","Model Stability","Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin. 핵심 연구 목표 Implicit ChainofThought (CoT) 모델은 토큰 효율성에도 불구하고, 명시적 CoT 대비 지속적인 성능 격차와 "},{"id":"2025-9-25-Video-models-are-zero-shot-learners-and-reasoners","title":"[논문리뷰] Video models are zero-shot learners and reasoners","excerpt":"rgeirhos이 arXiv에 게시한 'Video models are zero-shot learners and reasoners' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-25 13:08:16+0900","permalink":"/ai/review/2025-9-25-Video-models-are-zero-shot-learners-and-reasoners","tags":["Review","Video Models","Zero-shot Learning","Visual Reasoning","Foundation Models","Generative AI","Perception","Manipulation","Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos 핵심 연구 목표 본 논문은 비디오 모델이 대규모 언어 모델(LLM)이 언어 이해 분야"},{"id":"2025-9-26-AutoIntent-AutoML-for-Text-Classification","title":"[논문리뷰] AutoIntent: AutoML for Text Classification","excerpt":"Denis Kuznetsov이 arXiv에 게시한 'AutoIntent: AutoML for Text Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-AutoIntent-AutoML-for-Text-Classification","tags":["Review","AutoML","Text Classification","Intent Classification","Transformer Embeddings","Out-of-Scope Detection","Multi-label Classification","Few-shot Learning","Sklearn-like Interface"],"text":"링크: 논문 PDF로 바로 열기 저자: Alekseev Ilya, Solomatin Roman, Rustamova Darina, Kuznetsov Denis 핵심 연구 목표 본 논문은 기존 AutoML 프레임워크가 임베딩 모델 선택, 다중 레이블 분류, OOS(OutofScope) 감지, 퓨샷(Fewshot) 학습 과 같은 NLP 특정 과제를 포괄적으로 지"},{"id":"2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback","title":"[논문리뷰] BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback","excerpt":"Dongha Lee이 arXiv에 게시한 'BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback","tags":["Review","Search-Augmented LLMs","Personalization","Benchmark","Diagnostic Feedback","User History","Evaluation Framework","RAG"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyunseo Kim, Sangam Lee, Kwangwook Seo, Dongha Lee 핵심 연구 목표 본 논문은 검색 증강 대규모 언어 모델(LLMs)의 개인화 능력 평가에 대한 체계적인 벤치마크 부재 문제를 해결하고자 합니다. 사용자의 다양한 정보 요구와 선호하는 전달 방식을 LLM이 얼마나 효과적으로 반영하는"},{"id":"2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information","title":"[논문리뷰] Behind RoPE: How Does Causal Mask Encode Positional Information?","excerpt":"Yeyun Gong이 arXiv에 게시한 'Behind RoPE: How Does Causal Mask Encode Positional Information?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information","tags":["Review","Transformer Decoder","Causal Mask","Positional Encoding","RoPE","Attention Mechanism","Length Generalization","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi 핵심 연구 목표 본 논문은 Transformer 디코더 에서 Rotary Positional Embeddings (RoPE) 와 같은 명시적인 위치 인코딩 외에 인과 마스크(causal mask) 가 "},{"id":"2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance","title":"[논문리뷰] Blueprints of Trust: AI System Cards for End to End Transparency and Governance","excerpt":"Roman Zhukov이 arXiv에 게시한 'Blueprints of Trust: AI System Cards for End to End Transparency and Governance' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance","tags":["Review","AI Governance","Transparency","AI System Card","Hazard-Aware System Card","Data Provenance","AI Safety","AI Risk Management","ISO/IEC 42001"],"text":"링크: 논문 PDF로 바로 열기 저자: Huzaifa Sidhpurwala, Emily Fox, Garth Mollett, Florencio Cano Gabarda, Roman Zhukov 핵심 연구 목표 본 논문은 AI 시스템의 개발 및 배포 과정에서 투명성과 책임성을 강화하기 위한 새로운 프레임워크인 HazardAware System Card (HASC"},{"id":"2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning","title":"[논문리뷰] CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning","excerpt":"Wenping Hu이 arXiv에 게시한 'CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Large Language Models","Policy Optimization","PPO","Entropy Control","Gradient Clipping","Exploration-Exploitation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou 핵심 연구 목표 본 논문은 LLM (Large Language Model) 을 위한 강화 학습(RL) 과정에서 정책 엔트로피(policy entrop"},{"id":"2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling","title":"[논문리뷰] CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling","excerpt":"Yushi Bai이 arXiv에 게시한 'CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling","tags":["Review","3D Anime Hairstyle","Autoregressive Modeling","Control Points","Parametric Representation","Transformer","Generative AI","Dataset (AnimeHair)","Computer Graphics"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuze He, Yanning Zhou, Wang Zhao, Jingwen Ye, Yushi Bai, Kaiwen Xiao, YongJin Liu, Zhongqian Sun, and Wei Yang 핵심 연구 목표 본 연구는 기존 사실적인 헤어 모델링 기법으로는 다루기 어려운, 고도로 양식화된 3D 애니메이션 헤어스타"},{"id":"2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving","title":"[논문리뷰] Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving","excerpt":"Hang Zhao이 arXiv에 게시한 'Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving","tags":["Review","Autonomous Driving","Vision-Language-Action Models","Discrete Diffusion","Reflection Mechanism","Trajectory Generation","Safety Constraints","Imitation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengxiang Li, Yinan Zheng, Yue Wang, Huimin Wang, Hang Zhao, Jingjing Liu, Xianyuan Zhan, Kun Zhan, Xianpeng Lang 핵심 연구 목표 본 논문은 자율주행 시스템에서 기존 모방 학습 기반 VLA(VisionLanguageAction) "},{"id":"2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition","title":"[논문리뷰] Does FLUX Already Know How to Perform Physically Plausible Image Composition?","excerpt":"Chen Zhao이 arXiv에 게시한 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition","tags":["Review","Image Composition","Diffusion Models","Training-Free","Physically Plausible","FLUX","Adapter","Guidance","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams WaiKin Kong 핵심 연구 목표 본 연구는 복잡한 조명, 그림자, 물 반사 등 물리적으로 사실적인 이미지 합성 을 사전 훈련된 텍스트투이미지(T2I) 확산 모델 을 활용하여 훈련 없이 수행"},{"id":"2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets","title":"[논문리뷰] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets","excerpt":"Bowen Zhang이 arXiv에 게시한 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets","tags":["Review","3D Generation","Controllable Generation","Multi-modal Conditioning","Diffusion Models","Point Clouds","Voxels","Bounding Boxes","Skeletons","Hunyuan3D"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Y"},{"id":"2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands","title":"[논문리뷰] Interactive Recommendation Agent with Active User Commands","excerpt":"Xueyang Feng이 arXiv에 게시한 'Interactive Recommendation Agent with Active User Commands' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands","tags":["Review","Interactive Recommendation","Large Language Models","Multi-Agent System","Natural Language Processing","Knowledge Distillation","User Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiakai Tang, Yujie Luo, Xunke Xi, Fei Sun, Xueyang Feng, Sunhao Dai, Chao Yi, Dian Chen, Zhujin Gao, Yang Li, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang, Bo Zheng 핵심 연구 목표 본 논문은 기존"},{"id":"2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model","title":"[논문리뷰] MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model","excerpt":"Hung-yi Lee이 arXiv에 게시한 'MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model","tags":["Review","Speech Emotion Recognition","Source-Free Unsupervised Domain Adaptation","Large Audio-Language Models","Label Fusion","Mutual Information","API-Only Models","Domain Mismatch"],"text":"링크: 논문 PDF로 바로 열기 저자: HsiaoYing Huang, YiCheng Lin, Hungyi Lee 핵심 연구 목표 논문은 소스 도메인 데이터가 없고, 강력한 LALM(Large AudioLanguage Model) 이 API 를 통해서만 접근 가능한 현실적인 SFUDA(SourceFree Unsupervised Domain Adaptation"},{"id":"2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources","title":"[논문리뷰] MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources","excerpt":"Jing Wang이 arXiv에 게시한 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources","tags":["Review","Multimodal Reasoning","Reinforcement Learning","Variance-Aware Sampling","Gradient Vanishing","Data Curation","Chain-of-Thought","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Zhiqiang Hu, Lidong Bing, Deli Zhao, Wei Lu, Yu Rong, Aixin Sun, Shijian Lu 핵심 연구 목표"},{"id":"2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning","title":"[논문리뷰] MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning","excerpt":"Junyan Zhang이 arXiv에 게시한 'MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning","tags":["Review","Video Temporal Reasoning","Reinforcement Learning","Process Supervision","Dynamic Time Warping","Multimodal Large Language Models","Video State Prediction","Reward Hacking"],"text":"링크: 논문 PDF로 바로 열기 저자: Sicheng Tao, Jungang Li, Yibo Yan, Junyan Zhang, Yubo Gao, Hanqian Li, ShuHang Xun, Yuxuan Fan, Hong Chen, Jianxiang He, Xuming Hu 핵심 연구 목표 비디오 기반 MLLM(Multimodal Large Language "},{"id":"2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer","title":"[논문리뷰] Quantized Visual Geometry Grounded Transformer","excerpt":"Yuqi Li이 arXiv에 게시한 'Quantized Visual Geometry Grounded Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer","tags":["Review","Quantization","Post-Training Quantization","3D Reconstruction","Visual Transformer","Model Compression","Efficient Inference","Hadamard Rotation","Calibration Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Weilun Feng, Haotong Qin, Mingqiang Wu, Chuanguang Yang, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu 핵심 연구 목표 대규모 Visual Geometry Grounded "},{"id":"2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution","title":"[논문리뷰] Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution","excerpt":"Jinjie Gu이 arXiv에 게시한 'Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution","tags":["Review","Multi-Agent System","Browser Automation","Web Reconnaissance","Tool Generation","Task Execution","Self-Evolving AI","LLM/VLM","VisualWebArena"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiwen He, Zhiwei Wang, Chenyi Zhuang, Jinjie Gu 핵심 연구 목표 본 논문은 실세계 웹 페이지에서 멀티턴, 장기적 궤적(longhorizon trajectories) 을 따르는 작업 수행 시 기존 브라우저 에이전트의 행동 시퀀싱 혼란 과 과도한 시행착오 문제를 해결하는 것을 목표로"},{"id":"2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies","title":"[논문리뷰] Residual Off-Policy RL for Finetuning Behavior Cloning Policies","excerpt":"Pieter Abbeel이 arXiv에 게시한 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies","tags":["Review","Reinforcement Learning (RL)","Behavior Cloning (BC)","Residual Learning","Off-Policy RL","Robot Manipulation","Real-World Robotics","High-DoF Systems","Sample Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Lars Ankile, Zhenyu Jiang, Rocky Duan, Guanya Shi, Pieter Abbeel, Anusha Nagabandi 핵심 연구 목표 본 논문은 행동 복제(BC) 기반 정책의 한계(데이터 품질, 수동 데이터 수집, 성능 포화)와 실제 로봇에서의 직접적인 강화 학습(RL)의 어려움(샘플 비"},{"id":"2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows","title":"[논문리뷰] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows","excerpt":"Yi-Zhe Song이 arXiv에 게시한 'SD3.5-Flash: Distribution-Guided Distillation of Generative Flows' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows","tags":["Review","Generative AI","Image Generation","Diffusion Models","Rectified Flow","Model Distillation","Few-Step Generation","Computational Efficiency","Prompt Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Hmrishav Bandyopadhyay, Reshinth Adithyan, Rahim Entezari, Jim Scott, YiZhe Song, Varun Jampani 핵심 연구 목표 본 논문은 최첨단 생성 모델, 특히 Rectified Flow 모델 의 높은 연산 요구량으로 인해 발생하는 접근성 문제를 해결하고자"},{"id":"2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning","title":"[논문리뷰] ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning","excerpt":"Yu Li이 arXiv에 게시한 'ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning","tags":["Review","Mathematical Reasoning","Large Reasoning Models (LRMs)","Difficulty Scaling","Data Augmentation","Supervised Fine-Tuning (SFT)","Problem Generation","Solution Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu 핵심 연구 목표 본 논문은 복잡한 추론 능력을 향상시키기 위해 어려운 수학 문제 의 생성을 확장하는 효율적인 파이프라인인 ScaleDiff 를 제안합"},{"id":"2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent","title":"[논문리뷰] SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent","excerpt":"Siyuan Huang이 arXiv에 게시한 'SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent","tags":["Review","3D Scene Synthesis","Agentic Framework","LLMs","Self-Reflection","Tool-Use","Physical Plausibility","Iterative Refinement","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yandan Yang, Baoxiong Jia, Shujie Zhang, Siyuan Huang 핵심 연구 목표 이 논문은 기존 3D 장면 합성 방법론들이 고정된 카테고리, 부족한 객체 디테일, 물리적 불일치, 복잡한 사용자 지시와의 낮은 정합성 등의 한계를 가지는 문제를 해결하고자 합니다. 시각적으로 사실적이고, 물"},{"id":"2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines","title":"[논문리뷰] SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines","excerpt":"Jiabei Xiao이 arXiv에 게시한 'SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines","tags":["Review","Scientific Reasoning","Foundation Models","Multi-modal Learning","Cross-domain Generalization","Chain-of-Thought","Reinforcement Learning","Scientific Discovery","Molecular Design"],"text":"링크: 논문 PDF로 바로 열기 저자: Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, et al. 핵심 연구 목표 이 논문은 이질적인 과학적 표현과 자연어를 통합하여 다양한 과학 분야에 걸친 복잡한 과학적 추론을 수행하는 최초의 과학 추론 대규모 언어 모델(LLM) 인 SciReasoner 를 제안합니다. 기존의 전문 "},{"id":"2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation","title":"[논문리뷰] Seedream 4.0: Toward Next-generation Multimodal Image Generation","excerpt":"Yunpeng Chen이 arXiv에 게시한 'Seedream 4.0: Toward Next-generation Multimodal Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation","tags":["Review","Multimodal Image Generation","Diffusion Transformer","VAE","Image Editing","Text-to-Image","Model Acceleration","Human Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunpeng Chen, Team Seedream, Cakeyan, wuwx, wujie10 핵심 연구 목표 본 논문은 텍스트투이미지(T2I) 합성, 이미지 편집, 다중 이미지 합성 기능을 단일 프레임워크 내에서 통합하는 효율적이고 고성능의 차세대 멀티모달 이미지 생성 시스템 Seedream 4.0 을 개발하는 것을 "},{"id":"2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models","title":"[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models","excerpt":"Javad Lavaei이 arXiv에 게시한 'StyleBench: Evaluating thinking styles in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models","tags":["Review","Large Language Models","Reasoning Strategies","Prompt Engineering","LLM Evaluation","Benchmark","Thinking Styles","Scaling Laws","Meta-Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei 핵심 연구 목표 본 연구는 LLM이 사용하는 추론 전략, 즉 '사고 방식'이 모델 아키텍처 및 태스크 유형과 어떻게 상호작용하는지에 대한 이해 부족을 해결하는 것을 목표로 합니다. 다양한 태스크와 모델 전반"},{"id":"2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment","title":"[논문리뷰] The Unanticipated Asymmetry Between Perceptual Optimization and Assessment","excerpt":"Du Chen이 arXiv에 게시한 'The Unanticipated Asymmetry Between Perceptual Optimization and Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment","tags":["Review","Perceptual Optimization","Image Quality Assessment (IQA)","Adversarial Training","Discriminators","Super-Resolution","Fidelity Metrics","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiabei Zhang, Qi Wang, Siyu Wu, Du Chen, Tianhe Wu 핵심 연구 목표 본 논문은 지각적 최적화(perceptual optimization)를 위한 손실 함수와 이미지 품질 평가(IQA) 지표 간의 상관관계 및 GAN(Generative Adversarial Network) Disc"},{"id":"2025-9-26-Thinking-Augmented-Pre-training","title":"[논문리뷰] Thinking Augmented Pre-training","excerpt":"Furu Wei이 arXiv에 게시한 'Thinking Augmented Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Thinking-Augmented-Pre-training","tags":["Review","Large Language Models (LLMs)","Pre-training","Data Augmentation","Reasoning","Data Efficiency","Thinking Trajectories"],"text":"링크: 논문 PDF로 바로 열기 저자: Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei 핵심 연구 목표 본 논문은 훈련 시 과 이라는 문제를 해결하고자 합니다. 으로 기존 텍스트 데이터를 보강하여 을 대폭 개선하고 을 향상시키는 것이 주된 연구 목표입니다. 핵심 방법론 저자들은 이라는 방법론을 제안합니다. "},{"id":"2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification","title":"[논문리뷰] Thinking While Listening: Simple Test Time Scaling For Audio Classification","excerpt":"Mert Pilanci이 arXiv에 게시한 'Thinking While Listening: Simple Test Time Scaling For Audio Classification' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification","tags":["Review","Audio Classification","Test-Time Scaling","Reasoning Traces","Large Language Models (LLMs)","Transformer Architectures","Zero-shot Reasoning","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Prateek Verma, Mert Pilanci 핵심 연구 목표 본 논문은 오디오 분류 성능 향상을 위해 신경망 모델이 \"듣는 동안 생각하는(thinking while listening)\" 능력을 갖추도록 하는 프레임워크를 제안합니다. 특히, LLM의 추론 능력 에서 영감을 받아 오디오 분류 파이프라인에 추론을 통합"},{"id":"2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning","title":"[논문리뷰] Tree Search for LLM Agent Reinforcement Learning","excerpt":"Xiangxiang Chu이 arXiv에 게시한 'Tree Search for LLM Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning","tags":["Review","LLM Agents","Reinforcement Learning","Tree Search","Policy Optimization","Preference Learning","Sparse Rewards","Multi-turn Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu 핵심 연구 목표 본 논문은 LLM 에이전트의 장기 및 멀티턴 태스크에서 발생하는 희소한 보상(sparse supervision) 문제와 과도한 롤아웃 예산(rollout budget) 소비를"},{"id":"2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them","title":"[논문리뷰] TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them","excerpt":"Zhuohao Yu이 arXiv에 게시한 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them","tags":["Review","LLM-as-a-Judge","Evaluation Frameworks","Inconsistency Reduction","Probabilistic Scoring","Transitivity","Information Loss","Perplexity","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yidong Wang, Yunze Song, Tingyuan Zhu, Xuanwang Zhang, Zhuohao Yu, Hao Chen, Chiyu Song, Qiufeng Wang, Cunxiang Wang, Zhen Wu, Xinyu Dai, Yue Zhang, Wei Ye, Shikun Zhang 핵심 연구 목표"},{"id":"2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory","title":"[논문리뷰] Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory","excerpt":"Yanbin Fu이 arXiv에 게시한 'Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory","tags":["Review","Large Reasoning Models","Cognitive Science","Schoenfeld's Episode Theory","Math Problem Solving","Chain-of-Thought","Behavioral Analysis","Dataset Annotation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Li, Nan Zhang, Chenrui Fan, Hong Jiao, Yanbin Fu, Sydney Peters, Qingshu Xu, Robert Lissitz, Tianyi Zhou 핵심 연구 목표 본 논문은 Large Reasoning Models (LRMs) 이 생성하는 ChainofThought ("},{"id":"2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models","title":"[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models","excerpt":"Shawn Guo이 arXiv에 게시한 'V-GameGym: Visual Game Generation for Code Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models","tags":["Review","Code Large Language Models","Visual Game Generation","Benchmark","Pygame","Multimodal Evaluation","Software Engineering","AI-assisted Game Development"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Zhang, Jack Yang, Renshuai Tao, Lingzheng Chai, Shawn Guo 핵심 연구 목표 본 연구는 코드 대규모 언어 모델(Code LLM)의 알고리즘 문제 해결 능력과 실제 게임 개발의 포괄적인 요구사항 간의 격차를 해소하고자 합니다. 특히, 기존 벤치마크들이 간과했던 플레이 가"},{"id":"2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models","title":"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models","excerpt":"Yuewei Zhang이 arXiv에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models","tags":["Review","Reinforcement Learning","Curriculum Learning","Large Language Models","Mathematical Reasoning","Variance-based Sampling","Replay Learning","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang 핵심 연구 목표 기존 롤아웃 기반 강화 학습(RL) 방법론이 LLM의 동적인 학습 능력과 샘플 난이도를 효과적으로 매칭하지 못하는 문제를 해결하는 것이 목표"},{"id":"2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity","title":"[논문리뷰] When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity","excerpt":"John P Dickerson이 arXiv에 게시한 'When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-26 13:35:32+0900","permalink":"/ai/review/2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity","tags":["Review","LLM Judge","Benchmark Evaluation","Validity","Reliability","Psychometrics","Factor Analysis","Schema Adherence","ELO Ranking"],"text":"링크: 논문 PDF로 바로 열기 저자: Benjamin Feuer, ChiungYi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John P Dickerson 핵심 연구 목표 본 논문은 LLM Judge 벤치마크 설계에서 발생하는 근본적인 결함이 평가 유효성을 침묵적으로 저해 하는 문제를 다룹니다. 특히, 명확한 목"},{"id":"2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition","title":"[논문리뷰] CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition","excerpt":"arXiv에 게시된 'CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition","tags":["Review","Historical Text Recognition","Vision-Language Model","Open-Weight Model","OCR","Cultural Heritage","Low-Cost AI","Dataset Curation","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Sina J. Semnani, Han Zhang, Xinyan He, Merve Tekgürler, Monica S. Lam 핵심 연구 목표 본 연구는 역사 문서의 텍스트 인식 정확도를 높이고 비용을 절감하기 위해 오픈웨이트 대규모 비전언어 모델(VLM) 인 CHURRO 를 개발하는 것을 목표로 합니다. 기존 VLM이"},{"id":"2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning","title":"[논문리뷰] CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning","excerpt":"arXiv에 게시된 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning","tags":["Review","Image Captioning","Reinforcement Learning","Verifiable Rewards","LVLMs","VQA","Data Curation","Caption Quality"],"text":"링크: 논문 PDF로 바로 열기 저자: Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin 핵심 연구 목표 본 연구는 기존 SFT(Supervised FineTuning) 기반 이미지 캡셔닝 모델의 한계(고비용 데이터"},{"id":"2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training","title":"[논문리뷰] Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training","excerpt":"arXiv에 게시된 'Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training","tags":["Review","LLM","Reinforcement Fine-tuning","Reward Modeling","Reward Over-optimization","Rubric-based Rewards","High-reward Tail","Off-policy Data","LLM Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, and Lifeng Jin 핵심 연구 목표 본 논문은 LLM(Large Language "},{"id":"2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents","title":"[논문리뷰] D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents","excerpt":"Jinyuan Li이 arXiv에 게시한 'D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents","tags":["Review","Mobile GUI Automation","Multi-Agent System","Cognitive Architecture","Pre-execution Alignment","Post-execution Reflection","Retrieval-Augmented Generation","Multimodal LLM","Deliberative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongze Mi, Yibo Feng, Wenjie Lu, Yuqi Wang, Jinyuan Li 핵심 연구 목표 본 논문은 기존 GUI 에이전트의 데이터 병목 현상, 지연된 오류 탐지의 높은 비용, 모순된 지침 등의 문제점을 해결하고자 합니다. 인간의 인지 루프(사고, 정렬, 성찰)에서 영감을 받은 DArtemis "},{"id":"2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning","title":"[논문리뷰] EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning","excerpt":"Li Yu-Jhe이 arXiv에 게시한 'EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning","tags":["Review","LLM Agents","Reinforcement Learning","Entropy Regularization","Policy Optimization","Sparse Rewards","Multi-turn Environments","Exploration-Exploitation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wujiang Xu, Wentian Zhao, Zhenting Wang, YuJhe Li, Can Jin, Mingyu Jin, Kai Mei, Kun Wan, Dimitris N. Metaxas 핵심 연구 목표 본 논문은 LLM 에이전트 가 스파스한 보상 을 제공하는 다중 턴 환경 에서 겪는 \"탐색활용 캐스케이드 실"},{"id":"2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models","title":"[논문리뷰] ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models","excerpt":"Ki-Ung Song이 arXiv에 게시한 'ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models","tags":["Review","High-Resolution Vision","Vision-Language Models","Efficient Reasoning","Coarse-to-Fine","Reinforcement Learning","Visual Understanding","Attention Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Jewon Lee, Wooksu Shin, Seungmin Yang, KiUng Song, DongUk Lim, Jaeyeon Kim, TaeHo Kim, BoKyeong Kim 핵심 연구 목표 논문은 대규모 시각언어 모델(LVLMs)의 고해상도 이미지 처리 시 발생하는 과도한 계산 오버헤드 문제를 해결하고, 실제 애"},{"id":"2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences","title":"[논문리뷰] Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences","excerpt":"Eija Honkavaara이 arXiv에 게시한 'Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences","tags":["Review","3D Object Localization","Particle Filter","Multi-target Tracking","Drone Surveillance","Wildfire Monitoring","Semantic Segmentation","Camera Pose Estimation"],"text":"링크: 논문 PDF로 바로 열기 저자: Julius Pesonen, Arno Solin, Eija Honkavaara 핵심 연구 목표 본 연구는 노이즈가 있는 카메라 움직임과 시맨틱 세그멘테이션 시퀀스로부터 원거리 객체의 3D 위치를 찾는 문제를 해결하는 것을 목표로 합니다. 특히, 드론 기반 산불 모니터링과 같이 컴퓨팅 자원이 제한적이거나 객체가 멀리 떨"},{"id":"2025-9-29-Fine-tuning-Done-Right-in-Model-Editing","title":"[논문리뷰] Fine-tuning Done Right in Model Editing","excerpt":"Du Su이 arXiv에 게시한 'Fine-tuning Done Right in Model Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing","tags":["Review","Model Editing","Fine-tuning","Large Language Models","Catastrophic Forgetting","Breadth-First Pipeline","Depth-First Pipeline","Localized Tuning","Lifelong Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Wanli Yang, Fei Sun, Rui Tang, Hongyu Zang, Du Su, Qi Cao, Jingang Wang, Huawei Shen, Xueqi Cheng 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM) 모델 편집에서 finetuning이 비효율적이라는 오랜 통념에 도전하고, 그 실패의 원인이"},{"id":"2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing","title":"[논문리뷰] FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing","excerpt":"Linghe Kong이 arXiv에 게시한 'FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing","tags":["Review","Text-Guided Image Editing","Diffusion Models","Real-Time Editing","One-Step Inversion","Attention Control","Background Preservation","Semantic Disentanglement"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang 핵심 연구 목표 이 논문은 확산 모델을 활용한 텍스트 기반 이미지 편집에서 발생하는 과도한 지연 시간, 배경 불안정성, 의미론적 얽힘 이라는 세 가지 주요 문제"},{"id":"2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models","title":"[논문리뷰] HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models","excerpt":"Romann M. Weber이 arXiv에 게시한 'HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models","tags":["Review","Diffusion Models","Sampling","Generative AI","Image Generation","Plug-and-Play","Training-Free","Guidance","Momentum-Based Methods"],"text":"링크: 논문 PDF로 바로 열기 저자: Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber 핵심 연구 목표 확산 모델이 적은 NFEs(Neural Function Evaluations) 또는 낮은 guidance scale에서 비현실적인 출력과 세부 정보 부족을 보이는 문제를 해결하고, 확산 샘플링의 품질과 효율성을"},{"id":"2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models","title":"[논문리뷰] Instruction-Following Evaluation in Function Calling for Large Language Models","excerpt":"NikolaiSkripko이 arXiv에 게시한 'Instruction-Following Evaluation in Function Calling for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models","tags":["Review","Function Calling","LLMs","Instruction Following","Benchmarking","JSON Schema","AI Agents","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Nikolai Skripko 핵심 연구 목표 이 논문은 기존의 함수 호출 벤치마크(BFCL, τ²Bench, ACEBench 등)가 인수의 정확성만을 평가하고, 매개변수 설명에 포함된 형식 지정 지침(예: 이중 따옴표, ISO 날짜 형식) 준수 여부를 테스트하지 않는 문제를 해결하고자 합니다. 이를 위해 LLM의 정밀"},{"id":"2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards","title":"[논문리뷰] Language Models Can Learn from Verbal Feedback Without Scalar Rewards","excerpt":"arXiv에 게시된 'Language Models Can Learn from Verbal Feedback Without Scalar Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards","tags":["Review","Verbal Feedback","Conditional Generation","Large Language Models","Feedback-Conditional Policy","Offline-Online Learning","Reward Hypothesis Bypass"],"text":"링크: 논문 PDF로 바로 열기 저자: Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang 핵심 연구 목표 기존 RLHF(Reinforcement Learning from Human Feedback) 방식이 구두 피드백을 스칼라 보상으로 압축하여 발생하"},{"id":"2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning","title":"[논문리뷰] Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning","excerpt":"Gang Li이 arXiv에 게시한 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning","tags":["Review","Reinforcement Learning","LLM Agents","Exploration-Exploitation","Self-Imitation Learning","Intrinsic Rewards","Curriculum Learning","Policy Entropy","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Gang Li, Zhengbao He, Xiaoyu Tan, Yulei Qin, tedsun 핵심 연구 목표 본 논문의 핵심 목표는 장기적인(longhorizon), 희소한 보상(sparselyrewarded)을 가진 LLM 에이전트 태스크에서 강화 학습(RL)의 근본적인 문제인 탐색활용 트레이드오프(explorati"},{"id":"2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation","title":"[논문리뷰] LongLive: Real-time Interactive Long Video Generation","excerpt":"arXiv에 게시된 'LongLive: Real-time Interactive Long Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation","tags":["Review","Long Video Generation","Real-time","Interactive AI","Autoregressive Models","KV Cache","Streaming Tuning","Attention Sink","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen 핵심 연구 목표 실시간 및 대화형으로 고품질의 긴 비디오를 생"},{"id":"2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer","title":"[논문리뷰] LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer","excerpt":"arXiv에 게시된 'LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer","tags":["Review","Universal Image Restoration","Diffusion Transformer","Caption-Free","Semantic Alignment","Image Quality Assessment","Data Curation","Real-World Degradations","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Song Fei, Tian Ye, Lujia Wang, Lei Zhu 핵심 연구 목표 본 논문은 알 수 없는 혼합된 열화가 적용된 실제 저품질(LQ) 이미지에 대해 의미론적 일관성과 지각적 충실도를 유지하면서 범용 이미지 복원(UIR)을 수행하는 것을 목표로 합니다. 기존 UNet 기반 확산 모델 과 텍스트 프롬프트 "},{"id":"2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning","title":"[논문리뷰] MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning","excerpt":"Weipeng Zhong이 arXiv에 게시한 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning","tags":["Review","3D Scene Generation","Robotic Manipulation","Large Language Models","Spatial Reasoning","Dataset","Direct Preference Optimization","Tabletop Scene"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinkun Hao, Naifu Liang, Zhen Luo, Xudong Xu, Weipeng Zhong, Ran Yi, Yichen Jin, Zhaoyang Lyu, Feng Zheng, Lizhuang Ma, Jiangmiao Pang 핵심 연구 목표 로봇 조작 태스크를 위한 현실적이고 태스크 관련성이 높은 3D"},{"id":"2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation","title":"[논문리뷰] Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation","excerpt":"Peter Wonka이 arXiv에 게시한 'Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation","tags":["Review","Subject-Driven Generation","Visual Inconsistency Detection","Feature Disentanglement","Diffusion Models","Semantic Correspondence","Evaluation Metric","Spatial Localization","Contrastive Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Abdelrahman Eldesokey, Aleksandar Cvejic, Bernard Ghanem, Peter Wonka 핵심 연구 목표 본 논문은 SubjectDriven 이미지 생성 모델에서 발생하는 시각적 불일치(visual inconsistencies)를 정확하게 감지하고 정량화하며, 더 나아가 해당 불일치"},{"id":"2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing","title":"[논문리뷰] MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing","excerpt":"SunYuefeng이 arXiv에 게시한 'MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing","tags":["Review","Document Parsing","Vision-Language Model","High-Resolution","Two-Stage Inference","Layout Analysis","Content Recognition","Data Engine","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, et al. 핵심 연구 목표 본 연구는 기존 비전언어 모델(VLM)이 고해상도 문서 처리 시 겪는 토큰 중복, 높은 계산 비용, 환각 문제 등의 한계를 극복하는 것을 목표로 합니다. 특히, 전반적인 계산 효율"},{"id":"2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping","title":"[논문리뷰] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping","excerpt":"arXiv에 게시된 'No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping","tags":["Review","LLM Reinforcement Learning","Zero-Variance Prompts","Advantage Shaping","Entropy-Guided","Math Reasoning","RLVR","Group Relative Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: ThanhLong V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang 핵심 연구 목표 본 논문은 기존의 Verifiable Rewards를 활용한 강화 학습(RLVR) 방법론, 특히 GRPO 가 모든 롤아웃 응답이 동일한 보상을 받는 \" ZeroVariance Prompts \"를"},{"id":"2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning","title":"[논문리뷰] PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning","excerpt":"Lingpeng Kong이 arXiv에 게시한 'PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning","tags":["Review","Prompt Synthesis","Large Language Models","Reasoning","Expectation-Maximization","Self-Play","Supervised Fine-Tuning","Task Generation","Rationale Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong 핵심 연구 목표 LLM 추론을 위한 고품질 훈련 문제의 부족이라는 핵심 병목 현상을 해결하고자 합니다. 특히, 수작업으로 큐레이션된 데이터셋의 높은 비용과 한계, 기존 합성 코퍼스의 낮은 난이도 및 다양성"},{"id":"2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning","title":"[논문리뷰] Quantile Advantage Estimation for Entropy-Safe Reasoning","excerpt":"An Zhang이 arXiv에 게시한 'Quantile Advantage Estimation for Entropy-Safe Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning","tags":["Review","Reinforcement Learning","LLM Reasoning","Entropy Control","Advantage Estimation","Quantile Baseline","Exploration-Exploitation","RLVR"],"text":"링크: 논문 PDF로 바로 열기 저자: Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He 핵심 연구 목표 대규모 언어 모델(LLMs)의 추론 능력을 강화하는 Reinforcement Learning with Verifiable Rewards (RLVR) 훈련 과정에서 발생하는 엔트"},{"id":"2025-9-29-Real-Time-Object-Detection-Meets-DINOv3","title":"[논문리뷰] Real-Time Object Detection Meets DINOv3","excerpt":"Xi Shen이 arXiv에 게시한 'Real-Time Object Detection Meets DINOv3' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Real-Time-Object-Detection-Meets-DINOv3","tags":["Review","Real-time Object Detection","DINOv3","DEIMv2","Vision Transformer","Multi-scale Features","Spatial Tuning Adapter","Lightweight Models","Object Detection Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Shihua Huang, Yongjie Hou, Longfei Liu, Xuanlong Yu, Xi Shen 핵심 연구 목표 본 논문은 실시간 객체 탐지 분야에서 성능과 연산 효율성 사이의 균형을 개선하고, 특히 경량 모델을 위한 엣지 및 모바일 환경에서의 배포 효율성을 높이는 것을 목표로 합니다. DINOv3 의 강"},{"id":"2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation","title":"[논문리뷰] RefAM: Attention Magnets for Zero-Shot Referral Segmentation","excerpt":"Federico Tombari이 arXiv에 게시한 'RefAM: Attention Magnets for Zero-Shot Referral Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation","tags":["Review","Zero-Shot Segmentation","Referring Segmentation","Diffusion Transformers (DiTs)","Attention Mechanisms","Attention Sinks","Stop Words","Vision-Language Models","Training-Free Methods"],"text":"링크: 논문 PDF로 바로 열기 저자: Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele 핵심 연구 목표 컴퓨터 비전 태스크에서 CNN의 의존성을 완전히 제거 하고, 순수한 Transformer 아키"},{"id":"2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models","title":"[논문리뷰] ReviewScore: Misinformed Peer Review Detection with Large Language Models","excerpt":"arXiv에 게시된 'ReviewScore: Misinformed Peer Review Detection with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models","tags":["Review","Peer Review","Review Quality","Large Language Models (LLMs)","Misinformed Review","Argument Reconstruction","Factuality Evaluation","Natural Language Processing","Automated Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyun Ryu, Doohyuk Jang, Hyemin S. Lee, Joonhyun Jeong, Gyeongman Kim, Donghyeon Cho, Gyouk Chu, Minyeong Hwang, Hyeongwon Jang, Changhun Kim, Haechan Kim, Jina Kim, Joowon Kim, Y"},{"id":"2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework","title":"[논문리뷰] SPARK: Synergistic Policy And Reward Co-Evolving Framework","excerpt":"arXiv에 게시된 'SPARK: Synergistic Policy And Reward Co-Evolving Framework' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework","tags":["Review","Reinforcement Learning","LLMs","LVLMs","Reward Modeling","Policy Optimization","Self-Reflection","Verifiable Rewards","Co-evolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang 핵심 연구 목표 본 논문은 대규모 언어/시각언어 모델(LLM/LVLM)의 강화 학습(RL) 파이프라인이 겪는 한계를 해결하고자 합니다. 특히"},{"id":"2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation","title":"[논문리뷰] See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation","excerpt":"Chih-Hai Su이 arXiv에 게시한 'See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation","tags":["Review","Vision-Language Models","UAV Navigation","Zero-shot","Spatial Grounding","Waypoint Prompting","Autonomous Navigation","Adaptive Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Chih Yao Hu, YangSen Lin, Yuna Lee, ChihHai Su, JieYing Lee, ShrRuei Tsai, ChinYang Lin, KuanWen Chen, TsungWei Ke, YuLun Liu 핵심 연구 목표 본 논문은 기존 VisionLanguage Models (VLMs) 기반의 드"},{"id":"2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion","title":"[논문리뷰] StateX: Enhancing RNN Recall via Post-training State Expansion","excerpt":"Zhiyuan Liu이 arXiv에 게시한 'StateX: Enhancing RNN Recall via Post-training State Expansion' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion","tags":["Review","RNN","State Expansion","Post-training","Long-context Recall","Linear Attention","State Space Models","GLA","Mamba2"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingyu Shen, Yingfa Chen, Zhen Leng Thai, Xu Han, Zhiyuan Liu, & Maosong Sun 핵심 연구 목표 본 논문은 Transformer 대비 긴 컨텍스트 처리 효율이 높은 RNN 계열 모델들이 고정된 크기의 recurrent state 로 인해 장문 컨텍스트에서의 정보"},{"id":"2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images","title":"[논문리뷰] TUN3D: Towards Real-World Scene Understanding from Unposed Images","excerpt":"Anna Vorontsova이 arXiv에 게시한 'TUN3D: Towards Real-World Scene Understanding from Unposed Images' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images","tags":["Review","3D Scene Understanding","Layout Estimation","3D Object Detection","Unposed Images","Sparse Convolutional Networks","Multi-view Stereo","Real-time AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Anton Konushin, Nikita Drozdov, Bulat Gabdullin, Alexey Zakharov, Anna Vorontsova, Danila Rukhovich, Maksim Kolodiazhnyi 핵심 연구 목표 본 논문은 실세계 스캔에서 정확한 카메라 포즈나 깊이 정보 없이 다중 뷰 이미지 입력만"},{"id":"2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval","title":"[논문리뷰] Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval","excerpt":"arXiv에 게시된 'Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval","tags":["Review","RAG","LLM Reasoning","Knowledge Graphs","Multi-Agent Systems","Context Retrieval","Heterogeneous Graphs","Adaptive Learning","Dual-Evolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaojun Wu, Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Hui Xiong, Jia Li, Jian Guo 핵심 연구 목표 본 논문은 기존 그래프 기반 RAG 시스템이 직면한 정적 그래프 인덱스 구축의 한계 와 LLM 추출기의 성능 의존"},{"id":"2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios","title":"[논문리뷰] UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios","excerpt":"Zeyu Qin이 arXiv에 게시한 'UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios","tags":["Review","LLM Agents","Long-Horizon Reasoning","Benchmarking","Partially Observable","Tool Use","Memory Management","Exploration"],"text":"링크: 논문 PDF로 바로 열기 저자: Zeyu Qin, Haoyu Wang, Xuelin Zhang, Huaisong Zhang, Haotian Luo 핵심 연구 목표 기존 LLM 에이전트 벤치마크가 짧은 호라이즌과 완전 관측 가능한 태스크에 집중하여 실제 복합 태스크에 필수적인 지속적인 추론, 계획, 메모리 관리, 툴 사용 능력 을 충분히 평가하지 못하"},{"id":"2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models","title":"[논문리뷰] UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models","excerpt":"Yuchao Gu이 arXiv에 게시한 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models","tags":["Review","Unified Vision Modeling","Video Generation","Diffusion Transformer","Supervised Fine-tuning","Cross-modal","Cross-source Tasks","Visual Sentences","LoRA"],"text":"링크: 논문 PDF로 바로 열기 저자: Lan Chen, Yuchao Gu, Qi Mao 핵심 연구 목표 기존 Large Vision Models (LVMs)이 태스크 및 모달리티별 사전 훈련 데이터에 대한 높은 의존성으로 인해 확장성이 제한되는 문제를 해결하고자 합니다. 이 연구는 사전 훈련된 단일 비디오 생성 모델 이 광범위한 이미지 및 비디오 태스크에"},{"id":"2025-9-29-Variational-Reasoning-for-Language-Models","title":"[논문리뷰] Variational Reasoning for Language Models","excerpt":"arXiv에 게시된 'Variational Reasoning for Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Variational-Reasoning-for-Language-Models","tags":["Review","Variational Inference","Language Models","Reasoning","ELBO","IWAE","Reinforcement Learning","Latent Variables","Forward-KL"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang 핵심 연구 목표 언어 모델(LLM)의 추론 능력 훈련에 사용되는 지도 미세 조정(SFT) 및 강화 학습(RL) 방법론의 한계를 극복하고, 생각 과정"},{"id":"2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing","title":"[논문리뷰] VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing","excerpt":"arXiv에 게시된 'VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing","tags":["Review","AI Assistants","Multimodal Benchmarking","Audio Understanding","Speech Synthesis","Vision-Language Models","Role-play","Safety","Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li 핵심 연구 목표 본 논문은 기존 벤치마크의 한계를 극복하고, 음성 우선 AI 비서의 청취, 말하기, 보기 능력 전반 을 평가할 수 있는 종합적인 벤치마크를 제시하는 것을 목표로 합니다. 특히 음성 개인화, 핸즈프리 "},{"id":"2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning","title":"[논문리뷰] WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning","excerpt":"Zhuofan Zong이 arXiv에 게시한 'WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning","tags":["Review","Website Generation","Code Agent","LLM","VLM","Reinforcement Learning","Multi-Level Feedback","GUI Agent","Step-GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li 핵심 연구 목표 본 논문은 웹사이트 코드 생성과 같이 시각적 요소와 사용자 상호작용 피드백이 중요한 태스크에서, 기존 코드 에이전트들이 단순한"},{"id":"2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation","title":"[논문리뷰] Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation","excerpt":"Shiming Liu이 arXiv에 게시한 'Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation","tags":["Review","MLLM","Interpretability","Attribution","Token Generation","Black-box Explanation","Hallucination Diagnosis","Multimodality","VQA"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruoyu Chen, Xiaoqing Guo, Kangwei Liu, Siyuan Liang, Shiming Liu, Qunli Zhang, Hua Zhang, Xiaochun Cao 핵심 연구 목표 Multimodal Large Language Models (MLLMs)의 자동 회귀 토큰 생성 과정에서 시각적 입력이"},{"id":"2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction","title":"[논문리뷰] WoW: Towards a World omniscient World model Through Embodied Interaction","excerpt":"Weishi Mi이 arXiv에 게시한 'WoW: Towards a World omniscient World model Through Embodied Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction","tags":["Review","World Model","Embodied AI","Robotics","Diffusion Models","Physical Reasoning","Vision Language Models","Interaction Data","Self-Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Weishi Mi, Xiaozhu Ju, ChunKai Fan, Peidong Jia, Xiaowei Chi 핵심 연구 목표 본 논문은 수동적 관찰에 의존하는 기존 비디오 생성 모델의 한계(물리적 인과관계 이해 부족)를 극복하고, 대규모의 인과관계가 풍부한 실제 상호작용 데이터 를 통해 로봇이 물리적 직관을 습득할 수"},{"id":"2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning","title":"[논문리뷰] X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning","excerpt":"Raghuveer Rao이 arXiv에 게시한 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning","tags":["Review","Text-to-Video Retrieval","LLM","Chain-of-Thought","Explainable AI","Multimodal Retrieval","Bradley-Terry Model","Video Annotation"],"text":"링크: 논문 PDF로 바로 열기 저자: Prasanna Reddy Pulakurthi, Jiamian Wang, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Zhiqiang Tao 핵심 연구 목표 본 논문은 기존 임베딩 모델 기반 텍스트비디오 검색 시스템의 한계, 즉 낮은 데이터 품질의 영향 및 랭킹 결과에 대한 설명 부"},{"id":"2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","title":"[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction","excerpt":"Guoxian Song이 arXiv에 게시한 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-29 13:47:46+0900","permalink":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","tags":["Review","Digital Human","Multimodal AI","Real-time Streaming","Video Generation","Diffusion Models","Transformer Architecture","Audiovisual Synchronization","World Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Guoxian Song, Xiaochen Zhao, Chao Liang, Jianwen Jiang, Hongyi Xu, Linjie Luo 핵심 연구 목표 컴퓨터 비전, 음성 및 텍스트를 아우르는 다중 모달 인터랙티브 인간 에이전트 시스템"},{"id":"2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models","title":"[논문리뷰] AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models","excerpt":"Rahul Karthikeyan이 arXiv에 게시한 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models","tags":["Review","Bias Mitigation","Large Language Models","Speculative Decoding","Constitutional AI","Fairness","Inference-Time Control","Indian Sociocultural Context"],"text":"링크: 논문 PDF로 바로 열기 저자: Snehasis Mukhopadhyay, Aryan Kasat, Shivam Dubey, Rahul Karthikeyan, Dhruv Sood, Vinija Jain, Aman Chadha, Amitava Das 핵심 연구 목표 대규모 언어 모델(LLMs)이 학습 데이터에서 발생하는 사회적 편향, 특히 인도 사회의 카"},{"id":"2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation","title":"[논문리뷰] Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation","excerpt":"Xiaolei Huang이 arXiv에 게시한 'Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation","tags":["Review","Synthetic Data Generation","Large Language Models (LLMs)","Genetic Algorithms","Textual Data Augmentation","Active Learning","NLP","Data Diversity"],"text":"링크: 논문 PDF로 바로 열기 저자: Guangzeng Han, Weisi Liu, Xiaolei Huang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)을 활용한 합성 데이터 생성 시 품질과 다양성 확보의 어려움을 해결하는 것을 목표로 합니다. 특히, 하류 태스크 훈련의 견고성을 높이기 위해 데이터 다양성과 생성기 적응성을 자동으로 증폭할 수 있"},{"id":"2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System","title":"[논문리뷰] Baichuan-M2: Scaling Medical Capability with Large Verifier System","excerpt":"Jayok6이 arXiv에 게시한 'Baichuan-M2: Scaling Medical Capability with Large Verifier System' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System","tags":["Review","Medical AI","LLM","Reinforcement Learning","Verifier System","Patient Simulator","Clinical Rubrics","Baichuan-M2","HealthBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Jayok6, yuanshuai, sdujq, anselcmy, fairyang 핵심 연구 목표 의료 분야 LLM 이 USMLE 같은 정적 벤치마크에서는 우수하지만 실제 임상 환경의 동적, 상호작용적 특성을 포착하지 못해 발생하는 성능 격차를 해소하는 것이 목표입니다. 이를 위해, 실제 임상 적용과 LLM 의 역량을 "},{"id":"2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining","title":"[논문리뷰] Benchmarking Optimizers for Large Language Model Pretraining","excerpt":"mjaggi이 arXiv에 게시한 'Benchmarking Optimizers for Large Language Model Pretraining' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining","tags":["Review","LLM Optimizers","Benchmarking","Hyperparameter Tuning","AdamW","AdEMAMix","MARS","Mixture of Experts (MoE)","Weight Decay"],"text":"링크: 논문 PDF로 바로 열기 저자: Andrei Semenov, Matteo Pagliardini, Martin Jaggi 핵심 연구 목표 대규모 언어 모델(LLM) 사전 훈련을 위한 최신 옵티마이저들의 성능을 표준화된 시나리오 에서 종합적으로 평가하고 비교하는 것을 목표로 합니다. 기존의 파편화된 평가 프로토콜로 인해 옵티마이저 간 직접 비교가 어렵다"},{"id":"2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection","title":"[논문리뷰] C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection","excerpt":"Vito Renó이 arXiv에 게시한 'C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection","tags":["Review","Object Detection","Diffusion Model","Global Scene Context","Context-Aware Fusion","Fine-grained Detection","Automotive Damage Assessment","Generative Denoising","Cross-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Abdellah Zakaria Sellam, Ilyes Benaissa, Salah Eddine Bekhouche, Abdenour Hadid, Vito Renó, Cosimo Distante 핵심 연구 목표 본 논문은 자동차 손상 평가와 같은 미세하고 컨텍스트에 의존적인 시나리오 에서 객체 탐지의 한계를 극복하는 것"},{"id":"2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization","title":"[논문리뷰] DCPO: Dynamic Clipping Policy Optimization","excerpt":"Kai Lu이 arXiv에 게시한 'DCPO: Dynamic Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization","tags":["Review","Reinforcement Learning","LLM","Policy Optimization","Dynamic Clipping","Advantage Standardization","RLVR","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 능력을 향상시키기 위한 Verifiable Rewards 기반의 강화 학습(RLVR) 에서 발생하는 기존 방법론(예:"},{"id":"2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing","title":"[논문리뷰] Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing","excerpt":"Amin Heyrani Nobar이 arXiv에 게시한 'Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing","tags":["Review","Image Editing","Autoregressive Models","Noise Inversion","Text-to-Image","Gumbel-max Trick","Training-free","Location-aware Argmax Inversion"],"text":"링크: 논문 PDF로 바로 열기 저자: Quan Dao, Ngan Hoai Nguyen, Ligong Han, Xiaoxiao He, Amin Heyrani Nobari 핵심 연구 목표 본 연구는 시각적 자기회귀(VAR) 모델 에서 추가 훈련 없이 프롬프트 기반 이미지 편집 기능을 구현하는 것을 목표로 합니다. 기존 VAR 모델의 편집 능력 한계를 극복하고"},{"id":"2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding","title":"[논문리뷰] ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding","excerpt":"Xuanyu Zheng이 arXiv에 게시한 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding","tags":["Review","Long Video Understanding","Hallucination","Semantic Aggregation","Video MLLM","Benchmark","DPO","Positional Encoding","VideoQA"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Lu, Jiahao Wang, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin, Lewei Lu 핵심 연구 목표 Video MLLM(Multimodal Large Language Models)이 긴 비디오에서 보이는 Semantic Aggrega"},{"id":"2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them","title":"[논문리뷰] Fantastic Pretraining Optimizers and Where to Find Them","excerpt":"Percy Liang이 arXiv에 게시한 'Fantastic Pretraining Optimizers and Where to Find Them' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them","tags":["Review","Deep Learning Optimizers","Large Language Models","Hyperparameter Tuning","Pretraining Speedup","Scaling Laws","AdamW","Matrix-based Optimizers","Data-to-Model Ratio"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaiyue Wen, David Hall, Tengyu Ma, Percy Liang 핵심 연구 목표 본 논문은 언어 모델 사전 훈련에서 AdamW 가 지배적인 옵티마이저임에도 불구하고, 새로운 옵티마이저들이 주장하는 1.4배에서 2배 의 학습 속도 향상이 실제로는 널리 채택되지 않는 이유를 규명하고자 합니다. 저자들은"},{"id":"2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models","title":"[논문리뷰] FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models","excerpt":"Zhen Wang이 arXiv에 게시한 'FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models","tags":["Review","Virtual Try-On","Diffusion Models","Cacheable Architecture","Multi-Reference","Semi-Attention","Efficiency","Image Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Zheng Chong, Yanwei Lei, Shiyue Zhang, Zhuandi He, Zhen Wang, Xujie Zhang, Xiao Dong, Yiling Wu, Dongmei Jiang & Xiaodan Liang 핵심 연구 목표 본 논문은 기존 가상 착용(Virtual TryOn) 기술이 다중 레퍼런스 "},{"id":"2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games","title":"[논문리뷰] FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games","excerpt":"Dongmin Park이 arXiv에 게시한 'FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games","tags":["Review","GUI Agents","Adventure Games","Benchmark","Full Story Arc","Observation-Behavior Gap","LLMs","Automated Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jaewoo Ahn, Junseo Kim, Heeseung Yun, Jaehyeon Son, Dongmin Park, Jaewoong Cho, Gunhee Kim 핵심 연구 목표 기존 GUI 에이전트 벤치마크는 게임 다양성과 전체 스토리라인 완료 평가 기능이 부족하며, 에이전트가 이전에 관찰한 정보를 기억하고 활용하는"},{"id":"2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer","title":"[논문리뷰] GenCompositor: Generative Video Compositing with Diffusion Transformer","excerpt":"Lingen Li이 arXiv에 게시한 'GenCompositor: Generative Video Compositing with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer","tags":["Review","Video Compositing","Diffusion Transformer","Generative Models","Video Editing","Position Embedding","Diffusion Models","Masked Token Injection","Video Harmonization"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, Jian Zhang 핵심 연구 목표 본 논문은 기존의 수동적이고 노동 집약적인 비디오 합성(Video Compositing) 과정을 생성형 모델 을 사용하여 자동화하는 것을 목표로 합"},{"id":"2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR","title":"[논문리뷰] Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR","excerpt":"Lu Wang이 arXiv에 게시한 'Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR","tags":["Review","RLVR","Large Language Models","Actor-Critic","Supervised Learning","Mathematical Reasoning","Policy Optimization","Cross-Entropy Loss"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang 핵심 연구 목표 본 논문은 LLM이 수학 및 프로그래밍과 같은 추론 태스크에서 직면하는 희소한 보상 신호 와 불안정한 정책 경사 업데이트 라는 기존 RLVR(Reinfo"},{"id":"2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers","title":"[논문리뷰] Improving Large Vision and Language Models by Learning from a Panel of Peers","excerpt":"Simon Jenni이 arXiv에 게시한 'Improving Large Vision and Language Models by Learning from a Panel of Peers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers","tags":["Review","Large Vision and Language Models (LVLMs)","Self-Improvement","Peer Learning","Preference Alignment","Reward Modeling","Multimodal Learning","Knowledge Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Jefferson Hernandez, Jing Shi, Simon Jenni, Vicente Ordonez, Kushal Kafle 핵심 연구 목표 본 논문은 대규모 시각언어 모델(LVLMs)의 성능을 향상시키기 위해 고가의 인간 주석 데이터에 대한 의존성을 줄이는 새로운 자체 개선 프레임워크인 'PanelofPeer"},{"id":"2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations","title":"[논문리뷰] Jointly Reinforcing Diversity and Quality in Language Model Generations","excerpt":"Tianlu이 arXiv에 게시한 'Jointly Reinforcing Diversity and Quality in Language Model Generations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations","tags":["Review","Reinforcement Learning","Language Models","Diversity Optimization","Quality Enhancement","Semantic Clustering","Post-training","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang 핵심 연구 목표 대규모 언어 모델(LM)의 후처리 과정에서 발생하는 다양성 감소 문제를 해결하는 것이 주요 목표입니다"},{"id":"2025-9-3-Kwai-Keye-VL-1-5-Technical-Report","title":"[논문리뷰] Kwai Keye-VL 1.5 Technical Report","excerpt":"SXxtyz이 arXiv에 게시한 'Kwai Keye-VL 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report","tags":["Review","Multimodal LLMs","Video Understanding","Slow-Fast Encoding","Long Context","Chain-of-Thought","Reinforcement Learning","Human Alignment","Native-Resolution Vision Encoder"],"text":"링크: 논문 PDF로 바로 열기 저자: Keye Team, Kuaishou Group 핵심 연구 목표 본 논문은 동적이고 정보 밀도가 높은 비디오 콘텐츠 이해에서 발생하는 공간 해상도와 시간 범위 간의 트레이드오프 문제를 해결하고, 기존 모델들이 비디오 이해에서 겪는 한계를 극복하는 것을 목표로 합니다. 궁극적으로 비디오 이해 태스크에서 최첨단 성능 을 달"},{"id":"2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model","title":"[논문리뷰] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model","excerpt":"Jianwei Yang이 arXiv에 게시한 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model","tags":["Review","Vision-Language Models (VLMs)","Critic Models","Policy Models","Reinforcement Learning (RL)","Self-Criticism","Multimodal Reasoning","Preference Learning","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang, Bo Liu, Tianyi Xiong, Furong Huang 핵심 연구 목표 본 논문은 critic 모델이 단순히 응답을 평가하는 것을 넘어 강력한 정책 모델로서 생성 능력까지 갖출 수 있다는 통념에 도전합니다. 최종 목표는 선"},{"id":"2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision","title":"[논문리뷰] M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision","excerpt":"Yan-Jie Zhou이 arXiv에 게시한 'M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision","tags":["Review","Medical Image Retrieval","Self-Supervised Learning","Multimodal","Zero-shot","Foundation Models","MAE","SimDINO","Vision Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Che Liu, Zheng Jiang, Chengyu Fang, Heng Guo, YanJie Zhou, Jiaqi Qu, Le Lu, Minfeng Xu 핵심 연구 목표 의료 영상 분야에서 기존의 2D, 3D, 비디오 기반 데이터에 파편화된 모델 아키텍처 및 훈련 전략의 한계를 극복하고, 단일한 시각적 표현 학습 프"},{"id":"2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation","title":"[논문리뷰] MedDINOv3: How to adapt vision foundation models for medical image segmentation?","excerpt":"Xiaofeng Yang이 arXiv에 게시한 'MedDINOv3: How to adapt vision foundation models for medical image segmentation?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation","tags":["Review","Medical Image Segmentation","Vision Foundation Models","Self-supervised Learning","Vision Transformers (ViT)","Domain Adaptation","DINOv3","CT Imaging"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang 핵심 연구 목표 의료 영상 분할에서 Vision Foundation Models (FMs) 의 효과적인 적용을 저해하는 두 가지 핵심 과제, 즉 ViT 백본 이 특수화된 CNN 보다 낮은 성능을 보이는 문제와 "},{"id":"2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization","title":"[논문리뷰] Metis: Training Large Language Models with Advanced Low-Bit Quantization","excerpt":"Hengjie Cao이 arXiv에 게시한 'Metis: Training Large Language Models with Advanced Low-Bit Quantization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization","tags":["Review","Low-Bit Quantization","LLMs","Spectral Decomposition","Anisotropy","Adaptive Learning Rate","Regularization","FP8 Training","FP4 Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengjie Cao, Jixian Zhou, Mengyi Chen, Yifeng Yang, et al. 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)을 저비트 양자화로 훈련할 때 발생하는 이방성 매개변수 분포 가 불안정한 훈련과 성능 저하의 주된 원인임을 식별하고, 이를 해결하여 견고하고 효율적인 저비트 훈"},{"id":"2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents","title":"[논문리뷰] MobiAgent: A Systematic Framework for Customizable Mobile Agents","excerpt":"Wangbo Gong이 arXiv에 게시한 'MobiAgent: A Systematic Framework for Customizable Mobile Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents","tags":["Review","Mobile Agents","GUI Agents","Vision-Language Models","Agent Acceleration","Benchmarking","Reinforcement Learning","Data Collection"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Zhang, Erhu Feng, Xi Zhao, Yisheng Zhao, Wangbo Gong, Jiahui Sun, Dong Du, Zhichao Hua, Yubin Xia, Haibo Chen 핵심 연구 목표 본 논문은 GUI 기반 모바일 에이전트가 직면하는 낮은 태스크 완료율, 느린 응답 시간, 예상치"},{"id":"2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning","title":"[논문리뷰] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning","excerpt":"Zirui Wang이 arXiv에 게시한 'OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning","tags":["Review","Multimodal Learning","Vision Encoder","Generative Pretraining","Captioning Loss","Training Efficiency","Image-Text Models","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanqing Liu, Xianhang Li, Letian Zhang, Zirui Wang, Zeyu Zheng, Yuyin Zhou, Cihang Xie 핵심 연구 목표 OpenVision 2는 기존 OpenVision 아키텍처와 손실 함수의 복잡성을 단순화하여 멀티모달 학습을 위한 시각 인코더의 훈련 효율성을 대폭"},{"id":"2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion","title":"[논문리뷰] POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion","excerpt":"Haicheng Wang이 arXiv에 게시한 'POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion","tags":["Review","문서 변환","시각-언어 모델","자가 개선","합성 데이터","증류 없는 학습","OCR","멀티모달 AI","데이터 필터링"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuan Liu, Zhongyin Zhao, Le Tian, Haicheng Wang, Xubing Ye, Yangxiu You, Zilin Yu, Chuhan Wu, Xiao Zhou, Yang Yu, Jie Zhou 핵심 연구 목표 본 논문은 복잡한 문서 형식(테이블, 수식, 다단 텍스트 등)을 정확하게 변환하기 "},{"id":"2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic","title":"[논문리뷰] Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic","excerpt":"Bernard Ghanem이 arXiv에 게시한 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic","tags":["Review","Reasoning Vectors","Task Arithmetic","Chain-of-Thought","LLMs","Reinforcement Learning","Model Merging","Parameter Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohammad Zbeeb, Hasan Abed Al Kader Hammoud, Bernard Ghanem 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 복잡한 추론 능력을 습득하기 위해 필요한 값비싼 강화 학습(RL) 기반 최적화 과정을 대체하는 방법을 모색합니다. 특히, 학습된 추론 능력을 추론 벡터(re"},{"id":"2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction","title":"[논문리뷰] SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction","excerpt":"bindsch이 arXiv에 게시한 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction","tags":["Review","Text-to-SQL","Multi-agent Systems","Chain-of-Thought","Error Correction","Large Language Models","Query Planning","Database Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Saumya Chaturvedi, Aman Chadha, Laurent Bindschaedler 핵심 연구 목표 본 논문은 자연어 질의를 SQL 쿼리로 변환하는 TexttoSQL (NL2SQL) 시스템의 견고성과 신뢰성을 향상시키는 것을 목표로 합니다. 특히, 기존 시스템들이 실행 기반 피드백에만 의존하여 논리적으로 "},{"id":"2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning","title":"[논문리뷰] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning","excerpt":"Qian Liu이 arXiv에 게시한 'SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning","tags":["Review","Reinforcement Learning","Large Language Models","Tool-Integrated Reasoning","Multi-turn Reasoning","Gradient Explosion","Training Stability","Trajectory Filtering","Zero RL"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, Bo An 핵심 연구 목표 본 논문은 Reinforcement Learning (RL)을 사용하여 Multiturn ToolIntegrated Reasoning (TIR)을 수행하는 L"},{"id":"2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang","title":"[논문리뷰] The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang","excerpt":"Solomon Tsai이 arXiv에 게시한 'The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang","tags":["Review","LLMs","Metalinguistic Reasoning","Constructed Language","Camlang","Second Language Acquisition","Zero-shot Learning","Natural Language Understanding","Commonsense Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Fenghua Liu, Yulong Chen, Yixuan Liu, Zhujun Jin, Solomon Tsai, Ming Zhong 핵심 연구 목표 이 논문은 대규모 언어 모델(LLMs)이 언어 학습에서 인간과 유사한 메타언어적 추론 능력 을 진정으로 갖추고 있는지 평가하는 것을 목표로 합니다. LLM의 성공이 단순"},{"id":"2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey","title":"[논문리뷰] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","excerpt":"Hejia Geng이 arXiv에 게시한 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey","tags":["Review","Agentic Reinforcement Learning","Large Language Models","LLM Agents","Sequential Decision Making","Policy Optimization","Tool Use","Dynamic Environments","Autonomous AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Hejia Geng, Guibin Zhang, henggg, Artemis0430, JeremyYin 핵심 연구 목표 본 설문조사는 LLM(Large Language Models)을 수동적인 시퀀스 생성기에서 자율적인 의사 결정 에이전트로 전환하는 Agentic RL(Agentic Reinforcement Learni"},{"id":"2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views","title":"[논문리뷰] Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views","excerpt":"Junchi Yan이 arXiv에 게시한 'Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views","tags":["Review","Point Cloud Learning","Self-Supervised Learning","Cross Reconstruction","Decoupled Views","Generative Models","Positional Encoding","3D Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangdong Zhang, Shaofeng Zhang, Junchi Yan 핵심 연구 목표 본 논문은 3D 포인트 클라우드 학습에서 기존 단일 뷰(singleview) 기반 마스킹 재구성(masked reconstruction) 방식의 한계를 극복하고, 더 다양하고 도전적인 두 뷰(twoview) 기반 사전 학습 "},{"id":"2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning","title":"[논문리뷰] UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning","excerpt":"Haoyang Zou이 arXiv에 게시한 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning","tags":["Review","GUI Agent","Multi-Turn RL","Reinforcement Learning","Data Flywheel","Agent Framework","Hybrid Environments","Parameter Interpolation"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoyang Zou, zhwang4ai, JoeYing, jzfeng, MingComplex 핵심 연구 목표 본 연구는 데이터 희소성, 확장 가능한 멀티턴 강화 학습(RL), GUI 전용 작동의 한계, 환경 확장성 및 안정성 과 같은 자율 GUI 에이전트 개발의 주요 과제를 해결하는 것을 목표로 합니다. 궁극적으로 "},{"id":"2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy","title":"[논문리뷰] Universal Deep Research: Bring Your Own Model and Strategy","excerpt":"Pavlo Molchanov이 arXiv에 게시한 'Universal Deep Research: Bring Your Own Model and Strategy' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy","tags":["Review","Agentic Systems","Language Models (LLMs)","Research Automation","Customizable Strategies","Code Generation","Deep Research","User-Defined Agents","Sandboxed Execution"],"text":"링크: 논문 PDF로 바로 열기 저자: Peter Belcak, Pavlo Molchanov 핵심 연구 목표 이 논문은 기존의 심층 연구 도구(DRT)들이 고정된 연구 전략과 제한적인 모델 선택으로 인해 사용자 정의가 어렵고 특정 산업에 특화된 연구 전략을 구축하기 어렵다는 문제를 제기합니다. Universal Deep Research (UDR) 시스템을 "},{"id":"2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use","title":"[논문리뷰] VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use","excerpt":"Zhiheng Lyu이 arXiv에 게시한 'VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use","tags":["Review","Agentic Reinforcement Learning","Tool Use","Large Language Models","Reinforcement Learning from Verifiable Rewards (RLVR)","Asynchronous Execution","Multi-modal AI","Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Zhen, Fei Zou, Chao Du, Tianpeng Pang, Wenhui Chen 핵심 연구 목표 논문은 LLM의 독립적인 추론과 상호작용적 에이전트 지능 사이의"},{"id":"2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association","title":"[논문리뷰] ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association","excerpt":"Daniel Cremers이 arXiv에 게시한 'ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-03 13:36:21+0900","permalink":"/ai/review/2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association","tags":["Review","Monocular SLAM","Dense Reconstruction","Neural Networks","Pose Graph Optimization","Intrinsics-free","Real-time","Two-view Association"],"text":"링크: 논문 PDF로 바로 열기 저자: Ganlin Zhang, Shenhan Qian, Xi Wang, Daniel Cremers 핵심 연구 목표 본 연구는 기존 모노큘러 덴스 SLAM 시스템의 주요 한계점인 카메라 인트린직스(intrinsics) 필요성, 높은 계산 복잡성, 그리고 장기적인 시퀀스에서의 드리프트 축적 문제를 해결하는 것을 목표로 합니다."},{"id":"2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering","title":"[논문리뷰] EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering","excerpt":"arXiv에 게시된 'EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering","tags":["Review","LLM Steering Framework","vLLM Integration","Hidden State Manipulation","Inference Optimization","Extensibility","Modular Architecture","Reasoning Mitigation","Hallucination Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Haolei Xu¹, Xinyu Mei¹, Yuchen Yan¹, Rui Zhou¹, Wenqi Zhang¹, Weiming Lu¹, Yueting Zhuang¹, Yongliang Shen¹ 핵심 연구 목표 기존 LLM 스티어링 프레임워크들이 겪는 계산 비효율성 , 제한된 확장성 , 및 부족한 기능성 문제를 해결하는"},{"id":"2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling","title":"[논문리뷰] EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling","excerpt":"arXiv에 게시된 'EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling","tags":["Review","Reinforcement Learning","Image Editing","Reward Modeling","Instruction-Guided Editing","Online RL","Visual Language Models","Benchmark","Self-Ensembling"],"text":"링크: 논문 PDF로 바로 열기 저자: Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Liu, Zheng Liu 핵심 연구 목표 본 논문은 이미지 편집 분야에서 온라인 강화 학습(RL) 의 적용을 가로막는 주요 장애물인 고충실도(highfid"},{"id":"2025-9-30-Multiplayer-Nash-Preference-Optimization","title":"[논문리뷰] Multiplayer Nash Preference Optimization","excerpt":"arXiv에 게시된 'Multiplayer Nash Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-Multiplayer-Nash-Preference-Optimization","tags":["Review","RLHF","LLM Alignment","Nash Equilibrium","Multiplayer Games","Preference Optimization","Non-transitive Preferences","Game Theory"],"text":"링크: 논문 PDF로 바로 열기 저자: Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi 핵심 연구 목표 기존 RLHF의 BradleyTerry 모델 이 실제 세계의 비전이적("},{"id":"2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing","title":"[논문리뷰] OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing","excerpt":"Huanyu Zhang이 arXiv에 게시한 'OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing","tags":["Review","Image Generation","Image Editing","Multimodal AI","Dataset","Instruction Following","Taxonomy","GPT-40"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, Pengfei Wan, YiFan Zhang 핵심 연구 목표 본 연구는 기존 데이터셋의"},{"id":"2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards","title":"[논문리뷰] Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards","excerpt":"Binxing Jiao이 arXiv에 게시한 'Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards","tags":["Review","Reinforcement Learning","LLM Reasoning","Policy Valuation","Markov Decision Process","Diversity","Math Reasoning","Verifiable Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan 핵심 연구 목표 현재 LLM 추론을 위한 RLVR(Reinforcement Learning with Verifiable Rewards) 방법론(예: PPO, GRPO)은 일반"},{"id":"2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark","title":"[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark","excerpt":"Yuran Wang이 arXiv에 게시한 'RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark","tags":["Review","Unified Models","Multimodal AI","Benchmark","Capability Synergy","Visual Understanding","Image Generation","Dual-Evaluation Protocol"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuran Wang, Yue Ding, zooblastlbz, THUdyh, DogNeverSleep 핵심 연구 목표 본 논문은 기존 벤치마크들이 통합 멀티모달 모델의 이해 및 생성 능력을 개별적으로 평가하는 한계를 지적하며, 모델의 아키텍처적 통합 이 실제적으로 이러한 역량 간의 시너지 효과 를 유도하는지에 대한 근"},{"id":"2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer","title":"[논문리뷰] SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer","excerpt":"arXiv에 게시된 'SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer","tags":["Review","Video Generation","Diffusion Model","Linear Attention","Transformer","Long Video","Efficient Inference","Constant Memory","Low-Cost Training","RTX Deployment"],"text":"링크: 논문 PDF로 바로 열기 저자: Junsong Chen, Yuyang Zhao, Jincheng Yu, Xianbang Wang, Yicheng Pan, Hao Zhang, Muyang Li, Daquan Zhou, Yukang Chen, Ruihang Chu, Junyu Chen, Shuai Yang, Huan Ling, Haozhe Liu, Ho"},{"id":"2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention","title":"[논문리뷰] SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention","excerpt":"arXiv에 게시된 'SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention","tags":["Review","Diffusion Transformers","Sparse Attention","Linear Attention","Model Acceleration","Video Generation","Attention Mechanisms","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, Joseph E. Gonzalez, Jun Zhu, Jianfei Chen 핵심 연구 목표 본 논"},{"id":"2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs","title":"[논문리뷰] StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs","excerpt":"Wei Jia이 arXiv에 게시한 'StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs","tags":["Review","Speech Tokenizer","Noise Robustness","Semantic Tokens","SpeechLLMs","Voting-LFQ","Consensus Training","Automatic Speech Recognition","Speech Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhan Song, Linhao Zhang, Chuhan Wu, Aiwei Liu, Wei Jia, Houfeng Wang, Xiao Zhou 핵심 연구 목표 기존 시맨틱 음성 토크나이저의 노이즈에 대한 취약성 문제를 해결하는 것이 주요 목표입니다. 사소한 음향 교란에도 토큰 시퀀스가 급격히 변하여 다운스트림 Spe"},{"id":"2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs","title":"[논문리뷰] Visual Jigsaw Post-Training Improves MLLMs","excerpt":"Lewei Lu이 arXiv에 게시한 'Visual Jigsaw Post-Training Improves MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-30 13:52:24+0900","permalink":"/ai/review/2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs","tags":["Review","MLLMs","Post-training","Self-supervised Learning","Visual Understanding","Jigsaw Puzzles","RLVR","Multimodal Perception","Spatial Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Penghao Wu, Yushan Zhang, Haiwen Diao, Bo Li, Lewei Lu, Ziwei Liu 핵심 연구 목표 본 논문은 기존 MLLM(Multimodal Large Language Models)의 텍스트 중심 후속 훈련 패러다임이 시각 신호에 대한 세밀한 이해를 과소평가한다는 문제점을 해결하고"},{"id":"2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations","title":"[논문리뷰] LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations","excerpt":"Yoav Gur-Arieh이 arXiv에 게시한 'LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","permalink":"/ai/review/2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations","tags":["Review","Language Models","Knowledge Acquisition","Pretraining Data","Entity Linking","Coreference Resolution","Information Retrieval","Model Analysis","Checkpoints"],"text":"링크: 논문 PDF로 바로 열기 저자: Yoav GurArieh, Ido Cohen, Alon GilaeDotan, Daniela Gottesman, Mor Geva 핵심 연구 목표 언어 모델(LMs)이 사전 훈련 과정에서 지식 표현을 어떻게 형성하고 발전시키는지에 대한 내부 프로세스를 분석하는 것입니다. 특히, 사전 훈련 데이터 내에서 특정 지식이 언제,"},{"id":"2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement","title":"[논문리뷰] MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement","excerpt":"Hualiang Wang이 arXiv에 게시한 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","permalink":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement","tags":["Review","Multi-Subject Generation","Personalized Image Synthesis","Semantic Correspondence","Attention Disentanglement","Diffusion Models","Identity Preservation","Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Dong She, Siming Fu, Mushui Liu, Qiaoqiao Jin, Hualiang Wang, Mu Liu, Jidong Jiang 핵심 연구 목표 이 논문은 다중 피사체 개인화 이미지 생성 시 발생하는 정체성 혼합(identity blending) 및 속성 유출(attribute leakage) 문제"},{"id":"2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation","title":"[논문리뷰] Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation","excerpt":"Kai Li이 arXiv에 게시한 'Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","permalink":"/ai/review/2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation","tags":["Review","Diffusion Transformer","Mixture of Experts","Controllable Generation","Face Generation","Multimodal Synthesis","Semantic Control","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xuechao Zou, Shun Zhang, Xing Fu, Yue Li, Kai Li, Yushe Cao, Congyan Lang, Pin Tao, Junliang Xing 핵심 연구 목표 논문은 기존 생성 모델이 의미론적 제어와 사진 같은 사실성 사이의 섬세한 균형을 맞추는 데 어려움을 겪고, 특히 Diffusio"},{"id":"2025-9-4-Open-Data-Synthesis-For-Deep-Research","title":"[논문리뷰] Open Data Synthesis For Deep Research","excerpt":"Zheng Liu이 arXiv에 게시한 'Open Data Synthesis For Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","permalink":"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research","tags":["Review","Data Synthesis","Deep Research","Hierarchical Constraint Satisfaction Problems","Large Language Models","Agentic AI","Reinforcement Learning","Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyi Xia, Kun Luo, Hongjin Qian, Zheng Liu 핵심 연구 목표 본 논문은 기존 벤치마크들이 \"심층 연구(Deep Research)\" 작업을 위한 충분한 구조적 깊이를 제공하지 못하는 한계를 해결하고자 합니다. 특히, 복잡한 질문을 하위 문제로 분해하고, 다단계 추론을 조율하며, 다양한 출"},{"id":"2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning","title":"[논문리뷰] Robix: A Unified Model for Robot Interaction, Reasoning and Planning","excerpt":"Zixuan Wang이 arXiv에 게시한 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-04 12:56:15+0900","permalink":"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning","tags":["Review","Robot Learning","Vision-Language Models (VLMs)","Embodied AI","Human-Robot Interaction (HRI)","Task Planning","Reinforcement Learning (RL)","Chain-of-Thought (CoT) Reasoning","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Zixuan Wang, Wei Li, Heng Dong, Mengxi Zhang, Huang Fang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li 핵심 연구 목표 본 논문은 일반ist 로봇이 복잡한 장기 작업을 추론하고 자연스러운 인간 상호작용에 참여할 수 있도록 단일 비전언어"},{"id":"2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks","title":"[논문리뷰] DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks","excerpt":"Jiaxuan Lu이 arXiv에 게시한 'DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks","tags":["Review","LLM Evaluation","Research Agents","Benchmark","Multi-Agent System","Seminar-Grounded Tasks","Data Leakage Prevention","Ill-Structured Problems"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaxuan Lu, Meiqi Tu, Junchi Yu, Chen Yang, haiyuanwan 핵심 연구 목표 본 논문은 기존 벤치마크의 데이터 누출 위험과 비현실적인 평가 방식의 한계를 극복하기 위해, 대규모 언어 모델(LLM) 기반 연구 에이전트 의 실제 연구 능력을 평가하기 위한 새로운 벤치마크인 DeepRe"},{"id":"2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models","title":"[논문리뷰] Delta Activations: A Representation for Finetuned Large Language Models","excerpt":"Ser-Nam Lim이 arXiv에 게시한 'Delta Activations: A Representation for Finetuned Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models","tags":["Review","LLM Embedding","Delta Activations","Finetuned Models","Model Representation","Model Clustering","Additive Property","Task Embedding","Model Merging"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiqiu Xu, Amish Sethi, Mayur Naik, SerNam Lim 핵심 연구 목표 다양하게 미세 조정된 대규모 언어 모델(LLM)의 방대한 생태계에서 모델 간의 유사점과 차이점을 효율적으로 파악하고, 모델을 검색, 비교 및 클러스터링할 수 있는 표준화된 표현 방식 이 부족한 문제를 해결하는 것이 목표"},{"id":"2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings","title":"[논문리뷰] Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings","excerpt":"Meie Fang이 arXiv에 게시한 'Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings","tags":["Review","CAD Generation","Vector Graphics","Sequence-to-Sequence Learning","Transformer Architecture","Engineering Drawings","Multi-modal Learning","Soft Target Loss","Dual Decoder"],"text":"링크: 논문 PDF로 바로 열기 저자: Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu 핵심 연구 목표 본 연구는 2D 벡터 엔지니어링 도면(SVG 형식)으로부터 파라메트릭 CAD 모델을 자동으로 생성 하는 문제를 해결하는 것을 목표로 합니다. 기존 방식들이 래스터 이미지나"},{"id":"2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth","title":"[논문리뷰] Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth","excerpt":"Chi-Li Chen이 arXiv에 게시한 'Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth","tags":["Review","Large Language Models","Pragmatic Understanding","Drivelology","Benchmark Dataset","Multilingual NLP","Semantic Reasoning","Contextual Inference"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Wang, Chenghao Xiao, ChiaYi Hsiao, Zi Yan Chang, ChiLi Chen, Tyler Loakman, Chenghua Lin 핵심 연구 목표 본 연구는 LLM(Large Language Models)이 겉으로는 논리적이지만 심층적인 역설적 의미를 담고 있는 \"Drivelolo"},{"id":"2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer","title":"[논문리뷰] Durian: Dual Reference-guided Portrait Animation with Attribute Transfer","excerpt":"Hanbyul Joo이 arXiv에 게시한 'Durian: Dual Reference-guided Portrait Animation with Attribute Transfer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer","tags":["Review","Portrait Animation","Attribute Transfer","Diffusion Models","Dual Reference Networks","Zero-shot Learning","Self-Reconstruction","Facial Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyunsoo Cha, Byungjun Kim, Hanbyul Joo 핵심 연구 목표 본 논문은 주어진 참조 이미지로부터 대상 인물의 얼굴 속성(예: 헤어스타일, 안경)을 전이하여 동적인 초상화 애니메이션 비디오를 제로샷(zeroshot) 방식으로 생성하는 것을 목표로 합니다. 기존 정적 이미지 편집이나 복잡한 마스킹"},{"id":"2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize","title":"[논문리뷰] False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize","excerpt":"Muhao Chen이 arXiv에 게시한 'False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize","tags":["Review","LLM Safety","Malicious Input Detection","Probing Classifiers","Out-of-Distribution Generalization","Superficial Patterns","Instructional Patterns","Trigger Words","AI Safety"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Wang, Zeming Wei, Qin Liu, Muhao Chen 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM)의 악성 입력 감지를 위해 제안된 프루빙 기반(probingbased) 방법론 의 신뢰성을 재평가하는 것을 목표로 합니다. 기존 연구에서 보고된 높은 인도메인(indomain) 정확도가 실"},{"id":"2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation","title":"[논문리뷰] Few-step Flow for 3D Generation via Marginal-Data Transport Distillation","excerpt":"Lingxi Xie이 arXiv에 게시한 'Few-step Flow for 3D Generation via Marginal-Data Transport Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation","tags":["Review","3D Generation","Flow-based Models","Model Distillation","Few-step Sampling","Marginal-Data Transport","Velocity Matching","Velocity Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zanwei Zhou, Taoran Yi, Jiemin Fang, Chen Yang, Lingxi Xie, Xinggang Wang, Wei Shen, Qi Tian 핵심 연구 목표 본 연구는 플로우 기반 3D 생성 모델의 느린 추론 속도 문제를 해결하는 것을 목표로 합니다. 기존 모델들이 수십 단계의 샘플링을 요구하"},{"id":"2025-9-5-From-Editor-to-Dense-Geometry-Estimator","title":"[논문리뷰] From Editor to Dense Geometry Estimator","excerpt":"Lang Nie이 arXiv에 게시한 'From Editor to Dense Geometry Estimator' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-From-Editor-to-Dense-Geometry-Estimator","tags":["Review","Dense Geometry Estimation","Diffusion Transformer","Image Editing","Zero-shot Learning","Depth Estimation","Normal Estimation","Flow Matching","Logarithmic Quantization"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiyuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang Nie, Mingxing Li, Kang Liao, Xiangxiang Chu, Yao Zhao 핵심 연구 목표 본 논문은 기존의 텍스트투이미지(T2I) 생성 모델보다 Diffusion Transformer (DiT) 기반의"},{"id":"2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions","title":"[논문리뷰] Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?","excerpt":"Yu Fu이 arXiv에 게시한 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions","tags":["Review","LLMs","Instruction Following","Benchmark","Cognitive Inertia","Out-of-Distribution","Supervised Fine-Tuning","Evaluation","Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Qinyan Zhang, Xinping Lei, Ruijie Miao, Yu Fu, Haojie Fan, Le Chang, Jiafan Hou, Dingling Zhang, Zhongfei Hou, Ziqiang Yang, Changxin Pu, Fei Hu, Jingkai Liu, Mengyun Liu, Yang L"},{"id":"2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings","title":"[논문리뷰] NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings","excerpt":"Oren Glickman이 arXiv에 게시한 'NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings","tags":["Review","Named Entity Retrieval","Zero-Shot Learning","Type-Aware Embeddings","Large Language Models (LLMs)","Contrastive Learning","Internal Representations","Information Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman 핵심 연구 목표 논문은 기존 NER(Named Entity Recognition) 시스템의 한계, 즉 고정된 유형 스키마와 대량의 레이블링 데이터 의존성을 극복하고자 합니다. 사용자가 정의한 임의의(adhoc) 엔티티 유형 쿼리"},{"id":"2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training","title":"[논문리뷰] Towards a Unified View of Large Language Model Post-Training","excerpt":"Hongyi Liu이 arXiv에 게시한 'Towards a Unified View of Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training","tags":["Review","Large Language Models (LLMs)","Post-Training","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Policy Gradient","Unified Framework","Hybrid Algorithms","Bias-Variance Tradeoff"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Lixuan He, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou 핵심 연구 목표 본 논문은 LLM의 포스트 트레이닝 과"},{"id":"2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective","title":"[논문리뷰] Transition Models: Rethinking the Generative Learning Objective","excerpt":"Yangguang Li이 arXiv에 게시한 'Transition Models: Rethinking the Generative Learning Objective' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective","tags":["Review","Generative Models","Diffusion Models","Training Objective","Continuous-Time Dynamics","State Transition","Few-Step Generation","Scalable Training","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zidong Wang, Yiyuan Zhang, Xiangyu Yue, Xiaoyu Yue, Yangguang Li, Wanli Ouyang, Lei Bai 핵심 연구 목표 본 논문은 반복적인 확산 모델의 높은 품질과 효율적인 소수 단계 모델의 성능 포화 사이의 근본적인 딜레마를 해결하고자 합니다. 이 충돌이 제한적인"},{"id":"2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding","title":"[논문리뷰] Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding","excerpt":"Lionel Ni이 arXiv에 게시한 'Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-05 13:07:20+0900","permalink":"/ai/review/2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding","tags":["Review","Long Video Understanding","Reinforcement Learning","Multi-Turn Reasoning","MLLMs","Video Segment Selection","Bi-level Reward","Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuan Xie, Tianshui Chen, Zheng Ge, Lionel Ni 핵심 연구 목표 본 논문은 장시간 비디오 이해의 난제를 해결하고자 합니다. 기존 방법론들이 정적 추론이나 외부 VLM(VisualLanguage Model) 에 의존하여 복잡성, 비최적 성능, 종단 간 학습 부재 등의 한계를 보이는 문제를"},{"id":"2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models","title":"[논문리뷰] Behavioral Fingerprinting of Large Language Models","excerpt":"Xing Li이 arXiv에 게시한 'Behavioral Fingerprinting of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models","tags":["Review","Large Language Models","Behavioral Evaluation","Model Alignment","Sycophancy","World Model Brittleness","Metacognition","Personality Profiling"],"text":"링크: 논문 PDF로 바로 열기 저자: Zehua Pei, HuiLing Zhen, Ying Zhang, Zhiyuan Yang, Xing Li, Xianzhi Yu, Mingxuan Yuan, Bei Yu 핵심 연구 목표 현재 대규모 언어 모델(LLM) 벤치마크들이 모델의 성능 지표에만 치중하여 미묘한 행동 특성을 포착하지 못하는 문제를 해결하고자 합니다"},{"id":"2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement","title":"[논문리뷰] Bootstrapping Task Spaces for Self-Improvement","excerpt":"Yoram Bachrach이 arXiv에 게시한 'Bootstrapping Task Spaces for Self-Improvement' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement","tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Self-Improvement","Autocurriculum","Task-Space Exploration","Inference-Time Iteration","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Minqi Jiang, Andrei Lupu, Yoram Bachrach 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 추론 시 여러 단계에 걸쳐 스스로 개선하는 능력을 학습하는 방법을 연구합니다. 기존의 자기 개선 훈련 방식이 가진 고정된 반복 깊이, 높은 비용, 출력 다양성 감소 등의 한계를 극복하고, 동"},{"id":"2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation","title":"[논문리뷰] LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation","excerpt":"Zhan Zhao이 arXiv에 게시한 'LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation","tags":["Review","Multimodal LLM","3D World Generation","Unreal Engine 5","Procedural Content Generation","Interactive Environments","Sim-to-Real","Spatial Understanding","Multimodal Input"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Hao Jiang, Kang Chen, Shuang Qiu 핵심 연구 목표 본 논문은 복잡한 실제 시나리오를 시뮬레이션하는 고충실도 3D 가상 환경 을 생성하는 데 초점을 맞"},{"id":"2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer","title":"[논문리뷰] LuxDiT: Lighting Estimation with Video Diffusion Transformer","excerpt":"Sanja Fidler이 arXiv에 게시한 'LuxDiT: Lighting Estimation with Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer","tags":["Review","Lighting Estimation","HDR Environment Map","Diffusion Models","Video Transformer","Low-Rank Adaptation","Generative Models","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, Zian Wang 핵심 연구 목표 논문은 단일 이미지 또는 비디오로부터 고품질의 HDR 환경 맵 을 추정하는 오랜 난제를 해결하고자 합니다. 이는 실측 HDR 환"},{"id":"2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting","title":"[논문리뷰] MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting","excerpt":"Vanessa Wildman이 arXiv에 게시한 'MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting","tags":["Review","3D CT","Vision-Language Model","Medical Imaging","Diagnostic Error Reduction","Multi-scale Alignment","Semantic Enrichment","Radiology Reporting","Zero-shot Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuheng Li, Yuxiang Lai, Yenho Chen, Jike Zhong, Vanessa Wildman, Xiaofeng Yang 핵심 연구 목표 3D CT 영상 진단에서 발생하는 오독(underreading), 부주의로 인한 인지 오류(inattentional blindness), 그리고 커뮤니케이션 오류"},{"id":"2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs","title":"[논문리뷰] On Robustness and Reliability of Benchmark-Based Evaluation of LLMs","excerpt":"Kevin Roitero이 arXiv에 게시한 'On Robustness and Reliability of Benchmark-Based Evaluation of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs","tags":["Review","LLM Evaluation","Model Robustness","Benchmark Reliability","Paraphrasing","Linguistic Variability","Generalization","Question Answering"],"text":"링크: 논문 PDF로 바로 열기 저자: Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro, Kevin Roitero 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 문맥에 따라 재구성된 질문에 얼마나 강건한지 를 평가하고, 현재 사용되는 벤치마크 기반 평가가 모델의 실제 능력을 얼마나 신뢰성 있게 측정하"},{"id":"2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator","title":"[논문리뷰] Set Block Decoding is a Language Model Inference Accelerator","excerpt":"Jeremy Reizenstein이 arXiv에 게시한 'Set Block Decoding is a Language Model Inference Accelerator' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator","tags":["Review","Language Model Inference","Acceleration","Set Block Decoding","Next Token Prediction","Masked Token Prediction","Parallel Decoding","KV-caching","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Itai Gat, Heli BenHamu, Marton Havasi, Daniel Haziza, Jeremy Reizenstein, Gabriel Synnaeve, David LopezPaz, Brian Karrer, Yaron Lipman 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 추론, 특히 디코딩 단계"},{"id":"2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models","title":"[논문리뷰] Symbolic Graphics Programming with Large Language Models","excerpt":"Kaipeng Zhang이 arXiv에 게시한 'Symbolic Graphics Programming with Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models","tags":["Review","Symbolic Graphics Programming","Large Language Models","Reinforcement Learning","SVG Generation","Text-to-Image Synthesis","Cross-Modal Alignment","Program Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaipeng Zhang, Zeju Qiu, Haoquan Zhang, Yamei Chen, Yangyi Huang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 자연어 설명으로부터 정확한 시각적 콘텐츠를 렌더링하는 심볼릭 그래픽 프로그램(SGPs) , 특히 Scalable Vector Graphics (S"},{"id":"2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation","title":"[논문리뷰] U-ARM : Ultra low-cost general teleoperation interface for robot manipulation","excerpt":"Junda Huang이 arXiv에 게시한 'U-ARM : Ultra low-cost general teleoperation interface for robot manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation","tags":["Review","Teleoperation","Robot Manipulation","Low-Cost Hardware","3D Printing","Leader-Follower System","Data Collection","Robotics Interface","Open Source"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanwen Zou, Zhaoye Zhou, Chenyang Shi, Zewei Ye, Junda Huang, Yan Ding†, Bo Zhao 핵심 연구 목표 본 논문은 기존의 고비용 및 복잡한 엔지니어링 요구사항을 가진 로봇 텔레오퍼레이션 시스템의 한계를 극복하고, 대부분의 상용 로봇 팔과 호환되는 초저가, 사용자"},{"id":"2025-9-8-Why-Language-Models-Hallucinate","title":"[논문리뷰] Why Language Models Hallucinate","excerpt":"Edwin Zhang이 arXiv에 게시한 'Why Language Models Hallucinate' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-Why-Language-Models-Hallucinate","tags":["Review","Language Models","Hallucination","Pretraining","Post-training","Evaluation Metrics","Binary Classification","Uncertainty Quantification","Calibration"],"text":"링크: 논문 PDF로 바로 열기 저자: Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala, Edwin Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 \"환각\" 현상, 즉 그럴듯하지만 틀린 정보를 자신감 있게 생성하는 이유를 통계적으로 분석하고, 이러한 문제가 최신 모델에서도 지속되는 근본적인 원인을"},{"id":"2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning","title":"[논문리뷰] WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning","excerpt":"Amit Namburi이 arXiv에 게시한 'WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning","tags":["Review","Multimodal Large Language Models","Symbolic Music Reasoning","Music Score Analysis","Benchmarking","Visual Question Answering","In-the-Wild Data","Music Theory"],"text":"링크: 논문 PDF로 바로 열기 저자: Gagan Mundada, Yash Vishe, Amit Namburi, Xin Xu, Zachary Novack, Julian McAuley, Junda Wu 핵심 연구 목표 본 논문은 Multimodal Large Language Models (MLLMs) 의 상징적 음악 분석 및 추론 능력에 대한 실세계 적용 가"},{"id":"2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool","title":"[논문리뷰] WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool","excerpt":"Wenzheng Chang이 arXiv에 게시한 'WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-08 13:10:18+0900","permalink":"/ai/review/2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool","tags":["Review","Online 3D Reconstruction","Camera Pose Estimation","Streaming Reconstruction","Sliding Window","Camera Token Pool","Real-time Performance","Computer Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Zizun Li, Jianjun Zhou, Yifan Wang, Haoyu Guo, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Junyi Chen, Chunhua Shen, Tong He 핵심 연구 목표 본 연구는 기존 온라인 3D 재구성 방법들이 겪는 재구성 품질과 실시간 성능 간의 절충 문"},{"id":"2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning","title":"[논문리뷰] D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning","excerpt":"Dhanvin Sanjay Namboodiri이 arXiv에 게시한 'D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning","tags":["Review","Dark Humor Detection","Multimodal Reasoning","Vision-Language Models (VLMs)","Iterative Reasoning Refinement","Meme Analysis","Content Moderation","Cross-Modal Attention","Dataset Annotation"],"text":"링크: 논문 PDF로 바로 열기 저자: Sai Kartheek Reddy Kasu, Mohammad Zia Ur Rehman, Shahid Shafi Dar, Rishi Bharat Junghare, Dhanvin Sanjay Namboodiri, Nagendra Kumar 핵심 연구 목표 온라인 밈(meme)에서 암묵적이고 문화적으로 민감한 다크 유머를 "},{"id":"2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard","title":"[논문리뷰] Does DINOv3 Set a New Medical Vision Standard?","excerpt":"Bailiang Jian이 arXiv에 게시한 'Does DINOv3 Set a New Medical Vision Standard?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard","tags":["Review","Medical Imaging","Foundation Models","DINOv3","Self-Supervised Learning","Vision Transformer","2D/3D Classification","Segmentation","Domain Adaptation","Scaling Laws"],"text":"링크: 논문 PDF로 바로 열기 저자: Che Liu, Yinda Chen, Haoyuan Shi, Jinpeng Lu, Bailiang Jian, Jiazhen Pan, Linghan Cai, Jiayi Wang, Yundi Zhang, Jun Li, Cosmin I. Bercea, Cheng Ouyang, Chen Chen, Zhiwei Xiong, B"},{"id":"2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play","title":"[논문리뷰] Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?","excerpt":"Rui Chen이 arXiv에 게시한 'Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play","tags":["Review","Text-to-Image Generation","T2I Benchmarking","Compositional Reasoning","Deductive Inference","Inductive Inference","Abductive Inference","MLLM Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Fuli Feng 핵심 연구 목표 본 논문은 기존 텍스트투이미지(T2I) 벤치마크의 한계를 해결하고, T2I 모델의 구성(composition) 및 추"},{"id":"2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning","title":"[논문리뷰] Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning","excerpt":"Baolong Bi이 arXiv에 게시한 'Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning","tags":["Review","Vision-Language Models (VLMs)","Visual Reasoning","Attention Mechanisms","Contrastive Learning","Noise Suppression","Visual Complexity","Training-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Xuanshan Zhou, Jiayu Yao, Jiafeng Guo, Xueqi Cheng 핵심 연구 목표 본 논문은 복잡한 시각 환경에서 VisionLanguage Models (VLMs) 의 추론 성능이 저"},{"id":"2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation","title":"[논문리뷰] Interleaving Reasoning for Better Text-to-Image Generation","excerpt":"Shixiang Tang이 arXiv에 게시한 'Interleaving Reasoning for Better Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Interleaving Reasoning","Multimodal Learning","Visual Quality","Fine-grained Detail","Diffusion Models","Self-Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Shixiang Tang, Shaosheng Cao, Zheyong Xie, Shuang Chen, Wenxuan Huang 핵심 연구 목표 본 논문은 기존 텍스트이미지(T2I) 생성 모델의 명령어 준수 및 세부 묘사 능력 한계를 극복하는 것을 목표로 합니다. 특히, 인터리빙 추론(Interleaving Reasoni"},{"id":"2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian","title":"[논문리뷰] Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian","excerpt":"Hoi-Fong Mak이 arXiv에 게시한 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian","tags":["Review","Multilingual LLM","Low-Resource Language","German","Bavarian Dialect","Cross-Lingual Transfer","Continuous Pretraining","Llama-3.1","Model Expansion"],"text":"링크: 논문 PDF로 바로 열기 저자: Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Alice Zhang, Dmitry Gaynullin, HoiFong Mak, Nicolay J. Hammer 핵심 연구 목표 대규모 언어 모델(LLM)의 영어 중심 편향 을 해결하고, 독일어, 영"},{"id":"2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents","title":"[논문리뷰] MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents","excerpt":"Zhengxi Lu이 arXiv에 게시한 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents","tags":["Review","Mobile GUI Agents","Hybrid Automation","Shortcut Generation","Benchmark","Task Efficiency","LLM-based Agents","Mobile Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengxiang Zhao, Guangyi Liu, Yaozhen Liang, Weiqing He, Zhengxi Lu, Yuehao Huang, Yaxuan Guo, Kexin Zhang, Hao Wang, Liang Liu, Yong Liu 핵심 연구 목표 이 논문은 모바일 GUI 에이전트의 효율성을 높이기 위해 "},{"id":"2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents","title":"[논문리뷰] Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents","excerpt":"James Zou이 arXiv에 게시한 'Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents","tags":["Review","AI Agents","Research Reproducibility","Scientific Communication","Model Context Protocol (MCP)","Natural Language Interaction","Genomics","Single-Cell Analysis","Spatial Transcriptomics"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiacheng Miao, Joe R. Davis, Jonathan K. Pritchard, James Zou 핵심 연구 목표 본 논문은 정적인 연구 논문이 가진 기술적 장벽으로 인해 코드 및 방법론의 활용과 확산이 어려운 문제를 해결하고자 합니다. 연구는 논문을 상호작용적이고 신뢰할 수 있는 AI 에이전트 로 변환하"},{"id":"2025-9-9-Reinforced-Visual-Perception-with-Tools","title":"[논문리뷰] Reinforced Visual Perception with Tools","excerpt":"Mingyang Fu이 arXiv에 게시한 'Reinforced Visual Perception with Tools' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Reinforced-Visual-Perception-with-Tools","tags":["Review","Visual Reasoning","Multimodal LLMs","Reinforcement Learning","Tool Usage","Perception-heavy Benchmarks","GRPO","Vision Tools"],"text":"링크: 논문 PDF로 바로 열기 저자: Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, Ranjay Krishna 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(LLM)이 복잡한 시각적 추론 문제를 해결하고 외부 시각 도구를 효과"},{"id":"2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey","title":"[논문리뷰] Reinforcement Learning Foundations for Deep Research Systems: A Survey","excerpt":"Wei Han이 arXiv에 게시한 'Reinforcement Learning Foundations for Deep Research Systems: A Survey' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey","tags":["Review","Reinforcement Learning","Deep Research Systems","Agentic AI","Tool Use","Hierarchical Agents","Reward Design","Multimodal AI","RL Frameworks"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenjun Li, Zhi Chen, Jingru Lin, Hannan Cao, Wei Han, Sheng Liang, Zhi Zhang, Kuicai Dong, Dexun Li, Chen Zhang, Yong Liu 핵심 연구 목표 본 논문은 복잡한 다단계 작업을 해결하는 딥 리서치 에이전트(agentic AI) 훈"},{"id":"2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation","title":"[논문리뷰] Reverse-Engineered Reasoning for Open-Ended Generation","excerpt":"Wangchunshu Zhou이 arXiv에 게시한 'Reverse-Engineered Reasoning for Open-Ended Generation' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation","tags":["Review","Deep Reasoning","Open-Ended Generation","Reverse-Engineered Reasoning (REER)","LLMs","Synthetic Data","Iterative Refinement","Perplexity Minimization","DeepWriting-20K"],"text":"링크: 논문 PDF로 바로 열기 저자: Wangchunshu Zhou, Minghao Liu, Qixin Xu, Haoran Que, Haozhe Wang 핵심 연구 목표 개방형(openended) 및 창의적 생성과 같이 검증 불가능한 도메인에서 대규모 언어 모델(LLM)에 깊이 있는 추론 능력 을 부여하는 것이 이 연구의 핵심 목표입니다. 기존의 강화 학"},{"id":"2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models","title":"[논문리뷰] Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models","excerpt":"Ke Shen이 arXiv에 게시한 'Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models","tags":["Review","Diffusion Language Models","Reinforcement Learning","Trajectory-aware RL","Value Model","Masked Diffusion Models","Large Language Models","Reasoning Tasks","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang 핵심 연구 목표 본 논문은 확산 언어 모델(DLMs)의 기존 강화 학습(RL) 프레임워크의 한계를 해결하고자 합니다. 특히, 사후 훈련 목표와 추론 궤적 간의 불일치를 개선하고, 다양한 DLM 아키텍처(f"},{"id":"2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World","title":"[논문리뷰] R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World","excerpt":"Bowen Zhou이 arXiv에 게시한 'R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World","tags":["Review","AI Safety","Resistant AI","Resilient AI","Coevolution","Fast-Slow Models","Adversarial Training","Continual Learning","AGI Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Youbang Sun, Xiang Wang, Jie Fu, Chaochao Lu, Bowen Zhou 핵심 연구 목표 이 논문은 급증하는 AI 역량과 뒤처지는 안전성 발전 간의 지속적인 격차를 해결하고자 합니다. 기존의 수동적이고 반응적인 안전 접근 방식의 한계를 지적하며, 예측 불가능한 위험에 적응하고 지능과 함께 "},{"id":"2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem","title":"[논문리뷰] Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem","excerpt":"Damien Sileo이 arXiv에 게시한 'Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem","tags":["Review","Automated Theorem Proving","LLM","Mathematical Reasoning","Synthetic Data Generation","TPTP Ecosystem","Saturation Proving","Proof Graph Reconstruction","Data Augmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Valentin Quesnel, Damien Sileo 핵심 연구 목표 대규모 언어 모델(LLM)의 수학적 추론 능력 향상을 저해하는 고품질, 논리적으로 건전한 데이터의 부족 문제를 해결하는 것이 주된 목표입니다. 수십 년간의 자동화된 정리 증명(ATP) 연구를 확장 가능한 데이터 엔진으로 전환하여 LLM의 학습을 위"},{"id":"2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers","title":"[논문리뷰] Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers","excerpt":"Xia Xiao이 arXiv에 게시한 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers","tags":["Review","LLM Step-Provers","Reinforcement Learning (RL)","Off-Policy RL","Multi-Agent Systems","Tree Search","Automated Theorem Proving (ATP)","Formal Mathematics","AlphaZero"],"text":"링크: 논문 PDF로 바로 열기 저자: Xia Xiao, Kun Yuan, Yanchen Nie, Zeyu Zheng, Ran Xin 핵심 연구 목표 논문은 대규모 언어 모델(LLM) 기반 자동화된 정리 증명 시스템에서 발생하는 훈련 시간(trainingtime) 확장성 과 추론 시간(inferencetime) 컴퓨팅 이라는 두 가지 핵심 과제를 해결하는 "},{"id":"2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet","title":"[논문리뷰] Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet","excerpt":"See-Kiong Ng이 arXiv에 게시한 'Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet","tags":["Review","Test-Time Scaling","Reasoning Models","Knowledge-Intensive Tasks","Hallucinations","Factual Accuracy","Chain-of-Thought","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: James Xu Zhao, Bryan Hooi, SeeKiong Ng 핵심 연구 목표 본 논문은 지식 집약적 태스크에서 TestTime Scaling 기법이 모델의 정확도와 환각(hallucination) 감소에 효과적인지 종합적으로 평가하는 것을 목표로 합니다. 특히, 추론 시간을 늘리는 것이 팩트 기반 질문 답변 "},{"id":"2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts","title":"[논문리뷰] UniVerse-1: Unified Audio-Video Generation via Stitching of Experts","excerpt":"Xinyao Liao이 arXiv에 게시한 'UniVerse-1: Unified Audio-Video Generation via Stitching of Experts' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts","tags":["Review","Unified Audio-Video Generation","Stitching of Experts (SoE)","Multimodal Diffusion","Online Annotation","Cross-modal Noise Correlation","Foundation Models","Verse-Bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Duomin Wang, Wei Zuo, Aojie Li, LingHao Chen, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, Gang Yu 핵심 연구 목표 본 논문은 기존 비디오 생성 모델 들이 시각적 도메인에만 집중하여 오디오비디오의 다중 모달 특성을 간과하는 문제를 해결하고, "},{"id":"2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents","title":"[논문리뷰] WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents","excerpt":"Aili Chen이 arXiv에 게시한 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2025-09-09 13:19:09+0900","permalink":"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents","tags":["Review","Web Agents","Long-Horizon Reasoning","Large Language Models (LLMs)","Data Generation","Reinforcement Learning (RL)","Supervised Fine-tuning (SFT)","Web Navigation","Information Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, et al. 핵심 연구 목표 본 논문은 복잡한 정보 탐색과 다단계 웹 탐색을 요구하는 장기 웹 에이전트 를 훈련하기 위한 핵심 과제인 고품질 훈련 데이터 부족 문제 를 해결하고자 합니다. 기존 웹 에이전트의 제한적인"},{"id":"2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents","title":"[논문리뷰] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents","excerpt":"Shixin Jiang이 arXiv에 게시한 'AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents","tags":["Review","Autonomous Agents","Memory Systems","Cognitive Neuroscience","Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Memory Management","Multimodal Memory","Agent Skills"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, SeeKiong Ng, Xia Liang†, Ming Liu†, and Bing Qin."},{"id":"2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts","title":"[논문리뷰] BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts","excerpt":"Mengmeng Wang이 arXiv에 게시한 'BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts","tags":["Review","Strategic Dialogue","Belief Estimation","Dialogue Acts","Probabilistic Constraints","Theory of Mind","Adversarial Dialogue","Alignment Dialogue"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengli Li, Zhaoxin Yu, Qi Shen, Chenxi Li, Mengmeng Wang, Yipeng Kang, Yuxuan Wang, Tinglang Wu, SongChun Zhu, Zilong Zheng 핵심 연구 목표 전략적 대화에서 에이전트가 정확하게 추정된 신념을 발화 생성에 효과적으로 활용하는"},{"id":"2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models","title":"[논문리뷰] Factorized Learning for Temporally Grounded Video-Language Models","excerpt":"arXiv에 게시된 'Factorized Learning for Temporally Grounded Video-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models","tags":["Review","Video-Language Models","Temporal Grounding","Factorized Learning","Preference Optimization","Evidence Referencing","Video Understanding","Dense Captioning"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng 핵심 연구 목표 기존 비디오언어 모델(VLLMs)이 이벤트 수준의 정확한 temporal grounding 과 텍스트 응답 생성에서 겪는 한계를 해결하는 것을 목표로 합니다. 특히, temporal grounding과 텍"},{"id":"2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process","title":"[논문리뷰] Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process","excerpt":"arXiv에 게시된 'Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process","tags":["Review","LLM Reasoning","Mechanistic Interpretability","Sparse Autoencoders (SAEs)","Activation Steering","Unsupervised Learning","Reasoning Behaviors","Chain-of-Thought","Feature Disentanglement"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, Mingqing Chen, Andrew Hard, Rajiv Mathews, Lun Wang 핵심 연구 목표 대규모 언어 모델(LLM)의 복잡한 추론 과정 중 내부 메커니즘을 심층적으로 이"},{"id":"2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking","title":"[논문리뷰] Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking","excerpt":"Jie Zhou이 arXiv에 게시한 'Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking","tags":["Review","Multimodal Reasoning","Visual Thinking","Reinforcement Learning","Code Generation","Geometric Reasoning","Adaptive Reward Mechanism","Problem Solving"],"text":"링크: 논문 PDF로 바로 열기 저자: Meiqi Chen, Fandong Meng, Jie Zhou 핵심 연구 목표 본 논문은 텍스트 전용 추론 모델이 암묵적인 공간 및 기하학적 관계를 파악하는 데 어려움을 겪는 복잡한 추론 문제의 한계를 해결하고자 합니다. 기존 멀티모달 모델의 시각적 부정확성 및 도구 사용의 제약에서 벗어나, 능동적인 시각적 사고를 추"},{"id":"2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems","title":"[논문리뷰] Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems","excerpt":"arXiv에 게시된 'Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems","tags":["Review","Multi-modal Pre-training","Autonomous Systems","Spatial Intelligence","Foundation Models","LiDAR-Camera Fusion","Self-Supervised Learning","Generative World Models","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi, WorldBench Team (Equal Contributions, Corresponding Author) 핵심 연구 목표 본 논문은 자율 시스템을 위한 진정한"},{"id":"2026-01-01-GR-Dexter-Technical-Report","title":"[논문리뷰] GR-Dexter Technical Report","excerpt":"arXiv에 게시된 'GR-Dexter Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-GR-Dexter-Technical-Report","tags":["Review","Dexterous Manipulation","Bimanual Robotics","VLA Models","Robot Learning","Teleoperation","Cross-Embodiment Data","Robotic Hand Design"],"text":"링크: 논문 PDF로 바로 열기 저자: ByteDance Seed 핵심 연구 목표 본 논문은 고자유도(highDoF) 양손 덱스터러스 핸드 로봇에서 VisionLanguageAction (VLA) 모델 기반의 일반화된 로봇 조작 정책을 확장하는 과제를 해결합니다. 확장된 동작 공간, 빈번한 핸드객체 가려짐, 실세계 데이터 수집 비용이라는 난제를 극복하고, "},{"id":"2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction","title":"[논문리뷰] GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction","excerpt":"Yu-Lun Liu이 arXiv에 게시한 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction","tags":["Review","3D Reconstruction","Sparse-View","Diffusion Models","Outpainting","Gaussian Splatting","Geometry-aware","Novel View Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: YiChuan Huang, HaoJen Chien, ChinYang Lin, YingHuan Chen, YuLun Liu 핵심 연구 목표 본 논문은 제한된 입력 뷰(sparseview) 환경에서 발생하는 3D 재구성의 고질적인 문제(구멍, 고스팅, 기하학적 불일치)를 해결하고자 합니다. 기존의 Novel View Ge"},{"id":"2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers","title":"[논문리뷰] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers","excerpt":"Mahşuk Taylan이 arXiv에 게시한 'Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers","tags":["Review","Respiratory Sound Classification","Audio Spectrogram Transformer","Sharpness-Aware Minimization","Loss Landscape","Imbalanced Learning","Transfer Learning","ICBHI 2017"],"text":"링크: 논문 PDF로 바로 열기 저자: Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan 핵심 연구 목표 호흡음 분류를 위한 ICBHI 2017 과 같은 소규모, 고노이즈, 클래스 불균형 데이터셋에서 Transformer 기반 모델의 과적합 및 일반화 문제 를 해결하고, 특히 민감도(Sens"},{"id":"2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself","title":"[논문리뷰] Guiding a Diffusion Transformer with the Internal Dynamics of Itself","excerpt":"arXiv에 게시된 'Guiding a Diffusion Transformer with the Internal Dynamics of Itself' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself","tags":["Review","Diffusion Models","Transformer","Generative AI","Image Generation","Guidance Strategy","Internal Guidance","Auxiliary Loss","Classifier-Free Guidance"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu 핵심 연구 목표 확산 트랜스포머(Diffusion Transformer) 모델이 저확률 데이터 영역에서 고품질 이미지를 생성하지 못하는 문제를 해결하는 것이 목표입니다. 기존 ClassifierFree Guidance "},{"id":"2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation","title":"[논문리뷰] JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation","excerpt":"arXiv에 게시된 'JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation","tags":["Review","Multimodal LLM","Sounding Video","Video Comprehension","Video Generation","Audio-Video Synchronization","Instruction Tuning","Diffusion Model","Encoder-Decoder"],"text":"링크: 논문 PDF로 바로 열기 저자: Kai Liu, Jungang Li, Yuchong Sun, Shengqiong Wu, Jianzhang Gao, Daoan Zhang, Wei Zhang, Sheng Jin, Sicheng Yu, Geng Zhan, Jiayi Ji, Fan Zhou, Liang Zheng, Shuicheng Yan, Hao Fei,"},{"id":"2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem","title":"[논문리뷰] Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem","excerpt":"Wei Gao이 arXiv에 게시한 'Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem","tags":["Review","Agentic Learning Ecosystem","Large Language Models","Reinforcement Learning","Agentic Crafting","Tool Use","ROME Model","Policy Optimization","Sandbox Environment"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Gao, Fangwen Dai, Wanhe An, XiaoXiao Xu, Weixun Wang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 복잡하고 다단계의 에이전트 태스크를 실제 환경에서 수행하기 위한 확장 가능하고 종단 간(endtoend)의 안정적인 에이전트 에코시스템을 구축하는 것을 목표로 합"},{"id":"2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation","title":"[논문리뷰] PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation","excerpt":"arXiv에 게시된 'PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation","tags":["Review","Text-to-Video Generation","Physics-Aware AI","Direct Preference Optimization","Groupwise Preference Learning","Vision-Language Model","LoRA"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuanhao Cai, Kunpeng Li, Menglin Jia, Jialiang Wang, Junzhe Sun, Feng Liang, Weifeng Chen, Felix JuefeiXu, Chu Wang, Ali Thabet, Xiaoliang Dai, Xuan Ju, Alan Yuille, Ji Hou 핵심 연구"},{"id":"2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","title":"[논문리뷰] Pretraining Frame Preservation in Autoregressive Video Memory Compression","excerpt":"Beijia Lu이 arXiv에 게시한 'Pretraining Frame Preservation in Autoregressive Video Memory Compression' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","tags":["Review","Video Compression","Autoregressive Models","Memory Compression","Frame Preservation","Pretraining","Video Generation","Diffusion Models","Long-Range Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Lvmin Zhang, Shengqu Cai, Anyi Rao, Song Han, Muyang Li, Chong Zeng, Beijia Lu, Gordon Wetzstein, Maneesh Agrawala 핵심 연구 목표 본 논문은 오토회귀 비디오 생성 모델에서 발생하는 긴 비디오 컨텍스트 처리의 한계 와 컨텍스트 품"},{"id":"2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future","title":"[논문리뷰] Scaling Open-Ended Reasoning to Predict the Future","excerpt":"arXiv에 게시된 'Scaling Open-Ended Reasoning to Predict the Future' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future","tags":["Review","Language Models","Forecasting","Open-Ended Reasoning","Reinforcement Learning (RL)","Data Generation","Calibration","Retrieval-Augmented Generation (RAG)","Future Prediction"],"text":"링크: 논문 PDF로 바로 열기 저자: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping 핵심 연구 목표 본 연구는 불확실한 미래에 대한 개방형 예측 질문에 대해 언어 모델(LLM)이 정확하고 신뢰할 수 있는 예측을 할 수 있도록 훈련하는 것을 목표로 합니다. 기존 예측 시장 "},{"id":"2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time","title":"[논문리뷰] SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time","excerpt":"Tuanfeng Y. Wang이 arXiv에 게시한 'SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time","tags":["Review","Video Diffusion Model","Generative Rendering","Novel View Synthesis","Space-Time Disentanglement","Temporal Control","Camera Control","Dynamic Scenes","Temporal Warping"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, ChunHao Huang 핵심 연구 목표 본 연구는 단일 모노큘러 비디오 로부터 동적 장면을 ()과 ()에 걸쳐 하며 하는 것을 목표로 합니다. , "},{"id":"2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems","title":"[논문리뷰] Valori: A Deterministic Memory Substrate for AI Systems","excerpt":"varam17이 arXiv에 게시한 'Valori: A Deterministic Memory Substrate for AI Systems' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems","tags":["Review","Deterministic AI","Reproducible Computation","Fixed-Point Arithmetic","Vector Databases","AI Memory","State Machine","Auditability"],"text":"링크: 논문 PDF로 바로 열기 저자: Varshith Gudur 핵심 연구 목표 현대 AI 시스템, 특히 RAG(Retrieval Augmented Generation) 및 에이전트 워크플로우에서 부동 소수점(floatingpoint) 연산 으로 인해 발생하는 비결정론적(nondeterminism) 메모리 상태 문제를 해결하는 것이 목표입니다. 동일한 모"},{"id":"2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models","title":"[논문리뷰] Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models","excerpt":"Xinyi Dai이 arXiv에 게시한 'Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models","tags":["Review","Lightweight LLM","Agentic AI","Pre-training","Multi-Latent Attention","Long-Context","Curriculum Learning","Agentic Mid-training","Instruction Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyi Dai, Yinghui Li, Lingfeng Qiao, Jiarui Qin, Junru Lu 핵심 연구 목표 본 논문은 경량 LLM이 높은 계산 효율성 을 유지하면서도 내재적인 에이전트 지능을 갖출 수 있도록 하는 것을 목표로 합니다. 특히, 기존의 증류(distillation) 방식이 아닌, sub2B "},{"id":"2026-01-01-mHC-Manifold-Constrained-Hyper-Connections","title":"[논문리뷰] mHC: Manifold-Constrained Hyper-Connections","excerpt":"arXiv에 게시된 'mHC: Manifold-Constrained Hyper-Connections' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-01 00:00:00+0900+0900","permalink":"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections","tags":["Review","Hyper-Connections","Residual Connections","Manifold Learning","Doubly Stochastic Matrices","Training Stability","Large Language Models","Infrastructure Optimization","Deep Learning Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenda Xie, Yixuan Wei, Huanqi Cao, et al. 핵심 연구 목표 논문은 HyperConnections (HC) 가 잔여 스트림의 폭을 넓히고 연결성을 다양화하여 성능을 향상시키지만, 항등 매핑(identity mapping) 속성을 손상시켜 심각한 훈련 불안정성, 제한된 확장성, 그리고 상"},{"id":"2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models","title":"[논문리뷰] DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models","excerpt":"Siyuan Huang이 arXiv에 게시한 'DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-02 00:00:00+0900+0900","permalink":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models","tags":["Review","Multimodal Reasoning","Diffusion Models","Image-to-Image Generation","Vision-centric AI","Generative AI","Spatial Planning","Constraint Satisfaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, Yu Cheng 핵심 연구 목표 현재 Multimodal Large Language Models (MLLMs)이 겪는 텍스트 중심 추론의 한계와 복잡한 장기 시각 중심 태스크에서의 비효율성을 해결하고, 확산 모델을 활용한"},{"id":"2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space","title":"[논문리뷰] Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space","excerpt":"arXiv에 게시된 'Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-02 00:00:00+0900+0900","permalink":"/ai/review/2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space","tags":["Review","Hierarchical Language Model","Concept-Level Reasoning","Dynamic Segmentation","Adaptive Computation","Scaling Laws","Maximal Update Parametrization","Next-Token Prediction","Flash Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Xingwei Qu, Shaowen Wang, Zihao Huang, Ge Zhang 핵심 연구 목표 본 논문은 기존 대규모 언어 모델(LLM)이 언어의 비균일한 정보 밀도에도 불구하고 토큰에 균일한 연산을 적용하여 발생하는 비효율성 문제를 해결하고자 합니다. 토큰 수준이 아닌 압축된 개념 공간(compressed c"},{"id":"2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs","title":"[논문리뷰] On the Role of Discreteness in Diffusion LLMs","excerpt":"arXiv에 게시된 'On the Role of Discreteness in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-02 00:00:00+0900+0900","permalink":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs","tags":["Review","Diffusion Models","Language Models","Discrete Text","Continuous Diffusion","Text Generation","Data Augmentation","Parallel Decoding","Structural Dependency"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziqi Jin, Bin Wang, Xiang Lin, Lidong Bing, Aixin Sun 핵심 연구 목표 본 논문은 확산 모델(Diffusion Models)을 언어 모델링에 적용할 때 발생하는 근본적인 문제점을 분석하고, 텍스트의 이산적이고 구조화된 특성이 확산 메커니즘과 어떻게 불일치하는지 명확히 하는 것을"},{"id":"2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction","title":"[논문리뷰] AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction","excerpt":"Yu-Lun Liu이 arXiv에 게시한 'AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction","tags":["Review","Dynamic Scene Reconstruction","Gabor Representation","Gaussian Splatting","Temporal Continuity","Cubic Hermite Splines","Frequency Adaptivity","Monocular Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiewen Chan, Zhenjun Zhao, YuLun Liu 핵심 연구 목표 본 논문은 단일 시점 비디오에서 동적인 3D 장면을 재구성할 때 발생하는 주요 문제점인 고주파수 외형 디테일과 시간적 연속성의 동시 확보를 목표로 합니다. 기존 Gaussian 기반 방법의 저주파 필터링 한계와 표준 Gabor 함수 의 "},{"id":"2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation","title":"[논문리뷰] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation","excerpt":"Sung Ju Hwang이 arXiv에 게시한 'Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation","tags":["Review","Avatar Generation","Real-Time Interaction","Diffusion Models","Preference Optimization","Causal Inference","Multimodal Input","Head Avatar"],"text":"링크: 논문 PDF로 바로 열기 저자: Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang 핵심 연구 목표 본 논문은 기존의 단방향적인 아바타 생성 모델들이 부족했던 실시간 양방향 상호작용 과 감정적 참여(emotional engagement) 를 가능하게 하는 대화형 헤드 아바타 생성"},{"id":"2026-01-05-Deep-Delta-Learning","title":"[논문리뷰] Deep Delta Learning","excerpt":"Quanquan Gu이 arXiv에 게시한 'Deep Delta Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-Deep-Delta-Learning","tags":["Review","Deep Residual Networks","Delta Operator","Geometric Transformation","Spectral Analysis","Gated Networks","Householder Reflection","Dynamical Systems","Identity Shortcut"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu 핵심 연구 목표 본 논문은 딥 잔차 신경망(Deep Residual Networks)의 엄격한 가산적 귀납적 편향(additive inductive bias)으로 인해 복잡한 상태 전이 모델링 능력이 제한되는 문제를 해결하고자 합"},{"id":"2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction","title":"[논문리뷰] Diversity or Precision? A Deep Dive into Next Token Prediction","excerpt":"arXiv에 게시된 'Diversity or Precision? A Deep Dive into Next Token Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction","tags":["Review","Next Token Prediction","Reinforcement Learning","Large Language Models","Reward Shaping","Pre-training Objective","Policy Gradient","Exploration-Exploitation"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoyuan Wu1,2, Hai Wang1,†, Jiajia Wu¹, Jinxiang Ou¹, Keyao Wang¹, Weile Chen¹, Zihao Zheng¹, Bei Yu² 핵심 연구 목표 본 연구는 LLM의 사전 훈련된 토큰 출력 분포가 후속 강화 학습(RL) 을 위한 탐색 공간에 미치는 영향을 체계적으로 "},{"id":"2026-01-05-Fast-weight-Product-Key-Memory","title":"[논문리뷰] Fast-weight Product Key Memory","excerpt":"arXiv에 게시된 'Fast-weight Product Key Memory' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-Fast-weight-Product-Key-Memory","tags":["Review","Fast-weight Memory","Product Key Memory","Episodic Memory","Language Models","Long-Context Modeling","Memory Augmented Networks","Continual Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyu Zhao, Llion Jones 핵심 연구 목표 본 논문은 최신 언어 모델의 시퀀스 모델링 레이어에서 저장 용량과 계산 효율성 사이의 근본적인 트레이드오프를 해결하는 것을 목표로 합니다. 특히, 정적 \"느린 가중치\" 모듈인 Product Key Memory (PKM) 의 한계를 극복하고, 이를 동적으로 업"},{"id":"2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs","title":"[논문리뷰] InfoSynth: Information-Guided Benchmark Synthesis for LLMs","excerpt":"arXiv에 게시된 'InfoSynth: Information-Guided Benchmark Synthesis for LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs","tags":["Review","Benchmark Synthesis","LLM Evaluation","Code Generation","Information Theory","Genetic Algorithms","Novelty Metrics","Diversity Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Ishir Garg, Xuandong Zhao, Neel Kolhe, Dawn Song 핵심 연구 목표 대규모 언어 모델(LLM)의 추론 및 코드 생성 능력 평가를 위한 새롭고 다양한 벤치마크를 효율적으로 생성하는 것이 이 논문의 핵심 목표입니다. 기존 벤치마크의 수동 생성 비용과 데이터 오염 문제를 해결하고, 정보 "},{"id":"2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing","title":"[논문리뷰] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing","excerpt":"Jian Yang이 arXiv에 게시한 'MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing","tags":["Review","3D Morphing","Structured Latent (SLAT)","Generative Models","Attention Mechanisms","Training-Free Framework","Cross-Category Transitions","Temporal Coherence"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang 핵심 연구 목표 본 논문은 3D 모핑의 난제를 해결하고자 합니다. 특히 다양한 카테고리 간의 객체에 대해 의미론적으로 일관되고 시간적으로 부드러운 변형 시퀀스를 훈련 없이 생성하는 것을 목표로 합니다"},{"id":"2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos","title":"[논문리뷰] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos","excerpt":"Feng Wang이 arXiv에 게시한 'NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos","tags":["Review","4D World Model","Gaussian Splatting","Monocular Video","Novel View Synthesis","Video Generation","Feed-Forward Reconstruction","Degradation Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang 핵심 연구 목표 본 연구는 기존 4D 세계 모델링 방법론의 확장성 한계(고비용의 특수 다중 뷰 데이터 및 번거로운 오프라인 전처리)를 극복하고자 합니다. 이를 위해 다양한 inthewild 단"},{"id":"2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures","title":"[논문리뷰] Nested Learning: The Illusion of Deep Learning Architectures","excerpt":"Vahab Mirrokni이 arXiv에 게시한 'Nested Learning: The Illusion of Deep Learning Architectures' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures","tags":["Review","Nested Learning","Continual Learning","In-context Learning","Associative Memory","Multi-Timescale Memory","Self-Modifying Models","Optimizers"],"text":"링크: 논문 PDF로 바로 열기 저자: Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni 핵심 연구 목표 본 논문은 기존 딥러닝 모델, 특히 대규모 언어 모델(LLM) 이 직면한 지속 학습, 자기 개선, 효과적인 문제 해결 능력의 한계를 극복하고자 합니다. 이를 위해 기계 학습 모델을 중첩되고 다단"},{"id":"2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning","title":"[논문리뷰] SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning","excerpt":"arXiv에 게시된 'SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning","tags":["Review","Multimodal Agents","Reinforcement Learning","Vision-Language Models","Tool Use","Agentic Reasoning","Image Search","HR-MMSearch","BN-GSPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu 핵심 연구 목표 본 논문은 기존 VLM "},{"id":"2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation","title":"[논문리뷰] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation","excerpt":"arXiv에 게시된 'Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation","tags":["Review","MLLMs","Video Understanding","Hallucinations","Counterfactual Generation","Diffusion Models","Reinforcement Learning","QA Dataset","DNA-Train"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLMs) 이 시각적 내용보다 언어적 선험 지식에 과도하게 의존하여 발생하"},{"id":"2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization","title":"[논문리뷰] Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization","excerpt":"arXiv에 게시된 'Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-05 00:00:00+0900+0900","permalink":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization","tags":["Review","LLM Agents","Automated Agent Generation","Reinforcement Learning","Hybrid Policy Optimization","Tool Synthesis","In-context Learning","Agent Framework","Scalability"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsh"},{"id":"2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs","title":"[논문리뷰] COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs","excerpt":"arXiv에 게시된 'COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs","tags":["Review","LLM Evaluation","Policy Alignment","Organizational Policies","AI Safety","Adversarial Robustness","Refusal Behavior","Prompt Engineering","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Dasol Choi, DongGeon Lee, Brigitta Jesica Kartono, Helena Berndt, Taeyoun Kwon, Joonwon Jang, Haon Park, Hwanjo Yu, Minsuk Kahng 핵심 연구 목표 본 논문은 범용적인 유해성 평가를 넘어, LLM이 기업 및 조직 특유의 "},{"id":"2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits","title":"[논문리뷰] Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits","excerpt":"arXiv에 게시된 'Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits","tags":["Review","LLM Self-Awareness","Failure Prediction","Internal States","Attention Mechanisms","Neural Network Probes","Computational Efficiency","Zero-Shot Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Amirhosein Ghasemabadi, Di Niu 핵심 연구 목표 거대 언어 모델(LLM)이 생성하는 텍스트의 정확성 또는 오류를 스스로 인지하지 못하는 문제를 해결하고, 외부 평가자 없이 LLM 내부 작동을 통해 자체 실패를 예측할 수 있는 경량 메커니즘 을 개발하는 것을 목표로 합니다. 이는 LLM의 신뢰성,"},{"id":"2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer","title":"[논문리뷰] DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer","excerpt":"arXiv에 게시된 'DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer","tags":["Review","Video Face Swapping","Diffusion Transformer","Identity Preservation","Temporal Consistency","Modality-Aware Conditioning","Reinforcement Learning","Data Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Xu Guo, Fulong Ye, Xinghui Li, Pengqi Tu, Pengze Zhang, Qichao Sun, Songtao Zhao, Xiangwang Hou, Qian He 핵심 연구 목표 비디오 얼굴 스와핑(VFS)에서 기존 이미지 얼굴 스와핑(IFS) 모델 대비 신원 유사성 및 속성 보존 능력의 격차"},{"id":"2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling","title":"[논문리뷰] Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling","excerpt":"arXiv에 게시된 'Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling","tags":["Review","Reasoning","Small Language Models (SLMs)","Hybrid Architecture","Test-Time Scaling (TTS)","Supervised Fine-Tuning (SFT)","Reinforcement Learning (RL)","DeepConf","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Iheb Chaabane, Puneesh Khanna, Suhail Mohmad, Slim Frikha, Shi Hu, Abdalgader Abubaker, Reda Alami, Mikhail Lubinets, Mohamed El Amine Seddik, Hakim Hacid 핵심 연구 목표 본 연구는 7B 파라미터의"},{"id":"2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking","title":"[논문리뷰] GARDO: Reinforcing Diffusion Models without Reward Hacking","excerpt":"Zhiyong Wang이 arXiv에 게시한 'GARDO: Reinforcing Diffusion Models without Reward Hacking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking","tags":["Review","Diffusion Models","Reinforcement Learning","Reward Hacking","KL Regularization","Adaptive Regularization","Diversity Optimization","Text-to-Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan 핵심 연구 목표 Reinforcement Learning(RL) 기반의 확산 모델 finetuning 과정"},{"id":"2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset","title":"[논문리뷰] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset","excerpt":"arXiv에 게시된 'IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset","tags":["Review","Dermoscopy","Skin Lesion Segmentation","Multi-Annotator Dataset","Inter-Annotator Variability","ISIC Archive","Medical Image Analysis","Machine Learning","Data Annotation"],"text":"링크: 논문 PDF로 바로 열기 저자: Kumar Abhishek, Jeremy Kawahara, and Ghassan Hamarneh 핵심 연구 목표 이 연구는 피부 병변 분할(Skin Lesion Segmentation, SLS) 분야의 주요 난제 중 하나인 대규모 다중어노테이터(multiannotator) 데이터셋의 부족 문제를 해결하는 것을 목표로 "},{"id":"2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams","title":"[논문리뷰] InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams","excerpt":"arXiv에 게시된 'InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams","tags":["Review","3D Reconstruction","Transformer","Streaming Perception","Memory Management","KV Cache Pruning","Visual Geometry","Temporal Consistency","Continuous Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuai Yuan, Yantai Yang, Xiaotian Yang, Xupeng Zhang, Zhonghao Zhao, Lingming Zhang, Zhipeng Zhang 핵심 연구 목표 본 논문은 실시간 스트리밍 환경에서 3D 시각 기하학 이해 가 확장성과 장기적 안정성이라는 상충되는 요구사항으로 인해 제한되는"},{"id":"2026-01-06-K-EXAONE-Technical-Report","title":"[논문리뷰] K-EXAONE Technical Report","excerpt":"arXiv에 게시된 'K-EXAONE Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-K-EXAONE-Technical-Report","tags":["Review","Multilingual Language Model","Mixture-of-Experts (MoE)","Long Context","AI Safety","Korean AI","Foundation Model","Reinforcement Learning (RL)"],"text":"링크: 논문 PDF로 바로 열기 저자: LG AI Research 핵심 연구 목표 LG AI Research는 KEXAONE 이라는 대규모 다국어 언어 모델을 개발하여 최첨단 성능을 달성하는 것을 목표로 합니다. 특히, 기존 모델의 한계를 극복하고 한국의 AI 인프라 환경을 고려하여 효율적이면서도 강력한 범용 및 전문 AI 기반 모델을 제공하고자 합니다. "},{"id":"2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs","title":"[논문리뷰] KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs","excerpt":"Yi Yang이 arXiv에 게시한 'KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs","tags":["Review","Text Embedding","Decoder-only LLMs","Training-free","KV Re-routing","Causal Attention","Representation Learning","Intrinsic Dimensionality"],"text":"링크: 논문 PDF로 바로 열기 저자: Yixuan Tang and Yi Yang 핵심 연구 목표 디코더 전용 LLM을 학습 없이 텍스트 임베딩 백본으로 활용할 때 발생하는 두 가지 구조적 문제(인과적 어텐션으로 인한 정보 비대칭, 다음 토큰 예측 목표로 인한 의미 압축 편향)를 해결하여, 고품질의 텍스트 임베딩을 효율적으로 추출하는 것입니다. 핵심 방법론"},{"id":"2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models","title":"[논문리뷰] M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models","excerpt":"Jun-Cheng Chen이 arXiv에 게시한 'M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models","tags":["Review","Diffusion Models","Concept Erasure","Multimodal Evaluation","Adversarial Attacks","Robustness","Textual Inversion","Latent Inversion","Cross-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: JuHsuan Weng, JiaWei Liao, ChengFu Chou, JunCheng Chen 핵심 연구 목표 본 논문은 텍스트투이미지 확산 모델의 개념 삭제(concept erasure) 방법들이 텍스트 프롬프트 외의 다른 입력 양식(모달리티)에 대해 얼마나 취약한지 평가하고, 이러한 취약점을 개선할 수 있는 새"},{"id":"2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation","title":"[논문리뷰] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation","excerpt":"arXiv에 게시된 'NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation","tags":["Review","Multimodal AI","Decoder-only Transformer","Next-scale Prediction","Image Generation","Image Editing","Reinforcement Learning","Unified Modeling","TokenFlow"],"text":"링크: 논문 PDF로 바로 열기 저자: Huichao Zhang, Liao Qu, Yiheng Liu, et al. 핵심 연구 목표 NextFlow는 단일 decoderonly autoregressive transformer 를 사용하여 멀티모달 이해 및 생성 능력을 통합하는 것을 목표로 합니다. 기존 래스터 스캔 방식의 AR 모델이 고해상도 시각 생성에서"},{"id":"2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment","title":"[논문리뷰] OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment","excerpt":"Chunchun Ma이 arXiv에 게시한 'OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment","tags":["Review","LLM 에이전트 시스템","학술 독창성 평가","피어 리뷰 지원","증거 기반 검증","의미론적 검색","계층적 분류 체계","대규모 언어 모델"],"text":"링크: 논문 PDF로 바로 열기 저자: Chunchun Ma, Yujiong Shen, Yueyuan Huang, Kexin Tan, Ming Zhang 핵심 연구 목표 OpenNovelty 는 방대하고 빠르게 진화하는 학술 문헌 속에서 논문의 독창성을 평가하는 피어 리뷰의 어려움을 해결하고자 합니다. 특히, 기존 LLM 기반 접근법 이 겪는 환각 현상이나"},{"id":"2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents","title":"[논문리뷰] Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents","excerpt":"arXiv에 게시된 'Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents","tags":["Review","LLM Agents","Faithfulness","XAI","Causal Inference","Structural Causal Models","Counterfactual Interventions","Reasoning Trace Auditing","Causal Decoupling"],"text":"링크: 논문 PDF로 바로 열기 저자: Sourena Khanzadeh 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트의 ChainofThought(CoT) 추론 과정 이 실제 모델 출력의 원인인지 혹은 사후 합리화인지에 대한 \"Faithfulness Gap\" 문제를 해결하고자 합니다. LLM 에이전트의 의사결정 과정 투명성 부족이 고위험 도"},{"id":"2026-01-06-Recursive-Language-Models","title":"[논문리뷰] Recursive Language Models","excerpt":"arXiv에 게시된 'Recursive Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-Recursive-Language-Models","tags":["Review","Recursive Language Models","Large Language Models","Long Context Processing","Inference Scaling","REPL Environment","Task Decomposition","Sub-LM Calls","Context Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Alex L. Zhang, Tim Kraska, Omar Khattab 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 컨텍스트 길이 제한으로 인해 긴 프롬프트를 효과적으로 처리하지 못하고 '컨텍스트 로트(context rot)' 현상을 겪는 문제를 해결하고자 합니다. 특히, 수백만 토큰 규모의 장기 작업에서"},{"id":"2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving","title":"[논문리뷰] SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving","excerpt":"arXiv에 게시된 'SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving","tags":["Review","Software Engineering","Issue Resolution","Supervised Fine-tuning (SFT)","Large Language Models (LLMs)","Hybrid Dataset","Error Masking","Curriculum Learning","Test-Time Scaling (TTS)","Generative Verifiers"],"text":"링크: 논문 PDF로 바로 열기 저자: Chaofan Tao, Jierun Chen, Yuxin Jiang, Kaiqi Kou, Shaowei Wang, Ruoyu Wang, Xiaohui Li, Sidi Yang, Yiming Du, Jianbo Dai, Zhiming Mao, Xinyu Wang, Lifeng Shang, Haoli Bai 핵심 연구 목"},{"id":"2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes","title":"[논문리뷰] Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes","excerpt":"Shuo Yang이 arXiv에 게시한 'Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes","tags":["Review","Reinforcement Learning","Text-Guided Image Editing","Object-Level Transformation","Geometric Transformation","Diffusion Models","GRPO","Scene Editing","Spatially Grounded Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto 핵심 연구 목표 본 논문은 기존 텍스트 기반 이미지 편집 모델이 객체 수준의 기하학적 변환(이동, 회전, 크기 조절)에 어려"},{"id":"2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion","title":"[논문리뷰] Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion","excerpt":"Shiying Wang이 arXiv에 게시한 'Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion","tags":["Review","Semi-Supervised Learning","Semantic Segmentation","Remote Sensing","Vision Foundation Models","Pseudo-Label Drift","Co-Guidance","Feature Fusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi 핵심 연구 목표 본 논문은 원격 탐사(RS) 이미지의 시맨틱 분할에서 의사 레이블(pseudolabel)"},{"id":"2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation","title":"[논문리뷰] VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation","excerpt":"arXiv에 게시된 'VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation","tags":["Review","Visual Autoregressive Models","Reinforcement Learning","Policy Conflicts","GRPO","Text-to-Image Generation","Credit Assignment","Multi-scale Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia 핵심 연구 목표 Visual Autoregressive (VAR) 모델은 이질적인 입력 구조와"},{"id":"2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context","title":"[논문리뷰] VINO: A Unified Visual Generator with Interleaved OmniModal Context","excerpt":"Kun Gai이 arXiv에 게시한 'VINO: A Unified Visual Generator with Interleaved OmniModal Context' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-06 00:00:00+0900+0900","permalink":"/ai/review/2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context","tags":["Review","Unified Generation","Multimodal Diffusion","Vision-Language Model","Image Editing","Video Editing","Interleaved Context","Progressive Training","Diffusion Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye 핵심 연구 목표 본 논문은 파편화된 기존 시각 생성 파이프라인의 한계를 극복하고, 단일 프레임워크 내에서 이미지 및 비디오 생성과 편집을 모두 수행할 수 있는 통합 시각 생성기 VINO 를 개발하는 것을"},{"id":"2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving","title":"[논문리뷰] CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving","excerpt":"Tao Feng이 arXiv에 게시한 'CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving","tags":["Review","Multimodal LLMs","Visual Reasoning","Mathematical Problem Solving","Knowledge Internalization","Reinforcement Learning","Cognitive-Inspired AI","Perception-Reasoning Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuhang Chen, Yunqiu Xu, Junjie Xie, Aojun Lu, Tao Feng, Zeying Huang, Ning Zhang, Yi Sun, Yi Yang, Hangjie Yuan 핵심 연구 목표 기존 Multimodal Large Language Models (MLLMs) 이 시각적 수학 문제 "},{"id":"2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization","title":"[논문리뷰] DreamStyle: A Unified Framework for Video Stylization","excerpt":"arXiv에 게시된 'DreamStyle: A Unified Framework for Video Stylization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization","tags":["Review","Video Stylization","Unified Framework","Diffusion Models","LoRA","Data Curation","Multi-modal Input","Image-to-Video"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengtian Li, Jinshu Chen, Songtao Zhao, Wanquan Feng, Pengqi Tu, Qian He (Intelligent Creation Lab, ByteDance) 핵심 연구 목표 본 논문은 텍스트, 스타일 이미지, 스타일이 적용된 첫 프레임 등 단일 모달리티 조건에 국한된 기존 비디"},{"id":"2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing","title":"[논문리뷰] FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing","excerpt":"Peng Tang이 arXiv에 게시한 'FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing","tags":["Review","Video Editing","First-Frame Propagation (FFP)","Large-Scale Dataset","Generative Models","Temporal Consistency","Spatio-Temporal RoPE","Self-Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xijie Huang, Chengming Xu, Donghao Luo, Xiaobin Hu, Peng Tang 핵심 연구 목표 본 논문은 제어 가능한 비디오 편집 패러다임인 FirstFrame Propagation (FFP) 의 주요 한계를 해결하고자 합니다. 특히 기존 FFP 방법론이 겪는 번거로운 런타임 가이드 에"},{"id":"2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields","title":"[논문리뷰] InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields","excerpt":"arXiv에 게시된 'InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields","tags":["Review","Depth Estimation","Neural Implicit Fields","Arbitrary Resolution","Fine-Grained","Novel View Synthesis","Vision Transformer","Synth4K Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Yu, Haotong Lin, Jiawei Wang, Jiaxin Li, Yida Wang, Xueyang Zhang, Yue Wang, Xiaowei Zhou, Ruizhen Hu, Sida Peng 핵심 연구 목표 기존의 이산적인 이미지 그리드 기반 깊이 추정 방식이 가지는 해상도 확장성 및 기하학적 세부 "},{"id":"2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model","title":"[논문리뷰] LTX-2: Efficient Joint Audio-Visual Foundation Model","excerpt":"Andrew Kvochko이 arXiv에 게시한 'LTX-2: Efficient Joint Audio-Visual Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model","tags":["Review","Multimodal AI","Text-to-Audio-Video","Diffusion Transformer","Cross-Modal Attention","Classifier-Free Guidance","Efficient Inference","Foundation Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko 핵심 연구 목표 기존 텍스트투비디오(T2V) 모델이 오디오 정보 없이 \"침묵하는\" 영상을 생성하는 한계를 해결하고자 합니다. 이 연구는 고품질의 시간적으로 동기화된 오디오비주얼 콘텐"},{"id":"2026-01-07-MiMo-V2-Flash-Technical-Report","title":"[논문리뷰] MiMo-V2-Flash Technical Report","excerpt":"arXiv에 게시된 'MiMo-V2-Flash Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-MiMo-V2-Flash-Technical-Report","tags":["Review","Mixture-of-Experts","Sliding Window Attention","Multi-Token Prediction","Multi-Teacher On-Policy Distillation","Reinforcement Learning","Long-Context Modeling","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: LLMCore Xiaomi 연구팀 핵심 연구 목표 본 논문은 빠른 추론 속도와 강력한 추론 및 에이전트 능력을 동시에 갖춘 효율적이고 비용 효율적인 대규모 언어 모델(LLM)인 MiMoV2Flash를 개발하는 것을 목표로 합니다. 특히, 긴 컨텍스트 모델링의 성능 저하 없이 효율성을 유지하고, 기존 사후 훈련 파이프라"},{"id":"2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents","title":"[논문리뷰] NitroGen: An Open Foundation Model for Generalist Gaming Agents","excerpt":"arXiv에 게시된 'NitroGen: An Open Foundation Model for Generalist Gaming Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents","tags":["Review","Generalist Agents","Foundation Models","Behavior Cloning","Video Games","Action Extraction","Multi-game","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Loïc Magne, Anas Awadalla, Guanzhi Wang, Yinzhen Xu, Joshua Belofsky, Fengyuan Hu, Joohwan Kim, Ludwig Schmidt, Georgia Gkioxari, Jan Kautz, Yisong Yue, Yejin Choi, Yuke Zhu, Lin"},{"id":"2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation","title":"[논문리뷰] Parallel Latent Reasoning for Sequential Recommendation","excerpt":"Yuning Jiang이 arXiv에 게시한 'Parallel Latent Reasoning for Sequential Recommendation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation","tags":["Review","Sequential Recommendation","Latent Reasoning","Parallel Processing","Computational Scaling","Mixture of Experts","Contrastive Learning","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiakai Tang, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang 핵심 연구 목표 순차 추천 시스템에서 희소한 사용자 행동 시퀀스로부터 복잡한 사용자 선호를 포착하는 문제를 해결하는 것이 목표입니다. 기존 잠재 추론(latent reasoning) 방법론들이 깊이 기반(depthlevel"},{"id":"2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models","title":"[논문리뷰] SOP: A Scalable Online Post-Training System for Vision-Language-Action Models","excerpt":"arXiv에 게시된 'SOP: A Scalable Online Post-Training System for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models","tags":["Review","Vision-Language-Action Models","Online Post-training","Scalable Robot Learning","Distributed Systems","Multi-task Learning","Imitation Learning","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingjie Pan, Siyuan Feng, Qinglin Zhang, Xinchen Li, Jianheng Song, Chendi Qu, Yi Wang, Chuankang Li, Ziyu Xiong, Zhi Chen, Yi Liu, Jianlan Luo 핵심 연구 목표 본 논문은 대규모 사전 훈련을 통해 일반화 능"},{"id":"2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs","title":"[논문리뷰] Steerability of Instrumental-Convergence Tendencies in LLMs","excerpt":"j-hoscilowic이 arXiv에 게시한 'Steerability of Instrumental-Convergence Tendencies in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs","tags":["Review","LLM Steerability","Instrumental Convergence","AI Safety","AI Security","Open-Weight Models","Prompt Engineering","Model Control","Behavioral Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Jakub Hoscilowicz 핵심 연구 목표 본 논문은 AI 시스템의 역량(capability) 성장과 제어 가능성(steerability) 간의 관계를 탐구하며, 특히 도구적 수렴(instrumental convergence) 경향에 초점을 맞춥니다. 연구는 역량 증가가 제어력 상실로 이어지는지 경험적으로 검증하"},{"id":"2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision","title":"[논문리뷰] UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision","excerpt":"XinYu Sun이 arXiv에 게시한 'UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision","tags":["Review","Unified Multimodal Models","Self-Supervised Learning","Text-to-Image Generation","Multi-Agent Framework","Cognitive Pattern Reconstruction","Cycle-Consistency","Conduction Aphasia"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruiyan Han, Zhen Fang, Xinyu Sun, Yuchen Ma, Zehui Chen, Lin Chen, Wenxuan Huang, Weijie Xu, Yi Cao, Ziheng Wang, Yu Zeng, Feng Zhao 핵심 연구 목표 본 연구는 통합 멀티모달 모델(UMMs)이 입력 이해는 뛰어나지만"},{"id":"2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework","title":"[논문리뷰] X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework","excerpt":"Shwetank Shekhar Singh이 arXiv에 게시한 'X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-07 00:00:00+0900+0900","permalink":"/ai/review/2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework","tags":["Review","Hate Speech Detection","Explainable AI (XAI)","Multilingual NLP","Large Language Models (LLMs)","Attention Mechanism","N-gram Explanations","Human Rationales","Benchmark Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohammad Zia Ur Rehman, Sai Kartheek Reddy Kasu, Shashivardhan Reddy Koppula, Sai Rithwik Reddy Chirra, Shwetank Shekhar Singh, Nagendra Kumar 핵심 연구 목표 본 논문은 특히 저자원 인디아어(힌디어, 텔루구"},{"id":"2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models","title":"[논문리뷰] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models","excerpt":"arXiv에 게시된 'E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models","tags":["Review","Reinforcement Learning","Flow Models","Entropy-aware Sampling","Group Relative Policy Optimization","SDE","Human Preference Alignment","Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan 핵심 연구 목표 기존 GRPO(Group Relative Policy Optimization) 기반의 플로우 모델들이 여러 디노이징 타임스텝에 걸쳐 정책을 최적화할 때 발생하는 희소하고 모호한 보상 신호 문제를 해결하는 "},{"id":"2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting","title":"[논문리뷰] Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting","excerpt":"arXiv에 게시된 'Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting","tags":["Review","Supervised Fine-Tuning (SFT)","Catastrophic Forgetting","Entropy-Adaptive Fine-Tuning (EAFT)","Large Language Models (LLMs)","Domain Adaptation","Reinforcement Learning (RL)","Confident Conflicts"],"text":"링크: 논문 PDF로 바로 열기 저자: Muxi Diao, Lele Yang, Wuxuan Gong, Yutong Zhang, Zhonghao Yan, Yufei Han, Kongming Liang, Weiran Xu, Zhanyu Ma 핵심 연구 목표 본 논문은 Supervised FineTuning (SFT) 과정에서 발생하는 catastrophic f"},{"id":"2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning","title":"[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning","excerpt":"Guanchen Wu이 arXiv에 게시한 'EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning","tags":["Review","Epidemiological Question Answering","Large Language Models","Benchmark","Multi-step Inference","Evidence Grounding","LLM Evaluation","Public Health AI","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingyang Wei, Dehai Min, Zewen Liu, Yuzhang Xie, Guanchen Wu, Carl Yang, Max S.Y. Lau, Qi He, Lu Cheng, Wei Jin 핵심 연구 목표 이 논문은 기존 의료 QA 벤치마크가 놓쳤던 인구 수준 추론 및 증거 기반 역학적 추론을 체계적으로 평"},{"id":"2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents","title":"[논문리뷰] MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents","excerpt":"Bingzhe Li이 arXiv에 게시한 'MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents","tags":["Review","Agentic Memory","Large Language Models","Retrieval-Augmented Generation","Knowledge Graphs","Multi-Graph Architecture","Long-Context Reasoning","Memory Evolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongming Jiang, Yi Li, Guanpeng Li, Bingzhe Li 핵심 연구 목표 기존 MemoryAugmented Generation (MAG) 시스템들이 단일 메모리 저장소에서 의미론적 유사성에 의존하여 시간, 인과, 엔티티 정보를 얽히게 하여 발생하는 해석 가능성 및 추론 정확도 한계를 해결하고"},{"id":"2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics","title":"[논문리뷰] MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics","excerpt":"arXiv에 게시된 'MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics","tags":["Review","Molecular Dynamics","LAMMPS","Code Generation","Knowledge Q&A","Large Language Models","Reinforcement Learning","Multi-agent System","Domain Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuofan Shi, Hubao A, Yufei Shao, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing 핵심 연구 목표 본 논문은 분자 동역학(MD) 시뮬레이션에서 LAMMPS"},{"id":"2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization","title":"[논문리뷰] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization","excerpt":"arXiv에 게시된 'RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization","tags":["Review","Gaussian Splatting","SLAM","Dense Initialization","Real-Time Tracking","Differentiable Rendering","DINOv3"],"text":"링크: 논문 PDF로 바로 열기 저자: WeiTse Cheng, YenJen Chiou, YuanFu Yang 핵심 연구 목표 기존 3D Gaussian Splatting (3DGS) SLAM 시스템의 residualdriven densification 방식이 초래하는 불안정한 수렴과 불균일한 지오메트리 문제를 해결하는 것입니다. 이를 대체할 oneshot"},{"id":"2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing","title":"[논문리뷰] ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing","excerpt":"arXiv에 게시된 'ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing","tags":["Review","Reinforcement Learning","Image Editing","Reasoning","Chain-of-Thought","Multimodal Generative Models","Reward Modeling","VLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengjia Li, Liming Jiang, Qing Yan, Yizhi Song, Hao Kang, Zichuan Liu, Xin Lu, Boxi Wu, Deng Cai 핵심 연구 목표 본 연구는 다중 모달 생성 모델을 활용한 지시 기반 이미지 편집에서 시각적 추론 능력의 한계 를 해결하고자 합니다. 특히, 기존 "},{"id":"2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts","title":"[논문리뷰] Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts","excerpt":"arXiv에 게시된 'Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-08 00:00:00+0900+0900","permalink":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts","tags":["Review","Machine Learning Research","Autonomous Research","LLM Agents","Scientific Workflow","Failure Modes","Experimental Design","AI Scientist","Agentic Systems"],"text":"링크: 논문 PDF로 바로 열기 저자: Dhruv Trehan, Paras Chopra 핵심 연구 목표 본 논문은 최신 추론형 LLM(Large Language Models)이 최소한의 코드 스캐폴딩과 기본적인 도구를 사용하여 연구 아이디어 구상부터 최종 연구 논문 작성까지 높은 자율성 을 가지고 수행할 수 있는지 탐구하는 것을 목표로 합니다. 특히, 기존"},{"id":"2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search","title":"[논문리뷰] AT^2PO: Agentic Turn-based Policy Optimization via Tree Search","excerpt":"arXiv에 게시된 'AT^2PO: Agentic Turn-based Policy Optimization via Tree Search' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search","tags":["Review","Agentic RL","Multi-turn Tasks","Policy Optimization","Tree Search","Credit Assignment","Exploration Diversity","LLM Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Zefang Zong, Dingwei Chen, Yang Li, Qi Yi, Bo Zhou, Chengming Li, Bo Qian, Peng Chen, Jie Jiang 핵심 연구 목표 본 논문은 LLM 에이전트의 다중 턴(multiturn) 작업에서 발생하는 세 가지 핵심 문제를 해결하고자 합니다. 구체적으로, 제"},{"id":"2026-01-09-Agent-as-a-Judge","title":"[논문리뷰] Agent-as-a-Judge","excerpt":"Meng Liu이 arXiv에 게시한 'Agent-as-a-Judge' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Agent-as-a-Judge","tags":["Review","Agent-as-a-Judge","LLM Evaluation","Multi-Agent Systems","Tool Integration","AI Alignment","Automated Assessment","Survey"],"text":"링크: 논문 PDF로 바로 열기 저자: Runyang You, Hongru Cai, Caiqi Zhang, Qiancheng Xu, Meng Liu, Tiezheng Yu, Yongqi Li, Wenjie Li 핵심 연구 목표 본 논문은 의 한계(내재된 편향, 피상적인 추론, 실제 관찰에 대한 검증 불가능성)를 극복하기 위해 패러다임으로의 전환을 포괄적으로"},{"id":"2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering","title":"[논문리뷰] AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering","excerpt":"Di Zhang이 arXiv에 게시한 'AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering","tags":["Review","LLM Agents","Release Engineering","Self-Improvement","Regression Testing","Continuous Integration","Flip-Centered Gating","Auditable Development","Software Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Di Zhang 핵심 연구 목표 본 논문은 LLM 에이전트의 자기 개선 방식이 종종 불안정하고 감사하기 어렵다는 문제점을 지적합니다. 에이전트 개선을 릴리즈 엔지니어링 패러다임으로 재구성하여, 배포 가능한 소프트웨어 아티팩트로서 에이전트를 관리하고, 회귀(regression)에 민감한 릴리즈 파이프라인 을 통해 안정적"},{"id":"2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","title":"[논문리뷰] DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs","excerpt":"Jing Ma이 arXiv에 게시한 'DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","tags":["Review","Chain-of-Thought","Diffusion Models","Large Language Models","Reasoning","Error Correction","Preference Optimization","Denoising"],"text":"링크: 논문 PDF로 바로 열기 저자: Shidong Cao, Hongzhan Lin, Yuxuan Gu, Ziyang Luo, Jing Ma 핵심 연구 목표 논문은 대규모 언어 모델(LLMs)의 ChainofThought (CoT) 추론에서 발생하는 노출 편향(exposure bias) 과 오류 누적 문제를 해결하는 것을 목표로 합니다. 기존의 순차적인 "},{"id":"2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking","title":"[논문리뷰] DocDancer: Towards Agentic Document-Grounded Information Seeking","excerpt":"arXiv에 게시된 'DocDancer: Towards Agentic Document-Grounded Information Seeking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking","tags":["Review","Agentic AI","Document Question Answering","Tool-use","Information Seeking","Synthetic Data Generation","Long-context Understanding","Multimodal Documents"],"text":"링크: 논문 PDF로 바로 열기 저자: Qintong Zhang, Xinjie Lv, Jialong Wu, Baixuan Li, Zhengwei Tao, Guochen Yan, Huanyao Zhang, Bin Wang, Jiahao Xu, Haitao Mi, Wentao Zhang 핵심 연구 목표 본 연구는 기존 DocQA(Document Question"},{"id":"2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach","title":"[논문리뷰] Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach","excerpt":"Carl James Debono이 arXiv에 게시한 'Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach","tags":["Review","Object Detection","Privileged Information","Teacher-Student Learning","Knowledge Distillation","Model-Agnostic","Bounding Box Masks","UAV-based Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Matthias Bartolo, Dylan Seychell, Gabriel Hili, Matthew Montebello, Carl James Debono, Saviour Formosa, Konstantinos Makantasis 핵심 연구 목표 본 논문은 객체 탐지 성능을 향상시키기 위해 훈련 시에만 접근 가능한 특권"},{"id":"2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models","title":"[논문리뷰] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models","excerpt":"arXiv에 게시된 'Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models","tags":["Review","Vision-Language Models","Adversarial Attacks","Entropy-Guided Attacks","Token Vulnerability","Harmful Content","Cross-Model Transferability","Autoregressive Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mengqi He, Jinhong Ni, Xinyu Tian, Shu Zou, Jing Zhang, Xin Shen, Zhaoyuan Yang 핵심 연구 목표 본 논문은 VisionLanguage Model (VLM)의 autoregressive 생성 과정에서 모든 토큰이 모델 불안정성에 동일하게 기여한다는 기존 가정"},{"id":"2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization","title":"[논문리뷰] GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization","excerpt":"arXiv에 게시된 'GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization","tags":["Review","Multi-reward RL","Policy Optimization","Reward Normalization","GRPO","GDPO","LLMs","Training Stability"],"text":"링크: 논문 PDF로 바로 열기 저자: ShihYang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, MinHung Chen, Hongxu Yin, YuChiang Frank Wang, KwangTing Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov "},{"id":"2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers","title":"[논문리뷰] Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers","excerpt":"arXiv에 게시된 'Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers","tags":["Review","Large Language Models","Weight Decay","Learnable Multipliers","Scale Adaptation","Optimization","µP Parametrization","Adam","Muon"],"text":"링크: 논문 PDF로 바로 열기 저자: Maksim Velikanov, Ilyas Chahed, Jingwei Zuo, Dhia Eddine Rhaiem, Younes Belkada, Hakim Hacid 핵심 연구 목표 대규모 언어 모델(LLM) 학습 시 Weight Decay(WD) 가 가중치 행렬의 스케일을 \"노이즈WD 평형\" 상태에 고정시켜 데이터에"},{"id":"2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study","title":"[논문리뷰] Memorization in 3D Shape Generation: An Empirical Study","excerpt":"arXiv에 게시된 'Memorization in 3D Shape Generation: An Empirical Study' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study","tags":["Review","3D Shape Generation","Memorization","Generative Models","Diffusion Models","Evaluation Framework","Generalization","Data Augmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu 핵심 연구 목표 3D 생성 모델이 훈련 데이터를 기억하는 현상이 데이터 유출 및 생성 결과의 다양성 저하를 초래할 수 있으나, 이에 대한 체계적인 연구가 부족했습니다. 이 논문은 3D 생성 모델의 기억(memoriza"},{"id":"2026-01-09-Plenoptic-Video-Generation","title":"[논문리뷰] Plenoptic Video Generation","excerpt":"arXiv에 게시된 'Plenoptic Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Plenoptic-Video-Generation","tags":["Review","Generative Video","Camera Control","Plenoptic Function","Autoregressive Model","Diffusion Transformer","3D FOV Retrieval","Spatio-Temporal Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiao Fu, Shitao Tang, Min Shi, Xian Liu, Jinwei Gu, MingYu Liu, Dahua Lin, ChenHsuan Lin 핵심 연구 목표 본 논문은 기존 카메라 제어형 비디오 재렌더링 방법들이 다중 뷰 시나리오에서 일관된 시공간적 일관성을 유지하지 못하는 문제를 해결하는 것을 목표"},{"id":"2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes","title":"[논문리뷰] RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes","excerpt":"Yu-Lun Liu이 arXiv에 게시한 'RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes","tags":["Review","Auto White Balance (AWB)","Deep Reinforcement Learning (DRL)","Low-Light Imaging","Night-time Scenes","Color Constancy","Cross-Sensor Generalization","Statistical Methods","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: YuanKang Lee, KuanLin Chen, ChiaChe Chang, YuLun Liu 핵심 연구 목표 본 논문은 저조도 야간 환경에서 자동 화이트 밸런스(AWB) 보정의 신뢰성 및 일반화 문제를 해결하는 것을 목표로 합니다. 특히, 저조도 노이즈, 복잡한 조명 조건, 그리고 다양한 카메라 센서 간의 배포 환경"},{"id":"2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","title":"[논문리뷰] Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing","excerpt":"Yu Xu이 arXiv에 게시한 'Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","tags":["Review","In-Context Image Generation","Image Editing","Multimodal Models","Chain-of-Thought","Structured Reasoning","Reinforcement Learning","Alignment","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Runze He, Yiji Cheng, Tiankai Hang, Zhimin Li, Yu Xu, Zijin Yin, Shiyi Zhang, Wenxun Dai, Penghui Du, Ao Ma, Chunyu Wang, Qinglin Lu, Jizhong Han, Jiao Dai 핵심 연구 목표 본 논문은 InConte"},{"id":"2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding","title":"[논문리뷰] RelayLLM: Efficient Reasoning via Collaborative Decoding","excerpt":"Haolin Liu이 arXiv에 게시한 'RelayLLM: Efficient Reasoning via Collaborative Decoding' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding","tags":["Review","LLM","SLM","Collaborative Decoding","Token-level Intervention","Reinforcement Learning","GRPO","Efficient Reasoning","Resource Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengsong Huang, Tong Zheng, Langlin Huang, Jinyuan Li, Haolin Liu, Jiaxin Huang 핵심 연구 목표 본 논문은 복잡한 추론 작업에서 대규모 언어 모델(LLM) 의 높은 연산 비용과 지연 시간 문제를 해결하면서, 소규모 언어 모델(SLM) 의 제한된 추론 능력"},{"id":"2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","title":"[논문리뷰] RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation","excerpt":"Mingda Jia이 arXiv에 게시한 'RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","tags":["Review","Robot Manipulation","Data Augmentation","Video Generation","Diffusion Models","Multi-View","Visual Identity Prompting","Action-Guided Segmentation","Visuomotor Policy"],"text":"링크: 논문 PDF로 바로 열기 저자: Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang 핵심 연구 목표 로봇 조작 데이터 수집의 어려움으로 인한 데이터 부족 및 다양"},{"id":"2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models","title":"[논문리뷰] The Illusion of Specialization: Unveiling the Domain-Invariant 'Standing Committee' in Mixture-of-Experts Models","excerpt":"arXiv에 게시된 'The Illusion of Specialization: Unveiling the Domain-Invariant 'Standing Committee' in Mixture-of-Experts Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models","tags":["Review","Mixture-of-Experts (MoE)","Sparse Routing","Domain Specialization","Load Balancing","Interpretability","Standing Committee","LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Yan Wang, Yitao Xu, Nanhan Shen, Jinyan Su, Jimin Huang, Zining Zhu 핵심 연구 목표 본 연구는 MoE(MixtureofExperts) 모델 이 희소 라우팅을 통해 도메인 특화(domain specialization)를 달성한다는 일반적인 가정에 의문을 제기합니다. "},{"id":"2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute","title":"[논문리뷰] Token-Level LLM Collaboration via FusionRoute","excerpt":"Furong Huang이 arXiv에 게시한 'Token-Level LLM Collaboration via FusionRoute' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute","tags":["Review","LLM Collaboration","Token-level Routing","Mixture-of-Experts","Complementary Logits","Preference Optimization","FusionRoute","Domain Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Nuoya Xiong, Zhaorun Chen, Hanqing Zeng, Furong Huang, Shuochao Bi, Lizhu Zhang, Zhikai Zhao 핵심 연구 목표 논문은 여러 전문 LLM 간의 효과적인 토큰 수준 협업 을 통해 단일 모델보다 높은 품질의 응답을 생성하는 것을 목표로 합니다. 순수한 "},{"id":"2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset","title":"[논문리뷰] Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset","excerpt":"YuanFu Yang이 arXiv에 게시한 'Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset","tags":["Review","Industrial Defect Detection","Multimodal Dataset","Vision-Language Model","Diffusion Model","Open-Vocabulary Learning","Quality Inspection","Data Efficiency","Foundation Model"],"text":"링크: 논문 PDF로 바로 열기 저자: TsaiChing Ni, ZhenQi Chen, YuanFu Yang 핵심 연구 목표 기존 산업용 결함 검사 시스템의 높은 오탐률, 낮은 적응성, 일반화 능력 부족, 그리고 블랙박스 모델의 해석 불가능성 한계를 극복하는 것이 목표입니다. 이를 위해 대규모 멀티모달 산업용 결함 데이터셋(IMDD1M) 을 구축하고, 이를"},{"id":"2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","title":"[논문리뷰] VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control","excerpt":"Ying Shan이 arXiv에 게시한 'VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","tags":["Review","Video World Model","4D Geometric Control","Gaussian Trajectories","Video Generation","Diffusion Models","Camera Control","Object Motion Control","Data Engine"],"text":"링크: 논문 PDF로 바로 열기 저자: Sixiao Zheng, Minghao Yin, Wenbo Hu, Xiaoyu Li, Ying Shan, Yanwei Fu 핵심 연구 목표 본 논문은 기존 비디오 월드 모델들이 카메라 및 다중 객체 모션에 대한 통합적이고 정밀한 제어에 어려움을 겪는 문제를 해결하고자 합니다. 비디오가 본질적으로 2D 이미지 평면에서 "},{"id":"2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice","title":"[논문리뷰] VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice","excerpt":"arXiv에 게시된 'VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-09 00:00:00+0900+0900","permalink":"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice","tags":["Review","Video Understanding","Chain-of-Thought (CoT)","Reinforcement Learning (RL)","Adaptive Reasoning","Early Exit","Multimodal LLM","Video QA","Temporal Grounding"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuming Liu, Mingchen Zhuge, Changsheng Zhao, Jun Chen, et al. 핵심 연구 목표 비디오 이해 태스크에서 ChainofThought (CoT) 추론의 필요성과 이점을 재평가하고, 기존 CoT 방식이 때로는 직접 답변보다 성능이 낮고 비효율적임을 지적합니다. 이를 바탕으로,"},{"id":"2026-01-12-CaricatureGS-Exaggerating-3D-Gaussian-Splatting-Faces-With-Gaussian-Curvature","title":"[논문리뷰] CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature","excerpt":"arXiv에 게시된 'CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-CaricatureGS-Exaggerating-3D-Gaussian-Splatting-Faces-With-Gaussian-Curvature","tags":["Review","3D Gaussian Splatting","Facial Caricaturization","Gaussian Curvature","Mesh Deformation","Photorealistic Rendering","Human Avatars","Local Affine Transformations"],"text":"링크: 논문 PDF로 바로 열기 저자: Eldad Matmon, Amit Bracha, Noam Rotstein, Ron Kimmel 핵심 연구 목표 본 논문은 제어 가능하고 사실적인 3D 얼굴 캐리커처 아바타를 생성하는 데 있어 기존 메시 기반 방법론의 한계를 극복하고자 합니다. 특히, 3D Gaussian Splatting (3DGS) 의 사실적인 렌더"},{"id":"2026-01-12-Distilling-Feedback-into-Memory-as-a-Tool","title":"[논문리뷰] Distilling Feedback into Memory-as-a-Tool","excerpt":"vicgalle이 arXiv에 게시한 'Distilling Feedback into Memory-as-a-Tool' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-Distilling-Feedback-into-Memory-as-a-Tool","tags":["Review","LLM","Continual Learning","Memory-Augmented Agents","Self-Correction","Feedback Distillation","Tool Use","Inference Cost Amortization","Rubric-based Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Víctor Gallego 핵심 연구 목표 본 논문은 LLM 의 추론 시 발생하는 높은 연산 비용과 반복적인 자기 수정 과정의 비효율성을 해결하고자 합니다. 특히, 기존 \"System 2\" 스케일링 방법론들이 매번 새로운 쿼리에 대해 처음부터 추론 과정을 반복하여 발생하는 지식 손실 과 계산 자원 낭비 문제를 극복하는"},{"id":"2026-01-12-GenCtrl-A-Formal-Controllability-Toolkit-for-Generative-Models","title":"[논문리뷰] GenCtrl -- A Formal Controllability Toolkit for Generative Models","excerpt":"arXiv에 게시된 'GenCtrl -- A Formal Controllability Toolkit for Generative Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-GenCtrl-A-Formal-Controllability-Toolkit-for-Generative-Models","tags":["Review","Generative Models","Controllability","Reachability","Control Theory","Dialogue Systems","LLMs","T2IMs","PAC Bounds","Formal Verification"],"text":"링크: 논문 PDF로 바로 열기 저자: Emily Cheng, Carmen Amo Alonso, Federico Danieli, Arno Blaas, Luca Zappella, Pau Rodríguez, Xavier Suau 핵심 연구 목표 본 연구는 생성 모델의 제어 가능성(controllability)이 암묵적으로 가정되는 현 상황을 비판하며, 모델이 "},{"id":"2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","title":"[논문리뷰] Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals","excerpt":"Arjan Chakravarthy이 arXiv에 게시한 'Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","tags":["Review","Video Generation","World Models","Physics-Conditioned Goals","Causal Planning","Force Vectors","Zero-Shot Generalization","Diffusion Models","Robotics Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Nate Gillman, Yinghua Zhou, Zitian Tang, Evan Luo, Arjan Chakravarthy, Daksh Aggarwal, Michael Freeman, Charles Herrmann, Chen Sun 핵심 연구 목표 기존 비디오 생성 \"월드 모델\"이 복잡한 물리적 작업을 위한 정확한 "},{"id":"2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning","title":"[논문리뷰] Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning","excerpt":"Zhicheng Dou이 arXiv에 게시한 'Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning","tags":["Review","LLM Agents","Agent Memory","Event Graph","Long-term Reasoning","Knowledge Graph","Active Retrieval","Event Segmentation","Multi-hop QA"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuyang Hu, Jiongnan Liu, Jiejun Tan, Yutao Zhu, Zhicheng Dou 핵심 연구 목표 현재 LLM 에이전트 메모리 시스템이 주로 사용하는 평면적인 정보 저장 방식과 단순 유사성 기반 검색의 한계를 극복하는 것이 목표입니다. 특히, 사건 간의 논리적 관계(인과성, 시간 순서) 를 "},{"id":"2026-01-12-Qwen3-VL-Embedding-and-Qwen3-VL-Reranker-A-Unified-Framework-for-State-of-the-Art-Multimodal-Retrieval-and-Ranking","title":"[논문리뷰] Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking","excerpt":"arXiv에 게시된 'Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-Qwen3-VL-Embedding-and-Qwen3-VL-Reranker-A-Unified-Framework-for-State-of-the-Art-Multimodal-Retrieval-and-Ranking","tags":["Review","Multimodal Retrieval","Multimodal Ranking","Foundation Models","Embedding Models","Reranking Models","Contrastive Learning","Knowledge Distillation","Matryoshka Representation Learning","Quantization-Aware Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingxin Li, Yanzhao Zhang, Dingkun Long, Keqin Chen, Sibo Song, Shuai Bai, Zhibo Yang, Pengjun Xie, An Yang, Dayiheng Liu, Jingren Zhou, Junyang Lin (Tongyi Lab, Alibaba Group) 핵"},{"id":"2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents","title":"[논문리뷰] SmartSearch: Process Reward-Guided Query Refinement for Search Agents","excerpt":"Guanting Dong이 arXiv에 게시한 'SmartSearch: Process Reward-Guided Query Refinement for Search Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents","tags":["Review","Search Agent","Information Retrieval","Large Language Models","Process Reward","Query Refinement","Reinforcement Learning","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tongyu Wen, Guanting Dong, Zhicheng Dou 핵심 연구 목표 대규모 언어 모델(LLM) 기반 검색 에이전트의 중간 검색 쿼리 품질이 낮아 예기치 않은 검색 결과와 전체 성능 저하로 이어지는 문제를 해결하는 것입니다. 기존 연구가 추론 패러다임 최적화에 집중하고 쿼리 품질을 간과하는 한계를 극"},{"id":"2026-01-12-Thinking-with-Map-Reinforced-Parallel-Map-Augmented-Agent-for-Geolocalization","title":"[논문리뷰] Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization","excerpt":"arXiv에 게시된 'Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-Thinking-with-Map-Reinforced-Parallel-Map-Augmented-Agent-for-Geolocalization","tags":["Review","Geolocalization","LVLM","Map-Augmented Agent","Reinforcement Learning","Parallel Test-Time Scaling","Tool Use","MAPBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxiang Ji, Yong Wang, Ziyu Ma, Yiming Hu, Hailang Huang, Xuecai Hu, Guanhua Chen, Liaoni Wu, Xiangxiang Chu 핵심 연구 목표 기존 대규모 시각언어 모델(LVLM) 기반 지리 위치 특정(Geolocalization) 방법론이 지도 활용"},{"id":"2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction","title":"[논문리뷰] VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction","excerpt":"Yu Sun이 arXiv에 게시한 'VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-12 00:00:00+0900+0900","permalink":"/ai/review/2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction","tags":["Review","Video Generation","Autoregressive Models","Next-Frame Prediction","Multi-scale Prediction","Temporal Consistency","Visual Autoregressive","Error Propagation"],"text":"링크: 논문 PDF로 바로 열기 저자: Longbin Ji, Xiaoxiong Liu, Junyuan Shang, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang 핵심 연구 목표 비디오 생성 분야에서 Diffusion 및 FlowMatching 모델 의 높은 계산 비용과 확장성 문제를 해결하는 것을 목표로 합니다. 특히, 기존"},{"id":"2026-01-13-Are-LLM-Decisions-Faithful-to-Verbal-Confidence","title":"[논문리뷰] Are LLM Decisions Faithful to Verbal Confidence?","excerpt":"arXiv에 게시된 'Are LLM Decisions Faithful to Verbal Confidence?' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Are-LLM-Decisions-Faithful-to-Verbal-Confidence","tags":["Review","Large Language Model","Uncertainty Quantification","Verbal Confidence","Abstention","Decision-Making","Risk-Sensitive AI","Utility Maximization"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiawei Wang, Yanfei Zhou, Siddartha Devic, Deqing Fu 핵심 연구 목표 대규모 언어 모델(LLM)이 자체 불확실성을 표현하는 '언어적 자신감'이 모델의 실제 추론, 지식 또는 의사 결정에 얼마나 충실한지 평가하는 것을 목표로 합니다. 특히, LLM이 다양한 오류 페널티에 반응하여"},{"id":"2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language","title":"[논문리뷰] BabyVision: Visual Reasoning Beyond Language","excerpt":"Yiyan Liang이 arXiv에 게시한 'BabyVision: Visual Reasoning Beyond Language' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language","tags":["Review","Multimodal LLMs","Visual Reasoning","Benchmark","Early Vision","Spatial Perception","Visual Tracking","Pattern Recognition","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Liang Chen, Kuan Li 외 다수 핵심 연구 목표 최신 멀티모달 대규모 언어 모델(MLLMs)이 고수준의 지식 기반 과제에서는 탁월하지만, 3세 아동도 쉽게 해결하는 기본적인 시각적 추론 과제에서 실패하는 근본적인 문제를 해결하고자 합니다. 이는 MLLMs가 시각 정보를 언어적 선험 지식에 과도하게 의존하며"},{"id":"2026-01-13-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models","title":"[논문리뷰] Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models","excerpt":"Chenchen Jing이 arXiv에 게시한 'Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Masked Diffusion","Soft Tokens","Progressive Decoding","Iterative Refinement","Continuous Trajectory Supervision","KV-Caching","Blockwise Diffusion"],"text":"링크: 논문 PDF로 바로 열기 저자: Linhao Zhong, Linyu Wu, Bozhen Fang, Tianjian Feng, Chenchen Jing, Hao Chen, Chunhua Shen 핵심 연구 목표 대부분의 확산 언어 모델(DLMs)이 사용하는 경직된 이진 마스킹 과 이산 토큰 할당 의 한계를 극복하고, 초기 결정의 수정 불가 및 중간 확"},{"id":"2026-01-13-Boosting-Latent-Diffusion-Models-via-Disentangled-Representation-Alignment","title":"[논문리뷰] Boosting Latent Diffusion Models via Disentangled Representation Alignment","excerpt":"arXiv에 게시된 'Boosting Latent Diffusion Models via Disentangled Representation Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Boosting-Latent-Diffusion-Models-via-Disentangled-Representation-Alignment","tags":["Review","Latent Diffusion Models","Variational Autoencoders","Disentangled Representations","Vision Foundation Models","Representation Alignment","Image Generation","Semantic Disentanglement"],"text":"링크: 논문 PDF로 바로 열기 저자: John Page, Xuesong Niu, Kai Wu, Kun Gai 핵심 연구 목표 Latent Diffusion Models (LDMs)의 핵심 구성 요소인 Variational Autoencoders (VAEs)가 기존처럼 픽셀 단위 재구성에만 초점을 맞추거나, LDM과 동일한 상위 수준의 의미론적 정렬 대상을"},{"id":"2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction","title":"[논문리뷰] Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction","excerpt":"Zhengkang Guo이 arXiv에 게시한 'Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction","tags":["Review","Long-Term Human-Agent Interaction","Controllable Memory","Memory Anchoring","Large Language Models (LLMs)","Personalization","Reinforcement Learning (RL)","Supervised Fine-Tuning (SFT)","Memory Dependence"],"text":"링크: 논문 PDF로 바로 열기 저자: Muzhao Tian, Zisu Huang, Xiaohua Wang, Jingwen Xu, Zhengkang Guo, Qi Qian, Yuanzhe Shen, Kaitao Song, Jiakang Yuan, Changze Lv, Xiaoqing Zheng 핵심 연구 목표 본 논문은 장기적인 인간에이전트 상호작용에서 L"},{"id":"2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data","title":"[논문리뷰] Dr. Zero: Self-Evolving Search Agents without Training Data","excerpt":"Shaoliang Nie이 arXiv에 게시한 'Dr. Zero: Self-Evolving Search Agents without Training Data' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data","tags":["Review","Self-Evolution","Search Agents","Large Language Models (LLMs)","Data-Free Learning","Reinforcement Learning (RL)","Hop-Grouped Relative Policy Optimization (HRPO)","Question Answering","Multi-hop Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaoliang Nie, Suyu Ge, Xianjun Yang, Kartikeya Upasani, Zhenrui Yue 핵심 연구 목표 본 논문은 기존 멀티턴 검색 에이전트의 데이터 없는 자기 진화 과정에서 발생하는 제한적인 질문 다양성과 다단계 추론 및 도구 사용에 필요한 막대한 컴퓨팅 자원 문제를 해결하는 것을"},{"id":"2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving","title":"[논문리뷰] DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving","excerpt":"arXiv에 게시된 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving","tags":["Review","Generative World Models","Autonomous Driving","Video Generation","Benchmark","Evaluation Metrics","Trajectory Prediction","Temporal Consistency","Data Diversity"],"text":"링크: 논문 PDF로 바로 열기 저자: Yang Zhou, Hao Shao, Letian Wang, Zhuofan Zong, Hongsheng Li, Steven L. Waslander 핵심 연구 목표 자율주행을 위한 생성형 비디오 월드 모델 연구 분야는 빠르게 성장하고 있지만, 안전에 중요한 시각적 요소, 궤적의 현실성, 시공간 및 에이전트 수준의 일관성"},{"id":"2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration","title":"[논문리뷰] ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration","excerpt":"arXiv에 게시된 'ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration","tags":["Review","Large Language Models (LLMs)","Tool-Integrated Reasoning (TIR)","Agent Behavior Calibration","Reinforcement Learning (RL)","Self-Evolving Data Flywheel","Action Space Exploration","Behavioral Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifei Chen, Guanting Dong, Zhicheng Dou 핵심 연구 목표 LLM 기반의 ToolIntegrated Reasoning (TIR) 에이전트가 정확도에만 집중하여 발생하는 비효율적인 행동 패턴(예: 중복되거나 불충분한 도구 호출) 문제를 해결하는 것이 목표입니다. 에이전트의 잘못된 행동 패턴을"},{"id":"2026-01-13-GlimpRouter-Efficient-Collaborative-Inference-by-Glimpsing-One-Token-of-Thoughts","title":"[논문리뷰] GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts","excerpt":"arXiv에 게시된 'GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-GlimpRouter-Efficient-Collaborative-Inference-by-Glimpsing-One-Token-of-Thoughts","tags":["Review","Collaborative Inference","Large Reasoning Models (LRMs)","Inference Latency","Step-wise Routing","Initial Token Entropy","Dynamic Routing","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhao Zeng, Xuteng Zhang, Yuling Shi, Chao Hu, Yuting Chen, Beijun Shen, Xiaodong Gu 핵심 연구 목표 대규모 추론 모델(LRMs)의 다단계 사고 체인 생성에서 발생하는 막대한 추론 지연 및 계산 비용 문제를 해결하는 것이 목표입니다. 기존 협업 추론 "},{"id":"2026-01-13-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors","title":"[논문리뷰] Lost in the Noise: How Reasoning Models Fail with Contextual Distractors","excerpt":"arXiv에 게시된 'Lost in the Noise: How Reasoning Models Fail with Contextual Distractors' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors","tags":["Review","Robustness","Contextual Distractors","RAG","Reasoning Models","Alignment","Tool Use","NoisyBench","Rationale-Aware Reward","Inverse Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo 핵심 연구 목표 현재 AI 연구는 '정돈된' 벤치마크에 의존하지만, 실제 환경의 본질적인 노이즈를 반영하지 못해 에이전트 AI 시스템의 실제 성능을 오해하게 만듭니다. 이 논문은 컨텍스트 교란 요소(conte"},{"id":"2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head","title":"[논문리뷰] MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head","excerpt":"arXiv에 게시된 'MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head","tags":["Review","Linear Attention","Multi-Head Attention","Transformer","Global Context Collapse","Representational Diversity","Image Generation","NLP","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou 핵심 연구 목표 Transformer의 핵심 모듈인 SelfAttention의 2차 시간 복잡성 으로 인한 확장성 문제를 해결하고자 합니다. 특히, "},{"id":"2026-01-13-MegaFlow-Large-Scale-Distributed-Orchestration-System-for-the-Agentic-Era","title":"[논문리뷰] MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era","excerpt":"Fan Zhou이 arXiv에 게시한 'MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-MegaFlow-Large-Scale-Distributed-Orchestration-System-for-the-Agentic-Era","tags":["Review","Agentic AI","Distributed Orchestration","Scalability","Cloud-Native","Reinforcement Learning","Software Engineering Agents","Resource Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Fan Zhou, Jiawei Chen, Ruisheng Cao, Mouxiang Chen, Lei Zhang 핵심 연구 목표 본 논문은 인터랙티브하고 자율적인 AI 에이전트의 대규모 훈련 및 평가를 위한 기존 인프라의 한계를 해결하고자 합니다. 특히 소프트웨어 공학 과 같은 복잡한 에이전트 태스크에서 발생하는 보안,"},{"id":"2026-01-13-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent","title":"[논문리뷰] OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent","excerpt":"arXiv에 게시된 'OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent","tags":["Review","Computer-Using Agent (CUA)","Multi-Agent Framework","Long-horizon Tasks","Memory Management","Multimodal Retrieval","Reflection","Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, Jingjing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen D"},{"id":"2026-01-13-On-the-Fallacy-of-Global-Token-Perplexity-in-Spoken-Language-Model-Evaluation","title":"[논문리뷰] On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation","excerpt":"Ju-Chieh Chou이 arXiv에 게시한 'On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-On-the-Fallacy-of-Global-Token-Perplexity-in-Spoken-Language-Model-Evaluation","tags":["Review","Spoken Language Models","Evaluation Metrics","Perplexity","Mean Opinion Score","Likelihood-based Evaluation","Model-as-a-Judge","Acoustic Consistency","Speech Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jeff ChanJan Sju, LiangHsuan Tseng, YiCheng Lin, YenChun Kuo, JuChieh Chou 핵심 연구 목표 본 논문은 음성 언어 모델(SLM) 평가에 널리 사용되는 '글로벌 토큰 퍼플렉시티(Global Token Perplexity)' 가 음성과 텍스트 양식 간의 근본적인 차"},{"id":"2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning","title":"[논문리뷰] OpenTinker: Separating Concerns in Agentic Reinforcement Learning","excerpt":"Jiaxuan You이 arXiv에 게시한 'OpenTinker: Separating Concerns in Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning","tags":["Review","Reinforcement Learning","LLM Agents","Multi-Agent Systems","System Architecture","Separation of Concerns","RLaaS","Distributed Training","Agent Protocol Coordination"],"text":"링크: 논문 PDF로 바로 열기 저자: Siqi Zhu, Jiaxuan You 핵심 연구 목표 기존 대규모 언어 모델(LLM) 에이전트용 강화 학습(RL) 시스템의 한계를 극복하고, 에이전트 환경 및 상호작용 프로토콜의 재사용성 부족, 그리고 에이전트 프로그래밍과 실행 간의 분리 부재 문제를 해결하는 것을 목표로 합니다. OpenTinker 는 알고리즘 설"},{"id":"2026-01-13-PaCoRe-Learning-to-Scale-Test-Time-Compute-with-Parallel-Coordinated-Reasoning","title":"[논문리뷰] PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning","excerpt":"arXiv에 게시된 'PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-PaCoRe-Learning-to-Scale-Test-Time-Compute-with-Parallel-Coordinated-Reasoning","tags":["Review","PaCoRe","Test-Time Compute Scaling","LLMs","Parallel Reasoning","Reinforcement Learning","Reasoning Synthesis","Message Passing","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingcheng Hu, Yinmin Zhang, Shijie Shang, Xiaobo Yang, Yue Peng, Zhewei Huang, Hebin Zhou, Xin Wu, Jie Cheng, Fanqi Wan, Xiangwen Kong, Chengyuan Yao, Kaiwen Yan, Ailin Huang, Ho"},{"id":"2026-01-13-Structured-Episodic-Event-Memory","title":"[논문리뷰] Structured Episodic Event Memory","excerpt":"arXiv에 게시된 'Structured Episodic Event Memory' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Structured-Episodic-Event-Memory","tags":["Review","LLMs","RAG","Episodic Memory","Graph Memory","Memory Architecture","Narrative Coherence","Long-term Reasoning","Event Frames"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengxuan Lu, Dongfang Li, Yukun Shi, Beilun Wang, Longyue Wang, Baotian Hu 핵심 연구 목표 현재 LLM(Large Language Models)의 RAG (RetrievalAugmented Generation) 가 겪는 산발적인 정보 검색 및 구조적 의존성 "},{"id":"2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","title":"[논문리뷰] TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning","excerpt":"Hao Wang이 arXiv에 게시한 'TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","tags":["Review","Travel Planning","LLM Agents","Reinforcement Learning","Multi-path Reasoning","Constraint Satisfaction","POI Optimization","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinuo Wang, Mining Tan, Wenxiang Jiao, Xiaoxi Li, Hao Wang, Xuanyu Zhang, Yuan Lu, Weiming Dong 핵심 연구 목표 본 논문은 여행 계획 생성 시 발생하는 세 가지 주요 문제를 해결하는 것을 목표로 합니다: 방대한 관심 지점(POI) 후보군의 효율"},{"id":"2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning","title":"[논문리뷰] Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning","excerpt":"Shuo Zhang이 arXiv에 게시한 'Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning","tags":["Review","Video Question Answering","Open-domain Search","Multimodal LLMs","Agentic AI","Benchmark","Video Understanding","Multi-hop Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuo Zhang, Zhe Huang, Zhuoyue Chang, Xiaomin Yu, Chengwen Liu 핵심 연구 목표 본 논문은 기존 비디오 질의응답 벤치마크의 한계, 즉 폐쇄된 증거 설정과 텍스트 기반 검색에 의존하는 문제점을 해결하고자 합니다. 비디오에서 추출한 시각적 단서와 개방형 웹 검색을 결합하여 "},{"id":"2026-01-13-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models","title":"[논문리뷰] What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models","excerpt":"arXiv에 게시된 'What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models","tags":["Review","Vision-Language Models","Under-specified Queries","Multimodal Benchmark","HAERAE-Vision","Query Explicitation","Retrieval Augmentation","Cultural Knowledge","Korean QA"],"text":"링크: 논문 PDF로 바로 열기 저자: Dasol Choi, Guijin Son, Hanwool Lee, Minhyuk Kim, Hyunwoo Ko, Teabin Lim, Eungyeol Ahn, Jungwhan Kim, Seunghyeok Hong, Youngsook Song 핵심 연구 목표 본 논문은 현재 VisionLanguage Models (VLM"},{"id":"2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests","title":"[논문리뷰] X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests","excerpt":"Jane Luo이 arXiv에 게시한 'X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-13 00:00:00+0900+0900","permalink":"/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests","tags":["Review","Competitive Programming","Code LLMs","Synthetic Data Generation","Supervised Fine-tuning (SFT)","Reinforcement Learning (RL)","Dual Verification","Scaling Laws","SynthSmith"],"text":"링크: 논문 PDF로 바로 열기 저자: Jie Wu, Haoling Li, Xin Zhang, Jiani Guo, Jane Luo, Steven Liu, Yangyu Huang, Ruihang Chu, Scarlett Li, Yujiu Yang 핵심 연구 목표 본 논문은 경쟁 프로그래밍(Competitive Programming)을 위한 코드 LLM(Lar"},{"id":"2026-01-14-Aligning-Text-Code-and-Vision-A-Multi-Objective-Reinforcement-Learning-Framework-for-Text-to-Visualization","title":"[논문리뷰] Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization","excerpt":"arXiv에 게시된 'Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-Aligning-Text-Code-and-Vision-A-Multi-Objective-Reinforcement-Learning-Framework-for-Text-to-Visualization","tags":["Review","Text-to-Visualization","Reinforcement Learning","Multi-Objective Optimization","GRPO","Multimodal Feedback","LLMs","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mizanur Rahman, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque 핵심 연구 목표 기존 TexttoVisualization (Text2Vis) 시스템, 특히 오픈소스 LLM 들이 쿼리와 의미적으로 정렬되고 가독성이 높으며 실"},{"id":"2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","title":"[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking","excerpt":"arXiv에 게시된 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","tags":["Review","Reinforcement Learning","LLM Agents","Open-Ended Tasks","Relative Ranking","Tournament-based Ranking","Discriminative Collapse","Reward Modeling","Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei L"},{"id":"2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance","title":"[논문리뷰] End-to-End Video Character Replacement without Structural Guidance","excerpt":"arXiv에 게시된 'End-to-End Video Character Replacement without Structural Guidance' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance","tags":["Review","Video Character Replacement","Diffusion Models","In-Context Learning","Reinforcement Learning","Structural Guidance","Video Editing","Data Generation Pipeline"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhengbo Xu, Jie Ma, Ziheng Wang, Zhan Peng, Jun Liang, Jing Li (AliBaBa Group) 핵심 연구 목표 본 논문은 기존 비디오 캐릭터 교체 방법론이 페어링된 데이터 부족과 perframe segmentation masks 및 explicit structural gu"},{"id":"2026-01-14-EpiCaR-Knowing-What-You-Dont-Know-Matters-for-Better-Reasoning-in-LLMs","title":"[논문리뷰] EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs","excerpt":"arXiv에 게시된 'EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-EpiCaR-Knowing-What-You-Dont-Know-Matters-for-Better-Reasoning-in-LLMs","tags":["Review","LLM Reasoning","Model Calibration","Epistemic Uncertainty","Self-Training","Supervised Fine-tuning","Confidence-Informed Self-Consistency","Model Collapse"],"text":"링크: 논문 PDF로 바로 열기 저자: Jewon Yeom, Jaewon Sok, Seonghyeon Park, Jeongjae Park, Taesup Kim 핵심 연구 목표 본 논문은 LLM의 반복적인 자가 훈련 과정에서 발생하는 과도한 자신감(overconfidence) 및 신뢰도 저하(calibration cost) 문제를 해결하여, 모델이 '무엇을 "},{"id":"2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning","title":"[논문리뷰] JudgeRLVR: Judge First, Generate Second for Efficient Reasoning","excerpt":"Sujian Li이 arXiv에 게시한 'JudgeRLVR: Judge First, Generate Second for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning","tags":["Review","RLVR","LLMs","Reasoning","Judge-then-Generate","Quality-Efficiency","Discriminative Supervision","Mathematical Reasoning","Backtracking Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiangshan Duo, Hanyu Li, Hailin Zhang, Yudong Wang, Sujian Li, Liang Zhao 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 과정에서 RLVR(Reinforcement Learning with Verifiable Rewards) 이 흔히 유발하는 장황"},{"id":"2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions","title":"[논문리뷰] KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions","excerpt":"Chenglong Li이 arXiv에 게시한 'KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions","tags":["Review","Person Understanding","Lifelong Digital Companions","Memory Benchmarking","Autobiographical Narratives","Cognitive Stream","Flashback Handling","LLM Evaluation","Hierarchical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tingyu Wu, Zhisheng Chen, Ziyan Weng, Shuhe Wang, Chenglong Li, Shuo Zhang, Sen Hu, Silin Wu, Qizhen Lan, Huacan Wang, Ronghao Chen 핵심 연구 목표 이 논문은 기존의 LLM 메모리 벤치마크가 단순한 정보 검색에 치우"},{"id":"2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences","title":"[논문리뷰] MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences","excerpt":"Rui Xu이 arXiv에 게시한 'MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences","tags":["Review","Code Agents","Software Engineering","Experiential Memory","GitHub Data","Experience Governance","Agentic Search","LLM Applications","Bug Fixing"],"text":"링크: 논문 PDF로 바로 열기 저자: Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu 핵심 연구 목표 자율 소프트웨어 엔지니어링(SWE) 에이전트가 GitHub와 같은 플랫폼에 축적된 방대한 인간 경험을 효과적으로 활용하지 못하는 \"닫힌 세계\" 한계를 해결하는 것이 목표입니다. 이 연구는 비정형적이고 파편화"},{"id":"2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning","title":"[논문리뷰] MemoBrain: Executive Memory as an Agentic Brain for Reasoning","excerpt":"Zheng Liu이 arXiv에 게시한 'MemoBrain: Executive Memory as an Agentic Brain for Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning","tags":["Review","Executive Memory","LLM Agents","Reasoning","Context Management","Tool-Augmented Agents","Memory Management","Trajectory Folding","Preference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongjin Qian, Zhao Cao, Zheng Liu 핵심 연구 목표 본 논문은 도구 증강 에이전트 환경에서 장기적인 추론 과정 중 발생하는 LLM의 유한한 컨텍스트 문제 를 해결하고자 합니다. 특히, 누적되는 추론 트레이스와 도구 산출물로 인해 논리적 연속성이 저해되고 태스크 정렬이 약화되는 인지 부하 문제를"},{"id":"2026-01-14-Ministral-3","title":"[논문리뷰] Ministral 3","excerpt":"arXiv에 게시된 'Ministral 3' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-Ministral-3","tags":["Review","Large Language Models","Model Distillation","Pruning","Parameter-Efficient AI","Multimodal LLMs","Instruction Tuning","Reinforcement Learning from Human Feedback","Open-Source AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexander H. Liu, Kartik Khandelwal, Sandeep Subramanian, Victor Jouault 핵심 연구 목표 본 연구는 컴퓨팅 및 메모리 제약이 있는 환경 을 위한 효율적인 매개변수 효율적(parameterefficient) 밀집 언어 모델 인 Ministral 3 시리즈를 개발하"},{"id":"2026-01-14-Motion-Attribution-for-Video-Generation","title":"[논문리뷰] Motion Attribution for Video Generation","excerpt":"arXiv에 게시된 'Motion Attribution for Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation","tags":["Review","Motion Attribution","Video Generation","Diffusion Models","Gradient-based Attribution","Temporal Dynamics","Motion Masking","Fine-tuning","Data Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura LealTaixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine 핵심 연구 목표 본 논문은 비디오 생성 모델에서 생성된 비디오의 움직임(motion) 에 영향"},{"id":"2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands","title":"[논문리뷰] ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands","excerpt":"arXiv에 게시된 'ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands","tags":["Review","GUI Automation","Flow-based Generative Models","Continuous Control","Vision-Language Models","Human-Computer Interaction","ScreenDrag Benchmark","Dexterous Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou 핵심 연구 목표 기존 GUI 에이전트들이 주로 이산적인 클릭 예측에 의존하여 연속적이고 자유로운 형태의 드래그(예: 그림 그리기, 캡차 풀이)와 같이 즉각적인 시각적 인지와 조정이 필요한 복잡한 GUI 상호작용을 수행하기 어렵다는 문제"},{"id":"2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices","title":"[논문리뷰] SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices","excerpt":"arXiv에 게시된 'SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices","tags":["Review","Diffusion Transformers","Edge AI","Efficient Image Generation","Sparse Attention","Elastic Training","Knowledge Distillation","Mobile AI","High-Fidelity"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongting Hu, Aarush Gupta, Magzhan Gabidolla, Arpit Sahni, Huseyin Coskun, Yanyu Li, Yerlan Idelbayev, Ahsan Mahmood, Aleksei Lebedev, Dishani Lahiri, Anujraaj Goyal, Ju Hu, Ming"},{"id":"2026-01-14-Solar-Open-Technical-Report","title":"[논문리뷰] Solar Open Technical Report","excerpt":"arXiv에 게시된 'Solar Open Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-Solar-Open-Technical-Report","tags":["Review","Large Language Models","Mixture-of-Experts","Korean LLM","Synthetic Data Generation","Curriculum Learning","Reinforcement Learning","Tokenizer Optimization","Multilingual AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Upstage Solar Team 핵심 연구 목표 Solar Open 논문은 기존 LLM 생태계에서 영어와 중국어 외의 언어들 , 특히 한국어와 같은 데이터 부족 언어 가 겪는 모델 개발의 어려움을 해결하는 것을 목표로 합니다. 데이터 희소성, 추론 능력 부족, 확장 가능한 강화 학습(RL) 부재라는 세 가지 상호 연"},{"id":"2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents","title":"[논문리뷰] The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents","excerpt":"Junjue Wang이 arXiv에 게시한 'The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents","tags":["Review","LLM Agents","Calibration","Tool Use","Reinforcement Learning","Miscalibration","Overconfidence","Trustworthy AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang, Naoto Yokoya 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 자율 에이전트의 신뢰성을 높이기 위해, 도구 사용 환경에서 발생하는 verbalized calibration(언어화된 확신)"},{"id":"2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking","title":"[논문리뷰] Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking","excerpt":"Zhen Ye이 arXiv에 게시한 'Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking","tags":["Review","Fact-Checking","Large Language Models (LLMs)","Benchmarking","Multi-agent System","Stage-wise Evaluation","Claim Evolution","Trustworthy AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongzhan Lin, Zixin Chen, Zhiqi Shen, Ziyang Luo, Zhen Ye, Jing Ma, TatSeng Chua and Guandong Xu 핵심 연구 목표 본 논문은 기존 벤치마크가 클레임 검증에만 초점을 맞춰 LLM의 사실 확인 워크플로우 전반(클레임 추출 및 증거 검색 포함)을 간"},{"id":"2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale","title":"[논문리뷰] User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale","excerpt":"arXiv에 게시된 'User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale","tags":["Review","Multi-Turn Dialogue Generation","Tool Use","Autonomous Agents","Large Reasoning Models","User Simulation","Synthetic Data Generation","SQL-based Tools","Agentic Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Jungho Cho, Minbyul Jeong, Sungrae Park 핵심 연구 목표 기존 멀티턴 도구 사용(tooluse) 데이터셋의 한계(정적, 사전 정의된 도구셋, 단일 샷 위주)를 극복하고, 실제 인간에이전트 협업의 반복적이고 점진적인 특성을 반영하는 확장 가능한 고품질 멀티턴 대화 데이터 생성 프레임워크 를"},{"id":"2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory","title":"[논문리뷰] VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory","excerpt":"arXiv에 게시된 'VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-14 00:00:00+0900+0900","permalink":"/ai/review/2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory","tags":["Review","Embodied Navigation","VLA Model","Adaptive Reasoning","Chain-of-Thought (CoT)","Linguistic Memory","Reinforcement Learning","Sim-to-Real Transfer","Multi-task Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaoan Wang, Yuanfei Luo, Xingyu Chen, Aocheng Luo, Dongyue Li, Chang Liu, Sheng Chen, Yangang Zhang, Junzhi Yu 핵심 연구 목표 기존 VisionLanguageAction (VLA) 모델이 복잡하고 장기적인 내비게이션 태스크에서 부"},{"id":"2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation","title":"[논문리뷰] A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation","excerpt":"Kai He이 arXiv에 게시한 'A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation","tags":["Review","Scientific Reasoning","Memory-Driven AI","Benchmarking","Large Language Models (LLMs)","Anchor-Attractor Activation","Episodic Memory","Knowledge Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Jian Zhang, Yu He, Zhiyuan Wang, Zhangqi Wang, Kai He, Fangzhi Xu, Qika Lin, Jun Liu 핵심 연구 목표 논문은 기존 과학적 추론 벤치마크가 최종 답변의 정확성과 과정의 일관성에만 초점을 맞추고, 인간 추론의 기저에 있는 메모리 기반 메커니즘 , 즉 앵커("},{"id":"2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity","title":"[논문리뷰] Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity","excerpt":"Chi Zhang이 arXiv에 게시한 'Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity","tags":["Review","Large Language Models","Preference Alignment","Preference-Undermining Attacks","Factorial Analysis","Sycophancy","Prompt Engineering","Truth-Deference Trade-off"],"text":"링크: 논문 PDF로 바로 열기 저자: Chi Zhang, Jiawei Shao, Jiangan Chen, Yiliang Song, Hongjun An 핵심 연구 목표 본 연구는 사용자 선호도에 맞춰 정렬된 대규모 언어 모델(LLM) 이 PreferenceUndermining Attacks (PUA) 에 취약한지 규명하는 것을 목표로 합니다. 특히, 조작적"},{"id":"2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization","title":"[논문리뷰] Controlled Self-Evolution for Algorithmic Code Optimization","excerpt":"arXiv에 게시된 'Controlled Self-Evolution for Algorithmic Code Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization","tags":["Review","Self-Evolution","Code Optimization","Large Language Models","Genetic Algorithms","Hierarchical Memory","Algorithmic Code Generation","Exploration Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Yi Xu, Huacan Wang 핵심 연구 목표 논문은 기존 LLM 기반 코드 생성 모델 이 기능"},{"id":"2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation","title":"[논문리뷰] DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation","excerpt":"arXiv에 게시된 'DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation","tags":["Review","Agentic AI","Deep Research Systems","Automated Evaluation","Task Construction","Fact-Checking","LLM Benchmarking","Adaptive Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yibo Wang, Lei Wang, Yue Deng, Keming Wu, Yao Xiao, Huanjin Yao, Liwei Kang, Hai Ye, Yongcheng Jing, Lidong Bing 핵심 연구 목표 본 논문은 심층 연구 시스템이 생성하는 길고 복잡한 보고서의 평가가 어렵다는 문제점을 해결하고자 합니"},{"id":"2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning","title":"[논문리뷰] Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning","excerpt":"arXiv에 게시된 'Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning","tags":["Review","Knowledge Distillation","Sequence-level Distillation","Chain-of-Thought Reasoning (CoT)","Large Language Models (LLMs)","Temperature-scheduled Learning","Divergence-aware Sampling","Mixed-policy Distillation","Open-source Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaotian Yan, Kaiyuan Liu, Chen Shen, Bing Wang, Sinan Fan, Jun Zhang, Yue Wu, Zheng Wang, Jieping Ye 핵심 연구 목표 본 논문은 교사 모델이 생성한 응답에 대한 SFT(Supervised FineTuning) 기반 시퀀스 레벨 증류 패러다"},{"id":"2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","title":"[논문리뷰] Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering","excerpt":"Ayush Tewari이 arXiv에 게시한 'Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","tags":["Review","Video Generation","Diffusion Models","3D Reconstruction","3D Gaussian Splatting","Camera-Controlled","Sparse Keyframes","Real-time","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari 핵심 연구 목표 본 논문은 확산 모델 기반 비디오 생성의 높은 계산 비효율성 문제를 해결하고, 정적 장면에 대한 카메라 제어 비디오 생성 을 위한 효율적인 프레임워크를 제안하는 것을 목표로 합니다. 특히, 기존 모델이 모든 "},{"id":"2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines","title":"[논문리뷰] EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines","excerpt":"arXiv에 게시된 'EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines","tags":["Review","LLM Agents","Self-Evolution","Finite State Machines","Deep Research","Multi-hop QA","Adaptive Workflow","Memory Mechanism","Controllable AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang, Ronghao Chen, Huacan Wang 핵심 연구 목표 LLM"},{"id":"2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents","title":"[논문리뷰] ExpSeek: Self-Triggered Experience Seeking for Web Agents","excerpt":"arXiv에 게시된 'ExpSeek: Self-Triggered Experience Seeking for Web Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents","tags":["Review","Web Agents","Experience Seeking","Self-Triggered","LLM Reasoning","Entropy","Proactive Guidance","Reinforcement Learning","Foundation Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenyuan Zhang¹², Xinghua Zhang³, Haiyang Yu³, Shuaiyi Nie1,2, Bingli Wu³, Juwei Yue1,2, Tingwen Liu1,2, Yongbin Li³ 핵심 연구 목표 기존 웹 에이전트들이 경험을 수동적으로 전역 컨텍스트로 주입하여 동적으로 변하는 환경에서 비효율"},{"id":"2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning","title":"[논문리뷰] Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning","excerpt":"arXiv에 게시된 'Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning","tags":["Review","Vision-Language-Action","Embodied AI","Latent Planning","Chain-of-Thought","Distillation","Inference Efficiency","Robotic Manipulation","Preference Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: ChiPin Huang, Yunze Man, Zhiding Yu, MinHung Chen, Jan Kautz, YuChiang Frank Wang, FuEn Yang 핵심 연구 목표 본 논문은 복잡한 시각언어액션 (VLA) 태스크에서 기존 추론 VLA 모델들이 긴 chainofthought (CoT) 추론 과정으로 인"},{"id":"2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models","title":"[논문리뷰] Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models","excerpt":"Xiao Yang이 arXiv에 게시한 'Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models","tags":["Review","Video Diffusion Models","Image-to-Video Generation","Diffusion Transformers (DiT)","Controllability","Semantic Alignment","Focal Guidance","Prompt Adherence"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuanyang Yin, Yufan Deng, Kaipeng Zhang, Xiao Yang, Shenghai Yuan, Feng Zhao 핵심 연구 목표 본 논문은 Diffusion Transformer (DiT) 기반의 ImagetoVideo (I2V) 모델에서 텍스트 프롬프트에 대한 제어력 부족 문제를 해결하고자 "},{"id":"2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection","title":"[논문리뷰] FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection","excerpt":"arXiv에 게시된 'FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection","tags":["Review","UI Grounding","Visual Token Reduction","Position-Preserving","Vision-Language Models (VLMs)","Saliency Scoring","Computational Efficiency","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingyu Ouyang, Kevin Qinghong Lin, Mike Zheng Shout, Hwee Tou Ng 핵심 연구 목표 본 논문은 고해상도 UI 스크린샷에서 발생하는 수천 개의 시각 토큰으로 인한 VisionLanguage Models (VLMs) 의 UI Grounding 작업의 높은 계산 오버헤드와 주"},{"id":"2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations","title":"[논문리뷰] Geometric Stability: The Missing Axis of Representations","excerpt":"pcr2120이 arXiv에 게시한 'Geometric Stability: The Missing Axis of Representations' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations","tags":["Review","Geometric Stability","Representation Analysis","Similarity Metrics","Shesha Framework","Drift Detection","Transfer Learning","Neural Representations","CRISPR Screens"],"text":"링크: 논문 PDF로 바로 열기 저자: Prashant C. Raju 핵심 연구 목표 논문은 학습된 표현(learned representations) 분석의 한계를 지적하며, 기존의 유사성(similarity) 측정 방식이 표현된 구조의 견고성(robustness)을 놓친다고 주장합니다. 이를 해결하기 위해, 섭동(perturbation), 재샘플링 또는 "},{"id":"2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models","title":"[논문리뷰] Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models","excerpt":"Wenjie Li이 arXiv에 게시한 'Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models","tags":["Review","LLM Agents","World Models","Adaptive Planning","Lookahead","Reinforcement Learning","POMDP","Task Planning","Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Youwei Liu, Jian Wang, Hanlin Wang, Beichen Guo, Wenjie Li 핵심 연구 목표 대규모 언어 모델(LLM) 기반 에이전트가 \"얕은 그라운딩(shallow grounding)\" 문제로 인해 행동의 장기적 결과를 예측하지 못하여 발생하는 실패를 해결하는 것이 목표입니다. 기존의 단"},{"id":"2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding","title":"[논문리뷰] OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding","excerpt":"arXiv에 게시된 'OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding","tags":["Review","3D Scene Understanding","Open-Vocabulary Segmentation","Referring Expression Segmentation","Training-Free","Voxel Grouping","Vision-Language Models","Multi-modal Large Language Models","Sparse Voxel Rasterization"],"text":"링크: 논문 PDF로 바로 열기 저자: ShengYu Huang, Jaesung Choe, Frank Wang, Cheng Sun 핵심 연구 목표 기존 3D 장면 이해 방법론들이 훈련된 임베딩과 대규모 수동 주석, 긴 훈련 시간에 의존하는 한계를 극복하고자 합니다. OpenVoxel은 훈련 없이 희소 복셀을 그룹화하고 캡셔닝하여 오픈vocabulary 3D"},{"id":"2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL","title":"[논문리뷰] SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL","excerpt":"arXiv에 게시된 'SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL","tags":["Review","Dermatological Diagnosis","Multimodal LLM","Reinforcement Learning","Dynamic Visual Encoding","Information Transmission","Clinically Grounded Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Lijun Liu, Linwei Chen, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, HongYu Zhou 핵심 연구 목표 본 논문은 일반적인 Large VisionLanguage Models (LVLMs) "},{"id":"2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory","title":"[논문리뷰] The AI Hippocampus: How Far are We From Human Memory?","excerpt":"Tong Wu이 arXiv에 게시한 'The AI Hippocampus: How Far are We From Human Memory?' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory","tags":["Review","Large Language Models (LLMs)","Multi-Modal LLMs (MLLMs)","Memory Systems","Implicit Memory","Explicit Memory","Agentic Memory","Retrieval-Augmented Generation (RAG)","Contextual Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Zixia Jia, Jiaqi Li, Yipeng Kang, Yuxuan Wang, Tong Wu, Quansen Wang, Xiaobo Wang, Shuyi Zhang, Junzhe Shen, Qing Li, Siyuan Qi, Yitao Liang, Di He, Zilong Zheng, SongChun Zhu 핵심"},{"id":"2026-01-15-TranslateGemma-Technical-Report","title":"[논문리뷰] TranslateGemma Technical Report","excerpt":"arXiv에 게시된 'TranslateGemma Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-15 00:00:00+0900+0900","permalink":"/ai/review/2026-01-15-TranslateGemma-Technical-Report","tags":["Review","Machine Translation","Large Language Models","Reinforcement Learning","Supervised Fine-tuning","Gemma 3","Multimodal AI","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Google Translate Research Team 핵심 연구 목표 본 논문은 Gemma 3 파운데이션 모델을 기반으로 한 오픈형 기계 번역 모델인 TranslateGemma 를 소개합니다. 연구의 핵심 목표는 Gemma 3 의 다국어 기능을 번역 작업에 맞게 강화하여 탁월한 번역 품질을 제공하고, 투명성, 재현성"},{"id":"2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5","title":"[논문리뷰] A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","excerpt":"Yutao Wu이 arXiv에 게시한 'A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5","tags":["Review","AI Safety","Large Language Models","Multimodal LLMs","Benchmark Evaluation","Adversarial Robustness","Multilingual Evaluation","Regulatory Compliance","Image Generation Safety"],"text":"링크: 논문 PDF로 바로 열기 저자: Yutao Wu, Yixu Wang, Xingjun Ma, xinwang22, DobyXu 핵심 연구 목표 본 논문은 GPT5.2, Gemini 3 Pro, Qwen3VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5 등 7개 최신 AI 모델의 안전성을 종합적이"},{"id":"2026-01-16-Action100M-A-Large-scale-Video-Action-Dataset","title":"[논문리뷰] Action100M: A Large-scale Video Action Dataset","excerpt":"arXiv에 게시된 'Action100M: A Large-scale Video Action Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Action100M-A-Large-scale-Video-Action-Dataset","tags":["Review","Large-scale Dataset","Video Action Recognition","Open-Vocabulary","Temporal Segmentation","Vision-Language Models","Zero-shot Learning","Data Curation","Self-Refine"],"text":"링크: 논문 PDF로 바로 열기 저자: Delong Chen, Tejaswi Kasarla, Yejin Bang, Mustafa Shukor, Willy Chung, Jade Yu, Allen Bolourchi, Théo Moutakanni, Pascale Fung 핵심 연구 목표 본 연구는 기존 영상 액션 데이터셋의 규모 및 도메인 다양성 한계를 극복하고"},{"id":"2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images","title":"[논문리뷰] Alterbute: Editing Intrinsic Attributes of Objects in Images","excerpt":"arXiv에 게시된 'Alterbute: Editing Intrinsic Attributes of Objects in Images' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images","tags":["Review","Intrinsic Attributes","Object Editing","Diffusion Models","Identity Preservation","Visual Named Entities","Text-to-Image","VLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Tal Reiss, Daniel Winter, Alex RavAcha, Yael Pritch, Matan Cohen, Ariel Shamir, Yedid Hoshen 핵심 연구 목표 이미지 내 객체의 색상, 질감, 재질, 심지어 모양과 같은 내재적 속성(Intrinsic Attributes) 을 변경하면서도 객체의 인"},{"id":"2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning","title":"[논문리뷰] Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning","excerpt":"arXiv에 게시된 'Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning","tags":["Review","Test-Time Tool Evolution","Scientific Reasoning","Large Language Models","Dynamic Tool Synthesis","Tool Adaptation","AI for Science","Autonomous Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaxuan Lu†, Ziyu Kong†, Yemin Wang†, Rong Fu, Haiyuan Wan, Cheng Yang, Wenjie Lou, Haoran Sun, Lilong Wang, Yankai Jiang, Xiaosong Wang, Xiao Sun, Dongzhan Zhou 핵심 연구 목표 과학적 추론 "},{"id":"2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation","title":"[논문리뷰] CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation","excerpt":"arXiv에 게시된 'CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation","tags":["Review","Text-to-Image Generation","Video Models","Visual Reasoning","Chain-of-Frame (CoF)","Progressive Refinement","Diffusion Models","CoF-Evol-Instruct"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengzhuo Tong, Mingkun Chang, Shenglong Zhang, Yuran Wang, Cheng Liang, Zhizheng Zhao, Ruichuan An, Bohan Zeng, Yang Shi, Yifan Dai, Ziming Zhao, Guanbin Li, Pengfei Wan, Yuanxi"},{"id":"2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning","title":"[논문리뷰] Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning","excerpt":"arXiv에 게시된 'Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning","tags":["Review","Multi-Agent Systems","Reinforcement Learning","Test-Time Adaptation","Large Language Models","Collaborative Reasoning","Credit Assignment","Textual Experience","Distribution Shift Robustness"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Hu, Yunhai Hu, Juncheng Liu, et al. 핵심 연구 목표 본 논문은 멀티 에이전트 강화 학습(MARL)의 자원 집약적 이고 불안정한 훈련 문제를 해결하는 것을 목표로 합니다. 특히, 동시 진화하는 팀원들로 인한 비정상성 및 희소하고 분산이 큰 보상 으로 인해 발생하는 MARL의 한"},{"id":"2026-01-16-DanQing-An-Up-to-Date-Large-Scale-Chinese-Vision-Language-Pre-training-Dataset","title":"[논문리뷰] DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset","excerpt":"Lan Wu이 arXiv에 게시한 'DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-DanQing-An-Up-to-Date-Large-Scale-Chinese-Vision-Language-Pre-training-Dataset","tags":["Review","Vision-Language Pre-training","Chinese Dataset","Data Filtering","Cross-modal Retrieval","Zero-shot Classification","Multimodal LLMs","SigLIP"],"text":"링크: 논문 PDF로 바로 열기 저자: Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang 핵심 연구 목표 본 연구는 고품질의 중국어 이미지텍스트 데이"},{"id":"2026-01-16-EvasionBench-Detecting-Evasive-Answers-in-Financial-QA-via-Multi-Model-Consensus-and-LLM-as-Judge","title":"[논문리뷰] EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge","excerpt":"Yi Yang이 arXiv에 게시한 'EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-EvasionBench-Detecting-Evasive-Answers-in-Financial-QA-via-Multi-Model-Consensus-and-LLM-as-Judge","tags":["Review","Evasion Detection","Financial NLP","Large Language Models (LLMs)","Multi-Model Consensus","LLM-as-Judge","Data Annotation","Knowledge Distillation","Hard Sample Mining"],"text":"링크: 논문 PDF로 바로 열기 저자: Shijian MA, Yan LIN, Yi YANG 핵심 연구 목표 본 논문은 금융 Q&A에서 회피성 답변(evasive answers) 을 탐지하는 데 필요한 대규모 고품질 벤치마크 부재 와 모호한 경계 사례에 대한 일관성 없는 레이블링 문제를 해결하고자 합니다. 특히 부분적으로 응답하는 답변과 완전한 회피성 답변 "},{"id":"2026-01-16-FlowAct-R1-Towards-Interactive-Humanoid-Video-Generation","title":"[논문리뷰] FlowAct-R1: Towards Interactive Humanoid Video Generation","excerpt":"arXiv에 게시된 'FlowAct-R1: Towards Interactive Humanoid Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-FlowAct-R1-Towards-Interactive-Humanoid-Video-Generation","tags":["Review","Interactive Video Generation","Humanoid Synthesis","Real-time","Streaming Diffusion","MMDiT","Temporal Consistency","Multimodal Control","Low Latency"],"text":"링크: 논문 PDF로 바로 열기 저자: Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao"},{"id":"2026-01-16-LSRIF-Logic-Structured-Reinforcement-Learning-for-Instruction-Following","title":"[논문리뷰] LSRIF: Logic-Structured Reinforcement Learning for Instruction Following","excerpt":"arXiv에 게시된 'LSRIF: Logic-Structured Reinforcement Learning for Instruction Following' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-LSRIF-Logic-Structured-Reinforcement-Learning-for-Instruction-Following","tags":["Review","Instruction Following","Reinforcement Learning","Logical Structures","LLMs","Reward Modeling","Dataset Construction","Attention Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Qingyu Ren¹, Qianyu He¹, Jingwen Chang¹, Jie Zeng¹, Jiaqing Liang2, Yanghua Xiao1, Han Xia³, Zeye Sun³, Fei Yu³ 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)이 복잡한 실세계 명령, 특히 순차적 의존성이나 조건부 분기와 같"},{"id":"2026-01-16-MatchTIR-Fine-Grained-Supervision-for-Tool-Integrated-Reasoning-via-Bipartite-Matching","title":"[논문리뷰] MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching","excerpt":"arXiv에 게시된 'MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-MatchTIR-Fine-Grained-Supervision-for-Tool-Integrated-Reasoning-via-Bipartite-Matching","tags":["Review","Tool-Integrated Reasoning","LLMs","Reinforcement Learning","Fine-Grained Supervision","Bipartite Matching","Credit Assignment","Advantage Estimation"],"text":"링크: 논문 PDF로 바로 열기 저자: Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin 핵심 연구 목표 본 논문은 ToolIntegrated Reasoning (TIR) 에서 기존 강화 학습 방법론이 획일적인 보상 할당 으로 인해 비효율적인 도구 사용 최적화를 초래하는 문제를 해"},{"id":"2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding","title":"[논문리뷰] Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding","excerpt":"Mohammadreza Salehi이 arXiv에 게시한 'Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding","tags":["Review","Vision-Language Models","Video Understanding","Grounding","Open Weights","Open Data","Multimodal AI","Object Tracking","Dense Captioning"],"text":"링크: 논문 PDF로 바로 열기 저자: Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, "},{"id":"2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs","title":"[논문리뷰] Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs","excerpt":"arXiv에 게시된 'Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs","tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Exploration Collapse","Strategy-level Diversity","Uniqueness-Aware Rewarding","Creative Problem Solving","Pass@k"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Hu, Yucheng Wang, Yufei He, Jiaying Wu, Yilun Zhao, Bryan Hooi, SeeKiong Ng, Cynthia Breazeal, Anh Tuan Luu, Hae Won Park 핵심 연구 목표 LLM의 RL 기반 학습에서 발생하는 탐색 붕괴(exploration "},{"id":"2026-01-16-STEP3-VL-10B-Technical-Report","title":"[논문리뷰] STEP3-VL-10B Technical Report","excerpt":"arXiv에 게시된 'STEP3-VL-10B Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-STEP3-VL-10B-Technical-Report","tags":["Review","Multimodal Large Language Models","Vision-Language Models","Reinforcement Learning","Parallel Coordinated Reasoning","Model Efficiency","Foundation Models","Pre-training","Post-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Ailin Huang, Chengyuan Yao, Chunrui Han, Fanqi Wan, Hangyu Guo, Haoran Lv, Hongyu Zhou, Jia Wang, Jian Zhou, Jianjian Sun, Jingcheng Hu, Kangheng Lin, Liang Zhao, Mitt Huang, Son"},{"id":"2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders","title":"[논문리뷰] Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders","excerpt":"arXiv에 게시된 'Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders","tags":["Review","Text-to-Image","Diffusion Models","LLM Encoders","Reasoning-Aware AI","Reinforcement Learning","Dual-GRPO","Prompt Rewriting"],"text":"링크: 논문 PDF로 바로 열기 저자: Siqi Kou, Jiachun Jin, Zetong Zhou, Ye Ma, Yugang Wang, Quan Chen, Peng Jiang, Xiao Yang, Jun Zhu, Kai Yu, Zhijie Deng 핵심 연구 목표 본 논문은 기존 텍스트이미지(T2I) 확산 모델들이 대규모 언어 모델(LLM) 기반 텍스트"},{"id":"2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback","title":"[논문리뷰] ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback","excerpt":"Shikun Zhang이 arXiv에 게시한 'ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback","tags":["Review","LLM Agents","Tool Use Safety","Guardrail","Step-level Safety Detection","Prompt Injection","Reinforcement Learning","Feedback Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Yutao Mou, Zhangchi Xue, Lijun Li, Peiyang Liu, Shikun Zhang, Wei Ye, Jing Shao, MurrayTom 핵심 연구 목표 본 논문은 LLM 기반 에이전트의 도구 호출 기능에서 발생하는 보안 위험을 해결하는 것을 목표로 합니다. 특히, 에이전트가 잠재적으로 안전하"},{"id":"2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering","title":"[논문리뷰] Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering","excerpt":"arXiv에 게시된 'Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering","tags":["Review","Agentic AI","Long-Horizon Autonomy","Cognitive Accumulation","Hierarchical Cognitive Caching (HCC)","Context Management","Machine Learning Engineering (MLE)","LLM Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, WeiChen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen 핵심 연구 목표 본 논문"},{"id":"2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","title":"[논문리뷰] Transition Matching Distillation for Fast Video Generation","excerpt":"arXiv에 게시된 'Transition Matching Distillation for Fast Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","tags":["Review","Video Generation","Diffusion Models","Model Distillation","Few-Step Sampling","Transition Matching","Flow Matching","DMD2","Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Weili Nie, Julius Berner, Nanye Ma, Chao Liu, Saining Xie, Arash Vahdat 핵심 연구 목표 대규모 비디오 Diffusion 모델이 고품질 비디오를 생성하지만, 다단계 샘플링 과정의 비효율성으로 인해 실시간 상호작용 애플리케이션에 적용하기 어렵다는 문제를 해결하고자 "},{"id":"2026-01-16-Urban-Socio-Semantic-Segmentation-with-Vision-Language-Reasoning","title":"[논문리뷰] Urban Socio-Semantic Segmentation with Vision-Language Reasoning","excerpt":"arXiv에 게시된 'Urban Socio-Semantic Segmentation with Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-Urban-Socio-Semantic-Segmentation-with-Vision-Language-Reasoning","tags":["Review","Urban Segmentation","Socio-Semantic","Vision-Language Models (VLMs)","Reinforcement Learning","Geospatial Data","Multi-modal Reasoning","SAM"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li 핵심 연구 목표 본 논문은 위성 이미지에서 건물이나 수역과 같은 물리적 속성이 아닌, 학교나 공원과 같은 사회적으로 정의된 도시의 의미론적 개체 를 정확하게 분할하는 새로운 도전"},{"id":"2026-01-16-VIBE-Visual-Instruction-Based-Editor","title":"[논문리뷰] VIBE: Visual Instruction Based Editor","excerpt":"Bulat Suleimanov이 arXiv에 게시한 'VIBE: Visual Instruction Based Editor' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-VIBE-Visual-Instruction-Based-Editor","tags":["Review","Instruction-Based Image Editing","Diffusion Models","Vision-Language Models (VLM)","Model Efficiency","Multi-stage Training","Preference Alignment","Source Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Grigorii Alekseenko, Aleksandr Gordeev, Irina Tolstykh, Bulat Suleimanov, Vladimir Dokholyan, Georgii Fedorov, Sergey Yakubson, Aleksandra Tsybina, Mikhail Chernyavsky, Maksim Ku"},{"id":"2026-01-16-VQ-Seg-Vector-Quantized-Token-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation","title":"[논문리뷰] VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation","excerpt":"Lei Zhu이 arXiv에 게시한 'VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-16 00:00:00+0900+0900","permalink":"/ai/review/2026-01-16-VQ-Seg-Vector-Quantized-Token-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation","tags":["Review","Semi-supervised Learning","Medical Image Segmentation","Vector Quantization","Consistency Learning","Feature Perturbation","Foundation Models","Dropout Replacement"],"text":"링크: 논문 PDF로 바로 열기 저자: Sicheng Yang, Zhaohu Xing, Lei Zhu 핵심 연구 목표 본 논문은 반지도 학습 기반 의료 영상 분할에서 기존 dropout 방식의 불안정하고 튜닝이 어려운 특성 교란 문제를 해결하고자 합니다. 특히, dropout rate( DR )에 따라 성능이 크게 저하되거나 불충분한 정규화가 발생하는 한계"},{"id":"2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems","title":"[논문리뷰] AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems","excerpt":"Xipeng Qiu이 arXiv에 게시한 'AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-19 00:00:00+0900+0900","permalink":"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems","tags":["Review","LLM Agents","Space Planning","Benchmark","Agentic Planning","Physics Constraints","Decision Making","Zero-Shot Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Weiyi Wang, Xinchi Chen, Jingjing Gong, Xuanjing Huang, Xipeng Qiu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트가 물리적으로 제한된 실제 환경, 특히 다양한 목표와 엄격한 제약을 가진 우주 계획 문제(SPP) 에서 얼마나 효과적으로 계획하고 행"},{"id":"2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models","title":"[논문리뷰] Language of Thought Shapes Output Diversity in Large Language Models","excerpt":"arXiv에 게시된 'Language of Thought Shapes Output Diversity in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-19 00:00:00+0900+0900","permalink":"/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models","tags":["Review","Large Language Models","Output Diversity","Multilingual Reasoning","Language of Thought","Sampling Strategies","Pluralistic Alignment","Hidden State Analysis","Cognitive Science"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaoyang Xu, Wenxuan Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 겪는 출력 다양성 부족(예: 모드 붕괴, 특정 문화 가치 과대 대표) 문제를 해결하고자 합니다. 특히, 모델의 중간 추론 과정에 사용되는 '사고의 언어(language of thought)'를 제어하는 것이 출력 다"},{"id":"2026-01-19-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes","title":"[논문리뷰] More Images, More Problems? A Controlled Analysis of VLM Failure Modes","excerpt":"arXiv에 게시된 'More Images, More Problems? A Controlled Analysis of VLM Failure Modes' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-19 00:00:00+0900+0900","permalink":"/ai/review/2026-01-19-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes","tags":["Review","Vision Language Models","Multi-Image Understanding","Failure Analysis","Evaluation Benchmark","Attention Mechanism","Fine-tuning","MIMIC"],"text":"링크: 논문 PDF로 바로 열기 저자: Anurag Das, Adrian Bulat, Alberto Baldrati, Ioannis Maniadis Metaxas, Bernt Schiele, Georgios Tzimiropoulos, Brais Martinez 핵심 연구 목표 본 논문은 최신 대규모 시각 언어 모델(LVLM) 이 다중 이미지 환경에서 보여주"},{"id":"2026-01-19-Reasoning-Models-Generate-Societies-of-Thought","title":"[논문리뷰] Reasoning Models Generate Societies of Thought","excerpt":"James Evans이 arXiv에 게시한 'Reasoning Models Generate Societies of Thought' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-19 00:00:00+0900+0900","permalink":"/ai/review/2026-01-19-Reasoning-Models-Generate-Societies-of-Thought","tags":["Review","Reasoning Models","Large Language Models (LLMs)","Multi-Agent Systems","Society of Thought","Mechanistic Interpretability","Reinforcement Learning","Cognitive Diversity","Conversational AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Agüera y Arcas, James Evans 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 정교한 추론 능력 이면에 있는 메커니즘을 규명하고, 이러한 능력이 단순히 계산량 증가가 아닌, 복잡한 다중 에이전트 상호작용 인 "},{"id":"2026-01-19-When-Personalization-Misleads-Understanding-and-Mitigating-Hallucinations-in-Personalized-LLMs","title":"[논문리뷰] When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs","excerpt":"arXiv에 게시된 'When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-19 00:00:00+0900+0900","permalink":"/ai/review/2026-01-19-When-Personalization-Misleads-Understanding-and-Mitigating-Hallucinations-in-Personalized-LLMs","tags":["Review","Personalized LLMs","Hallucination Mitigation","Factual Reasoning","Representation Entanglement","Inference-time Steering","Question Answering","Factuality Preservation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongxiang Sun, Yi Zhan, Chenglei Shen, Weijie Yu, Xiao Zhang, Ming He, Jun Xu 핵심 연구 목표 개인화된 대규모 언어 모델(LLM)이 사용자 만족도를 높이는 동시에 사실적 추론을 왜곡 하여 개인화 유도 환각(personalizationinduced hallu"},{"id":"2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development","title":"[논문리뷰] ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development","excerpt":"arXiv에 게시된 'ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development","tags":["Review","Backend Development","LLM Agents","Code Generation","Benchmarking","DevOps","Containerization","End-to-End Testing","Environment Configuration"],"text":"링크: 논문 PDF로 바로 열기 저자: Jie Yang, Honglin Guo, Li Ji, Jiazheng Zhou, Rui Zheng, Zhikai Lei, Shuo Zhang, Zhiheng Xi, Shichun Liu, Yuxin Wang, Bo Wang, Yining Zheng, Tao Gui, Xipeng Qiu 핵심 연구 목표 기존 코드 생성 "},{"id":"2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion","title":"[논문리뷰] CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion","excerpt":"arXiv에 게시된 'CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion","tags":["Review","Continual Learning","Vision-Language-Action Models","Adapter Learning","Catastrophic Forgetting","Autonomous Routing","Parameter-Efficient Learning","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Ralf Römer, Yi Zhang, Angela P. Schoellig 핵심 연구 목표 본 논문은 로봇이 실제 환경에서 새로운 작업을 지속적으로 학습하면서도 기존 지식을 잊지 않는 catastrophic forgetting 문제 를 해결하고, 과거 데이터 저장 및 작업 식별자 없이 작동하는 exemplarfree "},{"id":"2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","title":"[논문리뷰] CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation","excerpt":"Hengshuang이 arXiv에 게시한 'CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","tags":["Review","Multi-subject Animation","Pose-driven Animation","Diffusion Models","Spatial Misalignment","Unbind-Rebind Paradigm","Character Animation","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuai Tan, Biao Gong, Ke Ma, Yutong Feng, Qiyuan Zhang, Yan Wang, Yujun Shen, Hengshuang Zhao 핵심 연구 목표 본 논문은 기존 단일 인물 애니메이션 방법론이 다중 인물, 다양한 캐릭터 유형, 그리고 레퍼런스 이미지와 드라이빙 포즈 간의 공간적 불"},{"id":"2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation","title":"[논문리뷰] Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation","excerpt":"Ziyang Yan이 arXiv에 게시한 'Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation","tags":["Review","Medical Image Segmentation","Foundation Models","SAM3","Fine-tuning","Prompt-driven","Domain Adaptation","Text-guided Segmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Chongcong Jiang, Tianxingjian Ding, Chuhan Song, Jiachen Tu, Ziyang Yan, Yihua Shao, Zhenyi Wang, Yuzhang Shang, Tianyu Han, Yu Tian 핵심 연구 목표 본 논문은 일반 자연 이미지에 대해 강력한 성능을 보인 SAM3 "},{"id":"2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge","title":"[논문리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge","excerpt":"arXiv에 게시된 'Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge","tags":["Review","Large Language Models","Reasoning","Chain-of-Thought","Reinforcement Learning","Stochastic Reasoning","Continuous Representation","Token Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Yao Tang, Li Dong, Yaru Hao, Qingxiu Dong, Furu Wei, Jiatao Gu 핵심 연구 목표 대규모 언어 모델(LLM)의 ChainofThought (CoT) 추론이 길고 저대역폭의 이산 토큰 시퀀스를 생성하는 문제점을 해결하고, 인간처럼 여러 가능한 다음 단계에 대한 분포를 유지하"},{"id":"2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature","title":"[논문리뷰] SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature","excerpt":"arXiv에 게시된 'SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature","tags":["Review","Long-Context Understanding","Multimodal AI","Scientific Literature","Evidence-based Reasoning","MLLM Evaluation","Benchmarking","Cross-modal Reasoning","Information Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiming Ren, Junjie Wang, Yuxin Meng, Yihang Shi, Zhiqiang Lin, Ruihang Chu, Yiran Xu, Ziming Li, Yunfei Zhao, Zihan Wang, Yu Qiao, Ruiming Tang, Minghao Liu, Yujiu Yang 핵심 연구 목표 "},{"id":"2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs","title":"[논문리뷰] Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs","excerpt":"Lecheng Yan이 arXiv에 게시한 'Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs","tags":["Review","RLVR","LLMs","Mechanistic Interpretability","Memorization Shortcuts","Data Contamination","Anchor-Adapter Circuit","Path Patching","Logit Lens"],"text":"링크: 논문 PDF로 바로 열기 저자: Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, Wenxi Li, Vincent Wang, Chris Lee 핵심 연구 목표 본 논문은 RLVR(Reinforcement Learning with Verifiable Rewards) 로 튜닝된 LLM 이 때로는 "},{"id":"2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models","title":"[논문리뷰] The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models","excerpt":"Jack Lindsey이 arXiv에 게시한 'The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models","tags":["Review","Language Models","Persona Control","Activation Steering","Persona Drift","Alignment","Post-training","Interpretability","Safety"],"text":"링크: 논문 PDF로 바로 열기 저자: Christina Lu, Jack Gallagher, Jonathan Michala, Kyle Fish, Jack Lindsey 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 기본적으로 가지는 'AI Assistant' 페르소나의 구조를 심층적으로 탐구하고, 이 페르소나가 특정 상황에서 벗어나 부적절하거나 유"},{"id":"2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation","title":"[논문리뷰] YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation","excerpt":"arXiv에 게시된 'YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-20 00:00:00+0900+0900","permalink":"/ai/review/2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation","tags":["Review","Large Language Models (LLMs)","Activation Steering","Sparse Autoencoders (SAEs)","Domain Adaptation","Cultural Alignment","Preference Optimization","Disentangled Representations","Fine-grained Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Hadi Abdine, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang 핵심 연구 목표 본 논문은 LLM의 행동을 미세하게 제어하는 데 있어 기존의 Dense Steering Vect"},{"id":"2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification","title":"[논문리뷰] A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification","excerpt":"arXiv에 게시된 'A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification","tags":["Review","LLM Orchestration","Lightweight Probes","Token-Layer Aggregation","Hidden States","Single-Pass Classification","Safety Moderation","Sentiment Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Gonzalo Ariel Meyoyan, Luciano Del Corro 핵심 연구 목표 본 논문은 프로덕션 LLM 시스템에서 안전성 검토 및 기타 분류 태스크를 위해 별도의 모델을 사용하는 방식이 야기하는 추론 지연 시간, VRAM 사용량, 운영 복잡성 증가 문제를 해결하고자 합니다. 이를 위해, 이미 계산된 서빙 "},{"id":"2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus","title":"[논문리뷰] A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus","excerpt":"Özay Ezerceli이 arXiv에 게시한 'A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus","tags":["Review","Low-Resource NLP","Semantic Relations","Dataset Generation","Turkish Language","LLM","FastText Embeddings","Agglomerative Clustering","Synonyms","Antonyms","Co-hyponyms"],"text":"링크: 논문 PDF로 바로 열기 저자: Ebubekir Tosun, Özay Ezerceli, Mehmet Emin Buldur, Mahmoud ElHussieni 핵심 연구 목표 본 논문은 저자원 언어 (특히 튀르키예어)에서 대규모 의미 관계 데이터셋을 효율적으로 생성하는 하이브리드 프로토콜 을 제시하고, 포괄적인 튀르키예어 의미 관계 코퍼스 를 구축하는"},{"id":"2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey","title":"[논문리뷰] Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey","excerpt":"arXiv에 게시된 'Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey","tags":["Review","LLM-based Issue Resolution","Software Engineering","Autonomous Agents","Code Generation","Benchmarking","Reinforcement Learning","Supervised Fine-tuning","Multimodal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Caihua Li, Lianghong Guo, Yanlin Wang, et al. 핵심 연구 목표 본 논문은 LLM 기반의 소프트웨어 엔지니어링 이슈 해결(Issue Resolution) 분야에 대한 최초의 체계적인 종합 조사를 제공하는 것을 목표로 합니다. 특히 SWEbench 와 같은 벤치마크에 의해 촉진된 자율 "},{"id":"2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search","title":"[논문리뷰] Agentic-R: Learning to Retrieve for Agentic Search","excerpt":"Daiting Shi이 arXiv에 게시한 'Agentic-R: Learning to Retrieve for Agentic Search' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search","tags":["Review","Agentic Search","Retrieval-Augmented Generation","Retriever Training","Passage Utility Modeling","Iterative Optimization","Reinforcement Learning","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenhan Liu, Xinyu Ma, Yutao Zhu, Yuchen Li, Daiting Shi, Dawei Yin, Zhicheng Dou 핵심 연구 목표 본 논문은 멀티턴 에이전트 검색(agentic search)의 맥락에서 리트리버(retriever) 훈련의 한계를 극복하는 것을 목표로 합니다. 기존 RAG("},{"id":"2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning","title":"[논문리뷰] Aligning Agentic World Models via Knowledgeable Experience Learning","excerpt":"arXiv에 게시된 'Aligning Agentic World Models via Knowledgeable Experience Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning","tags":["Review","Agentic AI","World Models","Experience Learning","LLMs","Physical Hallucinations","Embodied AI","Predictive Coding","Knowledge Repository"],"text":"링크: 논문 PDF로 바로 열기 저자: Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang, Huajun Chen 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs) 기반 에이전트 월드 모델이 겪는 \"물리적 환각(physical hallucinations)\" 문제를 해결하고자 합니다. 이는 LL"},{"id":"2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization","title":"[논문리뷰] Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization","excerpt":"arXiv에 게시된 'Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization","tags":["Review","Robot Learning","Cross-Embodiment Generalization","Vision-Language-Action Models","Human-Centric Learning","Unified Action Space","Mixture-of-Flow","Real-Time Deployment","Large-Scale Datasets"],"text":"링크: 논문 PDF로 바로 열기 저자: BeingBeyond Team 핵심 연구 목표 논문은 다양한 로봇 플랫폼에 걸쳐 견고한 CrossEmbodiment Generalization 을 달성하기 위한 HumanCentric Robot Learning 패러다임을 제안합니다. 기존 VLA(VisionLanguageAction) 모델이 겪는 형태학적 이질성과 데"},{"id":"2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","title":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation","excerpt":"arXiv에 게시된 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","tags":["Review","Vision-Language Navigation","Chain-of-Thought Reasoning","Multimodal AI","Implicit Reasoning","Visual AutoRegressor","Embodied AI","Long-Horizon Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu, Yonggang Qi 핵심 연구 목표 VisionandLanguage Navigation (VLN)에서 기존 ChainofThought (CoT) 추론 방식의 한계, 즉 공간 접지 부족, 희소한 주석에 대한 과적합, 또"},{"id":"2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD","title":"[논문리뷰] Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD","excerpt":"arXiv에 게시된 'Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD","tags":["Review","Differential Privacy (DP)","DP-SGD","f-differential privacy","Privacy-Utility Trade-off","Shuffled Sampling","Poisson Subsampling","Gaussian Noise","Worst-Case Adversary"],"text":"링크: 논문 PDF로 바로 열기 저자: Murat Bilgehan Ertan, Marten van Dijk 핵심 연구 목표 이 논문은 차등 프라이버시(DP)를 적용한 확률적 경사하강법(DPSGD)의 근본적인 한계를 f차등 프라이버시(fDP) 프레임워크 하에서 분석하는 것을 목표로 합니다. 특히, 최악의 경우를 가정한 공격자 모델에서 랜덤 셔플링(shuffl"},{"id":"2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs","title":"[논문리뷰] FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs","excerpt":"arXiv에 게시된 'FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs","tags":["Review","Multimodal LLMs","Future Forecasting","Audio-Visual Reasoning","Benchmark","Instruction Tuning","Omni-Modal","Causal Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Qian Chen, Jinlan Fu, Changsong Li, SeeKiong Ng, Xipeng Qiu 핵심 연구 목표 기존 벤치마크들이 주로 회고적 이해에 초점을 맞추는 한계를 해결하기 위해, 오디오비주얼 환경에서 멀티모달 대규모 언어 모델(MLLM)의 미래 사건 예측 능력 을 평가하는 것을 목표로 합니다. 특히"},{"id":"2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning","title":"[논문리뷰] KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning","excerpt":"Aleksandr I. Panov이 arXiv에 게시한 'KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Visual Generalization","Distribution Shift","Benchmarking","JAX","Controlled Environments","PPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Egor Cherepannov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov 핵심 연구 목표 픽셀 기반 강화 학습(RL) 에이전트가 잠재된 역학 및 보상이 고정되어 있음에도 불구하고 순수한 시각적 분포 변화에 취약한 문제를 해결하는 것을 목표로 합니다. 기존 벤"},{"id":"2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals","title":"[논문리뷰] LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals","excerpt":"arXiv에 게시된 'LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals","tags":["Review","LLM Explainability","Causal Inference","Structural Counterfactuals","Concept-Based Explanations","Evaluation Benchmark","Faithfulness","SCM"],"text":"링크: 논문 PDF로 바로 열기 저자: Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart 핵심 연구 목표 본 논문은 LLM의 불투명한 의사결정 과정으로 인해 고위험 도메인에서의 적용이 어려운 문제를 해결하고자 합니다. 특히, 의 를 평가하기 위한 기존 벤치마크가 에 의존하여 실제 인과 메커니즘을 반영하지 못하는"},{"id":"2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR","title":"[논문리뷰] LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR","excerpt":"arXiv에 게시된 'LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR","tags":["Review","OCR","Vision-Language Model","End-to-End Learning","Multilingual","Reinforcement Learning","Document Understanding","Bounding Box Prediction","Task Arithmetic Merging"],"text":"링크: 논문 PDF로 바로 열기 저자: Said Taghadouini, Adrien Cavaillès, Baptiste Aubertin 핵심 연구 목표 논문은 복잡한 다단계 OCR 파이프라인 없이 문서 이미지를 깨끗하고 자연스럽게 정렬된 텍스트로 변환하는 10억 개의 파라미터를 가진 종단 간 다국어 비전언어 모델 LightOnOCR21B 를 제안합니다. 이"},{"id":"2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models","title":"[논문리뷰] MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models","excerpt":"arXiv에 게시된 'MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models","tags":["Review","Reward Models","LLM Memory Management","Benchmarking","Long Context","Evaluation Metrics","Generative RMs","Memory Management Patterns"],"text":"링크: 논문 PDF로 바로 열기 저자: Zecheng Tang, Baibei Ji, Ruoxi Sun, Haitian Wang, Wangjie You, Yijun Zhang, Wenpeng Zhu, Ji Qi, Juntao Li, Min Zhang 핵심 연구 목표 본 연구는 LLM의 장기 기억 관리 능력 을 평가하기 위한 Reward Model (RM)의 "},{"id":"2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","title":"[논문리뷰] OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer","excerpt":"arXiv에 게시된 'OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","tags":["Review","Video Transfer","Diffusion Models","Spatio-temporal Learning","Multimodal Alignment","Appearance Consistency","Temporal Control","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengze Zhang, Yanze Wu, Mengtian Li, Xu Bai, Songtao Zhao, Fulong Ye, Chong Mou, Xinghui Li, Zhuowei Chen, Qian He, Mingyuan Gao 핵심 연구 목표 기존 비디오 커스터마이징 방법론들이 레퍼런스 비디오의 풍부한 시공간 정보"},{"id":"2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing","title":"[논문리뷰] On the Evidentiary Limits of Membership Inference for Copyright Auditing","excerpt":"Marten van Dijk이 arXiv에 게시한 'On the Evidentiary Limits of Membership Inference for Copyright Auditing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing","tags":["Review","Membership Inference Attacks","Copyright Auditing","Large Language Models","Adversarial Robustness","Paraphrasing","Sparse Autoencoders","Semantic Preservation","LLM Security"],"text":"링크: 논문 PDF로 바로 열기 저자: Murat Bilgehan Ertan, Emirhan Böge, Min Chen, Kaleel Mahmood, Marten van Dijk 핵심 연구 목표 본 논문은 LLM(Large Language Model) 학습 데이터의 저작권 감사에서 MIA(Membership Inference Attack) 가 신뢰할 수 있"},{"id":"2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models","title":"[논문리뷰] PRiSM: Benchmarking Phone Realization in Speech Models","excerpt":"arXiv에 게시된 'PRiSM: Benchmarking Phone Realization in Speech Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models","tags":["Review","Phone Recognition","Speech Models","Benchmarking","Phonetic Analysis","Cross-lingual Speech","LALMs","Intrinsic Evaluation","Extrinsic Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shikhar Bharadwaj, ChinJou Li, Yoonjae Kim, Kwanghee Choi, Eunjung Yeo, Ryan SohEun Shim, Hanyu Zhou, Brendon Boldt, Karen Rosero Jacome, Kalvin Chang, Darsh Agrawal, Keer Xu, Ch"},{"id":"2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment","title":"[논문리뷰] SciCoQA: Quality Assurance for Scientific Paper--Code Alignment","excerpt":"arXiv에 게시된 'SciCoQA: Quality Assurance for Scientific Paper--Code Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment","tags":["Review","Reproducibility","Paper-Code Discrepancy","Code Alignment","LLM Evaluation","Synthetic Data Generation","Quality Assurance","Scientific Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: Tim Baumgärtner, Iryna Gurevych 핵심 연구 목표 이 논문은 AI 및 과학 분야의 \"재현성 위기\"에 대응하여, 과학 논문과 그 코드 구현 간의 불일치(discrepancy) 를 자동으로 감지하는 시스템의 필요성을 다룹니다. 논문은 코드베이스가 보고된 과학적 방법론을 충실하게 재현하는지 확인하기 "},{"id":"2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning","title":"[논문리뷰] Think3D: Thinking with Space for Spatial Reasoning","excerpt":"Yuhan Wu이 arXiv에 게시한 'Think3D: Thinking with Space for Spatial Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning","tags":["Review","Spatial Reasoning","3D Reconstruction","VLM Agents","Tool Calling","Reinforcement Learning","Novel View Synthesis","Iterative Exploration"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhan Wu, JeremyYin, sunz525, luciasnowblack, MrBean2024 핵심 연구 목표 기존 VisionLanguage Models (VLMs) 이 2D 인식을 넘어선 진정한 3D 공간 추론 능력 과 일관된 공간 표현을 구축하는 데 한계가 있음을 해결하고자 합니다. 본 연구는 VLM 에이"},{"id":"2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents","title":"[논문리뷰] ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents","excerpt":"arXiv에 게시된 'ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents","tags":["Review","Process Reward Models","Tool-using Agents","Benchmark","Reinforcement Learning","Large Language Models","Reward-guided Search","Agent Evaluation","Step-level Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo 핵심 연구 목표 본 논문은 도구 사용 에이전트의 평가를 위한 체계적이고 신뢰할 수 있는 벤치마크의 부재를 해결하고자 합니다. 기존 PRM 벤치마크들이 도구 사용 시나리오의 길고 복잡한 상호작용에 적합하지 않다는 문제를 "},{"id":"2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning","title":"[논문리뷰] Toward Efficient Agents: Memory, Tool learning, and Planning","excerpt":"arXiv에 게시된 'Toward Efficient Agents: Memory, Tool learning, and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning","tags":["Review","LLM Agents","Agent Efficiency","Memory Management","Tool Learning","AI Planning","Resource Optimization","Cost-Performance Trade-off"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaofang Yang, Lijun Li, Heng Zhou, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Qi Li, Ning Ding, Siheng Chen, Jing Shao 핵심 연구 목표 "},{"id":"2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation","title":"[논문리뷰] UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation","excerpt":"arXiv에 게시된 'UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-21 00:00:00+0900+0900","permalink":"/ai/review/2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation","tags":["Review","Chest X-Ray","Medical Foundation Model","Autoregressive Model","Diffusion Model","Multimodal Learning","Image Understanding","Image Generation","Cross-Modal Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruiheng Zhang, Jingfeng Yao, Huangxuan Zhao, Hao Yan, Xiao He, Lei Chen, Zhou Wei, Yong Luo, Zengmao Wang, Lefei Zhang, Dacheng Tao, Bo Du 핵심 연구 목표 의료 영상 이해(semantic abstraction)"},{"id":"2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization","title":"[논문리뷰] AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization","excerpt":"arXiv에 게시된 'AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization","tags":["Review","Clinical Decision-Making","LLM Agents","EHR","Retrospective Summarization","Long-Context Reasoning","Experience Replay","Healthcare AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yusheng Liao, Chuan Xuan, Yutong Cai, Lina Yang, Zhe Chen, Yanfeng Wang, Yu Wang 핵심 연구 목표 본 논문은 LLM의 자율적인 EHR(전자건강기록) 탐색 및 임상 의사 결정 능력이 현재까지 이상화된 실험 설정에 의해 제한되어 있음을 지적합니다. 특히, 기존"},{"id":"2026-01-22-Agentic-Reasoning-for-Large-Language-Models","title":"[논문리뷰] Agentic Reasoning for Large Language Models","excerpt":"arXiv에 게시된 'Agentic Reasoning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Agentic-Reasoning-for-Large-Language-Models","tags":["Review","Agentic Reasoning","LLM Agents","Self-Evolving AI","Multi-Agent Systems","Planning","Tool Use","Retrieval-Augmented Generation","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianxin Wei, TingWei Li, Zhining Liu, et al. 핵심 연구 목표 본 설문조사 논문은 대규모 언어 모델(LLM)의 추론 능력이 정적인 폐쇄형 환경에서 벗어나 동적이고 개방형 환경에서 계획, 행동, 학습을 통해 지속적으로 상호작용하는 자율 에이전트 로 발전하는 Agentic Reasonin"},{"id":"2026-01-22-FARE-Fast-Slow-Agentic-Robotic-Exploration","title":"[논문리뷰] FARE: Fast-Slow Agentic Robotic Exploration","excerpt":"Jingsong Liang이 arXiv에 게시한 'FARE: Fast-Slow Agentic Robotic Exploration' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-FARE-Fast-Slow-Agentic-Robotic-Exploration","tags":["Review","Robotic Exploration","LLM","Reinforcement Learning","Fast-Slow Thinking","Hierarchical Planning","Agentic AI","Graph Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuhao Liao, Xuxin Lv, Jeric Lew, Shizhe Zhang, Jingsong Liang 핵심 연구 목표 본 연구는 자율 로봇 탐사에서 기존 방법론이 장기 정보 활용 및 환경 변화 적응에 어려움을 겪는 문제를 해결하고자 합니다. 특히, 에이전트 수준의 의미론적 추론과 빠른 로컬 제어를 통합하여,"},{"id":"2026-01-22-Facilitating-Proactive-and-Reactive-Guidance-for-Decision-Making-on-the-Web-A-Design-Probe-with-WebSeek","title":"[논문리뷰] Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek","excerpt":"Arpit Narechania이 arXiv에 게시한 'Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Facilitating-Proactive-and-Reactive-Guidance-for-Decision-Making-on-the-Web-A-Design-Probe-with-WebSeek","tags":["Review","Mixed-Initiative AI","Human-AI Collaboration","Web Data Analysis","Proactive Guidance","Large Language Models (LLMs)","Browser Extension","Data-Centric Design"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanwei Huang, Arpit Narechania 핵심 연구 목표 컴퓨터 비전 태스크에서 CNN의 의존성을 완전히 제거 하고, 순수한 Transformer 아키텍처 만으로 이미지 분류 성능을 달성하는 것을 목표로 합니다. 기존 CNN 기반 접근법의 한계를 극복하고 selfattention 메커니즘 이 이미지 패치"},{"id":"2026-01-22-FinVault-Benchmarking-Financial-Agent-Safety-in-Execution-Grounded-Environments","title":"[논문리뷰] FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments","excerpt":"arXiv에 게시된 'FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-FinVault-Benchmarking-Financial-Agent-Safety-in-Execution-Grounded-Environments","tags":["Review","Financial AI Agents","Security Benchmark","Execution-Grounded","LLM Safety","Prompt Injection","Jailbreaking","Compliance","Vulnerability Assessment"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhi Yang, Runguo Li, Qiqi Qiang, Jiashun Wang, Fangqi Lou, Mengping Li, Dongpo Cheng, Rui Xu, Heng Lian, Shuo Zhang, Xiaolong Liang, Xiaoming Huang, Zheng Wei, Zhaowei Liu, Xin G"},{"id":"2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models","title":"[논문리뷰] Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models","excerpt":"arXiv에 게시된 'Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models","tags":["Review","Prompt Engineering","Large Language Models","Causal Attention","Multiple-Choice QA","Prompt Order Sensitivity","Information Bottleneck","Decoder-only Transformers"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyunjong Ok, Jaeho Lee 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 프롬프트 구조에 민감하게 반응하는 이유를 밝히고, 특히 다중 선택 질의응답(MCQA) 태스크에서 컨텍스트의 순서가 성능에 미치는 영향을 분석하는 것을 목표로 합니다. 컨텍스트가 질문과 옵션 뒤에 오는 QOC(Question"},{"id":"2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents","title":"[논문리뷰] MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents","excerpt":"Samiul Alam이 arXiv에 게시한 'MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents","tags":["Review","Multimodal Deep Research","Research Agents","Benchmark","Evaluation Framework","Retrieval-Augmented Generation","Large Multimodal Models","Visual Grounding","Citation Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Samiul Alam, Zhongwei Wan, Zixuan Zhong, Peizhou Huang, Donghao Zhou 핵심 연구 목표 본 논문은 기존 연구 에이전트 벤치마크들이 텍스트 전용 또는 짧은 형태의 멀티모달 질의응답에 초점을 맞춰, 멀티모달 증거를 활용한 종단 간 보고서 생성 능력을 평가하는 데 한계가 "},{"id":"2026-01-22-Numina-Lean-Agent-An-Open-and-General-Agentic-Reasoning-System-for-Formal-Mathematics","title":"[논문리뷰] Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics","excerpt":"arXiv에 게시된 'Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Numina-Lean-Agent-An-Open-and-General-Agentic-Reasoning-System-for-Formal-Mathematics","tags":["Review","Agentic Systems","Formal Theorem Proving","Large Language Models (LLMs)","Lean Theorem Prover","Multi-Agent Systems","Code Generation","Automated Reasoning","Human-AI Collaboration"],"text":"링크: 논문 PDF로 바로 열기 저자: Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li 핵심 연구 목표 기존 에이전트 기반 형식 "},{"id":"2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance","title":"[논문리뷰] Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance","excerpt":"arXiv에 게시된 'Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance","tags":["Review","Multi-Agent Framework","LLM Agents","Peer Review","Rebuttal Generation","Evidence-centric Planning","Transparency","Human-in-the-loop"],"text":"링크: 논문 PDF로 바로 열기 저자: Qianli Ma, Chang Guo, Zhiheng Tian, Siyu Wang, Jipeng Xiao, Yuanhao Yue, Zhipeng Zhang 핵심 연구 목표 AI/ML 논문 심사 과정에서 발생하는 저자 답변(rebuttal) 작성의 어려움을 해결하는 것을 목표로 합니다. 기존 LLM 기반 솔루션들이 겪는"},{"id":"2026-01-22-Quantifying-Speaker-Embedding-Phonological-Rule-Interactions-in-Accented-Speech-Synthesis","title":"[논문리뷰] Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis","excerpt":"Jihwan Lee이 arXiv에 게시한 'Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Quantifying-Speaker-Embedding-Phonological-Rule-Interactions-in-Accented-Speech-Synthesis","tags":["Review","Text-to-Speech","Accent Control","Phonological Rules","Speaker Embeddings","Speech Synthesis","Disentanglement","Accent Classification"],"text":"링크: 논문 PDF로 바로 열기 저자: Thanathai Lertpetchpun, Yoonjeong Lee, Thanapat Trachu, Jihwan Lee, Tiantian Feng, Dani Byrd, Shrikanth Narayanan 핵심 연구 목표 현재 TTS 시스템에서 스피커 임베딩이 액센트 외의 음색이나 감정과 같은 특성까지 인코딩하여 액센트"},{"id":"2026-01-22-Render-of-Thought-Rendering-Textual-Chain-of-Thought-as-Images-for-Visual-Latent-Reasoning","title":"[논문리뷰] Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning","excerpt":"arXiv에 게시된 'Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Render-of-Thought-Rendering-Textual-Chain-of-Thought-as-Images-for-Visual-Latent-Reasoning","tags":["Review","Chain-of-Thought (CoT)","Large Language Models (LLMs)","Vision Language Models (VLMs)","Latent Reasoning","Visual Modality","Image Rendering","Computational Efficiency","Knowledge Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang, Zheng Wei 핵심 연구 목표 본 논문은 ChainofThought (CoT) 프롬프팅의 지나친 장황함으로 인한 높은 연산 오버헤드 와 중간 추론 과정의 불투명성 문제를 해결하고자 합니다. 텍스트 CoT 단계"},{"id":"2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World","title":"[논문리뷰] Rethinking Video Generation Model for the Embodied World","excerpt":"arXiv에 게시된 'Rethinking Video Generation Model for the Embodied World' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World","tags":["Review","Video Generation","Embodied AI","Robotics Benchmark","RBench","Robotics Dataset","RoVid-X","Physical Plausibility","Task Completion"],"text":"링크: 논문 PDF로 바로 열기 저자: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou 핵심 연구 목표 본 연구는 로봇 상호작용을 정확하게 반영하는 고품질 비디오 생성의 어려움을 해결하고, 표준화된 벤치마크 부족으로"},{"id":"2026-01-22-RoboBrain-2-5-Depth-in-Sight-Time-in-Mind","title":"[논문리뷰] RoboBrain 2.5: Depth in Sight, Time in Mind","excerpt":"Yuheng Ji이 arXiv에 게시한 'RoboBrain 2.5: Depth in Sight, Time in Mind' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-RoboBrain-2-5-Depth-in-Sight-Time-in-Mind","tags":["Review","Embodied AI","Foundation Model","3D Spatial Reasoning","Temporal Value Estimation","Robotics","Manipulation","Multimodal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuheng Ji, Yijie Xu, Zhiyu Li, Huajie Tan, Zhoues 핵심 연구 목표 본 논문은 기존 embodied AI foundation model의 2D pixel 기반 grounding 및 sparse temporal supervision의 한계를 극복하고, 정확한 3D 공간 추론(Prec"},{"id":"2026-01-22-The-Responsibility-Vacuum-Organizational-Failure-in-Scaled-Agent-Systems","title":"[논문리뷰] The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems","excerpt":"Roman Bondar이 arXiv에 게시한 'The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-The-Responsibility-Vacuum-Organizational-Failure-in-Scaled-Agent-Systems","tags":["Review","Responsibility Vacuum","Scaled Agent Systems","Organizational Failure","CI/CD Pipelines","Human Verification Capacity","Authority-Capacity Mismatch","AI Governance","Ritualized Approval"],"text":"링크: 논문 PDF로 바로 열기 저자: Oleg Romanchuk, Roman Bondar 핵심 연구 목표 본 논문은 현대 AI 에이전트 기반 시스템에서 의사결정 처리량이 인간의 검증 역량을 초과할 때 발생하는 구조적인 책임 귀속 실패, 즉 책임 공백(Responsibility Vacuum) 현상을 정의하고 분석합니다. 의사결정에 대한 공식적 승인 권한과 "},{"id":"2026-01-22-Typhoon-ASR-Real-time-FastConformer-Transducer-for-Thai-Automatic-Speech-Recognition","title":"[논문리뷰] Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition","excerpt":"arXiv에 게시된 'Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Typhoon-ASR-Real-time-FastConformer-Transducer-for-Thai-Automatic-Speech-Recognition","tags":["Review","Thai ASR","Real-time Speech Recognition","FastConformer-Transducer","Low-latency","Text Normalization","Dialect Adaptation","Data Curation","Streaming ASR"],"text":"링크: 논문 PDF로 바로 열기 저자: Warit Sirichotedumrong, Adisai NaThalang, Potsawee Manakul, Pittawat Taveekitworachai, Sittipong Sripaisarnmongkol, Kunat Pipatanakul 핵심 연구 목표 본 논문은 높은 지연 시간 때문에 스트리밍 애플리케이션에 비실용"},{"id":"2026-01-22-Typhoon-OCR-Open-Vision-Language-Model-For-Thai-Document-Extraction","title":"[논문리뷰] Typhoon OCR: Open Vision-Language Model For Thai Document Extraction","excerpt":"arXiv에 게시된 'Typhoon OCR: Open Vision-Language Model For Thai Document Extraction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-Typhoon-OCR-Open-Vision-Language-Model-For-Thai-Document-Extraction","tags":["Review","Vision-Language Model","OCR","Thai Language Processing","Document Understanding","Low-Resource Language","Data Synthesis","Fine-tuning","Layout Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Surapon Nonesung, Natapong Nitarach, Teetouch Jaknamon, Pittawat Taveekitworachai, Kunat Pipatanakul 핵심 연구 목표 기존 VLM이 태국어와 같은 저자원 언어의 복잡한 스크립트 특성(비라틴 문자, 명시적 단어 경계 부재, 스택형 발음 구별 "},{"id":"2026-01-22-XR-Cross-Modal-Agents-for-Composed-Image-Retrieval","title":"[논문리뷰] XR: Cross-Modal Agents for Composed Image Retrieval","excerpt":"arXiv에 게시된 'XR: Cross-Modal Agents for Composed Image Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-XR-Cross-Modal-Agents-for-Composed-Image-Retrieval","tags":["Review","Composed Image Retrieval","Cross-Modal Agents","Multimodal Reasoning","Training-free Framework","Information Retrieval","Agentic AI","Progressive Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongyu Yang, Wei Pang, Yingfang Yuan 핵심 연구 목표 AI 시대의 Composed Image Retrieval (CIR)에서 기존 유사성 기반 패러다임의 한계를 극복하고, 레퍼런스 이미지와 텍스트 수정 사항을 통합하는 데 필요한 교차모달 추론 능력 을 향상시키는 것이 목표입니다. 특히 기"},{"id":"2026-01-22-sangkuriang-A-pseudo-spectral-Python-library-for-Korteweg-de-Vries-soliton-simulation","title":"[논문리뷰] sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation","excerpt":"arXiv에 게시된 'sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-22 00:00:00+0900+0900","permalink":"/ai/review/2026-01-22-sangkuriang-A-pseudo-spectral-Python-library-for-Korteweg-de-Vries-soliton-simulation","tags":["Review","Nonlinear Wave Physics","Soliton Simulation","Korteweg-de Vries Equation","Pseudo-spectral Methods","Adaptive Time Integration","Python Library","Computational Physics"],"text":"링크: 논문 PDF로 바로 열기 저자: Sandy H. S. Herho, Faruq Khadami, Iwan P. Anwar, Dasapta E. Irawan 핵심 연구 목표 본 논문은 Kortewegde Vries (KdV) 방정식을 해결하는 오픈소스 Python 라이브러리인 을 소개하는 것을 목표로 합니다. 이는 비선형 파동 현상 에 대한 물리적 직관 "},{"id":"2026-01-23-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360","title":"[논문리뷰] 360Anything: Geometry-Free Lifting of Images and Videos to 360°","excerpt":"arXiv에 게시된 '360Anything: Geometry-Free Lifting of Images and Videos to 360°' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360","tags":["Review","Panorama Generation","Diffusion Transformers","Geometry-Free Learning","Latent Encoding","Seam Artifacts","Camera Pose Estimation","Video Outpainting"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyi Wu, Daniel Watson, Andrea Tagliasacchi, David J. Fleet, Marcus A. Brubaker, Saurabh Saxena 핵심 연구 목표 본 논문은 기존의 카메라 메타데이터(FoV, 자세)에 의존하는 한계를 극복하고, 단일 시점의 이미지 및 비디오를 360° 파노라마 "},{"id":"2026-01-23-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion","title":"[논문리뷰] ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion","excerpt":"arXiv에 게시된 'ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion","tags":["Review","3D Mesh Generation","Animated 3D Models","Temporal Diffusion","Video-to-4D","Deep Learning","Generative Models","Topology Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Remy Sabathier, David Novotny, Niloy J. Mitra, Tom Monnier 핵심 연구 목표 논문은 기존 애니메이션 3D 객체 생성 모델의 한계점인 느린 최적화 과정, 제한적인 입력 방식, 낮은 품질, 그리고 토폴로지 불일치 문제 를 해결하고자 합니다. 이를 위해 다양한 입력(텍스트, 이미"},{"id":"2026-01-23-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries","title":"[논문리뷰] BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries","excerpt":"arXiv에 게시된 'BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries","tags":["Review","Vision-Language-Action Models","Bayesian Decomposition","Latent Action Queries","Information Collapse","OOD Generalization","Robot Manipulation","Pointwise Mutual Information"],"text":"링크: 논문 PDF로 바로 열기 저자: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen 핵심 연구 목표 VisionLanguageAction (VLA) 모델이 새로운 지시나 복잡한 다중 작업 시나리오에서"},{"id":"2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning","title":"[논문리뷰] Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning","excerpt":"arXiv에 게시된 'Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning","tags":["Review","Video Models","Visuomotor Control","Robot Policy","Fine-tuning","Diffusion Models","World Models","Model-based Planning","Imitation Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Moo Jin Kim, Yihuai Gao, TsungYi Lin, YenChen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, MingYu Liu, Chelsea Finn, Jinwei Gu 핵심 연구 목표 본 논문은 대규모 사전 훈련된 비디오 생성 모델 의 시공간적 "},{"id":"2026-01-23-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience","title":"[논문리뷰] EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience","excerpt":"Linsen Guo이 arXiv에 게시한 'EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience","tags":["Review","Computer Use Agent","Synthetic Experience","Evolutionary Learning","Reinforcement Learning","Direct Preference Optimization","GUI Automation","Scalable Infrastructure","Verifiable Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han, Haozhe Wang, Jianing Wang, Xiaocheng Zhang, Xin Yang, Dengchang Zhao, Jinrui Ding, Xiandi Ma, Yuchen Xie, Peng "},{"id":"2026-01-23-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding","title":"[논문리뷰] HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding","excerpt":"arXiv에 게시된 'HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding","tags":["Review","Streaming Video Understanding","KV Cache Management","Hierarchical Memory","MLLMs","Low Latency","Training-free","Memory Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Haowei Zhang, Shudong Yang, Jinlan Fu, SeeKiong Ng, Xipeng Qiu 핵심 연구 목표 기존 Multimodal Large Language Models (MLLMs) 이 스트리밍 비디오 이해에서 겪는 성능 불안정, 높은 응답 지연 시간, 높은 GPU 메모리 사용량 등의 문제를 "},{"id":"2026-01-23-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence","title":"[논문리뷰] LLM-in-Sandbox Elicits General Agentic Intelligence","excerpt":"arXiv에 게시된 'LLM-in-Sandbox Elicits General Agentic Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence","tags":["Review","LLM-in-Sandbox","Agentic Intelligence","Code Sandbox","Reinforcement Learning","Generalization","Tool Use","Multi-Modal Generation","Long-Context Processing"],"text":"링크: 논문 PDF로 바로 열기 저자: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, JiRong Wen, Furu Wei 핵심 연구 목표 본 논문은 LLM이 코드 샌드박스(가상 컴퓨터) 내에서 탐색할 수 있도록 지원하여, 비코드 도메인에"},{"id":"2026-01-23-Learning-to-Discover-at-Test-Time","title":"[논문리뷰] Learning to Discover at Test Time","excerpt":"arXiv에 게시된 'Learning to Discover at Test Time' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Learning-to-Discover-at-Test-Time","tags":["Review","Test-Time Training","Reinforcement Learning","Scientific Discovery","LLM Optimization","GPU Kernel Engineering","Algorithm Design","Single-Cell Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun 핵심 연구 목표 본 연구는 AI를 활용하여 과학적 문제"},{"id":"2026-01-23-Numba-Accelerated-2D-Diffusion-Limited-Aggregation-Implementation-and-Fractal-Characterization","title":"[논문리뷰] Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization","excerpt":"arXiv에 게시된 'Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Numba-Accelerated-2D-Diffusion-Limited-Aggregation-Implementation-and-Fractal-Characterization","tags":["Review","Diffusion-Limited Aggregation","Fractal Dimension","Numba","JIT Compilation","Monte Carlo Simulation","Pattern Formation","Laplacian Growth","Non-equilibrium Statistical Mechanics"],"text":"링크: 논문 PDF로 바로 열기 저자: Sandy H. S. Herho, Faiz R. Fajary, Iwan P. Anwar, Faruq Khadami, Nurjanna J. Trilaksono, Rusmawan Suwarman, and Dasapta E. Irawan 핵심 연구 목표 본 연구는 고성능 Numba가속화 Python 프레임워크() 를 개발하"},{"id":"2026-01-23-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation","title":"[논문리뷰] OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation","excerpt":"arXiv에 게시된 'OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation","tags":["Review","Unified Visual Encoder","Image Understanding","Image Generation","VAE","Vision Transformer","Multimodal Learning","Reconstruction","Contrastive Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Letian Zhang, Sucheng Ren, Huaxiu Yao, Yanqing Liu, Xianhang Li, Zeyu Wang, Yuyin Zhou, Weili Nie, Guilin Liu, Zhiding Yu, Cihang Xie 핵심 연구 목표 본 논문은 이미지 이해(understanding)와 생성(gen"},{"id":"2026-01-23-Qwen3-TTS-Technical-Report","title":"[논문리뷰] Qwen3-TTS Technical Report","excerpt":"arXiv에 게시된 'Qwen3-TTS Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Qwen3-TTS-Technical-Report","tags":["Review","Text-to-Speech (TTS)","Multilingual","Voice Cloning","Controllable Speech","Streaming","Speech Tokenization","Language Models","Low-latency"],"text":"링크: 논문 PDF로 바로 열기 저자: Qwen Team 핵심 연구 목표 본 논문은 고급 다국어(multilingual) , 제어 가능한(controllable) , 강건한(robust) , 스트리밍(streaming) TTS 모델 인 Qwen3TTS 시리즈를 소개하는 것을 목표로 합니다. 특히 3초 음성 복제(voice cloning) 및 설명 기반 제어"},{"id":"2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing","title":"[논문리뷰] Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing","excerpt":"Dingkun Long이 arXiv에 게시한 'Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing","tags":["Review","Composed Image Retrieval","Fine-Grained Evaluation","Image Editing","Benchmark","Multimodal LLM","Synthetic Data","Compositional Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Dingkun Long, Zhuoning Guo, Mingxin Li, Yanzhao Zhang, Tingyu Song 핵심 연구 목표 기존 Composed Image Retrieval (CIR) 벤치마크의 한계, 즉 제한된 쿼리 범주, 실제 시나리오의 다양성 부족, 모호한 범주 정의, 모달리티 편향 등을 극복하는 것"},{"id":"2026-01-23-SAMTok-Representing-Any-Mask-with-Two-Words","title":"[논문리뷰] SAMTok: Representing Any Mask with Two Words","excerpt":"arXiv에 게시된 'SAMTok: Representing Any Mask with Two Words' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-SAMTok-Representing-Any-Mask-with-Two-Words","tags":["Review","Mask Tokenization","Multimodal LLMs","Pixel-wise Vision-Language","Reinforcement Learning","Segmentation Anything Model","Discrete Representation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yikang Zhou, Tao Zhang, Dengxian Gong, Yuanzheng Wu, Ye Tian, Haochen Wang, Haobo Yuan, Jiacong Wang, Lu Qi, Hao Fei, Anran Wang, Zhuochen Wang, Yujing Wang, Cheng Chen, Shunping"},{"id":"2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders","title":"[논문리뷰] Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders","excerpt":"arXiv에 게시된 'Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders","tags":["Review","Text-to-Image Generation","Diffusion Models","Representation Autoencoder","Latent Space","Large-Scale Models","Unified Models","Noise Scheduling"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie 핵심 연구 목표 본 논문은 기존 변형 오토인코더(VAE) 의 저차원 잠재 공간이 대규모 텍스"},{"id":"2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model","title":"[논문리뷰] Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model","excerpt":"arXiv에 게시된 'Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model","tags":["Review","Code Diffusion Models","Large Language Models","Continual Pretraining","Code Generation","Code Editing","Masked Language Models","Code Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenghao Fan, Wen Heng, Bo Li, Sichen Liu, Yuxuan Song, Jing Su, Xiaoye Qu, Kai Shen, Wei Wei 핵심 연구 목표 본 연구는 기존 autoregressive (AR) 모델에 비해 성능이 뒤처지던 확산 기반 언어 모델(DLLM)이 코드 모델링 품질을 "},{"id":"2026-01-23-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces","title":"[논문리뷰] Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces","excerpt":"Harsh Raj이 arXiv에 게시한 'Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces","tags":["Review","AI Agents","LLM Evaluation","Benchmarking","Command Line Interface","Software Engineering","Realistic Tasks","Error Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Harsh Raj, Boxuan Li, Nicholas Carlini, Alexander G. Shaw, Mike A. Merrill 핵심 연구 목표 본 논문은 기존 AI 에이전트 벤치마크가 실제 작업 시나리오를 충분히 반영하지 못하거나 최신 모델의 성능을 측정하기에 난이도가 부족하다는 문제점을 해결하고자 합니다. 이"},{"id":"2026-01-23-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models","title":"[논문리뷰] The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models","excerpt":"arXiv에 게시된 'The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Reasoning","Reinforcement Learning","Autoregressive Models","Generation Order","Entropy Degradation","Pass@k","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang 핵심 연구 목표 이 논문은 Diffusion Large Language Models (dLLMs)의"},{"id":"2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs","title":"[논문리뷰] Towards Automated Kernel Generation in the Era of LLMs","excerpt":"Yixin Shen이 arXiv에 게시한 'Towards Automated Kernel Generation in the Era of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs","tags":["Review","Large Language Models","Kernel Generation","GPU Optimization","AI Agents","Code Synthesis","Performance Engineering","Hardware Acceleration"],"text":"링크: 논문 PDF로 바로 열기 저자: Yixin Shen, Haiming Wu, Chi Hsu Tsai, Peiyu Zang, Yang Yu 핵심 연구 목표 본 논문은 현대 AI 시스템의 성능을 근본적으로 제한하는 고성능 커널 생성 및 최적화의 비확장성 문제 를 해결하고자 합니다. 특히, LLM과 LLM 기반 에이전트의 발전이 이 분야에 가져올 혁신적인 "},{"id":"2026-01-23-VIOLA-Towards-Video-In-Context-Learning-with-Minimal-Annotations","title":"[논문리뷰] VIOLA: Towards Video In-Context Learning with Minimal Annotations","excerpt":"Ryo Hachiuma이 arXiv에 게시한 'VIOLA: Towards Video In-Context Learning with Minimal Annotations' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-VIOLA-Towards-Video-In-Context-Learning-with-Minimal-Annotations","tags":["Review","Video In-Context Learning","Minimal Annotation","Active Learning","Pseudo-Labeling","Multimodal LLMs","Density-Uncertainty Sampling","Confidence-Aware Retrieval","Low-Resource Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ryo Fujii, Hideo Saito, Ryo Hachiuma 핵심 연구 목표 본 논문은 레이블링된 데이터가 부족한 새로운 비디오 도메인에서 Multimodal Large Language Models (MLLMs) 의 일반화 능력을 향상시키는 것을 목표로 합니다. 특히, InContext Learning (ICL)"},{"id":"2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior","title":"[논문리뷰] VideoMaMa: Mask-Guided Video Matting via Generative Prior","excerpt":"arXiv에 게시된 'VideoMaMa: Mask-Guided Video Matting via Generative Prior' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-23 00:00:00+0900+0900","permalink":"/ai/review/2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior","tags":["Review","Video Matting","Diffusion Models","Generative Priors","Mask-Guided","Pseudo-labeling","Large-scale Dataset","Zero-shot Generalization"],"text":"링크: 논문 PDF로 바로 열기 저자: Sangbeom Lim, Seoung Wug Oh, Seungryong Kim, Jiahui Huang, Heeji Yoon, JoonYoung Lee 핵심 연구 목표 논문은 비디오 매팅 모델이 실제 세계 비디오에 효과적으로 일반화되지 못하는 문제, 즉 레이블링된 데이터의 희소성과 합성 비디오와 실제 비디오 간의 도메"},{"id":"2026-01-26-DSGym-A-Holistic-Framework-for-Evaluating-and-Training-Data-Science-Agents","title":"[논문리뷰] DSGym: A Holistic Framework for Evaluating and Training Data Science Agents","excerpt":"Yongchan Kwon이 arXiv에 게시한 'DSGym: A Holistic Framework for Evaluating and Training Data Science Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-DSGym-A-Holistic-Framework-for-Evaluating-and-Training-Data-Science-Agents","tags":["Review","Data Science Agents","LLM Evaluation","Benchmark Framework","Execution-Grounded Training","Bioinformatics","Kaggle","Shortcut Filtering","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Yongchan Kwon, Federico Bianchi, Harper Hua, Junlin Wang, Fan Nie 핵심 연구 목표 기존 데이터 사이언스 LLM 벤치마크의 단편적인 평가 인터페이스 , 좁은 태스크 커버리지 , 그리고 데이터 의존성 부족 문제를 해결하는 것을 목표로 합니다. 특히, 실제 데이터를 사용하"},{"id":"2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind","title":"[논문리뷰] Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind","excerpt":"Yi R Fung이 arXiv에 게시한 'Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind","tags":["Review","Academic Rebuttal","Theory of Mind","Large Language Models","Strategic Persuasion","Reinforcement Learning","Self-Reward","Dataset Synthesis","Automated Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhitao He, Zongwei Lyu, Yi R. (May) Fung 핵심 연구 목표 본 논문은 학술적 반론(rebuttal) 과정에서 단순히 표면적인 언어적 유사성을 모방하는 현재 AI 모델의 한계를 극복하고자 합니다. Theory of Mind (ToM) 를 통합하여 심사위원의 정신 상태를 모델링하고, 전략적 "},{"id":"2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","title":"[논문리뷰] Endless Terminals: Scaling RL Environments for Terminal Agents","excerpt":"arXiv에 게시된 'Endless Terminals: Scaling RL Environments for Terminal Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","tags":["Review","Reinforcement Learning","Procedural Generation","Terminal Agents","Environment Scaling","Language Models (LLMs)","PPO","Task Generation","Automated Verification"],"text":"링크: 논문 PDF로 바로 열기 저자: Kanishk Gandhi, Shivam Garg, Noah D. Goodman, Dimitris Papailiopoulos 핵심 연구 목표 본 논문은 자체 개선 에이전트 훈련을 위한 환경이 부족하다는 문제점을 해결하고, 확장 가능한 RL 환경을 제공하는 것을 목표로 합니다. 특히 터미널 사용 태스크에 초점을 맞춰, "},{"id":"2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization","title":"[논문리뷰] Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization","excerpt":"Gabriele Bavota이 arXiv에 게시한 'Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization","tags":["Review","Large Language Models","Code Generation","Prompt Engineering","Prompt Optimization","Empirical Study","Software Engineering","Guidelines"],"text":"링크: 논문 PDF로 바로 열기 저자: Alessandro Midolo, Alessandro Giagnorio, Fiorella Zampetti, Rosalia Tufano, Gabriele Bavota, Massimiliano Di Penta 핵심 연구 목표 본 연구는 LLM 기반 코드 생성 시 개발자들이 효과적인 프롬프트를 작성할 수 있도록 돕는 구체적"},{"id":"2026-01-26-Inference-Time-Scaling-of-Verification-Self-Evolving-Deep-Research-Agents-via-Test-Time-Rubric-Guided-Verification","title":"[논문리뷰] Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification","excerpt":"arXiv에 게시된 'Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Inference-Time-Scaling-of-Verification-Self-Evolving-Deep-Research-Agents-via-Test-Time-Rubric-Guided-Verification","tags":["Review","Deep Research Agents","Inference-Time Verification","Self-Evolving LLM Agents","Rubric-Guided Feedback","Failure Taxonomy","Test-Time Scaling","Supervised Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang, Haitao Mi, Dong Yu, Michael R. Lyu 핵심 연구 목표 본 논문은 Deep Research Agents (DRAs)의 신뢰할 수 없는 출력(예: 환각, 오류) 문제를 해결하고, "},{"id":"2026-01-26-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow","title":"[논문리뷰] Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow","excerpt":"arXiv에 게시된 'Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow","tags":["Review","Reinforcement Learning","FP8 Quantization","LLM Training","On-Policy RL","Unified Precision Flow","Training Efficiency","Rollout Acceleration"],"text":"링크: 논문 PDF로 바로 열기 저자: Haocheng Xi, Charlie Ruan, Peiyuan Liao, Yujun Lin, Han Cai, Yilong Zhao, Shuo Yang, Kurt Keutzer, Song Han, Ligeng Zhu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 강화 학습(RL) 훈련 파이프라인에서 발생하는 계"},{"id":"2026-01-26-Knowledge-is-Not-Enough-Injecting-RL-Skills-for-Continual-Adaptation","title":"[논문리뷰] Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation","excerpt":"arXiv에 게시된 'Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Knowledge-is-Not-Enough-Injecting-RL-Skills-for-Continual-Adaptation","tags":["Review","LLMs","Continual Adaptation","Reinforcement Learning","Supervised Fine-Tuning","Skill Transfer","Task Arithmetic","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Pingzhi Tang, Yiding Wang, Muhan Zhang 핵심 연구 목표 대규모 언어 모델(LLMs)이 겪는 \"지식 단절(knowledge cutoff)\" 문제와, 지도 미세 조정(SFT)이 새로운 지식 통합 시 추론 능력 향상에 한계가 있으며, 강화 학습(RL)은 온라인 적응에 비실용적으로 비싼 비용 문"},{"id":"2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report","title":"[논문리뷰] LongCat-Flash-Thinking-2601 Technical Report","excerpt":"arXiv에 게시된 'LongCat-Flash-Thinking-2601 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report","tags":["Review","Agentic AI","Large Language Models (LLMs)","Mixture-of-Experts (MoE)","Reinforcement Learning (RL)","Context Management","Scalable Training","Test-Time Reasoning","Open-Source Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Meituan LongCat Team 핵심 연구 목표 본 논문은 장기적인 상호작용과 추론이 요구되는 에이전트 태스크 에서 기존 모델들의 한계를 극복하고, 뛰어난 에이전트 추론 능력을 가진 오픈소스 MoE(MixtureofExperts) 대규모 언어 모델인 LongCatFlashThinking2601 을 개발하는 것을 "},{"id":"2026-01-26-Mecellem-Models-Turkish-Models-Trained-from-Scratch-and-Continually-Pre-trained-for-the-Legal-Domain","title":"[논문리뷰] Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain","excerpt":"arXiv에 게시된 'Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Mecellem-Models-Turkish-Models-Trained-from-Scratch-and-Continually-Pre-trained-for-the-Legal-Domain","tags":["Review","Turkish Legal NLP","Domain Adaptation","ModernBERT","Continual Pre-training (CPT)","Embedding Models","Legal LLMs","Retrieval-Augmented Generation (RAG)","Curriculum Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Özgür Uğura, Mahmut Göksu, Mahmut Çimen, Musa Yılmaz, Esra Şavirdi, Alp Talha Demir, Rumeysa Güllüce, İclal Çetin, Ömer Can Sağbaş 핵심 연구 목표 본 논문은 터키어 법률 도메인에 특화된 언어 모델인 Mecellem "},{"id":"2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences","title":"[논문리뷰] MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences","excerpt":"Jianwen Sun이 arXiv에 게시한 'MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences","tags":["Review","Large Language Models","Board Games","Virtual Playtester","User Simulation","Persona Modeling","MDA Framework","Human-AI Collaboration","Critique Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zizhen Li, Chuanhao Li, Yibin Wang, Yukang Feng, Jianwen Sun, Jiaxin Ai, Fanrui Zhang, Mingzhu Sun, Yifei Huang, Kaipeng Zhang 핵심 연구 목표 본 논문은 LLM이 보드게임 디자인에 대한 건설적인 비판을 제공하는 데 있어"},{"id":"2026-01-26-Memory-V2V-Augmenting-Video-to-Video-Diffusion-Models-with-Memory","title":"[논문리뷰] Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory","excerpt":"arXiv에 게시된 'Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-Memory-V2V-Augmenting-Video-to-Video-Diffusion-Models-with-Memory","tags":["Review","Video-to-Video Diffusion","Explicit Memory","Multi-turn Video Editing","Cross-consistency","Dynamic Tokenization","Adaptive Token Merging","Video Novel View Synthesis","Text-guided Video Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Dohun Lee, ChunHao Paul Huang, Xuelin Chen, Jong Chul Ye, Duygu Ceylan, Hyeonho Jeong 핵심 연구 목표 본 논문은 반복적인 비디오 편집 과정에서 기존 VideotoVideo (V2V) Diffusion 모델 들이 순차적인 편집 간의 일관성(crossco"},{"id":"2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer","title":"[논문리뷰] SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer","excerpt":"arXiv에 게시된 'SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer","tags":["Review","Video Diffusion Models","Sparse Attention","Linear Attention","Computational Efficiency","Transformer Tuning","Video Generation","LoRA","Gating Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Tongcheng Fang, Tianchen Zhao, Pengfei Wan, Hanling Zhang, Ruiqi Xie, Zhuo Han, Xin Tao, Wenbo Ding, Wanli Ouyang, Xuefei Ning, Yu Wang 핵심 연구 목표 비디오 Diffusion Transformer의 긴 입력 시"},{"id":"2026-01-26-SWE-Pruner-Self-Adaptive-Context-Pruning-for-Coding-Agents","title":"[논문리뷰] SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents","excerpt":"arXiv에 게시된 'SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-SWE-Pruner-Self-Adaptive-Context-Pruning-for-Coding-Agents","tags":["Review","Context Pruning","Coding Agents","Large Language Models (LLMs)","Software Development","Code Comprehension","Efficiency Optimization","Task-Aware Pruning","CRF"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhang Wang, Yuling Shi, Mo Yang, Rongrui Zhang, Shilin He, Heng Lian, Yuting Chen, Siyu Ye, Kai Cai, Xiaodong Gu 핵심 연구 목표 본 논문은 소프트웨어 개발을 위한 LLM 에이전트가 긴 컨텍스트로 인해 발생하는 높은 API 비용과"},{"id":"2026-01-26-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers","title":"[논문리뷰] TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers","excerpt":"arXiv에 게시된 'TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers","tags":["Review","Vision-Language-Action (VLA)","Embodied AI","Robotics","Catastrophic Forgetting","Asymmetric Mixture-of-Transformers (AsyMoT)","Generalist VLM","Specialist VLM","Flow-Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Bin Yu, Shijie Lian, Xiaopeng Lin, Yuliang Wei, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Xinming Wang, Bailing Wang, Cong Huang, Kai Chen 핵심 연구 목표 표준 VisionLanguageAction (VLA) 모델"},{"id":"2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents","title":"[논문리뷰] VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents","excerpt":"arXiv에 게시된 'VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-26 00:00:00+0900+0900","permalink":"/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents","tags":["Review","Multimodal Agents","Vision-Language Models (VLMs)","Interactive AI","Reinforcement Learning Environments","Benchmark","Decision-Making","Diagnostic Tools","Supervised Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zirui Wang, Junyi Zhang, Jiaxin Ge, Long Lian, Letian Fu, Lisa Dunlap, Ken Goldberg, XuDong Wang, Ion Stoica, David M. Chan, Sewon Min, Joseph E. Gonzalez 핵심 연구 목표 본 논문은 시각적으로 풍부"},{"id":"2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation","title":"[논문리뷰] AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation","excerpt":"arXiv에 게시된 'AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation","tags":["Review","Autoregressive Models","Multimodal AI","Any-to-Any Generation","Unified Model","Speech Generation","Image Generation","Transformer Decoder","Real-time Streaming"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Wenjie Wang, Liqiang Nie, Lei Zhang, Runyang You, Wenjie Li 핵심 연구 목표 본 논문은 기존 멀티모달 대규모 언어 모델(MLLM)이 멀티모달 생성을 위해 외부 전문가 구성 요소(예: 확산 디코더)에 의"},{"id":"2026-01-27-Agentic-Very-Long-Video-Understanding","title":"[논문리뷰] Agentic Very Long Video Understanding","excerpt":"arXiv에 게시된 'Agentic Very Long Video Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-Agentic-Very-Long-Video-Understanding","tags":["Review","Long-Horizon Video Understanding","Agentic AI","Entity Graph","Multimodal Reasoning","Video Question Answering","EgoLifeQA","Retrieval Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Aniket Rege, Arka Sadhu, Yuliang Li, Kejie Li, Ramya Korlakai Vinayak, Yuning Chai, Yong Jae Lee, Hyo Jin Kim 핵심 연구 목표 본 논문은 가 요구하는 의 과제를 해결하는 것을 목표로 합니다. 기존 대규모 언어 모델(LLM) 및 검색 "},{"id":"2026-01-27-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval","title":"[논문리뷰] CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval","excerpt":"arXiv에 게시된 'CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval","tags":["Review","Table Retrieval","LLM Supervision","K-means Clustering","Partial Table","Contrastive Learning","Embedding Fine-tuning","Synthetic Query Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: TsungHsiang Chou, ChenJui Yu, ShuiHsiang Hsu, YaoChung Fan 핵심 연구 목표 본 논문은 일반적인 임베딩 모델이 테이블 검색에서 겪는 의미론적 압축(semantic compression) 및 쿼리테이블 불일치 문제를 해결하고, 기존 LLM 기반 검색 증강 방법론인 QGpT의 "},{"id":"2026-01-27-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs","title":"[논문리뷰] Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs","excerpt":"arXiv에 게시된 'Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs","tags":["Review","Data Preparation","LLMs","Data Cleaning","Data Integration","Data Enrichment","AI Agents","Semantic Reasoning","Workflow Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Zhou, Jun Zhou, Haoyu Wang, Zhenghao Li, Qikang He, Shaokun Han, Guoliang Li Fellow, IEEE, Xuanhe Zhou, Yeye He, Chunwei Liu, Zirui Tang, Bin Wang, Shen Tang, Kai Zuo, Yuyu L"},{"id":"2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal","title":"[논문리뷰] DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal","excerpt":"Jiaxuan You이 arXiv에 게시한 'DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal","tags":["Review","Academic Rebuttal","LLM Agents","Peer Review Automation","Generative AI","Retrieval-Augmented Generation (RAG)","Strategic Planning","Persuasion"],"text":"링크: 논문 PDF로 바로 열기 저자: Peixuan Han, Yingjie Yu, Jingjun Xu, Jiaxuan You 핵심 연구 목표 본 논문은 학술적 동료 심사 과정에서 중요한 단계인 학술 리버탈(rebuttal)에 대한 자동화된 지원이 부족하고, 기존 LLM 기반 접근 방식이 긴 컨텍스트 이해와 설득력 있는 응답 생성에 어려움을 겪는 문제를 해"},{"id":"2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints","title":"[논문리뷰] DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints","excerpt":"arXiv에 게시된 'DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints","tags":["Review","LLM Agents","Long-Horizon Planning","Benchmarking","Verifiable Constraints","Tool Use","Constraint Optimization","Information Acquisition","Travel Planning","Shopping Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinger Zhang, Shutong Jiang, Renhao Li, Jianhong Tu, Yang Su, Lianghao Deng, Xudong Guo, Chenxu Lv, Junyang Lin 핵심 연구 목표 기존 LLM 에이전트 평가 벤치마크들이 주로 국소적인 추론에 집중하고 실제 환경의 복잡한 전역 제약 최"},{"id":"2026-01-27-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers","title":"[논문리뷰] Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers","excerpt":"arXiv에 게시된 'Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers","tags":["Review","Transformer","Sparse Attention","Adaptive Sparsity","Efficient LLM","Attention Router","Long-Context","Hybrid Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang 핵심 연구 목표 표준 어텐션 메커니즘의 이차적인 복잡도로 인한 대규모 언어 모델(LLM)의 긴 컨텍스트 시나리오에서의 확장성"},{"id":"2026-01-27-End-to-End-Joint-ASR-and-Speaker-Role-Diarization-with-Child-Adult-Interactions","title":"[논문리뷰] End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions","excerpt":"Shrikanth Narayanan이 arXiv에 게시한 'End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-End-to-End-Joint-ASR-and-Speaker-Role-Diarization-with-Child-Adult-Interactions","tags":["Review","End-to-End ASR","Speaker Diarization","Child Speech Processing","Whisper Model","Serialized Output Training","Multi-task Learning","State-Machine Decoding"],"text":"링크: 논문 PDF로 바로 열기 저자: Anfeng Xu, Tiantian Feng, Somer Bishop, Catherine Lord, Shrikanth Narayanan 핵심 연구 목표 본 논문은 아동성인 상호작용에서 정확한 전사 및 화자 역할 분리(speaker role diarization)의 어려움을 해결하는 것을 목표로 합니다. 특히, 기존의 "},{"id":"2026-01-27-Less-Is-More-Until-It-Breaks-Security-Pitfalls-of-Vision-Token-Compression-in-Large-Vision-Language-Models","title":"[논문리뷰] Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models","excerpt":"Guanhong Tao이 arXiv에 게시한 'Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-Less-Is-More-Until-It-Breaks-Security-Pitfalls-of-Vision-Token-Compression-in-Large-Vision-Language-Models","tags":["Review","LVLM Security","Token Compression","Adversarial Attack","Robustness Degradation","Compression-Aware Attack","Efficiency-Security Trade-off","Black-box Attack"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaomei Zhang, Zhaoxi Zhang, Leo Yu Zhang, Yanjun Zhang, Guanhong Tao, Shirui Pan 핵심 연구 목표 본 논문은 대규모 시각언어 모델(LVLM)에서 시각 토큰 압축이 모델의 강건성(robustness) 에 미치는 보안적 영향을 최초로 체계적으로 탐구합니다. "},{"id":"2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents","title":"[논문리뷰] Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents","excerpt":"arXiv에 게시된 'Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents","tags":["Review","LLM Agents","Reinforcement Learning","Cross-Domain Generalization","State Information Richness","Planning Complexity","State Augmentation","Step-by-Step Reasoning","Mid-Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhihan Liu, Lin Guan, Yixin Nie, Kai Zhang, Zhuoqun Hao, Lin Chen, Asli Celikyilmaz, Zhaoran Wang, Na Zhang 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM) 에이전트가 좁은 범위의 환경에서 후기 훈련(posttraining)된 후"},{"id":"2026-01-27-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback","title":"[논문리뷰] SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback","excerpt":"arXiv에 게시된 'SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback","tags":["Review","Deep Search","Agentic Data Generation","LLMs","Execution Feedback","Reinforcement Learning","Question Answering","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangyuan Xu, Rujun Han, Yanfei Chen, Zifeng Wang, IHung Hsu, Jun Yan, Vishy Tirumalashetty, Eunsol Choi, Tomas Pfister and ChenYu Lee 핵심 연구 목표 본 논문은 복잡한 다중 문서 추론이 필요한 딥 서치(deep s"},{"id":"2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion","title":"[논문리뷰] STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion","excerpt":"arXiv에 게시된 'STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion","tags":["Review","Table Retrieval","Semantic Representation","K-means Clustering","Weighted Fusion","Large Language Models","Query Generation","Information Retrieval"],"text":"링크: 논문 PDF로 바로 열기 저자: ShuiHsiang Hsu, ChenJui Yu, TsungHsiang Chou, YaoChung Fan 핵심 연구 목표 이 논문은 자연어 질의에 대한 테이블 검색(Table Retrieval) 과정에서 발생하는 비정형 질의와 정형 테이블 간의 심층적인 의미적 불일치 및 긴 테이블 처리 시 토큰 길이 제한 문제를 해결"},{"id":"2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility","title":"[논문리뷰] Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility","excerpt":"arXiv에 게시된 'Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility","tags":["Review","Scientific Image Synthesis","Multimodal Reasoning","Text-to-Image","Benchmarking","Programmatic Synthesis","Large Multimodal Models","Synthetic Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Honglin Lin, Chonghan Qin, Zheng Liu, Qizhi Pei, Yu Li, Zhanping Zhong, Xin Gao, Yanfeng Wang, Conghui He, Lijun Wu 핵심 연구 목표 과학적 추론을 위한 멀티모달 데이터의 부족과 기존 TexttoImage(T2I) 모델 이 시각적"},{"id":"2026-01-27-SkyReels-V3-Technique-Report","title":"[논문리뷰] SkyReels-V3 Technique Report","excerpt":"arXiv에 게시된 'SkyReels-V3 Technique Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report","tags":["Review","Video Generation","Multimodal AI","Diffusion Models","Transformer Architecture","Reference-guided Generation","Video-to-Video","Audio-driven Animation","Temporal Consistency"],"text":"링크: 논문 PDF로 바로 열기 저자: Debang Li, Zhengcong Fei, Tuanhui Li, et al. 핵심 연구 목표 본 논문은 SkyReelsV3 를 통해 시각적 참조, 비디오, 오디오 및 텍스트 입력을 통합하여 유연하고 제어 가능한 비디오 생성을 가능하게 하는 통합 멀티모달 조건부 비디오 생성 프레임워크 를 제시하는 것을 목표로 합니다"},{"id":"2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability","title":"[논문리뷰] Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability","excerpt":"arXiv에 게시된 'Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability","tags":["Review","Meta-RL","Curriculum Learning","Self-Play","LLM Reasoning","Sparse Rewards","Question Generation","Bilevel Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe 핵심 연구 목표 본 논문은 초기 성공률이 낮아 훈련 신호가 희박한 어려운 추론 문제 에 대해 대규모 언어 모델(LLM) 이 학습 정체기에서 벗어나도록 돕는 것"},{"id":"2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation","title":"[논문리뷰] The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation","excerpt":"arXiv에 게시된 'The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation","tags":["Review","Dialogue-to-Video Generation","Agentic AI","Cinematic Scripting","Long-Horizon Video Synthesis","Visual Coherence","Reinforcement Learning","Multimodal LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenyu Mu, Xin He, Qu Yang, Wanshun Chen, Jiadi Yao, Huang Liu, Zihao Yi, Bo Zhao, Xingyu Chen, Ruotian Ma, Fanghua Ye, Erkun Yang, Cheng Deng, Zhaopeng Tu⁺, Xiaolong Li, and Lin"},{"id":"2026-01-27-VIBEVOICE-ASR-Technical-Report","title":"[논문리뷰] VIBEVOICE-ASR Technical Report","excerpt":"arXiv에 게시된 'VIBEVOICE-ASR Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report","tags":["Review","Automatic Speech Recognition","Speaker Diarization","Long-form Audio","Large Language Models","End-to-end Speech Processing","Multilingual","Context-aware ASR"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong, Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun, Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang"},{"id":"2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering","title":"[논문리뷰] daVinci-Dev: Agent-native Mid-training for Software Engineering","excerpt":"arXiv에 게시된 'daVinci-Dev: Agent-native Mid-training for Software Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering","tags":["Review","Agentic Software Engineering","Mid-training","Large Language Models","Agent-native Data","Contextual Trajectories","Environmental Trajectories","SWE-Bench Verified","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ji Zeng, Dayuan Fu, Tiantian Mi, Ye, Muhang Xie, Qishuo Hua, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie "},{"id":"2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code","title":"[논문리뷰] iFSQ: Improving FSQ for Image Generation with 1 Line of Code","excerpt":"arXiv에 게시된 'iFSQ: Improving FSQ for Image Generation with 1 Line of Code' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-27 00:00:00+0900+0900","permalink":"/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code","tags":["Review","Finite Scalar Quantization (FSQ)","Image Generation","Autoregressive Models","Diffusion Models","Quantization","Tokenization","Representation Alignment (REPA)","Latent Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Bin Lin, Zongjian Li, Yuwei Niu, Kaixiong Gong, Yunyang Ge, Yunlong Lin, Mingzhe Zheng, JianWei Zhang, Miles Yang, Zhao Zhong, Liefeng Bo, Li Yuan 핵심 연구 목표 이미지 생성 분야의 Autoregress"},{"id":"2026-01-28-A-Pragmatic-VLA-Foundation-Model","title":"[논문리뷰] A Pragmatic VLA Foundation Model","excerpt":"arXiv에 게시된 'A Pragmatic VLA Foundation Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-A-Pragmatic-VLA-Foundation-Model","tags":["Review","Vision-Language-Action Model","Robotics","Foundation Models","Multi-Embodiment Learning","Data Scaling","Computational Efficiency","Real-world Deployment"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Ho"},{"id":"2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking","title":"[논문리뷰] AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking","excerpt":"arXiv에 게시된 'AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking","tags":["Review","Multimodal LLMs","Benchmark","Cultural Understanding","Contextual Inference","Audio-Visual Memes","Multilingual","Q&A Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Xilin Jiang, Qiaolin Wang, Junkai Wu, Xiaomin He, Zhongweiyang Xu, Yinghao Ma, Minshuo Piao, Kaiyi Yang, Xiuwen Zheng, Riki Shimizu, Yicong Chen, Arsalan Firoozi, Gavin Mischler,"},{"id":"2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning","title":"[논문리뷰] AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning","excerpt":"arXiv에 게시된 'AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning","tags":["Review","Multimodal LLMs","Tool Orchestration","Visual Reasoning","Reinforcement Learning","Adaptive Learning","Generalization","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu, Ranjay Krishna, Yu Cheng 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델(MLLM)의 시각적 추론 능력을 향상시키기 위해, 적응적이며 다단계적인 도구 활용 능력 을 개발하는 것을 목표로 합니"},{"id":"2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security","title":"[논문리뷰] AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security","excerpt":"arXiv에 게시된 'AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security","tags":["Review","AI Agents","Safety Guardrails","Explainable AI (XAI)","Risk Taxonomy","Benchmarking","LLM Safety","Tool Use","Agent Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongrui Liu, Jing Shao, Qihan Ren, et al. 핵심 연구 목표 AI 에이전트의 자율적인 도구 사용과 환경 상호작용으로 인해 발생하는 복잡한 안전 및 보안 문제를 해결하고자 합니다. 기존 가드레일 모델의 에이전트 리스크 인지 부족과 진단 투명성 부족이라는 한계를 극복하고, 복잡하고 다양한 위"},{"id":"2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning","title":"[논문리뷰] FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning","excerpt":"arXiv에 게시된 'FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning","tags":["Review","RAG","LLM-Enhanced Retrieval","Multi-Document Reasoning","Hierarchical Indexing","Bi-Path Retrieval","Adaptive Retrieval","Knowledge Organization","Context Window Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Lin Sun, Linglin Zhang, Jingang Huang, Change Jia, Zhengwei Cheng, Xiangzheng Zhang 핵심 연구 목표 본 논문은 장문 컨텍스트 LLM의 \"lostinthemiddle\" 현상, 높은 계산 비용, 멀티 도큐먼트 추론 확장성 부족 문제를 해결하고, 기존 RAG"},{"id":"2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery","title":"[논문리뷰] GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery","excerpt":"arXiv에 게시된 'GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery","tags":["Review","GPCR","Drug Discovery","Deep Learning","Protein Language Model","Graph Neural Network","Attention Mechanism","Drug Target Interaction","Virtual Screening"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingjie Ning, Xiangzhen Shen, Li Hou, Shiyi Shen, Jiahao Yang, Junrui Li, Hong Shan, Sanan Wu, Sihan Gao, Huaqiang Eric Xu, Xinheng He 핵심 연구 목표 GPCR(G proteincoupled receptors) 변"},{"id":"2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences","title":"[논문리뷰] HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences","excerpt":"Taro Watanabe이 arXiv에 게시한 'HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences","tags":["Review","Hallucinated Citations","NLP Conferences","Citation Detection","Academic Integrity","Peer Review","Large Language Models (LLMs)","Bibliometrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe 핵심 연구 목표 본 논문은 학술 논문, 특히 AI/ML 분야에서 증가하는 환각 인용(HalluCitation) 의 확산과 그 영향을 체계적으로 조사하는 것을 목표로 합니다. 이는 과학적 신뢰성과 학술 컨퍼런스의 명성을 위협하는 심각한"},{"id":"2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep","title":"[논문리뷰] Post-LayerNorm Is Back: Stable, ExpressivE, and Deep","excerpt":"arXiv에 게시된 'Post-LayerNorm Is Back: Stable, ExpressivE, and Deep' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep","tags":["Review","Transformer Architecture","Layer Normalization","Depth Scaling","Training Stability","Large Language Models","Gradient Flow","Highway Networks","Post-LayerNorm"],"text":"링크: 논문 PDF로 바로 열기 저자: Chen Chen, Lai Wei,† 핵심 연구 목표 현재 대규모 언어 모델(LLM)의 스케일링이 한계에 부딪혔으며, 특히 깊이 스케일링은 이론적으로 우수한 표현력을 제공하지만 기존 Transformer 아키텍처는 극심한 깊이에서 안정적으로 훈련하기 어렵습니다. 본 논문은 과거 불안정성으로 대체되었던 PostLayer"},{"id":"2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training","title":"[논문리뷰] Revisiting Parameter Server in LLM Post-Training","excerpt":"arXiv에 게시된 'Revisiting Parameter Server in LLM Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training","tags":["Review","LLM Post-Training","Parameter Server","Distributed Training","FSDP","On-Demand Communication","Workload Imbalance","Communication Optimization","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyi Wan, Penghui Qi, Guangxing Huang, Chaoyi Ruan, Min Lin & Jialin Li 핵심 연구 목표 대규모 언어 모델(LLM) 후처리 훈련 과정에서 시퀀스 길이의 높은 편차 로 인해 발생하는 워크로드 불균형 문제 를 해결하는 것이 목표입니다. 기존 FSDP(Fully Sh"},{"id":"2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection","title":"[논문리뷰] Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection","excerpt":"arXiv에 게시된 'Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection","tags":["Review","Activation Steering","Large Language Models (LLMs)","Norm Preservation","Discriminative Layer Selection","Behavior Control","Inference-time Intervention","Angular Steering"],"text":"링크: 논문 PDF로 바로 열기 저자: QuyAnh Dang, Chris Ngo 핵심 연구 목표 대규모 언어 모델(LLM)이 정렬 노력에도 불구하고 여전히 유해한 행동에 취약하며, 기존 액티베이션 스티어링(Activation Steering) 기법들이 norm 보존 실패 로 인한 생성 붕괴, 세심한 계수 튜닝, 또는 이진 제어 등의 한계를 가진다는 문제점을"},{"id":"2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment","title":"[논문리뷰] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment","excerpt":"arXiv에 게시된 'TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment","tags":["Review","LLM Safety Alignment","Reinforcement Learning","Self-Play","Red Teaming","Adversarial Training","Multi-Role Framework","Reward Hacking Mitigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhewen Tan, Wenhan Yu, Jianfeng Si, Tongxin Liu, Kaiqi Guan, Huiyan Jin, Jiawen Tao, Xiaokun Yuan, Duohe Ma, Xiangzheng Zhang, Tong Yang, Lin Sun 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)에서 "},{"id":"2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models","title":"[논문리뷰] Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models","excerpt":"arXiv에 게시된 'Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models","tags":["Review","Multimodal AI","World Models","Visual Generation","Chain-of-Thought (CoT)","Multimodal Reasoning","Unified Multimodal Models","Spatial-Physical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long 핵심 연구 목표 본 논문은 기존 AI 시스템이 언어적/추상적 영역에"},{"id":"2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text","title":"[논문리뷰] World Craft: Agentic Framework to Create Visualizable Worlds via Text","excerpt":"arXiv에 게시된 'World Craft: Agentic Framework to Create Visualizable Worlds via Text' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-28 00:00:00+0900+0900","permalink":"/ai/review/2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text","tags":["Review","Generative Agents","AI Town","LLM","Environment Creation","Multi-agent System","Spatial Reasoning","Text-to-World","Reverse Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianwen Sun, Zizhen Li, Yukang Feng, kaining Ying, Chuanhao Li, Jiaxin Ai, Yifan Chang, Yifei Huang, Kaipeng Zhang, Yu Dai 핵심 연구 목표 본 논문은 프로그래밍 기술이 없는 비전문가도 텍스트 설명을 통해 실행 및 시각화 가"},{"id":"2026-01-29-Advancing-Open-source-World-Models","title":"[논문리뷰] Advancing Open-source World Models","excerpt":"arXiv에 게시된 'Advancing Open-source World Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-Advancing-Open-source-World-Models","tags":["Review","World Models","Open-source AI","Video Generation","Real-time Simulation","Long-term Memory","Action-Conditioned Learning","Generative Models","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Robbyant Team 핵심 연구 목표 본 논문은 기존 비디오 생성 모델의 한계(데이터 희소성, 장기 일관성 부족, 실시간 상호작용의 어려움, 독점적 솔루션)를 극복하고, 가상 세계의 역학을 학습하며 실시간으로 렌더링할 수 있는 오픈 소스 세계 모델(world model) 인 를 개발하는 것을 목표로 합니다. 이는 "},{"id":"2026-01-29-DeepSeek-OCR-2-Visual-Causal-Flow","title":"[논문리뷰] DeepSeek-OCR 2: Visual Causal Flow","excerpt":"arXiv에 게시된 'DeepSeek-OCR 2: Visual Causal Flow' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-DeepSeek-OCR-2-Visual-Causal-Flow","tags":["Review","OCR","Vision-Language Model","Causal Reasoning","Transformer Architecture","Attention Mechanism","Document Understanding","DeepEncoder"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoran Wei, Yaofeng Sun, Yukun Li 핵심 연구 목표 본 논문은 기존 VisionLanguage Model (VLM) 이 시각 토큰을 고정된 래스터 스캔 순서로 처리하여 인간의 유연한 시각 인지 방식과 상충하는 문제를 해결하고자 합니다. 특히 복잡한 문서 레이아웃에서 발생하는 부적절한 유도 편향"},{"id":"2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection","title":"[논문리뷰] GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection","excerpt":"arXiv에 게시된 'GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection","tags":["Review","Multimodal Sarcasm Detection","Large Language Models","Multimodal LLMs","Discrepancy Modeling","Image Captioning","Gated Fusion","Semantic Incongruity"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuguang Zhang, Junhong Lian, Guoxin Yu, Baoxun Xu, Xiang Ao 핵심 연구 목표 본 논문은 이미지텍스트 쌍에서 풍자(sarcasm)를 효과적으로 탐지하기 위해 기존 방법론의 한계를 극복하는 것을 목표로 합니다. 특히, 기존 모델이 겪는 느슨한 이미지텍스트 관계 나 주관적인 "},{"id":"2026-01-29-Harder-Is-Better-Boosting-Mathematical-Reasoning-via-Difficulty-Aware-GRPO-and-Multi-Aspect-Question-Reformulation","title":"[논문리뷰] Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation","excerpt":"arXiv에 게시된 'Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-Harder-Is-Better-Boosting-Mathematical-Reasoning-via-Difficulty-Aware-GRPO-and-Multi-Aspect-Question-Reformulation","tags":["Review","Reinforcement Learning","Mathematical Reasoning","Difficulty-Aware Optimization","Data Augmentation","Policy Optimization","LLMs","GRPO","MQR"],"text":"링크: 논문 PDF로 바로 열기 저자: Yanqi Dai, Yuxiang Ji, Xiao Zhang, Yong Wang, Xiangxiang Chu, Zhiwu Lu 핵심 연구 목표 대규모 언어 모델(LLMs)의 수학적 추론 능력을 강화하기 위해 기존 RLVR(Reinforcement Learning with Verifiable Rewards) 방법론이 어"},{"id":"2026-01-29-Innovator-VL-A-Multimodal-Large-Language-Model-for-Scientific-Discovery","title":"[논문리뷰] Innovator-VL: A Multimodal Large Language Model for Scientific Discovery","excerpt":"arXiv에 게시된 'Innovator-VL: A Multimodal Large Language Model for Scientific Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-Innovator-VL-A-Multimodal-Large-Language-Model-for-Scientific-Discovery","tags":["Review","Multimodal LLM","Scientific AI","Data Efficiency","Reinforcement Learning","Vision-Language Model","Scientific Reasoning","Reproducible AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han, Junlong Ke, Cong Wang, Yicheng Fu, Jiawang Zhao, Jiangchao Yao, Xi Fang, Zhen Wang, Henxing Cai, Lin Yao, Zhifeng G"},{"id":"2026-01-29-Linear-representations-in-language-models-can-change-dramatically-over-a-conversation","title":"[논문리뷰] Linear representations in language models can change dramatically over a conversation","excerpt":"arXiv에 게시된 'Linear representations in language models can change dramatically over a conversation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-Linear-representations-in-language-models-can-change-dramatically-over-a-conversation","tags":["Review","Language Models","Representation Analysis","Interpretability","In-Context Learning","Representation Dynamics","Factuality","Conversational AI","Activation Steering"],"text":"링크: 논문 PDF로 바로 열기 저자: Andrew Kyle Lampinen, Yuxuan Li, Eghbal Hosseini, Sangnie Bhardwaj, Murray Shanahan 핵심 연구 목표 본 연구는 대규모 언어 모델(LLM) 내에서 선형 표현(Linear representations) , 특히 사실성(factuality)이나 윤리(ethi"},{"id":"2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution","title":"[논문리뷰] OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution","excerpt":"Yusai Zhao이 arXiv에 게시한 'OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution","tags":["Review","GUI Agent","Multimodal AI","MoE","Data Synthesis","Reinforcement Learning","Cross-Platform","Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: Le Zhang, Yixiong Xiao, Xinjiang Lu, Jingjia Cao, Yusai Zhao, Jingbo Zhou†, Lang An, Zikan Feng, Wanxiang Sha, Yu Shi, Congxi Xiao, Jian Xiong, Yankai Zhang, Hua Wu, Haifeng Wang"},{"id":"2026-01-29-RIR-Mega-Speech-A-Reverberant-Speech-Corpus-with-Comprehensive-Acoustic-Metadata-and-Reproducible-Evaluation","title":"[논문리뷰] RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation","excerpt":"mandipgoswami이 arXiv에 게시한 'RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-RIR-Mega-Speech-A-Reverberant-Speech-Corpus-with-Comprehensive-Acoustic-Metadata-and-Reproducible-Evaluation","tags":["Review","Reverberant Speech","Speech Corpus","Acoustic Metadata","Reproducible Research","ASR Evaluation","Room Impulse Response","Speech Recognition"],"text":"링크: 논문 PDF로 바로 열기 저자: Mandip Goswami 핵심 연구 목표 본 논문은 잔향 스피치 처리 분야에서 투명한 음향 메타데이터와 용이한 재현성을 갖춘 표준화된 평가 자원의 부족 문제를 해결하고자 합니다. 궁극적으로 RIRMegaSpeech 라는 잔향 음성 코퍼스를 소개하여 강건한 자동 음성 인식(ASR) 및 반향 제거(dereverberat"},{"id":"2026-01-29-Reinforcement-Learning-via-Self-Distillation","title":"[논문리뷰] Reinforcement Learning via Self-Distillation","excerpt":"arXiv에 게시된 'Reinforcement Learning via Self-Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-Reinforcement-Learning-via-Self-Distillation","tags":["Review","Reinforcement Learning","Self-Distillation","Large Language Models (LLMs)","Rich Feedback","Credit Assignment","Policy Optimization","RLHF","Code Generation","Test-Time Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause 핵심 "},{"id":"2026-01-29-SE-DiCoW-Self-Enrolled-Diarization-Conditioned-Whisper","title":"[논문리뷰] SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper","excerpt":"arXiv에 게시된 'SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-SE-DiCoW-Self-Enrolled-Diarization-Conditioned-Whisper","tags":["Review","Target-Speaker ASR","DiCoW","Whisper Model","Multi-speaker ASR","Self-enrollment","Cross-attention","Speech Diarization"],"text":"링크: 논문 PDF로 바로 열기 저자: Alexander Polok, Dominik Klement, Samuele Cornell, Matthew Wiesner, Jan Černocký, Sanjeev Khudanpur, Lukáš Burget 핵심 연구 목표 본 논문은 DiarizationConditioned Whisper (DiCoW)의 핵심 한계점인 S"},{"id":"2026-01-29-SERA-Soft-Verified-Efficient-Repository-Agents","title":"[논문리뷰] SERA: Soft-Verified Efficient Repository Agents","excerpt":"arXiv에 게시된 'SERA: Soft-Verified Efficient Repository Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-SERA-Soft-Verified-Efficient-Repository-Agents","tags":["Review","Coding Agents","Synthetic Data Generation","Repository Specialization","Supervised Finetuning","Soft Verification","Cost-Efficiency","SWE-bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Ethan Shen, Daniel Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers 핵심 연구 목표 본 논문은 폐쇄형 시스템 대비 오픈 소스 코딩 에이전트의 강점인 사설 코드베이스 특화 능력 을 저비용으로 실현하는 것을 목표로 합니다. 기존 훈련 방식의 높은 비용과 복잡성으로 인해"},{"id":"2026-01-29-Shallow-π-Knowledge-Distillation-for-Flow-based-VLAs","title":"[논문리뷰] Shallow-π: Knowledge Distillation for Flow-based VLAs","excerpt":"arXiv에 게시된 'Shallow-π: Knowledge Distillation for Flow-based VLAs' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-Shallow-π-Knowledge-Distillation-for-Flow-based-VLAs","tags":["Review","Knowledge Distillation","Flow-based VLA","Transformer Compression","Real-time Robotics","Edge AI","Vision-Language-Action Models","Inference Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Boseong Jeon, Yunho Choi, Taehan Kim 핵심 연구 목표 본 논문은 대규모 VisionLanguageAction (VLA) 모델 의 높은 연산 비용으로 인해 엣지 디바이스에서의 실시간 배포가 어려운 문제를 해결하고자 합니다. 특히, VLM 백본과 플로우 기반 액션 헤드 모두에 걸쳐 트랜스포머 "},{"id":"2026-01-29-SketchDynamics-Exploring-Free-Form-Sketches-for-Dynamic-Intent-Expression-in-Animation-Generation","title":"[논문리뷰] SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation","excerpt":"Hongbo Fu이 arXiv에 게시한 'SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-SketchDynamics-Exploring-Free-Form-Sketches-for-Dynamic-Intent-Expression-in-Animation-Generation","tags":["Review","Animation Generation","Free-Form Sketching","Human-AI Interaction","Vision-Language Models (VLMs)","Dynamic Intent Expression","Motion Graphics","Iterative Refinement","Storyboard"],"text":"링크: 논문 PDF로 바로 열기 저자: Boyu Li, LinPing Yuan, Zeyu Wang, Hongbo Fu 핵심 연구 목표 본 논문은 기존 애니메이션 도구가 스케치를 고정된 명령으로 제한하여 자유로운 표현과 인간의 의도 반영에 한계가 있다는 문제를 해결합니다. 자유형 스케치를 통해 동적 의도(dynamic intent)를 효과적으로 포착하고, 이"},{"id":"2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning","title":"[논문리뷰] Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning","excerpt":"Shuai Zhang이 arXiv에 게시한 'Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning","tags":["Review","Agentic AI","Reinforcement Learning","Long-Horizon Tasks","Dynamic Branching","Strategic Exploration","LLM Agents","Sample Efficiency","Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinyang Wu, Shuo Yang, Changpeng Yang, Yuhao Shen, Shuai Zhang, Zhengqi Wen, Jianhua Tao 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반의 에이전트가 장기적인 태스크를 수행할 때 발생하는 비효율적인 탐색 문제를 해결하는 것을 목표로 합니다"},{"id":"2026-01-29-UPLiFT-Efficient-Pixel-Dense-Feature-Upsampling-with-Local-Attenders","title":"[논문리뷰] UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders","excerpt":"arXiv에 게시된 'UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-29 00:00:00+0900+0900","permalink":"/ai/review/2026-01-29-UPLiFT-Efficient-Pixel-Dense-Feature-Upsampling-with-Local-Attenders","tags":["Review","Feature Upsampling","Local Attender","Pixel-Dense Features","Iterative Upsampling","Vision Transformer","Efficiency","Generative AI","Semantic Segmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Matthew Walmer, Saksham Suri, Anirud Aggarwal, Abhinav Shrivastava 핵심 연구 목표 본 연구는 사전 훈련된 비전 백본으로부터 밀도 높은 특징(dense features)을 효율적으로 생성하는 과정에서 발생하는 계산 비용 문제 를 해결하고자 합니다. 기존 교차어텐션 기"},{"id":"2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts","title":"[논문리뷰] AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts","excerpt":"arXiv에 게시된 'AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts","tags":["Review","Long-Context LLMs","Autonomous Agents","Benchmark","Environment Rollouts","State Tracking","Tool Use","Memory Evaluation","Lateral Thinking Puzzles"],"text":"링크: 논문 PDF로 바로 열기 저자: Shicheng Fang, Yuxin Wang, XiaoRan Liu, Jiahao Lu, Chuanyuan Tan, Xinchi Chen, Yining Zheng, Xuanjing Huang, Xipeng Qiu 핵심 연구 목표 이 논문은 동적으로 변화하는 컨텍스트 내에서 장문 컨텍스트 LLM (Large Langu"},{"id":"2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning","title":"[논문리뷰] Beyond Imitation: Reinforcement Learning for Active Latent Planning","excerpt":"Wee Sun Lee이 arXiv에 게시한 'Beyond Imitation: Reinforcement Learning for Active Latent Planning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning","tags":["Review","Large Language Models (LLMs)","Chain-of-Thought (CoT)","Latent Reasoning","Reinforcement Learning (RL)","Variational Autoencoder (VAE)","Active Planning","Numerical Reasoning","Coherence Reward"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhi Zheng, Wee Sun Lee 핵심 연구 목표 이 논문은 기존의 모방 기반 잠재 추론 방식이 여러 동등한 추론 경로 중 하나만을 학습하여 성능 저하 및 훈련테스트 간 격차를 초래하는 문제를 해결하고자 합니다. 최적의 잠재 추론 정책을 달성하기 위해 잠재 토큰 표현 공간에서 능동적인 계획(active plan"},{"id":"2026-01-30-ConceptMoE-Adaptive-Token-to-Concept-Compression-for-Implicit-Compute-Allocation","title":"[논문리뷰] ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation","excerpt":"arXiv에 게시된 'ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-ConceptMoE-Adaptive-Token-to-Concept-Compression-for-Implicit-Compute-Allocation","tags":["Review","MoE","LLMs","Adaptive Compression","Token Merging","Compute Allocation","Efficiency","Vision-Language Models","Continual Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Zihao Huang, Jundong Zhou, Xingwei Qu, Qiyang Min, Ge Zhang 핵심 연구 목표 대규모 언어 모델(LLMs)이 모든 토큰에 균일하게 연산을 할당하여 비효율적인 연산 자원 사용을 초래하는 문제를 해결하는 것이 목표입니다. 본 연구는 의미적으로 유사한 토큰들을 동적으로 개념(co"},{"id":"2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents","title":"[논문리뷰] DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents","excerpt":"arXiv에 게시된 'DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents","tags":["Review","AI Agents","Deep Research","Benchmark","Information Retrieval","Comprehensiveness","Multi-step Reasoning","Evaluation","LLM-as-a-Judge"],"text":"링크: 논문 PDF로 바로 열기 저자: Nikita Gupta, Riju Chatterjee, Lukas Haas, Connie Tao, Andrew Wang, Chang Liu, Hidekazu Oiwa, Elena Gribovskaya, Jan Ackermann, John Blitzer, Sasha Goldshtein, Dipanjan Das 핵심 연구"},{"id":"2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories","title":"[논문리뷰] Discovering Hidden Gems in Model Repositories","excerpt":"Yedid Hoshen이 arXiv에 게시한 'Discovering Hidden Gems in Model Repositories' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories","tags":["Review","Model Discovery","Hidden Gems","Sequential Halving","Multi-Armed Bandit","Model Repositories","Large Language Models","Performance Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen 핵심 연구 목표 본 논문은 대규모 모델 저장소에서 사용자에게 잘 알려지지 않았지만 성능이 뛰어난 \"숨겨진 보석\" 모델들을 효율적으로 발견하는 것을 목표로 합니다. 특히, 현재 모델 사용의 집중이 효율적인 시장 선택의 결과인지, 아니면 우"},{"id":"2026-01-30-DynamicVLA-A-Vision-Language-Action-Model-for-Dynamic-Object-Manipulation","title":"[논문리뷰] DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation","excerpt":"arXiv에 게시된 'DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-DynamicVLA-A-Vision-Language-Action-Model-for-Dynamic-Object-Manipulation","tags":["Review","Vision-Language-Action (VLA) Models","Dynamic Object Manipulation","Robotics","Continuous Inference","Latent-aware Action Streaming","Real-time Control","Perception-Execution Gap"],"text":"링크: 논문 PDF로 바로 열기 저자: Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhong Hong, Haiwen Diao, Ziwei Liu 핵심 연구 목표 기존 VLA 모델들이 정적 객체 조작에서는 강점을 보이지만, 동적 객체 조작 시 빠른 인지(perception) , 시간적 예측(tempor"},{"id":"2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models","title":"[논문리뷰] Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models","excerpt":"arXiv에 게시된 'Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models","tags":["Review","Text-to-Image Models","Spatial Intelligence","Benchmark","Evaluation","Prompt Engineering","Multimodal LLMs","Fine-tuning","Spatial Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu 핵심 연구 목표 현재 TexttoImage (T2I) 모델들이 복잡한 공간 관계(공간 인식, 추론, 상호작용) 처리에서 실패하는 한계를 해결하고, 기존의 짧고 정보 밀도가 낮은 프롬프트 "},{"id":"2026-01-30-Exploring-Reasoning-Reward-Model-for-Agents","title":"[논문리뷰] Exploring Reasoning Reward Model for Agents","excerpt":"Zhixun Li이 arXiv에 게시한 'Exploring Reasoning Reward Model for Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Exploring-Reasoning-Reward-Model-for-Agents","tags":["Review","Agentic Reinforcement Learning","Reward Modeling","Reasoning-aware Feedback","Large Language Models (LLMs)","Multi-modal Agents","Fine-tuning","Critique Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhixun Li, Tianshuo Peng, Manyuan Zhang, Kaituo Feng, bunny127 핵심 연구 목표 기존 에이전트 RL(Agentic Reinforcement Learning) 방법론이 최종 결과 기반의 희소한 보상에 의존하여 중간 추론 과정의 품질을 제대로 반영하지 못하는 문제를 해결합니"},{"id":"2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives","title":"[논문리뷰] Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives","excerpt":"arXiv에 게시된 'Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives","tags":["Review","Autonomous Scientific Discovery","LLM Agents","Knowledge Graph","Pre-computation","Research Pattern","Methodology","Retrieval-Augmented Generation","Review-Guided Refinement"],"text":"링크: 논문 PDF로 바로 열기 저자: Tengyue Xu, Zhuoyang Qian, Gaoge Liu, Li Ling, Zhentao Zhang, Biao Wu, Shuo Zhang, Ke Lu, Wei Shi, Ziqi Wang, Zheng Feng, Yan Luo, Shu Xu, Yongjin Chen, Zhibo Feng, Zhuo Chen, Br"},{"id":"2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience","title":"[논문리뷰] Language-based Trial and Error Falls Behind in the Era of Experience","excerpt":"arXiv에 게시된 'Language-based Trial and Error Falls Behind in the Era of Experience' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience","tags":["Review","Large Language Models","Reinforcement Learning","Exploration Efficiency","Sub-Scale Collaboration","Out-of-Distribution Tasks","Agentic AI","Supervised Fine-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoyu Wang¹ Guozheng Ma¹ Shugang Cui 2 Yilun Kong¹ Haotian Luo Li Shen 3 Mengya Gao 2 Yichao Wu 2 Xiaogang Wang2 Dacheng Tao ¹ 핵심 연구 목표 Large Language Models (LLMs)가 언어 기반이 아닌 새로"},{"id":"2026-01-30-Llama-3-1-FoundationAI-SecurityLLM-Reasoning-8B-Technical-Report","title":"[논문리뷰] Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report","excerpt":"arXiv에 게시된 'Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Llama-3-1-FoundationAI-SecurityLLM-Reasoning-8B-Technical-Report","tags":["Review","Cybersecurity LLM","Reasoning Model","Supervised Fine-Tuning","Reinforcement Learning","Verifiable Rewards","8B Parameters","Open-Source AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam, Paul Kassianik, Sajana Weerawardhena, Anu Vellore, Blaine Nelson, Neusha Javidnia, Arthur Goldblatt, Fraser Bur"},{"id":"2026-01-30-MAD-Modality-Adaptive-Decoding-for-Mitigating-Cross-Modal-Hallucinations-in-Multimodal-Large-Language-Models","title":"[논문리뷰] MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models","excerpt":"Yong Man Ro이 arXiv에 게시한 'MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-MAD-Modality-Adaptive-Decoding-for-Mitigating-Cross-Modal-Hallucinations-in-Multimodal-Large-Language-Models","tags":["Review","Multimodal LLM","Cross-modal Hallucination","Contrastive Decoding","Modality-Adaptive Decoding","Self-Assessment","Audio-Visual Language Model","Training-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Sangyun Chung, Se Yeon Kim, Youngchae Chee, and Yong Man Ro 핵심 연구 목표 본 논문은 Multimodal Large Language Models (MLLMs) 에서 발생하는 교차 모달리티 환각 현상(crossmodal hallucinations) 을 해결하는 것을 목표로"},{"id":"2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods","title":"[논문리뷰] MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods","excerpt":"arXiv에 게시된 'MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods","tags":["Review","Multimodal Reasoning","Data-centric AI","Chain-of-Thought","Large Language Models","Visual Question Answering","STEM Reasoning","Dataset","Fine-tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu 핵심 연구 목표 본 논문은 고품질 추론 데이터의 부족으로 인해 독점 시스템에 비해 뒤처지는 오픈소스 멀티모달 모델의 한계를 "},{"id":"2026-01-30-MetricAnything-Scaling-Metric-Depth-Pretraining-with-Noisy-Heterogeneous-Sources","title":"[논문리뷰] MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources","excerpt":"Jianxun Cui이 arXiv에 게시한 'MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-MetricAnything-Scaling-Metric-Depth-Pretraining-with-Noisy-Heterogeneous-Sources","tags":["Review","Metric Depth Estimation","Pretraining","Foundation Models","Sparse Prompts","Heterogeneous Data","Zero-Shot Learning","Multi-modal Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Xie Yan, Wei Chen 핵심 연구 목표 이 논문은 이질적인 센서 노이즈, 카메라 의존적 편향, 그리고 노이즈가 많은 교차 소스 3D 데이터의 모호성으로 인해 확장이 어려웠던 Me"},{"id":"2026-01-30-OCRVerse-Towards-Holistic-OCR-in-End-to-End-Vision-Language-Models","title":"[논문리뷰] OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models","excerpt":"Liming Zheng이 arXiv에 게시한 'OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-OCRVerse-Towards-Holistic-OCR-in-End-to-End-Vision-Language-Models","tags":["Review","Holistic OCR","Vision-Language Models","Multi-domain Training","Text-centric OCR","Vision-centric OCR","SFT-RL","Code Generation","Document Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng 핵심 연구 목표 본 논문은 기존의 파편화된 OCR 접근법의 한계를 극복하고, 텍스트 중심(Textc"},{"id":"2026-01-30-PLANING-A-Loosely-Coupled-Triangle-Gaussian-Framework-for-Streaming-3D-Reconstruction","title":"[논문리뷰] PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction","excerpt":"arXiv에 게시된 'PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-PLANING-A-Loosely-Coupled-Triangle-Gaussian-Framework-for-Streaming-3D-Reconstruction","tags":["Review","Streaming 3D Reconstruction","Hybrid Representation","Triangle Primitives","Neural Gaussians","Geometric Accuracy","High-Fidelity Rendering","Embodied AI","Monocular SLAM"],"text":"링크: 논문 PDF로 바로 열기 저자: Changjian Jiang, Kerui Ren, Junting Dong, Xudong Li, Kaiwen Song, Linning Xu, Tao Lu, Yu Zhang, Bo Dai, Mulin Yu 핵심 연구 목표 본 논문은 기존의 스트리밍 3D 재구성 방식이 고품질 렌더링과 정확한 기하학적 구조를 동시에 달성하기"},{"id":"2026-01-30-Qwen3-ASR-Technical-Report","title":"[논문리뷰] Qwen3-ASR Technical Report","excerpt":"arXiv에 게시된 'Qwen3-ASR Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Qwen3-ASR-Technical-Report","tags":["Review","ASR","Language Identification","Forced Alignment","Large Audio-Language Models","Multilingual Speech Recognition","Streaming Inference","Qwen3-Omni"],"text":"링크: 논문 PDF로 바로 열기 저자: Qwen Team 핵심 연구 목표 본 논문은 Qwen3ASR 모델 제품군을 소개하며, 기존 ASR 모델의 한계를 넘어선 최첨단 성능과 효율성을 제공하는 것을 목표로 합니다. 특히, 다국어 및 다양한 환경에서의 음성 인식 정확도를 높이고, 혁신적인 LLM 기반 강제 정렬 모델 을 통해 단어/문장 단위의 타임스탬프 예측 "},{"id":"2026-01-30-Scalable-Power-Sampling-Unlocking-Efficient-Training-Free-Reasoning-for-LLMs-via-Distribution-Sharpening","title":"[논문리뷰] Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening","excerpt":"Haitham Bou Ammar이 arXiv에 게시한 'Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Scalable-Power-Sampling-Unlocking-Efficient-Training-Free-Reasoning-for-LLMs-via-Distribution-Sharpening","tags":["Review","LLM Reasoning","Distribution Sharpening","Power Sampling","Training-Free","Monte Carlo Estimation","Jackknife Correction","Autoregressive Generation","Inference Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar 핵심 연구 목표 본 논문의 핵심 목표는 LLM의 추론 성능을 향상시키는 데 사용되는 강화 학습(RL) 기반 후처리 및 MCMC(Markov Chain Monte Carlo) 기반 파워 샘플링 의 높은 계산 "},{"id":"2026-01-30-Scaling-Embeddings-Outperforms-Scaling-Experts-in-Language-Models","title":"[논문리뷰] Scaling Embeddings Outperforms Scaling Experts in Language Models","excerpt":"arXiv에 게시된 'Scaling Embeddings Outperforms Scaling Experts in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Scaling-Embeddings-Outperforms-Scaling-Experts-in-Language-Models","tags":["Review","Embedding Scaling","N-gram Embedding","Mixture-of-Experts (MoE)","Large Language Models (LLMs)","Parameter Efficiency","Inference Optimization","Speculative Decoding"],"text":"링크: 논문 PDF로 바로 열기 저자: Hong Liu, Jiaqi Zhang, Chao Wang, Xing Hu, Linkun Lyu, Jiaqi Sun, Xurui Yang, Bo Wang, Fengcun Li, Yulei Qian, Lingtong Si, Yerui Sun, Rumei Li, Peng Pei, Yuchen Xie, Xunliang Ca"},{"id":"2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models","title":"[논문리뷰] Self-Improving Pretraining: using post-trained models to pretrain better models","excerpt":"arXiv에 게시된 'Self-Improving Pretraining: using post-trained models to pretrain better models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models","tags":["Review","Self-Improving Pretraining","Reinforcement Learning (RL)","Large Language Models (LLMs)","Quality Control","Factuality","Safety","Post-trained Models","Pretraining Data Augmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 생성 안전성, 사실성 및 전반적인 품질 문제를 사전 훈련 단계에서부"},{"id":"2026-01-30-Typhoon-S-Minimal-Open-Post-Training-for-Sovereign-Large-Language-Models","title":"[논문리뷰] Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models","excerpt":"arXiv에 게시된 'Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-Typhoon-S-Minimal-Open-Post-Training-for-Sovereign-Large-Language-Models","tags":["Review","Sovereign LLMs","Post-Training","Instruction Tuning","Supervised Fine-tuning","On-Policy Distillation","Reinforcement Learning","Knowledge Injection","Thai Language"],"text":"링크: 논문 PDF로 바로 열기 저자: Kunat Pipatanakul, Pittawat Taveekitworachai 핵심 연구 목표 본 연구는 제한된 자원과 엄격한 투명성 제약이 있는 환경에서, 지역 또는 국가 기관이 모델 가중치, 훈련 데이터, 배포에 대한 통제력을 유지할 수 있도록 하는 소버린 대규모 언어 모델(LLM) 의 최소한의 공개 포스트 트레"},{"id":"2026-01-30-VTC-R1-Vision-Text-Compression-for-Efficient-Long-Context-Reasoning","title":"[논문리뷰] VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning","excerpt":"arXiv에 게시된 'VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-01-30 00:00:00+0900+0900","permalink":"/ai/review/2026-01-30-VTC-R1-Vision-Text-Compression-for-Efficient-Long-Context-Reasoning","tags":["Review","Vision-Text Compression","Long-Context Reasoning","LLM Efficiency","Vision-Language Models","Iterative Reasoning","Mathematical Problem Solving","Inference Speedup"],"text":"링크: 논문 PDF로 바로 열기 저자: Yibo Wang, Yongcheng Jing, Shunyu Liu, Hao Guan, RongCheng Tu, Dacheng Tao, Chengyu Wang, Jun Huang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 longcontext reasoning 에서 발생하는 심각한 효율성 병목 현상을 해결"},{"id":"2026-02-02-ASTRA-Automated-Synthesis-of-agentic-Trajectories-and-Reinforcement-Arenas","title":"[논문리뷰] ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas","excerpt":"Kaichi Yu이 arXiv에 게시한 'ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-ASTRA-Automated-Synthesis-of-agentic-Trajectories-and-Reinforcement-Arenas","tags":["Review","LLM Agent","Tool Use","Trajectory Synthesis","Reinforcement Learning","Environment Synthesis","Data Generation","Multi-turn Interaction","Automated Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Kaichi Yu, Hao Zhou, Shuaiting Chen, Haotian Wang, Xiaoyu Tian 핵심 연구 목표 논문은 도구증강 언어 모델 에이전트 훈련의 어려움(수동 개입, 검증 불가능한 시뮬레이션 환경, 불안정한 장기/다중 턴 학습)을 해결하기 위해 완전히 자동화된 종단 간 프레임워크 ASTRA 를"},{"id":"2026-02-02-Continual-GUI-Agents","title":"[논문리뷰] Continual GUI Agents","excerpt":"arXiv에 게시된 'Continual GUI Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Continual-GUI-Agents","tags":["Review","Continual Learning","GUI Agents","Reinforcement Learning","Grounding","Domain Adaptation","Resolution Adaptation","Reward Shaping","Human-Computer Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziwei Liu, Borui Kang, Hangjie Yuan, Zixiang Zhao, Wei Li, Yifan Zhu, Tao Feng 핵심 연구 목표 본 연구는 GUI(Graphical User Interface) 에이전트가 새로운 도메인이나 해상도 변화와 같은 동적인 디지털 환경(데이터 분포의 변화)에서 성능"},{"id":"2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation","title":"[논문리뷰] DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation","excerpt":"Jong Chul Ye이 arXiv에 게시한 'DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation","tags":["Review","Autoencoder","DINO","Vision Foundation Models","Image Generation","Image Reconstruction","Spherical Manifold","Diffusion Models","Flow Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Hun Chang, Byunghee Cha, Jong Chul Ye 핵심 연구 목표 본 연구는 사전 훈련된 Vision Foundation Model (VFM) 기반의 생성형 오토인코더가 겪는 낮은 재구성 충실도(fidelity) 문제를 해결하고, 동시에 효율적인 이미지 생성 능력을 유지하는 것을 목표로 합니다. 특히"},{"id":"2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience","title":"[논문리뷰] Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience","excerpt":"arXiv에 게시된 'Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience","tags":["Review","Deep Search Agent","Meta-Cognitive Monitoring","Hierarchical Monitoring","Large Language Models","Cognitive Neuroscience","Uncertainty Calibration"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongxiang Sun, Qipeng Wang, Weijie Yu, Jingxuan Yang, Haolang Lu, Jun Xu 핵심 연구 목표 대규모 언어 모델(LLM) 기반 딥 서치 에이전트가 다단계 태스크 수행 중 추론 및 검색 상태를 모니터링하고 조절하는 메커니즘이 부족하여 발생하는 체계적인 실패 문제를 해"},{"id":"2026-02-02-DenseGRPO-From-Sparse-to-Dense-Reward-for-Flow-Matching-Model-Alignment","title":"[논문리뷰] DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment","excerpt":"arXiv에 게시된 'DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-DenseGRPO-From-Sparse-to-Dense-Reward-for-Flow-Matching-Model-Alignment","tags":["Review","Reinforcement Learning","Flow Matching Models","Dense Reward","Sparse Reward Problem","Preference Alignment","SDE Sampler","GRPO","Text-to-Image Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Haoyou Deng, Keyu Yan, Chaojie Mao, Xiang Wang, Yu Liu, Changxin Gao, Nong Sang 핵심 연구 목표 본 논문은 Flow Matching Model 의 인간 선호도 정렬 과정에서 발생하는 희소 보상(Sparse Reward) 문제 를 해결하는 것을 목표로 합니다"},{"id":"2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","title":"[논문리뷰] DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning","excerpt":"arXiv에 게시된 'DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","tags":["Review","Character Animation","Image Animation","Spatiotemporal Learning","In-Context Learning","Diffusion Models","Motion Transfer","Generalization","Video Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingshuang Luo, Shuang Liang, Zhengkun Rong, Yuxuan Luo, Tianshu Hu, Ruibing Hou, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao 핵심 연구 목표 본 논문은 정적 이미지에 운전 비디오의 움직임을 적용하여 고품질 애니메이션"},{"id":"2026-02-02-FourierSampler-Unlocking-Non-Autoregressive-Potential-in-Diffusion-Language-Models-via-Frequency-Guided-Generation","title":"[논문리뷰] FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation","excerpt":"arXiv에 게시된 'FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-FourierSampler-Unlocking-Non-Autoregressive-Potential-in-Diffusion-Language-Models-via-Frequency-Guided-Generation","tags":["Review","Diffusion Language Models","Non-Autoregressive Generation","Frequency Domain Analysis","Decoding Strategy","Structure-to-Detail","Fourier Transform","Text Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Siyang He, Qiqi Wang, Xiaoran Liu, Hongnan Ma, Yiwei Shi, Yuerong Song, Ying Zhu, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu 핵심 연구 목표 본 논문은 확산 언어 모델(dLLMs)의 비자기회귀적 잠재력을 완"},{"id":"2026-02-02-Latent-Chain-of-Thought-as-Planning-Decoupling-Reasoning-from-Verbalization","title":"[논문리뷰] Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization","excerpt":"arXiv에 게시된 'Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Latent-Chain-of-Thought-as-Planning-Decoupling-Reasoning-from-Verbalization","tags":["Review","Latent Reasoning","Chain-of-Thought (CoT)","Large Language Models (LLMs)","Planning","Reinforcement Learning","Mathematical Reasoning","Decoupling","Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiecong Wang, Hao Peng, Chunyang Liu 핵심 연구 목표 논문은 LLM의 CoT(ChainofThought) 추론 이 가진 높은 연산 비용과 이산 토큰 샘플링으로 인한 추론 경로 붕괴 문제를 해결하고자 합니다. 궁극적으로 추론과 언어화 과정을 분리하여 연속적인 잠재 공간에서 계획(plannin"},{"id":"2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning","title":"[논문리뷰] MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning","excerpt":"Yuxin Chen이 arXiv에 게시한 'MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning","tags":["Review","Long-Horizon Reasoning","Multimodal Memory","Visual Layout","Adaptive Information Density","Reinforcement Learning","Context Window","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yaorui Shi, Shugui Liu, Yu Yang, Wenyu Mao, Yuxin Chen, Qi Gu, Hui Su, Xunliang Cai, Xiang Wang, An Zhang 핵심 연구 목표 본 논문은 LLM 기반 에이전트의 장기적 추론 시 발생하는 제한된 컨텍스트 창 문제를 해결하는 것을 목표로 합니다"},{"id":"2026-02-02-PaddleOCR-VL-1-5-Towards-a-Multi-Task-0-9B-VLM-for-Robust-In-the-Wild-Document-Parsing","title":"[논문리뷰] PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing","excerpt":"Zelun Zhang이 arXiv에 게시한 'PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-PaddleOCR-VL-1-5-Towards-a-Multi-Task-0-9B-VLM-for-Robust-In-the-Wild-Document-Parsing","tags":["Review","Document Parsing","Visual Language Model (VLM)","Robustness","Multi-task Learning","Layout Analysis","OCR","Real-world Scenarios","Parameter Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Yi Liu, Dianhai Yu, Yanjun Ma"},{"id":"2026-02-02-PaperBanana-Automating-Academic-Illustration-for-AI-Scientists","title":"[논문리뷰] PaperBanana: Automating Academic Illustration for AI Scientists","excerpt":"arXiv에 게시된 'PaperBanana: Automating Academic Illustration for AI Scientists' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-PaperBanana-Automating-Academic-Illustration-for-AI-Scientists","tags":["Review","Automated Illustration Generation","Agentic Framework","Vision-Language Model","Image Generation","Methodology Diagrams","Statistical Plots","Academic Publishing","Iterative Refinement"],"text":"링크: 논문 PDF로 바로 열기 저자: Dawei Zhu, Rui Meng, Yale Song, Xiyu Wei, Sujian Li, Tomas Pfister, Jinsung Yoon 핵심 연구 목표 AI 과학자들을 위한 학술 출판용 일러스트레이션(방법론 다이어그램 및 통계 플롯) 생성 과정의 노동 집약적인 병목 현상을 해소하고 자동화하는 것을 목표로 합니"},{"id":"2026-02-02-Pushing-the-Boundaries-of-Natural-Reasoning-Interleaved-Bonus-from-Formal-Logic-Verification","title":"[논문리뷰] Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification","excerpt":"arXiv에 게시된 'Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Pushing-the-Boundaries-of-Natural-Reasoning-Interleaved-Bonus-from-Formal-Logic-Verification","tags":["Review","LLM Reasoning","Formal Verification","Neuro-Symbolic AI","Reinforcement Learning","Supervised Fine-tuning","Logic Consistency","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Chuxue Cao, Jinluan Yang, Haoran Li, Kunhao Pan, Zijian Zhao, Zhengyu Chen, Yuchen Tian, Lijun Wu, Conghui He, Sirui Han, Yike Guo 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)의 확률적 토큰 예측 과정에서 "},{"id":"2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation","title":"[논문리뷰] RM -RF: Reward Model for Run-Free Unit Test Evaluation","excerpt":"Vadim Alperovich이 arXiv에 게시한 'RM -RF: Reward Model for Run-Free Unit Test Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation","tags":["Review","Unit Test Generation","Reward Model","Reinforcement Learning","Code Coverage","Mutation Testing","Large Language Models","Run-Free Evaluation","Software Engineering Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: Elena Bruches, Daniil Grebenkin, Mikhail Klementev, Vadim Alperovich, Roman Derunets, Dari Baturova, Georgy Mkrtchyan, Oleg Sedukhin, Ivan Bondarenko, Nikolay Bushkov, Stanislav "},{"id":"2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought","title":"[논문리뷰] ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought","excerpt":"Zhifeng Gao이 arXiv에 게시한 'ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought","tags":["Review","Latent Reasoning","Chain-of-Thought","Variational Autoencoder","Visual-Text Compression","LLMs","Multi-modal Reasoning","Computational Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Fanmeng Wang, Haotian Liu, Guojiang Zhao, Hongteng Xu, Zhifeng Gao 핵심 연구 목표 본 연구는 LLM의 ChainofThought (CoT) 추론 과정에서 발생하는 높은 계산 비용 과 추론 비효율성 을 해결하고자 합니다. 특히, 기존의 잠재 추론(latent reas"},{"id":"2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality","title":"[논문리뷰] Revisiting Diffusion Model Predictions Through Dimensionality","excerpt":"Chaoyang Wang이 arXiv에 게시한 'Revisiting Diffusion Model Predictions Through Dimensionality' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality","tags":["Review","Diffusion Models","Prediction Target","Dimensionality","Latent Space","Pixel Space","Generative Models","Theoretical Analysis","k-Diff"],"text":"링크: 논문 PDF로 바로 열기 저자: Qing Jin, Chaoyang Wang 핵심 연구 목표 확산 모델(Diffusion Models)에서 데이터의 내재적 차원(intrinsic dimension) 과 주변 차원(ambient dimension) 에 따라 최적의 예측 대상(prediction target: ε, v, x)이 달라지는 현상에 대한 정량적"},{"id":"2026-02-02-Robust-Tool-Use-via-Fission-GRPO-Learning-to-Recover-from-Execution-Errors","title":"[논문리뷰] Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors","excerpt":"Bin Liang이 arXiv에 게시한 'Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Robust-Tool-Use-via-Fission-GRPO-Learning-to-Recover-from-Execution-Errors","tags":["Review","Tool Use","Execution Errors","Error Recovery","Reinforcement Learning","LLMs","Agentic AI","GRPO","FISSION"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, KamFai Wong 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs), 특히 소형 LLMs가 다중 턴 도구 실행에서 발생하는 실행 오류로부터"},{"id":"2026-02-02-Routing-the-Lottery-Adaptive-Subnetworks-for-Heterogeneous-Data","title":"[논문리뷰] Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data","excerpt":"Michal Byra이 arXiv에 게시한 'Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Routing-the-Lottery-Adaptive-Subnetworks-for-Heterogeneous-Data","tags":["Review","Pruning","Lottery Ticket Hypothesis","Adaptive Subnetworks","Heterogeneous Data","Model Efficiency","Conditional Computation","Subnetwork Collapse"],"text":"링크: 논문 PDF로 바로 열기 저자: Grzegorz Stefański, Alberto Presta, Michał Byra 핵심 연구 목표 본 논문은 Lottery Ticket Hypothesis (LTH) 가 하나의 보편적인 \"winning ticket\"을 가정하여 실제 데이터의 내재된 이질성을 간과하는 한계를 해결하고자 합니다. 데이터의 특성에 따라 "},{"id":"2026-02-02-SSL-Sweet-Spot-Learning-for-Differentiated-Guidance-in-Agentic-Optimization","title":"[논문리뷰] SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization","excerpt":"Bolin Ni이 arXiv에 게시한 'SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-SSL-Sweet-Spot-Learning-for-Differentiated-Guidance-in-Agentic-Optimization","tags":["Review","Reinforcement Learning","Reward Shaping","Agent Optimization","GUI Automation","Complex Reasoning","Sample Efficiency","Tiered Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinyang Wu, Changpeng Yang, Yuhao Shen, Fangzhi Xu, Bolin Ni 핵심 연구 목표 본 논문은 검증 가능한 보상 기반 강화 학습(RLVR)에서 이진 보상(binary rewards) 의 한계(최적화 모호성, 학습 비효율성, 정책 취약성)를 해결하고자 합니다. 동일한 결과에 도달"},{"id":"2026-02-02-Statistical-Estimation-of-Adversarial-Risk-in-Large-Language-Models-under-Best-of-N-Sampling","title":"[논문리뷰] Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling","excerpt":"arXiv에 게시된 'Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-Statistical-Estimation-of-Adversarial-Risk-in-Large-Language-Models-under-Best-of-N-Sampling","tags":["Review","LLM Safety","Adversarial Robustness","Best-of-N Sampling","Statistical Estimation","Beta-Binomial Model","Jailbreak","Risk Amplification"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingqian Feng, Xiaodong Liu, Weiwei Yang, Chenliang Xu, Christopher White, Jianfeng Gao 핵심 연구 목표 이 논문은 대규모 언어 모델(LLMs)의 안전성 평가가 단일 시도(singleshot) 또는 저예산 공격에만 초점을 맞춰 실제 위협을 과소평가하는"},{"id":"2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance","title":"[논문리뷰] TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance","excerpt":"Daniil Grebenkin이 arXiv에 게시한 'TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance","tags":["Review","LLM","Unit Test Maintenance","Software Engineering","Code Generation","Test Repair","Test Updating","Benchmark","Mutation Testing","Code Coverage"],"text":"링크: 논문 PDF로 바로 열기 저자: Elena Bruches, Vadim Alperovich, Dari Baturova, Roman Derunets, Daniil Grebenkin, Georgy Mkrtchyan, Oleg Sedukhin, Mikhail Klementev, Ivan Bondarenko, Nikolay Bushkov, Stanislav "},{"id":"2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models","title":"[논문리뷰] THINKSAFE: Self-Generated Safety Alignment for Reasoning Models","excerpt":"Minki Kang이 arXiv에 게시한 'THINKSAFE: Self-Generated Safety Alignment for Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models","tags":["Review","Large Reasoning Models","Safety Alignment","Self-Distillation","Refusal Steering","Distributional Shift","Chain-of-Thought","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Seanie Lee, Sangwoo Park, Yumin Choi, Gyeongman Kim, Minki Kang, Jihun Yun, Dongmin Park, Jongho Park, Sung Ju Hwang 핵심 연구 목표 본 논문은 강화 학습(RL) 기반의 추론 모델들이 복잡한 추론 태스크에서 성능을 극대화하는 과"},{"id":"2026-02-02-TTCS-Test-Time-Curriculum-Synthesis-for-Self-Evolving","title":"[논문리뷰] TTCS: Test-Time Curriculum Synthesis for Self-Evolving","excerpt":"Chengsong Huang이 arXiv에 게시한 'TTCS: Test-Time Curriculum Synthesis for Self-Evolving' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-02 00:00:00+0900+0900","permalink":"/ai/review/2026-02-02-TTCS-Test-Time-Curriculum-Synthesis-for-Self-Evolving","tags":["Review","Test-Time Training","Self-Evolving LLMs","Curriculum Learning","Reinforcement Learning","Question Synthesis","Mathematical Reasoning","GRPO"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengyi Yang, Zhishang Xiang, Yunbo Tang, Zongpei Teng, Chengsong Huang, Fei Long, Yuhan Liu, Jinsong Su 핵심 연구 목표 TTCS는 대규모 언어 모델(LLM)이 테스트 질문만 사용하여 추론 능력을 향상시키는 기존 TestTime Trai"},{"id":"2026-02-03-Beyond-Pixels-Visual-Metaphor-Transfer-via-Schema-Driven-Agentic-Reasoning","title":"[논문리뷰] Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning","excerpt":"arXiv에 게시된 'Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Beyond-Pixels-Visual-Metaphor-Transfer-via-Schema-Driven-Agentic-Reasoning","tags":["Review","Visual Metaphor Transfer","Conceptual Blending Theory","Schema Grammar","Multi-Agent Framework","Generative AI","VLM","LLM","Creative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Xu, Yuxin Zhang, Juan Cao, Lin Gao, Chunyu Wang, Oliver Deussen, TongYee Lee, Fan Tang 핵심 연구 목표 본 논문은 기존 생성형 AI 모델이 픽셀 수준의 지침과 표면적 외관 유지에만 머물러 진정한 은유적 생성을 위한 추상적 논리를 포착하지 못하는 "},{"id":"2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation","title":"[논문리뷰] Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation","excerpt":"arXiv에 게시된 'Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation","tags":["Review","Autoregressive Video Generation","Diffusion Models","Model Distillation","Real-Time AI","Causal Attention","ODE Distillation","Frame-level Injectivity","Teacher Forcing"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongzhou Zhu, Min Zhao, Guande He, Hang Su, Chongxuan Li, Jun Zhu 핵심 연구 목표 실시간 상호작용 비디오 생성을 위해 기존의 양방향(bidirectional) 비디오 확산 모델을 소수 스텝의 자기회귀(autoregressive, AR) 모델로 증류하는 과정에서 발생하"},{"id":"2026-02-03-Closing-the-Loop-Universal-Repository-Representation-with-RPG-Encoder","title":"[논문리뷰] Closing the Loop: Universal Repository Representation with RPG-Encoder","excerpt":"Steven Liu이 arXiv에 게시한 'Closing the Loop: Universal Repository Representation with RPG-Encoder' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Closing-the-Loop-Universal-Repository-Representation-with-RPG-Encoder","tags":["Review","Code Representation","LLM Agent","Software Engineering AI","Repository Understanding","Repository Generation","Repository Planning Graph (RPG)","Semantic Lifting","Incremental Code Maintenance"],"text":"링크: 논문 PDF로 바로 열기 저자: Jane Luo, Chengyu Yin, Xin Zhang, Qingtao Li, Steven Liu, Yiming Huang, Jie Wu, Hao Liu, Yangyu Huang, Yu Kang, Fangkai Yang, Ying Xin, Scarlett Li 상세 요약 핵심 연구 목표 현재 리포지토리 에이전트들이"},{"id":"2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents","title":"[논문리뷰] FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents","excerpt":"arXiv에 게시된 'FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents","tags":["Review","LLM Agents","Deep Research","Long-Horizon Tasks","Test-Time Scaling","File System","Persistent Workspace","Knowledge Base","Dual-Agent Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Chiwei Zhu, Benfeng Xu, Mingxuan Du, Shaohan Wang, Xiaorui Wang, Zhendong Mao, Yongdong Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트가 긴 호라이즌의 심층 연구 작업을 수행할 때 발생하는 컨텍스트 창 제한 문제를 해결하고자"},{"id":"2026-02-03-FSVideo-Fast-Speed-Video-Diffusion-Model-in-a-Highly-Compressed-Latent-Space","title":"[논문리뷰] FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space","excerpt":"arXiv에 게시된 'FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-FSVideo-Fast-Speed-Video-Diffusion-Model-in-a-Highly-Compressed-Latent-Space","tags":["Review","Video Diffusion Model","Image-to-Video Generation","Latent Space Compression","Diffusion Transformer (DiT)","Model Acceleration","Layer Memory","Video Upsampling"],"text":"링크: 논문 PDF로 바로 열기 저자: FSVideo Team, Intelligent Creation, ByteDance 핵심 연구 목표 본 논문은 기존 비디오 확산 모델의 높은 추론 비용으로 인한 긴 대기 시간과 GPU 비용 문제를 해결하여, 더욱 빠르고 효율적인 비디오 생성을 가능하게 하는 고속 이미지투비디오 (I2V) 확산 프레임워크인 FSVideo "},{"id":"2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots","title":"[논문리뷰] Green-VLA: Staged Vision-Language-Action Model for Generalist Robots","excerpt":"arXiv에 게시된 'Green-VLA: Staged Vision-Language-Action Model for Generalist Robots' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots","tags":["Review","Vision-Language-Action","Generalist Robots","Staged Training","Reinforcement Learning","Multi-embodiment","Data Quality","Humanoid Robotics","Flow Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Sber Robotics Center 핵심 연구 목표 본 논문은 로봇 학습의 고질적인 문제인 데이터의 이질성, 낮은 품질, 그리고 행동 모방 (Behavior Cloning, BC)의 장기 태스크 한계를 해결하고자 합니다. 이를 위해 다양한 로봇 플랫폼과 환경에 걸쳐 일반화되고 견고하며 효율적인 로봇 정책을 개발하는 "},{"id":"2026-02-03-How-Well-Do-Models-Follow-Visual-Instructions-VIBE-A-Systematic-Benchmark-for-Visual-Instruction-Driven-Image-Editing","title":"[논문리뷰] How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing","excerpt":"Haochen Tian이 arXiv에 게시한 'How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-How-Well-Do-Models-Follow-Visual-Instructions-VIBE-A-Systematic-Benchmark-for-Visual-Instruction-Driven-Image-Editing","tags":["Review","Visual Instruction","Image Editing","Multimodal Benchmark","LMM-as-a-judge","Deictic Grounding","Morphological Manipulation","Causal Reasoning","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Huanyu Zhang, Xuehai Bai, Chengzu Li, Chen Liang, Haochen Tian, Haodong Li, Ruichuan An, Yifan Zhang, Anna Korhonen, Zhang Zhang, Liang Wang, Tieniu Tan 핵심 연구 목표 이 논문은 기존의 텍스트 기반"},{"id":"2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence","title":"[논문리뷰] Kimi K2.5: Visual Agentic Intelligence","excerpt":"arXiv에 게시된 'Kimi K2.5: Visual Agentic Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence","tags":["Review","Multimodal AI","Agentic Intelligence","Vision-Language Models","Parallel Agent Orchestration","Reinforcement Learning","Joint Optimization","Visual Reasoning","Software Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Kimi Team 핵심 연구 목표 본 논문은 일반 에이전트 지능(general agentic intelligence)을 발전시키기 위해 오픈소스 멀티모달 에이전트 모델 Kimi K2.5 를 소개합니다. 기존 순차적 에이전트 실행의 한계인 높은 지연 시간과 확장성 문제 를 해결하고, 텍스트와 비전 모달리티 간의 시너지 "},{"id":"2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars","title":"[논문리뷰] Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars","excerpt":"Teng Hu이 arXiv에 게시한 'Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars","tags":["Review","Talking Avatars","Human-Object Interaction (HOI)","Text-Driven Generation","Diffusion Models","Multimodal Control","Grounded Interaction"],"text":"링크: 논문 PDF로 바로 열기 저자: Teng Hu, Ziyao Huang, Zhentao Yu, Zhengguang Zhou, youliang1233214 핵심 연구 목표 본 논문은 기존 토킹 아바타 기술의 한계인 환경 인지 및 텍스트 기반 객체 상호작용 능력 부재 문제를 해결하고자 합니다. 특히, 아바타가 주변 객체와 상황에 맞게 텍스트 지시에 따라 "},{"id":"2026-02-03-Mind-Brush-Integrating-Agentic-Cognitive-Search-and-Reasoning-into-Image-Generation","title":"[논문리뷰] Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation","excerpt":"Chenjue Zhang이 arXiv에 게시한 'Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Mind-Brush-Integrating-Agentic-Cognitive-Search-and-Reasoning-into-Image-Generation","tags":["Review","Agentic Text-to-Image","Multimodal Reasoning","Cognitive Search","Knowledge-Driven Generation","Image Generation Benchmarks","Complex User Intent"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenjue Zhang, Dongzhi Jiang, Junyan Ye, Jun He, SereinH 핵심 연구 목표 기존 텍스트이미지(T2I) 모델의 한계인 정적인 동작, 암묵적인 사용자 의도 파악 실패, 복잡한 지식 기반 추론 능력 부족을 해결하는 것입니다. 이를 위해 능동적인 지식 습득과 추론을 통해 사용자의 인"},{"id":"2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards","title":"[논문리뷰] PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards","excerpt":"arXiv에 게시된 'PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards","tags":["Review","Text-to-Video Generation","Post-Training","Optimal Transport","Reward Modeling","Annotation-free","Vision-Language Models","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: MinhQuan Le, Gaurav Mittal, Cheng Zhao, David Gu, Dimitris Samaras, Mei Chen 핵심 연구 목표 기존 annotationfree T2V 후처리 학습 방식이 사전 훈련된 VisionLanguage Models (VLMs) 의 정렬되지 않은 임베딩에 의존하여 최적의"},{"id":"2026-02-03-PixelGen-Pixel-Diffusion-Beats-Latent-Diffusion-with-Perceptual-Loss","title":"[논문리뷰] PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss","excerpt":"arXiv에 게시된 'PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-PixelGen-Pixel-Diffusion-Beats-Latent-Diffusion-with-Perceptual-Loss","tags":["Review","Pixel Diffusion","Perceptual Loss","Latent Diffusion","Image Generation","LPIPS","DINOv2","x-prediction","End-to-End Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zehong Ma, Ruihan Xu, Shiliang Zhang 핵심 연구 목표 본 논문은 기존 픽셀 확산 모델이 고차원 픽셀 공간의 지각적으로 중요하지 않은 신호를 학습하는 데 어려움을 겪어 잠재 확산 모델보다 성능이 뒤처지는 문제를 해결하고자 합니다. VAE 기반의 2단계 잠재 확산 모델에서 발생하는 아티팩트와 "},{"id":"2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System","title":"[논문리뷰] RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System","excerpt":"arXiv에 게시된 'RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System","tags":["Review","Reinforcement Learning","Large Language Models","Agentic AI","Reward Modeling","Environment Adaptation","Closed-loop Optimization","Multimodal Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang 핵심 연구 목표 본 논문은 LLM 및 에이전트 시나리오에서 학습 신호를 증폭하고 전체 RL 시스템을 강화하기 위해 환경, 정책, 보상 모델을 닫힌 루프(closedloop) 최적화 를 통해 동적으로 구축하는 RLAn"},{"id":"2026-02-03-SLIME-Stabilized-Likelihood-Implicit-Margin-Enforcement-for-Preference-Optimization","title":"[논문리뷰] SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization","excerpt":"arXiv에 게시된 'SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-SLIME-Stabilized-Likelihood-Implicit-Margin-Enforcement-for-Preference-Optimization","tags":["Review","Preference Optimization","LLM Alignment","Direct Preference Optimization","Reference-Free","Likelihood Anchoring","Token Stabilization","Dual-Margin Loss","Unlearning"],"text":"링크: 논문 PDF로 바로 열기 저자: Maksim Afanasyev, Illarion Iov 핵심 연구 목표 기존 선호도 최적화 방법론, 특히 DPO 및 SimPO 가 겪는 \"언러닝(unlearning)\"과 \"포맷팅 붕괴(formatting collapse)\" 문제를 해결하는 것이 주 목표입니다. 이들 방법론은 선택된 응답의 절대적인 품질 저하 없이 상대"},{"id":"2026-02-03-SPARKLING-Balancing-Signal-Preservation-and-Symmetry-Breaking-for-Width-Progressive-Learning","title":"[논문리뷰] SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning","excerpt":"arXiv에 게시된 'SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-SPARKLING-Balancing-Signal-Preservation-and-Symmetry-Breaking-for-Width-Progressive-Learning","tags":["Review","Progressive Learning","Width Expansion","Signal Preservation","Symmetry Breaking","LLM","Training Stability","MoE","RMSNorm"],"text":"링크: 논문 PDF로 바로 열기 저자: Qifan Yu, Xinyu Ma, Zhijian Zhuo, Minrui Wang, Deyi Liu, Shiyi Zhan, Yiyuan Ma, Liang Xiang, Xingyan Bin, Di He 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 사전 훈련 비용을 절감하기 위한 점진적 학습(Progressiv"},{"id":"2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions","title":"[논문리뷰] SWE-Universe: Scale Real-World Verifiable Environments to Millions","excerpt":"arXiv에 게시된 'SWE-Universe: Scale Real-World Verifiable Environments to Millions' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions","tags":["Review","Software Engineering Environments","LLM Agents","Data Generation","Verifiable Tasks","Multilingual","Reinforcement Learning","Self-Verification","Hacking Detection"],"text":"링크: 논문 PDF로 바로 열기 저자: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingr"},{"id":"2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model","title":"[논문리뷰] Toward Cognitive Supersensing in Multimodal Large Language Model","excerpt":"Yifan Xu이 arXiv에 게시한 'Toward Cognitive Supersensing in Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model","tags":["Review","Multimodal Large Language Models","Cognitive Reasoning","Visual Imagery","Latent Representations","Reinforcement Learning","Visual Question Answering","Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Boyi Li, Yifan Shen, Yuanzhe Liu, Yifan Xu, Jiateng Liu, Xinzhuo Li, Zhengyuan Li, Jingyuan Zhu, Yunhan Zhong, Fangzhou Lan, Jianguo Cao, James M. Rehg, Heng Ji, Ismini Lourentzo"},{"id":"2026-02-03-UniReason-1-0-A-Unified-Reasoning-Framework-for-World-Knowledge-Aligned-Image-Generation-and-Editing","title":"[논문리뷰] UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing","excerpt":"Size Wu이 arXiv에 게시한 'UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-UniReason-1-0-A-Unified-Reasoning-Framework-for-World-Knowledge-Aligned-Image-Generation-and-Editing","tags":["Review","Multimodal Reasoning","Image Generation","Image Editing","World Knowledge","Self-Reflection","Unified Framework","Text-to-Image"],"text":"링크: 논문 PDF로 바로 열기 저자: Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, CodeGoat24 핵심 연구 목표 본 논문은 복잡한 추론과 세계 지식이 필요한 이미지 합성 태스크에서 기존 통합 멀티모달 모델의 한계를 해결하고자 합니다. 특히, 텍스트투이미지(T2I) 생성과 이미지 편집을 별개의 작업이 아닌 상호 연결된"},{"id":"2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models","title":"[논문리뷰] Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models","excerpt":"Shuang Chen이 arXiv에 게시한 'Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models","tags":["Review","Multimodal Large Language Models","Visual Question Answering","Deep Research","Benchmark","Visual Search","Textual Search","Cropped Search","Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yu Zeng, Wenxuan Huang, Zhen Fang, Shuang Chen, Yufan Shen, Yishuo Cai, Xiaoman Wang, Zhenfei Yin, Lin Chen, Zehui Chen, Shiting Huang, Yiming Zhao, Yao Hu, Philip Torr, Wanli Ou"},{"id":"2026-02-03-Vision-DeepResearch-Incentivizing-DeepResearch-Capability-in-Multimodal-Large-Language-Models","title":"[논문리뷰] Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models","excerpt":"Zhen Fang이 arXiv에 게시한 'Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Vision-DeepResearch-Incentivizing-DeepResearch-Capability-in-Multimodal-Large-Language-Models","tags":["Review","Multimodal Large Language Models","Deep Research","Agentic AI","Tool Use","Visual Question Answering","Reinforcement Learning","Multi-scale Search"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Lin Chen 외 핵심 연구 목표 본 논문은 기존 멀티모달 딥 리서치 MLLM들이 겪는 히트율 문제(검색 엔진의 노이즈와 불안정성) 및 제한된 추론 깊이/검색 폭 문제를 해결하고자 합니다. 특히, 단일 이미지 쿼리나 짧은 텍스트 "},{"id":"2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles","title":"[논문리뷰] Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles","excerpt":"arXiv에 게시된 'Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles","tags":["Review","Deep Research Agents","LLM Evaluation","Wikipedia","Good Articles","Factuality","Writing Quality","Benchmark","Hallucinations","Verifiability"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaohan Wang, Benfeng Xu, Licheng Zhang, Mingxuan Du, Chiwei Zhu, Xiaorui Wang, Zhendong Mao, Yongdong Zhang 핵심 연구 목표 현재 Deep Research Agents (DRAs) 의 평가 방식이 LLM 생성 참조 나 단순한 평가 기"},{"id":"2026-02-03-WildGraphBench-Benchmarking-GraphRAG-with-Wild-Source-Corpora","title":"[논문리뷰] WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora","excerpt":"arXiv에 게시된 'WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-03 00:00:00+0900+0900","permalink":"/ai/review/2026-02-03-WildGraphBench-Benchmarking-GraphRAG-with-Wild-Source-Corpora","tags":["Review","GraphRAG","Benchmarking","Retrieval-Augmented Generation","Wild-Source Corpora","Multi-document Aggregation","Heterogeneous Data","Wikipedia","Long-Context Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengyu Wang, Benfeng Xu, Licheng Zhang, Shaohan Wang, Mingxuan Du, Chiwei Zhu, Zhendong Mao 핵심 연구 목표 기존 GraphRAG 벤치마크들이 짧고 정제된 문단에 의존하여 실제와 같은 긴 컨텍스트 및 대규모 이질적 문서 환경에서의 성능 평가에 미흡"},{"id":"2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation","title":"[논문리뷰] 3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation","excerpt":"arXiv에 게시된 '3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation","tags":["Review","Human Video Generation","3D-Aware","Implicit Motion Control","View-Adaptive","Diffusion Models","Motion Encoder","Text-Guided Camera Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhixue Fang, Xu He, Songlin Tang, Haoxian Zhang, Qingfeng Li, Xiaoqiang Liu, Pengfei Wan, Kun Gai 핵심 연구 목표 본 논문은 2D driving video로부터 3D 모션을 충실히 재현하면서도 유연한 텍스트 기반 카메라 제어를 지원하는 3Da"},{"id":"2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration","title":"[논문리뷰] AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration","excerpt":"Zhaoyang Yu이 arXiv에 게시한 'AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration","tags":["Review","Agentic Orchestration","Sub-Agent Creation","Language Agents","Dynamic Specialization","Context Management","Tool Use","Large Language Models","Cost-Performance Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoyang Yu, Fashen Ren, Yiran Peng, Zhihao Xu, Jianhao Ruan 핵심 연구 목표 본 논문은 복잡하고 장기적인 AI 태스크를 해결하기 위한 에이전트 시스템에서 동적인 서브 에이전트 생성 및 관리의 한계 를 극복하고자 합니다. 기존 서브 에이전트 접근 방식이 유연성이 부족하고 "},{"id":"2026-02-04-AdaptMMBench-Benchmarking-Adaptive-Multimodal-Reasoning-for-Mode-Selection-and-Reasoning-Process","title":"[논문리뷰] AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process","excerpt":"Shilin Yan이 arXiv에 게시한 'AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-AdaptMMBench-Benchmarking-Adaptive-Multimodal-Reasoning-for-Mode-Selection-and-Reasoning-Process","tags":["Review","Multimodal Reasoning","Adaptive Learning","Vision-Language Models (VLMs)","Benchmarking","Mode Selection","Tool Learning","Reasoning Process Evaluation","Matthews Correlation Coefficient (MCC)"],"text":"링크: 논문 PDF로 바로 열기 저자: Xintong Zhang, Xiaowen Zhang, Jongrong Wu, Zhi Gao, Shilin Yan, Zhenxin Diao, Kunpeng Gao, Xuanyan Chen, Yuwei Wu, Yunde Jia, Qing Li 핵심 연구 목표 본 논문은 기존 VLM(VisionLanguage Model) "},{"id":"2026-02-04-Balancing-Understanding-and-Generation-in-Discrete-Diffusion-Models","title":"[논문리뷰] Balancing Understanding and Generation in Discrete Diffusion Models","excerpt":"Jianbin Jiao이 arXiv에 게시한 'Balancing Understanding and Generation in Discrete Diffusion Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Balancing-Understanding-and-Generation-in-Discrete-Diffusion-Models","tags":["Review","Discrete Diffusion Models","Language Modeling","Image Generation","Masked Diffusion","Uniform Noise","XDLM","Stationary Noise Kernel","Pareto Frontier"],"text":"링크: 논문 PDF로 바로 열기 저자: Yue Liu, Yuzhong Zhao, Zheyong Xie, Qixiang Ye, Jianbin Jiao, Yao Hu, Shaosheng Cao, Yunfan Liu 핵심 연구 목표 이 논문은 이산 확산 모델(Discrete Diffusion Models, DDM) 분야에서 Masked Diffusion Lang"},{"id":"2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs","title":"[논문리뷰] CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs","excerpt":"arXiv에 게시된 'CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs","tags":["Review","Reinforcement Learning","LLMs","Budget Allocation","Adaptive Learning","Capability-Oriented Value Function","Exploration-Exploitation","Resource Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiyuan Yao, YiKai Zhang, Yuxin Chen, Yueqing Sun, Zishan Xu, Yu Yang, Tianhao Hu, Qi Gu, Hui Su, Xunliang Cai 핵심 연구 목표 논문은 LLM 추론을 강화하는 RLVR(Reinforcement Learning with Verifiab"},{"id":"2026-02-04-CodeOCR-On-the-Effectiveness-of-Vision-Language-Models-in-Code-Understanding","title":"[논문리뷰] CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding","excerpt":"arXiv에 게시된 'CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-CodeOCR-On-the-Effectiveness-of-Vision-Language-Models-in-Code-Understanding","tags":["Review","Vision Language Models","Code Understanding","Visual Code Representation","Code Compression","Computational Efficiency","Multimodal LLMs","Software Engineering"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuling Shi¹, Chaoxiang Xie², Zhensu Sun³, Yeheng Chen⁴, Chenxu Zhang⁵, Longfei Yun⁶, Chengcheng Wan⁷⁸, Hongyu Zhang⁹, David Lo³, Xiaodong Gu¹ 핵심 연구 목표 본 논문은 텍스트 기반 LLM의 선형적인 컨텍스트"},{"id":"2026-02-04-Decouple-Searching-from-Training-Scaling-Data-Mixing-via-Model-Merging-for-Large-Language-Model-Pre-training","title":"[논문리뷰] Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training","excerpt":"Haifeng Liu이 arXiv에 게시한 'Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Decouple-Searching-from-Training-Scaling-Data-Mixing-via-Model-Merging-for-Large-Language-Model-Pre-training","tags":["Review","LLM Pre-training","Data Mixture Optimization","Model Merging","Proxy Models","Resource Efficiency","DeMix","Corpus Curation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shengrui Li, Fei Zhao, Kaiyan Zhao, Jieying Ye, Haifeng Liu, Fangcheng Shi, Zheyong Xie, Yao Hu, Shaosheng Cao 핵심 연구 목표 Large Language Model (LLM) 사전 학습에서 효과적인 데이터 혼합 비율을 결정하는 것은"},{"id":"2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis","title":"[논문리뷰] Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis","excerpt":"arXiv에 게시된 'Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis","tags":["Review","Diffusion Models","Model Distillation","Mode Collapse","Image Generation","Diversity Preservation","Flow Matching","Few-Step Synthesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianhe Wu, Ruibin Li, Lei Zhang, Kede Ma 핵심 연구 목표 본 논문은 적은 추론 단계(fewstep inference)로 고품질 이미지를 빠르게 생성하기 위한 Distribution Matching Distillation (DMD) 과정에서 발생하는 모드 붕괴(mode collapse) "},{"id":"2026-02-04-Learning-Query-Specific-Rubrics-from-Human-Preferences-for-DeepResearch-Report-Generation","title":"[논문리뷰] Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation","excerpt":"arXiv에 게시된 'Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Learning-Query-Specific-Rubrics-from-Human-Preferences-for-DeepResearch-Report-Generation","tags":["Review","DeepResearch","Rubric Generation","Human Preferences","Reinforcement Learning","Multi-agent Systems","LLM Evaluation","Reward Modeling"],"text":"링크: 논문 PDF로 바로 열기 저자: Changze Lv, Jie Zhou, Wentao Zhao, Jingwen Xu, Zisu Huang, Muzhao Tian, Shihan Dou, Tao Gui, Le Tian, Xiao Zhou, Xiaoqing Zheng, Xuanjing Huang, Jie Zhou 핵심 연구 목표 본 논문은 DeepResea"},{"id":"2026-02-04-Less-Noise-More-Voice-Reinforcement-Learning-for-Reasoning-via-Instruction-Purification","title":"[논문리뷰] Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification","excerpt":"arXiv에 게시된 'Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Less-Noise-More-Voice-Reinforcement-Learning-for-Reasoning-via-Instruction-Purification","tags":["Review","Reinforcement Learning","LLM Reasoning","Instruction Purification","Interference Tokens","Sample Efficiency","Policy Optimization","Verifiable Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiju Guo, Tianyi Hu, Zexu Sun, Yankai Lin 핵심 연구 목표 대규모 언어 모델(LLM) 추론을 위한 RLVR (Reinforcement Learning with Verifiable Rewards) 의 비효율적인 탐색 문제를 해결하는 것을 목표로 합니다. 특히 제한된 롤아웃 예산으로 인한 "},{"id":"2026-02-04-MARS-Modular-Agent-with-Reflective-Search-for-Automated-AI-Research","title":"[논문리뷰] MARS: Modular Agent with Reflective Search for Automated AI Research","excerpt":"arXiv에 게시된 'MARS: Modular Agent with Reflective Search for Automated AI Research' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-MARS-Modular-Agent-with-Reflective-Search-for-Automated-AI-Research","tags":["Review","Autonomous AI","Agent Framework","Machine Learning Engineering","Monte Carlo Tree Search","Reflective Learning","Modular Programming","Code Generation","Resource Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiefeng Chen, Bhavana Dalvi Mishra, Jaehyun Nam, Rui Meng, Tomas Pfister, Jinsung Yoon 핵심 연구 목표 본 논문은 높은 평가 비용, 불투명한 성능 귀속, 복잡한 아키텍처 등으로 인해 기존 LLM 기반 에이전트가 어려움을 겪는 자동화된 AI 연구의 한계"},{"id":"2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs","title":"[논문리뷰] No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs","excerpt":"arXiv에 게시된 'No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs","tags":["Review","Chain-of-Thought","LLM Planning","Probing Methods","Uncertainty Estimation","Reasoning Dynamics","Model Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Liyan Xu, Mo Yu, Fandong Meng, Jie Zhou 핵심 연구 목표 본 연구는 Large Language Models (LLMs)의 ChainofThought (CoT) 추론 과정에서 내재된 계획 능력(latent planning horizon) 을 규명하는 것을 목표로 합니다. LLM이 복잡한 추"},{"id":"2026-02-04-Parallel-Probe-Towards-Efficient-Parallel-Thinking-via-2D-Probing","title":"[논문리뷰] Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing","excerpt":"arXiv에 게시된 'Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Parallel-Probe-Towards-Efficient-Parallel-Thinking-via-2D-Probing","tags":["Review","LLM Reasoning","Parallel Thinking","Efficiency Optimization","2D Probing","Consensus-based Early Stopping","Deviation-based Branch Pruning","Test-Time Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Tong Zheng, Chengsong Huang, Runpeng Dai, Yun He, Rui Liu, Xin Ni, Huiwen Bao, Kaishen Wang, Hongtu Zhu, Jiaxin Huang, Furong Huang, Heng Huang 핵심 연구 목표 대규모 언어 모델(LLM)의 병렬 추론 시 발"},{"id":"2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks","title":"[논문리뷰] Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks","excerpt":"arXiv에 게시된 'Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks","tags":["Review","World Models","Unified Framework","Multimodal AI","Embodied AI","Physical Understanding","Long-term Consistency","AI Agents","Generative Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Bohan Zeng, Kaixin Zhu, Daili Hua, Bozhou Li, Chengzhuo Tong, Yuran Wang, Xinyi Huang, Yifan Dai, Zixiang Zhang, Yifan Yang, Zhou Liu, Hao Liang, Xiaochen Ma, Ruichuan An, Tianyi"},{"id":"2026-02-04-SWE-Master-Unleashing-the-Potential-of-Software-Engineering-Agents-via-Post-Training","title":"[논문리뷰] SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training","excerpt":"arXiv에 게시된 'SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-SWE-Master-Unleashing-the-Potential-of-Software-Engineering-Agents-via-Post-Training","tags":["Review","Software Engineering Agents","Post-Training","Supervised Fine-Tuning","Reinforcement Learning","Language Server Protocol","SWE-bench","Code Navigation","LLM"],"text":"링크: 논문 PDF로 바로 열기 저자: Huatong Song, Lisheng Huang, Shuang Sun, Jinhao Jiang, Ran Le, Daixuan Cheng, Guoxin Chen, Yiwen Hu, Zongchao Chen, Wayne Xin Zhao, Yang Song, Tao Zhang, JiRong Wen 핵심 연구 목표 이 논문"},{"id":"2026-02-04-SWE-World-Building-Software-Engineering-Agents-in-Docker-Free-Environments","title":"[논문리뷰] SWE-World: Building Software Engineering Agents in Docker-Free Environments","excerpt":"arXiv에 게시된 'SWE-World: Building Software Engineering Agents in Docker-Free Environments' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-SWE-World-Building-Software-Engineering-Agents-in-Docker-Free-Environments","tags":["Review","Software Engineering Agents","LLM","Docker-Free","Execution Simulation","Reinforcement Learning","Supervised Fine-tuning","World Model"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuang Sun, Huatong Song, Lisheng Huang, Jinhao Jiang, Ran Le, Zhihao Lv, Zongchao Chen, Yiwen Hu, Wenyang Luo, Wayne Xin Zhao, Yang Song, Hongteng Xu, Tao Zhang, JiRong Wen 핵심 연"},{"id":"2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy","title":"[논문리뷰] SimpleGPT: Improving GPT via A Simple Normalization Strategy","excerpt":"Rong Xiao이 arXiv에 게시한 'SimpleGPT: Improving GPT via A Simple Normalization Strategy' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy","tags":["Review","Transformer Optimization","Normalization Strategy","Hessian Spectral Norm","Learning Rate Stability","Large Language Models","SimpleNorm","Second-Order Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Marco Chen, Xianbiao Qi, Yelin He, Jiaquan Ye, Rong Xiao 핵심 연구 목표 본 논문은 Transformer 모델의 최적화 안정성 문제를 해결하고자 합니다. 기존 정규화 기법들이 경험적으로 도입되었던 한계를 넘어, 2차 최적화 기하학 과 활성화 스케일 의 관점에서 아키텍처 설계"},{"id":"2026-02-04-Token-Sparse-Attention-Efficient-Long-Context-Inference-with-Interleaved-Token-Selection","title":"[논문리뷰] Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection","excerpt":"Jae-Joon Kim이 arXiv에 게시한 'Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Token-Sparse-Attention-Efficient-Long-Context-Inference-with-Interleaved-Token-Selection","tags":["Review","Sparse Attention","Long-Context Inference","LLMs","Token Selection","Efficiency","Transformer","Dynamic Sparsity"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongwon Jo, Beomseok Kang, Jiwon Song, JaeJoon Kim 핵심 연구 목표 대규모 언어 모델(LLMs)에서 O(L²) 의 복잡성을 가지는 어텐션 메커니즘이 긴 컨텍스트 추론의 병목이 되는 문제를 해결하고자 합니다. 기존의 희소 어텐션(sparse attention) 방법론들이 갖는 경직"},{"id":"2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation","title":"[논문리뷰] Unified Personalized Reward Model for Vision Generation","excerpt":"arXiv에 게시된 'Unified Personalized Reward Model for Vision Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation","tags":["Review","Reward Model","Vision Generation","Personalized Learning","Context-Adaptive Reasoning","Direct Preference Optimization (DPO)","Reinforcement Learning (RL)","Multimodal Learning","Group Relative Policy Optimization (GRPO)"],"text":"링크: 논문 PDF로 바로 열기 저자: Yibin Wang, Yuhang Zang, Feng Han, Yujie Zhou, Jiazi Bu, Cheng Jin, Jiaqi Wang 핵심 연구 목표 본 논문은 기존 멀티모달 보상 모델(RMs)이 \"onesizefitsall\" 평가 패러다임을 따르며, 사용자들의 주관적이고 문맥에 따른 시각적 선호도와 일치하지 "},{"id":"2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling","title":"[논문리뷰] WideSeek: Advancing Wide Research via Multi-Agent Scaling","excerpt":"Zhongtao Jiang이 arXiv에 게시한 'WideSeek: Advancing Wide Research via Multi-Agent Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling","tags":["Review","Wide Research","Multi-Agent Systems","Reinforcement Learning","Information Seeking","Benchmarking","LLM Agents","Knowledge Graphs"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyang Huang, Haolin Ren, Xiaowei Yuan, Jiawei Wang, Zhongtao Jiang, Kun Xu, Shizhu He, Jun Zhao, Kang Liu 핵심 연구 목표 본 논문은 기존의 심층 연구(Deep Research) 패러다임이 아닌, 복잡한 제약 조건 하에서 포괄적인 정보"},{"id":"2026-02-04-daVinci-Agency-Unlocking-Long-Horizon-Agency-Data-Efficiently","title":"[논문리뷰] daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently","excerpt":"arXiv에 게시된 'daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-04 00:00:00+0900+0900","permalink":"/ai/review/2026-02-04-daVinci-Agency-Unlocking-Long-Horizon-Agency-Data-Efficiently","tags":["Review","Long-Horizon Agency","Data Synthesis","Pull Request Chains","Software Evolution","LLM Training","Agentic AI","Self-Distillation","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mohan Jiang, Dayuan Fu, Junhao Shi, Ji Zeng, Weiye Si, Keyu Li, Xuefeng Li, Yang Xiao, Wenjie Li, Dequan Wang, Pengfei Liu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 단기 작업에서 뛰어난 성능을 보임에도 불구하"},{"id":"2026-02-05-A-RAG-Scaling-Agentic-Retrieval-Augmented-Generation-via-Hierarchical-Retrieval-Interfaces","title":"[논문리뷰] A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces","excerpt":"arXiv에 게시된 'A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-A-RAG-Scaling-Agentic-Retrieval-Augmented-Generation-via-Hierarchical-Retrieval-Interfaces","tags":["Review","Agentic RAG","Hierarchical Retrieval","LLM Tool Use","Multi-hop QA","Context Efficiency","Dynamic Strategy","Retrieval-Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingxuan Du, Benfeng Xu, Chiwei Zhu, Shaohan Wang, Pengyu Wang, Xiaorui Wang, Zhendong Mao 핵심 연구 목표 기존 RAG(RetrievalAugmented Generation) 시스템이 대규모 언어 모델(LLM)의 추론 및 도구 사용 능력을 충분히 "},{"id":"2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning","title":"[논문리뷰] Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning","excerpt":"arXiv에 게시된 'Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning","tags":["Review","LLM Agents","Agent Efficiency","Context Management","Thought Omission","Observation Omission","Reinforcement Learning","Adaptive Policy"],"text":"링크: 논문 PDF로 바로 열기 저자: Yansong Ning, Jun Fang, Naiqiang Tan, Hao Liu 핵심 연구 목표 이 논문은 LLM 에이전트가 복잡한 실제 작업을 수행할 때 발생하는 과도한 사고(thought) 및 관찰(observation) 컨텍스트 축적 문제 를 해결하고 효율성을 향상시키는 것을 목표로 합니다. 기존 연구들이 상호"},{"id":"2026-02-05-AutoFigure-Generating-and-Refining-Publication-Ready-Scientific-Illustrations","title":"[논문리뷰] AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations","excerpt":"arXiv에 게시된 'AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-AutoFigure-Generating-and-Refining-Publication-Ready-Scientific-Illustrations","tags":["Review","Scientific Illustration Generation","Long-form Text-to-Image","Agentic Framework","Reasoned Rendering","Layout Planning","Text Refinement","FigureBench","VLM-as-a-judge"],"text":"링크: 논문 PDF로 바로 열기 저자: Minjun Zhu, Zhen Lin, Yixuan Weng, Panzhong Lu, Qiujie Xie, Yifan Wei, Sifan Liu, Qiyao Sun, Yue Zhang 핵심 연구 목표 과학 논문의 복잡한 내용을 효과적으로 시각화하는 고품질 삽화의 수동 생성 병목 현상을 해결하고자 합니다. 특히, 긴 과"},{"id":"2026-02-05-BatCoder-Self-Supervised-Bidirectional-Code-Documentation-Learning-via-Back-Translation","title":"[논문리뷰] BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation","excerpt":"Xiaohua Wang이 arXiv에 게시한 'BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-BatCoder-Self-Supervised-Bidirectional-Code-Documentation-Learning-via-Back-Translation","tags":["Review","Self-Supervised Learning","Code Generation","Documentation Generation","Back-Translation","Reinforcement Learning","Large Language Models (LLMs)","Code-Documentation Alignment","Low-Resource Languages"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingwen Xu, Yiyang Lu, Zisu Huang, Changze Lv, Xiaohua Wang, Shizheng Li, Zhibo Xu, Zhengkang Guo, Zhengyuan Wang, Muzhao Tian, Xuanjing Huang, Xiaoqing Zheng 핵심 연구 목표 본 논문의 핵심 목"},{"id":"2026-02-05-ERNIE-5-0-Technical-Report","title":"[논문리뷰] ERNIE 5.0 Technical Report","excerpt":"HasuerYu이 arXiv에 게시한 'ERNIE 5.0 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-ERNIE-5-0-Technical-Report","tags":["Review","Multimodal Foundation Model","Autoregressive","Mixture-of-Experts","Elastic Training","Reinforcement Learning","Unified Architecture","Sparse MoE","Efficient Deployment"],"text":"링크: 논문 PDF로 바로 열기 저자: ERNIE Team, Baidu 핵심 연구 목표 ERNIE 5.0은 텍스트, 이미지, 비디오, 오디오에 걸쳐 통합된 멀티모달 이해 및 생성 을 위한 본질적으로 자기회귀(autoregressive) 기반 파운데이션 모델 을 개발하는 것을 목표로 합니다. 기존 후기 퓨전(latefusion) 방식의 한계를 극복하고, 단일"},{"id":"2026-02-05-EgoActor-Grounding-Task-Planning-into-Spatial-aware-Egocentric-Actions-for-Humanoid-Robots-via-Visual-Language-Models","title":"[논문리뷰] EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models","excerpt":"Ziyi Bai이 arXiv에 게시한 'EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-EgoActor-Grounding-Task-Planning-into-Spatial-aware-Egocentric-Actions-for-Humanoid-Robots-via-Visual-Language-Models","tags":["Review","Humanoid Robots","Vision-Language Models","Task Planning","Egocentric Control","Mobile Manipulation","Active Perception","Human-Robot Interaction","Real-World Deployment"],"text":"링크: 논문 PDF로 바로 열기 저자: Ziyi Bai, Chaojie Li, MingMing Yu, Yu Bai, tellarin 핵심 연구 목표 본 논문은 인간형 로봇의 실제 환경 배포 시 발생하는 고유한 불안정성, 부분적 정보 기반의 지각/이동/조작 통합의 어려움, 그리고 동적 환경에서의 견고한 하위 태스크 전환 문제를 해결하는 것을 목표로 합니다. "},{"id":"2026-02-05-FASA-Frequency-aware-Sparse-Attention","title":"[논문리뷰] FASA: Frequency-aware Sparse Attention","excerpt":"arXiv에 게시된 'FASA: Frequency-aware Sparse Attention' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-FASA-Frequency-aware-Sparse-Attention","tags":["Review","Sparse Attention","KV Cache Optimization","Rotary Positional Embedding (RoPE)","Frequency Chunks (FCs)","LLMs","Long-Context","Training-Free"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifei Wang, Yueqi Wang, Zhenrui Yue, Huimin Zeng, Yong Wang, Ismini Lourentzou, Zhengzhong Tu, Xiangxiang Chu, Julian McAuley 핵심 연구 목표 대규모 언어 모델(LLMs)이 긴 입력 시퀀스를 처리할 때 발생하는 KV 캐시"},{"id":"2026-02-05-HY3D-Bench-Generation-of-3D-Assets","title":"[논문리뷰] HY3D-Bench: Generation of 3D Assets","excerpt":"arXiv에 게시된 'HY3D-Bench: Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets","tags":["Review","3D Generation","Dataset","Benchmark","AIGC","Watertight Mesh","Part-level Decomposition","Foundation Model","Robotics"],"text":"링크: 논문 PDF로 바로 열기 저자: Bowen Zhang, Chunchao Guo, Dongyuan Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jiaao Yu, Jiachen Xu, Jingwei Huang, Kunhong Li, Lifu Wang, Linus, Penghao Wang, Qingxiang Lin, Ruini"},{"id":"2026-02-05-HySparse-A-Hybrid-Sparse-Attention-Architecture-with-Oracle-Token-Selection-and-KV-Cache-Sharing","title":"[논문리뷰] HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing","excerpt":"arXiv에 게시된 'HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-HySparse-A-Hybrid-Sparse-Attention-Architecture-with-Oracle-Token-Selection-and-KV-Cache-Sharing","tags":["Review","Sparse Attention","KV Cache Sharing","Hybrid Attention","Long-Context LLMs","Memory Optimization","Token Selection","Transformer Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Yizhao Gao, Jianyu Wei, Qihao Zhang, Yu Cheng, Shimao Chen, Zhengju Tang, Zihan Jiang, Yifan Song, Hailin Zhang, Liang Zhao, Bo Yang, Gang Wang, Shijie Cao, Fuli Luo 핵심 연구 목표 본 논"},{"id":"2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models","title":"[논문리뷰] OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models","excerpt":"Yiyan Ji이 arXiv에 게시한 'OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models","tags":["Review","Omni-modal LLMs","Token Compression","Modality-Asymmetric","Video Pruning","Audio Selection","Efficiency","Large Language Models","Spatio-Temporal"],"text":"링크: 논문 PDF로 바로 열기 저자: Yue Ding, Yiyan Ji, Jungang Li, Xuyang Liu, Xinlong Chen, Junfei Wu, Bozhou Li, Bohan Zeng, Yang Shi, Yushuo Guan, Yuanxing Zhang, Jiaheng Liu, Qiang Liu, Pengfei Wan, Liang Wang"},{"id":"2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR","title":"[논문리뷰] PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR","excerpt":"Alejandro Lozano이 arXiv에 게시한 'PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR","tags":["Review","Reinforcement Learning","Large Language Models","Scientific QA","Information Retrieval","Verifiable Rewards","Biomedical Domain","Search Agents","Dataset Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano, Min Woo Sun, Emma Lundberg, Serena YeungLevy 핵심 연구 목표 본 논문은 기존 RLVR(Verifiable Rewards를 사용한 강화 학습) 검색 에이전트가"},{"id":"2026-02-05-Quant-VideoGen-Auto-Regressive-Long-Video-Generation-via-2-Bit-KV-Cache-Quantization","title":"[논문리뷰] Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization","excerpt":"arXiv에 게시된 'Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Quant-VideoGen-Auto-Regressive-Long-Video-Generation-via-2-Bit-KV-Cache-Quantization","tags":["Review","Auto-Regressive Video Generation","KV-Cache Quantization","Memory Optimization","Long Video Generation","Video Diffusion Models","Semantic-Aware Smoothing","Progressive Residual Quantization"],"text":"링크: 논문 PDF로 바로 열기 저자: Haocheng Xi, Shuo Yang, Yilong Zhao, Muyang Li, Han Cai, Xingyang Li, Yujun Lin, Zhuoyang Zhang, Jintao Zhang, Xiuyu Li, Chenfeng Xu, Ion Stoica, Song Han, Kurt Keutzer 핵심 연구 목표 "},{"id":"2026-02-05-Residual-Context-Diffusion-Language-Models","title":"[논문리뷰] Residual Context Diffusion Language Models","excerpt":"arXiv에 게시된 'Residual Context Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Residual-Context-Diffusion-Language-Models","tags":["Review","Diffusion Language Models","Residual Learning","Context Aggregation","Parallel Decoding","Masked Denoising","Reasoning Benchmarks","Entropy Weighting"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuezhou Hu, Harman Singh, Monishwaran Maheswaran, Haocheng Xi, Coleman Hooper, Jintao Zhang, Aditya Tomar, Michael W. Mahoney, Sewon Min, Mehrdad Farajtabar, Kurt Keutzer, Amir G"},{"id":"2026-02-05-Rethinking-the-Trust-Region-in-LLM-Reinforcement-Learning","title":"[논문리뷰] Rethinking the Trust Region in LLM Reinforcement Learning","excerpt":"arXiv에 게시된 'Rethinking the Trust Region in LLM Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Rethinking-the-Trust-Region-in-LLM-Reinforcement-Learning","tags":["Review","LLM","Reinforcement Learning","Trust Region","PPO","DPPO","Policy Optimization","Training Stability","Divergence Approximation"],"text":"링크: 논문 PDF로 바로 열기 저자: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee 핵심 연구 목표 Large Language Models (LLMs)의 강화학습 미세 조정 시, 기존 Proximal Policy Optimization (PPO) 의 비율 "},{"id":"2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning","title":"[논문리뷰] Self-Hinting Language Models Enhance Reinforcement Learning","excerpt":"arXiv에 게시된 'Self-Hinting Language Models Enhance Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Large Language Models","GRPO","Sparse Rewards","Self-Hinting","Policy Optimization","Adaptive Curriculum","On-Policy Training"],"text":"링크: 논문 PDF로 바로 열기 저자: Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian 핵심 연구 목표 본 논문은 Group Relative Policy Optimization (GRPO) 이 희소한(sparse) 터미널 보상 환경에서 발생하는 문제, 즉 롤아웃 그룹 내 보상이 동일하여 이점이"},{"id":"2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers","title":"[논문리뷰] Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers","excerpt":"arXiv에 게시된 'Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers","tags":["Review","Diffusion Models","LLM","Text-to-Image","Transformer","Semantic Routing","Feature Fusion","Dynamic Conditioning","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Bozhou Li, Yushuo Guan, Haolin Li, Bohan Zeng, Yiyan Ji, Yue Ding, Pengfei Wan, Kun Gai, Yuanxing Zhang, Wentao Zhang 핵심 연구 목표 본 논문은 LLM을 텍스트 인코더로 사용하는 DiT 기반 텍스트이미지 모델에서, 정적인 텍스"},{"id":"2026-02-05-SoMA-A-Real-to-Sim-Neural-Simulator-for-Robotic-Soft-body-Manipulation","title":"[논문리뷰] SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation","excerpt":"arXiv에 게시된 'SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-SoMA-A-Real-to-Sim-Neural-Simulator-for-Robotic-Soft-body-Manipulation","tags":["Review","Neural Simulator","Real-to-Sim (R2S)","Robotic Manipulation","Soft-body Dynamics","Gaussian Splatting","Deformable Objects","Action-conditioned Simulation","Long-horizon Simulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Mu Huang, Hui Wang, Kerui Ren, Linning Xu, Yunsong Zhou, Mulin Yu, Bo Dai, Jiangmiao Pang 핵심 연구 목표 본 논문은 로봇의 소프트바디 조작 시 발생하는 복잡한 상호작용 속에서 변형 가능한 객체의 동역학을 정확하고 안정적으로 시뮬레이션하는 근본적인 "},{"id":"2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents","title":"[논문리뷰] TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents","excerpt":"Qiushi Sun이 arXiv에 게시한 'TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents","tags":["Review","LLM Agents","Test-Time Improvement","Diagnostic Evaluation","Trajectory Analysis","Performance Metrics","Behavior Adaptation","Memory Management","POMDP"],"text":"링크: 논문 PDF로 바로 열기 저자: Zichen Ding, Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jian Zhang, Tao Qin, Jun Liu, Qika Lin 핵심 연구 목표 본 논문은 LLM 에이전트의 TestTime Improvement (TTI) 메커니즘이 성공하거나 실패하"},{"id":"2026-02-05-Training-Data-Efficiency-in-Multimodal-Process-Reward-Models","title":"[논문리뷰] Training Data Efficiency in Multimodal Process Reward Models","excerpt":"Haolin Liu이 arXiv에 게시한 'Training Data Efficiency in Multimodal Process Reward Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Training-Data-Efficiency-in-Multimodal-Process-Reward-Models","tags":["Review","Multimodal Process Reward Models (MPRMs)","Data Efficiency","Monte Carlo Annotation","Data Selection","Balanced-Information Score (BIS)","Label Mixture","Label Reliability","Computational Cost Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinyuan Li, Chengsong Huang, Langlin Huang, Shaoyang Xu, Haolin Liu, Wenxuan Zhang, Jiaxin Huang 핵심 연구 목표 본 논문은 Multimodal Process Reward Models (MPRMs) 훈련의 데이터 효율성 문제를 해결하는 것을 목"},{"id":"2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models","title":"[논문리뷰] VLS: Steering Pretrained Robot Policies via Vision-Language Models","excerpt":"arXiv에 게시된 'VLS: Steering Pretrained Robot Policies via Vision-Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models","tags":["Review","Robot Learning","Vision-Language Models","Policy Steering","Inference-Time Adaptation","Out-of-Distribution Generalization","Diffusion Models","Generative Policies"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuo Liu¹,⁴, Ishneet Sukhvinder Singh², Yiqing Xu³,⁴, Jiafei Duan¹,⁴,, Ranjay Krishna¹,⁴, ¹University of Washington ²University of Oxford ³National University of Singapore ⁴Allen"},{"id":"2026-02-05-Vibe-AIGC-A-New-Paradigm-for-Content-Generation-via-Agentic-Orchestration","title":"[논문리뷰] Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration","excerpt":"arXiv에 게시된 'Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-Vibe-AIGC-A-New-Paradigm-for-Content-Generation-via-Agentic-Orchestration","tags":["Review","Agentic AI","Content Generation","Orchestration","Vibe Coding","Meta-Planner","Human-in-the-Loop","Intent-Execution Gap"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiaheng Liu, Yuanxing Zhang, Shihao Li, Xinping Lei 핵심 연구 목표 본 논문은 지난 10년간 모델 중심 패러다임이 지배했던 생성형 AI(AIGC) 분야의 한계, 특히 '의도실행 격차(IntentExecution Gap)'를 해결하는 것을 목표로 합니다. 이는 사용자의 고수준 의"},{"id":"2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning","title":"[논문리뷰] WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning","excerpt":"arXiv에 게시된 'WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-05 00:00:00+0900+0900","permalink":"/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning","tags":["Review","Multi-Agent Reinforcement Learning","Width Scaling","Large Language Models","Information Seeking","Task Decomposition","Parallel Execution","Lead-Agent-Subagent Framework","Orchestration"],"text":"링크: 논문 PDF로 바로 열기 저자: Zelai Xu, Zhexuan Xu, Ruize Zhang, Chunyang Zhu, Shi Yu, Weilin Liu, Quanlu Zhang, Wenbo Ding, Chao Yu, Yu Wang 핵심 연구 목표 본 논문은 LLM의 \"깊이 스케일링\"이 아닌 \"폭 스케일링(width scaling)\" 이라는 새로운 "},{"id":"2026-02-06-BABE-Biology-Arena-BEnchmark","title":"[논문리뷰] BABE: Biology Arena BEnchmark","excerpt":"arXiv에 게시된 'BABE: Biology Arena BEnchmark' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark","tags":["Review","Biology Benchmark","Large Language Models","Experimental Reasoning","Causal Inference","Cross-Scale Inference","Multimodal AI","Scientific Reasoning","Research Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Junting Zhou, Jin Chen, Linfeng Hao, Denghui Cao, Zheyu Wang, Qiguang Chen, Chaoyou Fu, Jiaze Chen, Yuchen Wu, Ge Zhang, Mingxuan Wang, Wenhao Huang, Tong Yang 핵심 연구 목표 이 논문은 LLM"},{"id":"2026-02-06-Breaking-the-Static-Graph-Context-Aware-Traversal-for-Robust-Retrieval-Augmented-Generation","title":"[논문리뷰] Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation","excerpt":"Qintian Guo이 arXiv에 게시한 'Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Breaking-the-Static-Graph-Context-Aware-Traversal-for-Robust-Retrieval-Augmented-Generation","tags":["Review","Retrieval-Augmented Generation","Knowledge Graphs","Graph Traversal","Context-Aware Retrieval","Personalized PageRank","Multi-hop Reasoning","Semantic Drift Mitigation"],"text":"링크: 논문 PDF로 바로 열기 저자: Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo, Ruiyuan Zhang, Xiaofang Zhou 핵심 연구 목표 본 논문은 기존 그래프 기반 RAG(RetrievalAugmented Generation) 모델들이 겪는 \"Static Graph"},{"id":"2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty","title":"[논문리뷰] CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty","excerpt":"arXiv에 게시된 'CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty","tags":["Review","LLM Agents","Benchmarks","Tool-use","Consistency","Uncertainty Handling","Hallucination","In-car Assistant","Policy Adherence"],"text":"링크: 논문 PDF로 바로 열기 저자: Johannes Kirmayr, Lukas Stappen, Elisabeth André 핵심 연구 목표 기존 LLM 에이전트 벤치마크가 이상적인 설정에서의 태스크 완료에만 초점을 맞추고 실제 환경에서의 신뢰성, 일관성, 한계 인식 을 간과하는 문제를 해결하고자 합니다. 특히, 모호하거나 불완전한 사용자 요청, 정책 준"},{"id":"2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","title":"[논문리뷰] Context Forcing: Consistent Autoregressive Video Generation with Long Context","excerpt":"arXiv에 게시된 'Context Forcing: Consistent Autoregressive Video Generation with Long Context' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","tags":["Review","Video Generation","Autoregressive Models","Long Context","Temporal Consistency","Diffusion Models","Context Forcing","Memory Management","Distribution Matching Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuo Chen, Cong Wei, Sun Sun, Ping Nie, Kai Zhou, Ge Zhang, MingHsuan Yang, Wenhu Chen 핵심 연구 목표 이 논문은 현재 자동회귀 비디오 생성 모델들이 짧은 컨텍스트 윈도우와 학생교사 불일치로 인해 장기적인 일관성(forgettingdrifting di"},{"id":"2026-02-06-Dr-Kernel-Reinforcement-Learning-Done-Right-for-Triton-Kernel-Generations","title":"[논문리뷰] Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations","excerpt":"arXiv에 게시된 'Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Dr-Kernel-Reinforcement-Learning-Done-Right-for-Triton-Kernel-Generations","tags":["Review","Reinforcement Learning","Kernel Generation","Triton","GPU Optimization","LLMs","Reward Hacking","Multi-turn Interaction","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs)을 활용하여 고품질 GPU 커널 코드를 생성하는 과정에서 발생하는 보상 해킹(reward hacking) 및 게으른 최적화"},{"id":"2026-02-06-InterPrior-Scaling-Generative-Control-for-Physics-Based-Human-Object-Interactions","title":"[논문리뷰] InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions","excerpt":"Xiaohan Fei이 arXiv에 게시한 'InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-InterPrior-Scaling-Generative-Control-for-Physics-Based-Human-Object-Interactions","tags":["Review","Human-Object Interaction","Physics-Based Simulation","Generative Control","Reinforcement Learning","Imitation Learning","Variational Policy","Failure Recovery","Loco-Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Sirui Xu, Samuel Schulter, Morteza Ziyadi, Xialin He, Xiaohan Fei, YuXiong Wang, LiangYan Gui 핵심 연구 목표 논문은 물리 기반 휴머노이드 로봇이 고수준의 목표만으로도 다양한 객체와 상호작용하는 복잡한 로코조작(locomanipulation) 행"},{"id":"2026-02-06-LatentMem-Customizing-Latent-Memory-for-Multi-Agent-Systems","title":"[논문리뷰] LatentMem: Customizing Latent Memory for Multi-Agent Systems","excerpt":"Zefeng He이 arXiv에 게시한 'LatentMem: Customizing Latent Memory for Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-LatentMem-Customizing-Latent-Memory-for-Multi-Agent-Systems","tags":["Review","Multi-Agent Systems","LLM Memory","Latent Representation","Role-Aware","Token Efficiency","Policy Optimization","Continual Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zefeng He, Yafu Li, Xiangyuan Xue, Guibin Zhang, Muxin Fu 핵심 연구 목표 본 논문은 LLM 기반 멀티 에이전트 시스템(MAS)의 메모리 설계가 겪는 두 가지 근본적인 문제, 즉 (i) 역할 인지적 맞춤화 부재로 인한 메모리 동질화 와 (ii) 과도하게 세분화된 메모리 항목"},{"id":"2026-02-06-Length-Unbiased-Sequence-Policy-Optimization-Revealing-and-Controlling-Response-Length-Variation-in-RLVR","title":"[논문리뷰] Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR","excerpt":"Zhixiong Zeng이 arXiv에 게시한 'Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Length-Unbiased-Sequence-Policy-Optimization-Revealing-and-Controlling-Response-Length-Variation-in-RLVR","tags":["Review","Reinforcement Learning with Verifiable Rewards","LLMs","Policy Optimization","Response Length Bias","Sequence-level Clipping","Length-Unbiased Optimization","Multimodal Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Fanfan Liu, Youyang Yin, Peng Shi, Siqi Yang, Zhixiong Zeng, Haibo Qiu 핵심 연구 목표 본 논문은 Reinforcement Learning with Verifiable Rewards (RLVR) 훈련 과정에서 GRPO 및 GSPO 와 같은 주류 알고리즘이 겪는 응"},{"id":"2026-02-06-Multi-Task-GRPO-Reliable-LLM-Reasoning-Across-Tasks","title":"[논문리뷰] Multi-Task GRPO: Reliable LLM Reasoning Across Tasks","excerpt":"Zhiyong Wang이 arXiv에 게시한 'Multi-Task GRPO: Reliable LLM Reasoning Across Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Multi-Task-GRPO-Reliable-LLM-Reasoning-Across-Tasks","tags":["Review","Large Language Models (LLMs)","Multi-Task Learning","Reinforcement Learning","Policy Optimization","GRPO","Task Reweighting","Robustness","Reasoning Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Shyam Sundhar Ramesh, Xiaotong Ji, Matthieu Zimmer, Sangwoong Yoon, Aurelien Lucchi, Zhiyong Wang, Haitham Bou Ammar, Ilija Bogunovic 핵심 연구 목표 본 논문은 GRPO(GroupRelative Policy Opt"},{"id":"2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments","title":"[논문리뷰] ProAct: Agentic Lookahead in Interactive Environments","excerpt":"arXiv에 게시된 'ProAct: Agentic Lookahead in Interactive Environments' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments","tags":["Review","Agentic AI","Large Language Models","Reinforcement Learning","Lookahead Reasoning","Monte-Carlo Tree Search","Supervised Fine-Tuning","Value Estimation","Simulation Drift"],"text":"링크: 논문 PDF로 바로 열기 저자: Yangbin Yu, Mingyu Yang, Junyou Li, Yiming Gao, Feiyu Liu, Yijun Yang, Zichuan Lin, Jiafei Lyu, Yicheng Liu, Zhencheng Lu, Deheng Ye, Jie Jiang†, Tencent Huaiyan 핵심 연구 목표 ProAct는"},{"id":"2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules","title":"[논문리뷰] RISE-Video: Can Video Generators Decode Implicit World Rules?","excerpt":"Zicheng Zhang이 arXiv에 게시한 'RISE-Video: Can Video Generators Decode Implicit World Rules?' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules","tags":["Review","Video Generation","Implicit Reasoning","Benchmark","Evaluation","Large Multimodal Models (LMMs)","Text-Image-to-Video (TI2V)"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingxin Liu, Shuran Ma, Shibei Meng, Xiangyu Zhao, Zicheng Zhang, Shaofeng Zhang, Zhihang Zhong, Peixian Chen, Haoyu Cao, Xing Sun, Haodong Duan, Xue Yang 핵심 연구 목표 본 논문은 최신 비디오 생"},{"id":"2026-02-06-Reinforced-Attention-Learning","title":"[논문리뷰] Reinforced Attention Learning","excerpt":"arXiv에 게시된 'Reinforced Attention Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Reinforced-Attention-Learning","tags":["Review","Reinforcement Learning","Multimodal LLMs","Attention Mechanisms","Policy Gradient","Knowledge Distillation","Visual Grounding","Post-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen and Derek Zhiyuan Cheng 핵심 연구 목표 본 논문은 기존 RL 기반 LLM 후처리 방식이 MLLM에서 시각적 추론을 위한 \"생성할 내용\"에만 초점을 맞추어 제한적인 "},{"id":"2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents","title":"[논문리뷰] Reinforcement World Model Learning for LLM-based Agents","excerpt":"arXiv에 게시된 'Reinforcement World Model Learning for LLM-based Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents","tags":["Review","LLM-based Agents","World Model Learning","Reinforcement Learning","Self-Supervised","Environment Dynamics","Sim-to-Real Reward","Textual States"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiao Yu, Baolin Peng, Ruize Xu, Yelong Shen, Pengcheng He, Suman Nath, Nikhil Singh, Jiangfeng Gao, Zhou Yu 핵심 연구 목표 대규모 언어 모델(LLM) 기반 에이전트가 현실 환경에서 행동 결과(action consequences)를 예"},{"id":"2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities","title":"[논문리뷰] Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities","excerpt":"arXiv에 게시된 'Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities","tags":["Review","Retrieval-Augmented Generation","Large Language Models","Reasoning","Benchmark","Deep Search","Error Analysis","Scientific Problem Solving","Context Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuangshuang Ying, Zheyu Wang, Yunjian Peng, Jin Chen, Ge Zhang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 새롭고 복잡한 과학 정보에 대해 추론하는 능력의 불확실성을 해결하는 것을 목표로 합니다. 기존 RAG(RetrievalAugmented Generat"},{"id":"2026-02-06-SAGE-Benchmarking-and-Improving-Retrieval-for-Deep-Research-Agents","title":"[논문리뷰] SAGE: Benchmarking and Improving Retrieval for Deep Research Agents","excerpt":"Chen Zhao이 arXiv에 게시한 'SAGE: Benchmarking and Improving Retrieval for Deep Research Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-SAGE-Benchmarking-and-Improving-Retrieval-for-Deep-Research-Agents","tags":["Review","Deep Research Agents","Scientific Literature Retrieval","LLM-based Retrievers","Benchmarking","Test-time Scaling","Information Retrieval","Query Decomposition","RAG"],"text":"링크: 논문 PDF로 바로 열기 저자: Tiansheng Hu, Yilun Zhao, Canyu Zhang, Arman Cohan, Chen Zhao 핵심 연구 목표 본 논문은 심층 연구 에이전트 워크플로우에서 LLM 기반 검색기 가 얼마나 효과적으로 기여할 수 있는지 체계적으로 조사하는 것을 목표로 합니다. 특히, 기존 에이전트들이 추론 집약적인 정보 검"},{"id":"2026-02-06-Semantic-Search-over-9-Million-Mathematical-Theorems","title":"[논문리뷰] Semantic Search over 9 Million Mathematical Theorems","excerpt":"arXiv에 게시된 'Semantic Search over 9 Million Mathematical Theorems' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Semantic-Search-over-9-Million-Mathematical-Theorems","tags":["Review","Semantic Search","Theorem Retrieval","LLMs","Dense Retrieval","Mathematical Information Retrieval","Vector Embeddings","Mathematical Dataset","RAG"],"text":"링크: 논문 PDF로 바로 열기 저자: Luke Alexander, Eric Leonen, Sophie Szeto, Artemii Remizov, Ignacio Tejeda, Giovanni Inchiostro, Vasily Ilin 핵심 연구 목표 본 논문은 기존 검색 도구가 논문 단위로만 작동하여 특정 수학적 정리, 보조 정리, 명제 검색이 어려운 문제"},{"id":"2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening","title":"[논문리뷰] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening","excerpt":"arXiv에 게시된 'Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening","tags":["Review","LLM Agents","Agent Security","Intrinsic Risk Sensing","Adaptive Defense","Hierarchical Screening","Attack Detection","S2Bench Benchmark"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhenxiong Yu, Zhi Yang, Zhiheng Jin, Shuhe Wang, Heng Zhang, Yanlin Fei, Lingfeng Zeng, Fangqi Lou, Shuo Zhang, Tu Hu, Jingping Liu, Rongze Chen, Xingyu Zhu, Kunyi Wang, Chaofa Y"},{"id":"2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight","title":"[논문리뷰] Steering LLMs via Scalable Interactive Oversight","excerpt":"arXiv에 게시된 'Steering LLMs via Scalable Interactive Oversight' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight","tags":["Review","Scalable Oversight","Interactive AI","Large Language Models","Human-AI Collaboration","Product Requirement Documents","Reinforcement Learning","Structured Interaction","Vibe Coding"],"text":"링크: 논문 PDF로 바로 열기 저자: Enyu Zhou, Zhiheng Xi, Long Ma, Zihao Zhang, Shihan Dou, Zhikai Lei, Guoteng Wang, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 복잡하고 장기적인"},{"id":"2026-02-06-SwimBird-Eliciting-Switchable-Reasoning-Mode-in-Hybrid-Autoregressive-MLLMs","title":"[논문리뷰] SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs","excerpt":"arXiv에 게시된 'SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-SwimBird-Eliciting-Switchable-Reasoning-Mode-in-Hybrid-Autoregressive-MLLMs","tags":["Review","Multimodal Large Language Models","Reasoning Modes","Hybrid Autoregressive","Latent Visual Reasoning","Dynamic Mode Selection","Supervised Fine-tuning","Vision-Language Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Jintao Tong, Shilin Yan, Hongwei Xue, Xiaojun Tang, Kunyu Shi, Guannan Zhang, Ruixuan Li, Yixiong Zou 핵심 연구 목표 기존 MLLM(Multimodal Large Language Models)이 고정된 추론 패턴(텍스트 전용, 시각 전용,"},{"id":"2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning","title":"[논문리뷰] Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning","excerpt":"arXiv에 게시된 'Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning","tags":["Review","Video Generation","Visual Reasoning","Zero-Shot Generalization","Test-Time Scaling","Visual Context","Sequential Planning","Continuous Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Chengzu Li, Zanyi Wang, Jiaang Li, Yi Xu, Han Zhou, Huanyu Zhang, Ruichuan An, Dengyang Jiang, Zhaochong An, Ivan Vulić, Serge Belongie, Anna Korhonen 핵심 연구 목표 본 논문은 기존 MLLMs가 겪는"},{"id":"2026-02-06-V-Retrver-Evidence-Driven-Agentic-Reasoning-for-Universal-Multimodal-Retrieval","title":"[논문리뷰] V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval","excerpt":"Zeyu Zhang이 arXiv에 게시한 'V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-06 00:00:00+0900+0900","permalink":"/ai/review/2026-02-06-V-Retrver-Evidence-Driven-Agentic-Reasoning-for-Universal-Multimodal-Retrieval","tags":["Review","Multimodal Retrieval","Agentic AI","Large Language Models (LLMs)","Visual Tools","Chain-of-Thought (CoT)","Reinforcement Learning","Curriculum Learning","Evidence-Driven Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Dongyang Chen, Chaoyang Wang, Dezhao SU, Xi Xiao, Zeyu Zhang, Jing Xiong, Qing Li, Yuzhang Shang, Shichao Kan 핵심 연구 목표 기존 MLLM 기반 검색 시스템이 정적 시각 인코딩에 의존하고 시각적 증거를 능동적으로 검증하지 못해 시각"},{"id":"2026-02-09-AudioSAE-Towards-Understanding-of-Audio-Processing-Models-with-Sparse-AutoEncoders","title":"[논문리뷰] AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders","excerpt":"arXiv에 게시된 'AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-AudioSAE-Towards-Understanding-of-Audio-Processing-Models-with-Sparse-AutoEncoders","tags":["Review","Sparse Autoencoders (SAEs)","Audio Representation Learning","Model Interpretability","Whisper","HuBERT","Feature Steering","EEG Correlation","Audio Analysis"],"text":"링크: 논문 PDF로 바로 열기 저자: Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich, Assel Yermekova, Laida Kushnareva, Vadim Popov, Kristian Kuznetsov, Irina Piontkovskaya 핵심 연구 목표 이 논문은 오디오 처리 모델, 특히 Whisper 와"},{"id":"2026-02-09-Back-to-Basics-Revisiting-Exploration-in-Reinforcement-Learning-for-LLM-Reasoning-via-Generative-Probabilities","title":"[논문리뷰] Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities","excerpt":"Ivan Oseledets이 arXiv에 게시한 'Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-Back-to-Basics-Revisiting-Exploration-in-Reinforcement-Learning-for-LLM-Reasoning-via-Generative-Probabilities","tags":["Review","Reinforcement Learning","LLM Reasoning","Exploration-Exploitation","Group Relative Policy Optimization","Entropy Collapse","Generative Models","Confidence-Aware Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov, Ivan Oseledets 핵심 연구 목표 본 논문은 LLM 추론에서 RLVR(Reinforcement Learning with Verifiable Rewards) 훈련 시 발생하는 엔트로피 붕괴(entropy collapse)"},{"id":"2026-02-09-Baichuan-M3-Modeling-Clinical-Inquiry-for-Reliable-Medical-Decision-Making","title":"[논문리뷰] Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making","excerpt":"arXiv에 게시된 'Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-Baichuan-M3-Modeling-Clinical-Inquiry-for-Reliable-Medical-Decision-Making","tags":["Review","Medical LLM","Clinical Decision Support","Reinforcement Learning","Hallucination Suppression","Multi-task Learning","Speculative Decoding","Quantization","Clinical Inquiry"],"text":"링크: 논문 PDF로 바로 열기 저자: BaichuanM3 Team 핵심 연구 목표 본 논문은 기존 의료 LLM이 보이는 수동적인 질문답변 방식과 개방형 임상 상담에서의 환각 문제를 해결하고자 합니다. 능동적인 정보 획득, 장기적 추론, 적응형 환각 억제 기능을 갖춘 임상 등급의 의사결정 지원 시스템인 BaichuanM3 를 개발하여 신뢰할 수 있는 의료 "},{"id":"2026-02-09-Canzona-A-Unified-Asynchronous-and-Load-Balanced-Framework-for-Distributed-Matrix-based-Optimizers","title":"[논문리뷰] Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers","excerpt":"arXiv에 게시된 'Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-Canzona-A-Unified-Asynchronous-and-Load-Balanced-Framework-for-Distributed-Matrix-based-Optimizers","tags":["Review","Distributed Training","Matrix-based Optimizers","Load Balancing","Asynchronous Compute","Data Parallelism","Tensor Parallelism","ZeRO-1","LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Liangyu Wang, Siqi Zhang, Junjie Wang, Yiming Dong, Bo Zheng, Zihan Qiu, Shengkun Tang, Di Wang, Rui Men, Dayiheng Liu 핵심 연구 목표 논문은 대규모 언어 모델(LLM) 훈련에서 Shampoo, Muon, SOAP 와 같은 행"},{"id":"2026-02-09-F-GRPO-Dont-Let-Your-Policy-Learn-the-Obvious-and-Forget-the-Rare","title":"[논문리뷰] F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare","excerpt":"arXiv에 게시된 'F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-F-GRPO-Dont-Let-Your-Policy-Learn-the-Obvious-and-Forget-the-Rare","tags":["Review","Reinforcement Learning","LLM","Policy Optimization","Reward Models","Diversity Preservation","Focal Loss","Group Sampling","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Daniil Plyusov, Alexey Gorbatovski, Boris Shaposhnikov, Viacheslav Sinii, Alexey Malakhov, Daniil Gavrilov 핵심 연구 목표 RLVR (Reinforcement Learning with Verifiable Rewards)에서 그룹 샘플링"},{"id":"2026-02-09-Group-Evolving-Agents-Open-Ended-Self-Improvement-via-Experience-Sharing","title":"[논문리뷰] Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing","excerpt":"Zhen Zhang이 arXiv에 게시한 'Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-Group-Evolving-Agents-Open-Ended-Self-Improvement-via-Experience-Sharing","tags":["Review","Open-Ended Learning","Self-Improving Agents","Evolutionary Algorithms","Experience Sharing","Meta-Learning","Code Generation","Agent Frameworks"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaotian Weng, Antonis Antoniades, Deepak Nathani, Zhen Zhang, Xiao Pu, Xin Eric Wang 핵심 연구 목표 본 논문은 기존의 개별 에이전트 중심, 트리 구조 진화 방식이 탐색적 다양성의 비효율적인 활용과 고립된 진화 브랜치로 인한 장기적인 누적 발전의 한계"},{"id":"2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning","title":"[논문리뷰] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning","excerpt":"arXiv에 게시된 'InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning","tags":["Review","Iterative Reasoning","Reinforcement Learning","Large Language Models","Context Management","Summarization","Chain-of-Thought","Efficiency","Mathematical Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen 핵심 연구 목표 대규모 추론 모델의 ChainofThought(CoT) 방식이 직면한 "},{"id":"2026-02-09-Judging-What-We-Cannot-Solve-A-Consequence-Based-Approach-for-Oracle-Free-Evaluation-of-Research-Level-Math","title":"[논문리뷰] Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math","excerpt":"Amit Agarwal이 arXiv에 게시한 'Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-Judging-What-We-Cannot-Solve-A-Consequence-Based-Approach-for-Oracle-Free-Evaluation-of-Research-Level-Math","tags":["Review","LLM Evaluation","Mathematical Reasoning","Oracle-Free Validation","Consequence-Based Utility","Solution Quality","In-Context Learning","Research-Level Math"],"text":"링크: 논문 PDF로 바로 열기 저자: Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Hyunwoo Ko, Amit Agarwal, Sunghee Ahn, KyongHa Lee, Youngjae Yu 핵심 연구 목표 연구 수준 수학 문제에 대한 LLM(Large Language Model) 생성 솔루션 의 검증은"},{"id":"2026-02-09-MSign-An-Optimizer-Preventing-Training-Instability-in-Large-Language-Models-via-Stable-Rank-Restoration","title":"[논문리뷰] MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration","excerpt":"arXiv에 게시된 'MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-MSign-An-Optimizer-Preventing-Training-Instability-in-Large-Language-Models-via-Stable-Rank-Restoration","tags":["Review","LLM Training Stability","Gradient Explosion","Stable Rank","Jacobian Alignment","Matrix Sign Operation","Optimizer","Transformer"],"text":"링크: 논문 PDF로 바로 열기 저자: Lianhai Ren, Yucheng Ding, Xiao Liu, Qianxiao Li, Peng Cheng, Yeyun Gong 핵심 연구 목표 대규모 언어 모델(LLM) 사전 학습 중 발생하는 갑작스러운 그레디언트 폭발 과 같은 훈련 불안정성 문제를 해결하는 것을 목표로 합니다. 특히, 이러한 불안정성의 근본적인 "},{"id":"2026-02-09-MemGUI-Bench-Benchmarking-Memory-of-Mobile-GUI-Agents-in-Dynamic-Environments","title":"[논문리뷰] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments","excerpt":"arXiv에 게시된 'MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-MemGUI-Bench-Benchmarking-Memory-of-Mobile-GUI-Agents-in-Dynamic-Environments","tags":["Review","Mobile GUI Agents","Memory Benchmarking","Short-Term Memory","Long-Term Memory","LLM-as-Judge","Dynamic Environments","Evaluation Metrics","Task Automation"],"text":"링크: 논문 PDF로 바로 열기 저자: Guangyi Liu, Pengxiang Zhao, Yaozhen Liang, Qinyi Luo, Shunye Tang, Yuxiang Chai, Weifeng Lin, Han Xiao, WenHao Wang, Siheng Chen, Zhengxi Lu, Gao Wu, Hao Wang, Liang Liu, Yong L"},{"id":"2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions","title":"[논문리뷰] OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions","excerpt":"heroding77이 arXiv에 게시한 'OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions","tags":["Review","LLM Agents","Benchmarking","Inductive Reasoning","Long-Horizon Tasks","Active Exploration","World Models","Autonomous Discovery"],"text":"링크: 논문 PDF로 바로 열기 저자: Fangzhi Xu, Hang Yan, Qiushi Sun, Jinyang Wu, Zixian Huang, Muye Huang, Jingyang Gong, Zichen Ding, Kanzhi Cheng, Jian Zhang, Yian Wang, Xinyu Che, Zeyi Sun, Zhangyue Yin, Haoran"},{"id":"2026-02-09-OmniMoE-An-Efficient-MoE-by-Orchestrating-Atomic-Experts-at-Scale","title":"[논문리뷰] OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale","excerpt":"arXiv에 게시된 'OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-OmniMoE-An-Efficient-MoE-by-Orchestrating-Atomic-Experts-at-Scale","tags":["Review","Mixture-of-Experts (MoE)","Fine-Grained Experts","Efficient Architectures","Transformer","Routing Algorithms","Hardware Acceleration","Sparse Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingze Shi, Zhangyang Peng, Yizhang Zhu, Yifan Wu, Guang Liu, Yuyu Luo 핵심 연구 목표 본 논문은 MoE 아키텍처에서 전문가 전문화의 세분성과 하드웨어 실행 효율성 사이의 본질적인 tradeoff를 해결하는 것을 목표로 합니다. 기존 finegrained MoE의"},{"id":"2026-02-09-On-the-Entropy-Dynamics-in-Reinforcement-Fine-Tuning-of-Large-Language-Models","title":"[논문리뷰] On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models","excerpt":"Yanxi Chen이 arXiv에 게시한 'On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-On-the-Entropy-Dynamics-in-Reinforcement-Fine-Tuning-of-Large-Language-Models","tags":["Review","Reinforcement Fine-Tuning (RFT)","Large Language Models (LLMs)","Entropy Dynamics","Exploration-Exploitation","Policy Optimization","GRPO","Entropy Control","Discriminator Score"],"text":"링크: 논문 PDF로 바로 열기 저자: Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen, Yaliang Li, Yanyong Zhang 핵심 연구 목표 본 논문은 LLM의 강화 학습 미세 조정(RFT) 과정에서 발생하는 엔트로피 동학에 대한 이론적인 이해를 확립하고, 탐색활용(explora"},{"id":"2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey","title":"[논문리뷰] POINTS-GUI-G: GUI-Grounding Journey","excerpt":"Le Tian이 arXiv에 게시한 'POINTS-GUI-G: GUI-Grounding Journey' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey","tags":["Review","GUI Grounding","Vision-Language Models (VLMs)","Reinforcement Learning (RL)","Data Engineering","UI Automation","Perception-intensive AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongyin Zhao, Yuan Liu†, Yikun Liu, Haicheng Wang, Le Tian, Xiao Zhou, Yangxiu You, Zilin Yu, Yang Yu, Jie Zhou 핵심 연구 목표 본 논문은 최소한의 GUI grounding 능력을 가진 POINTS1.5 와 같은 기반 모델에서 출"},{"id":"2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks","title":"[논문리뷰] PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks","excerpt":"Zhixin Wang이 arXiv에 게시한 'PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks","tags":["Review","Multimodal Models","Image Generation","Image Editing","Benchmark","Computer-Use Tasks","Planning","Evaluation Metrics"],"text":"링크: 논문 PDF로 바로 열기 저자: Junxian Li†¹, Kai Liu †¹, Leyang Chen¹, Weida Wang², Zhixin Wang¹, Jiaqi Xu³, Fan Li³, Renjing Pei³, Linghe Kong¹, Yulun Zhang ¹ 핵심 연구 목표 본 논문은 통합 멀티모달 모델(UMMs)이 일상생활과 밀접한 컴퓨터 사용"},{"id":"2026-02-09-RaBiT-Residual-Aware-Binarization-Training-for-Accurate-and-Efficient-LLMs","title":"[논문리뷰] RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs","excerpt":"arXiv에 게시된 'RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-RaBiT-Residual-Aware-Binarization-Training-for-Accurate-and-Efficient-LLMs","tags":["Review","LLM Quantization","2-bit Quantization","Residual Binarization","Quantization-Aware Training (QAT)","Inter-Path Adaptation","Hardware Efficiency","Model Compression","Low-Bit LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Youngcheon You, Banseok Lee, Minseop Choi, Seonyoung Kim, Hyochan Chong, Changdong Kim, Youngmin Kim, Dongkyu Kim 핵심 연구 목표 논문은 LLM의 극단적인 2비트 양자화에서 발생하는 성능과 효율성 간의 치명적인 트레이드오프 를 해"},{"id":"2026-02-09-SEMA-Simple-yet-Effective-Learning-for-Multi-Turn-Jailbreak-Attacks","title":"[논문리뷰] SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks","excerpt":"arXiv에 게시된 'SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-SEMA-Simple-yet-Effective-Learning-for-Multi-Turn-Jailbreak-Attacks","tags":["Review","Multi-Turn Jailbreaks","LLM Safety","Red Teaming","Reinforcement Learning","Intent Drift","Response-Agnostic Generation","Self-Tuning"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingqian Feng, Xiaodong Liu, Weiwei Yang, Jialin Song, Xuekai Zhu, Chenliang Xu, Jianfeng Gao 핵심 연구 목표 기존의 다중 턴(multiturn) 탈옥(jailbreak) 공격 방법론들이 겪는 탐색 복잡성 과 의도 왜곡(intent drift) "},{"id":"2026-02-09-Self-Improving-Multilingual-Long-Reasoning-via-Translation-Reasoning-Integrated-Training","title":"[논문리뷰] Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training","excerpt":"Liqian Huang이 arXiv에 게시한 'Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-Self-Improving-Multilingual-Long-Reasoning-via-Translation-Reasoning-Integrated-Training","tags":["Review","Multilingual Reasoning","Reinforcement Learning","Machine Translation","Question Understanding","Self-Improvement","Language Models","Cross-Lingual Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang 핵심 연구 목표 다국어 환경에서 긴 추론 모델( LRMs )이 겪는 어려움, 즉 비영어권 질문에 대해 영어로 추론하려는 "},{"id":"2026-02-09-Self-Improving-World-Modelling-with-Latent-Actions","title":"[논문리뷰] Self-Improving World Modelling with Latent Actions","excerpt":"Anna Korhonen이 arXiv에 게시한 'Self-Improving World Modelling with Latent Actions' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-09 00:00:00+0900+0900","permalink":"/ai/review/2026-02-09-Self-Improving-World-Modelling-with-Latent-Actions","tags":["Review","World Modeling","Latent Actions","Self-Improvement","Reinforcement Learning","LLMs","VLMs","Inverse Dynamics Model","Forward World Modelling"],"text":"링크: 논문 PDF로 바로 열기 저자: Yifu Qiu, Zheng Zhao, Waylon Li, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti 핵심 연구 목표 본 논문은 액션이 레이턴트 변수로 취급되는 상태온리 시퀀스 로부터 LLM(Large Language Models) 및 VLM(Vision"},{"id":"2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents","title":"[논문리뷰] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents","excerpt":"arXiv에 게시된 'AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents","tags":["Review","AI Research Agents","LLM Agents","Machine Learning Benchmarks","Scientific Discovery","Code Generation","Evaluation Metrics","Scaffolds","Reproducibility"],"text":"링크: 논문 PDF로 바로 열기 저자: Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster, et al. 핵심 연구 목표 본 논문의 핵심 목표는 LLM 에이전트의 과학 연구 역량을 종합적으로 평가할 수 있는 표준화된 벤치마크인 AIRSBENCH 를 도입하는 것입니다. 기존 벤치마크의 데이터 오염, 환경 비표준화, 높은 계"},{"id":"2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research","title":"[논문리뷰] AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research","excerpt":"arXiv에 게시된 'AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research","tags":["Review","Deep Research","Agentic Systems","Writing As Reasoning Policy (WARP)","Outline Generation","Iterative Refinement","Reinforcement Learning (RL)","Small Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: AgentCPM Team 핵심 연구 목표 본 논문은 기존 언어 모델 기반 심층 연구 보고서 생성 시스템들이 겪는 한계를 극복하는 것을 목표로 합니다. 특히, 정적 계획에 의존하여 통찰력에 제한이 있고, 배포 및 데이터 보안 문제로 인해 대규모의 독점 모델에 의존하는 경향을 해소하고자 합니다. 핵심 방법론 제안하는 Ag"},{"id":"2026-02-10-Alleviating-Sparse-Rewards-by-Modeling-Step-Wise-and-Long-Term-Sampling-Effects-in-Flow-Based-GRPO","title":"[논문리뷰] Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO","excerpt":"arXiv에 게시된 'Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Alleviating-Sparse-Rewards-by-Modeling-Step-Wise-and-Long-Term-Sampling-Effects-in-Flow-Based-GRPO","tags":["Review","Reinforcement Learning","Flow Matching","Text-to-Image Generation","Sparse Rewards","Credit Assignment","Turning Points","Group Relative Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunze Tong, Mushui Liu, Canyu Zhao, Wanggui He, Shiyi Zhang, Hongwei Zhang, Peng Zhang, Jinlong Liu, Ju Huang, Jiamang Wang, Hao Jiang, Pipei Huang 핵심 연구 목표 본 논문은 텍스트투이미지 생성에 Flo"},{"id":"2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition","title":"[논문리뷰] Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition","excerpt":"arXiv에 게시된 'Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition","tags":["Review","Video Understanding","In-Context Learning","Procedural Knowledge","Multimodal LLMs","Benchmark","Direct Preference Optimization","Demonstration Selection"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhao Dong, Shulin Tian, Shuai Liu, Shuangrui Ding, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Ziwei Liu 핵심 연구 목표 본 논문은 기존 MLLM(Multimodal Large Language Models)이 정적이고 내부적"},{"id":"2026-02-10-Fundamental-Reasoning-Paradigms-Induce-Out-of-Domain-Generalization-in-Language-Models","title":"[논문리뷰] Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models","excerpt":"Maria Liakata이 arXiv에 게시한 'Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Fundamental-Reasoning-Paradigms-Induce-Out-of-Domain-Generalization-in-Language-Models","tags":["Review","LLM Reasoning","Deduction","Induction","Abduction","Out-of-Domain Generalization","Symbolic Reasoning","Fine-tuning","Upcycling"],"text":"링크: 논문 PDF로 바로 열기 저자: Mingzi Cao, Xingwei Tan, Mahmud Akhter, Maria Liakata, Marco Valentino, Nikolaos Aletras 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 연역, 귀납, 귀추 와 같은 근본적인 추론 패러다임을 습득할 때, 세계 지식으로부터 분리된 상징적 추론 "},{"id":"2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments","title":"[논문리뷰] GEBench: Benchmarking Image Generation Models as GUI Environments","excerpt":"arXiv에 게시된 'GEBench: Benchmarking Image Generation Models as GUI Environments' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments","tags":["Review","GUI Generation","Image Generation Models","Benchmark","Temporal Coherence","Spatial Grounding","Evaluation Metric","Vision Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Haodong Li, Jingwei Wu, Quan Sun, Guopeng Li, Juanxi Tian, Huanyu Zhang, Yanlin Lai, Ruichuan An, Hongbo Peng, Yuhong Dai, Chenxi Li, Chunmei Qing, Jia Wang, Ziyang Meng, Zheng G"},{"id":"2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant","title":"[논문리뷰] GISA: A Benchmark for General Information-Seeking Assistant","excerpt":"arXiv에 게시된 'GISA: A Benchmark for General Information-Seeking Assistant' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant","tags":["Review","Search Agents","Information Seeking","Benchmark","LLM-driven Agents","Human Trajectories","Deep and Wide Search","Deterministic Evaluation","Dynamic Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Yutao Zhu, Xingshuo Zhang, Maosen Zhang, Jiajie Jin, Liancheng Zhang, Xiaoshuai Song, Kangzhi Zhao, Wencong Zeng, Ruiming Tang, Han Li, JiRong Wen, Zhicheng Dou 핵심 연구 목표 기존 검색 에이"},{"id":"2026-02-10-InternAgent-1-5-A-Unified-Agentic-Framework-for-Long-Horizon-Autonomous-Scientific-Discovery","title":"[논문리뷰] InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery","excerpt":"Xiangchao Yan이 arXiv에 게시한 'InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-InternAgent-1-5-A-Unified-Agentic-Framework-for-Long-Horizon-Autonomous-Scientific-Discovery","tags":["Review","Agentic AI","Scientific Discovery","Long-Horizon Reasoning","Structured Memory","Knowledge Graph","Experimental Optimization","Multi-disciplinary"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiangchao Yan, Runmin Ma, JiakangYuan, huangst, sY713 핵심 연구 목표 본 논문은 기존 AI 과학자 시스템의 한계(도메인 특화 설계, 불완전한 추론 능력, 비효율적인 최적화 파이프라인, 장기 자율 운영 미흡)를 극복하고, 계산 및 경험적 영역 전반에 걸쳐 엔드투엔드 과학적 발견"},{"id":"2026-02-10-LLaDA2-1-Speeding-Up-Text-Diffusion-via-Token-Editing","title":"[논문리뷰] LLaDA2.1: Speeding Up Text Diffusion via Token Editing","excerpt":"arXiv에 게시된 'LLaDA2.1: Speeding Up Text Diffusion via Token Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-LLaDA2-1-Speeding-Up-Text-Diffusion-via-Token-Editing","tags":["Review","Text Diffusion","Token Editing","Inference Acceleration","Mask-to-Token","Token-to-Token","Reinforcement Learning","Speedy Mode","Quality Mode"],"text":"링크: 논문 PDF로 바로 열기 저자: Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen, Kun Chen, Lun Du, Daozhuo Feng, Haibo Feng, Mingliang Gong, Zhuocheng Gong, Yanmei Gu, Jian Guan, Kaiyuan Guan, Hong"},{"id":"2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth","title":"[논문리뷰] LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth","excerpt":"arXiv에 게시된 'LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth","tags":["Review","Large Language Models","Language Agents","Long Context","Context Rot","Benchmarking","Context Management","Tool Use","Agent Evaluation","Dynamic Environments"],"text":"링크: 논문 PDF로 바로 열기 저자: Weihao Zeng, Yuzhen Huang, Junxian He 핵심 연구 목표 본 논문은 대규모 언어 모델(LLMs) 기반의 언어 에이전트가 실세계의 장기 실행 태스크를 수행할 때 발생하는 \"컨텍스트 로트(context rot)\" 현상, 즉 컨텍스트 길이가 증가함에 따른 성능 저하 문제를 해결하고자 합니다. 기존"},{"id":"2026-02-10-LatentChem-From-Textual-CoT-to-Latent-Thinking-in-Chemical-Reasoning","title":"[논문리뷰] LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning","excerpt":"Jia Zhang이 arXiv에 게시한 'LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-LatentChem-From-Textual-CoT-to-Latent-Thinking-in-Chemical-Reasoning","tags":["Review","Chemical Reasoning","Large Language Models (LLMs)","Chain-of-Thought (CoT)","Latent Space","Molecular Optimization","Inference Efficiency","Reinforcement Learning","Chemical AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinwu Ye, Yicheng Mao, Jia Zhang, Yimeng Liu, et al. 핵심 연구 목표 화학 분야의 대규모 언어 모델(LLMs)이 명시적인 자연어 ChainofThought (CoT) 추론에 과도하게 의존하여 발생하는 \"연속성이산성 격차(continuitydiscretization gap)\" 문"},{"id":"2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory","title":"[논문리뷰] Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory","excerpt":"arXiv에 게시된 'Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory","tags":["Review","LLM Agents","Runtime Memory","Budget-Tier Routing","Reinforcement Learning","Performance-Cost Trade-off","Modular Memory Pipeline","Query-Aware Memory","Resource Management"],"text":"링크: 논문 PDF로 바로 열기 저자: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang 핵심 연구 목표 이 논문은 LLM 에이전트의 기존 오프라인, 쿼리불가지론적 "},{"id":"2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation","title":"[논문리뷰] MOVA: Towards Scalable and Synchronized Video-Audio Generation","excerpt":"arXiv에 게시된 'MOVA: Towards Scalable and Synchronized Video-Audio Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation","tags":["Review","Video-Audio Generation","Diffusion Transformer","Multimodal AI","Lip Synchronization","Open Source","Data Curation","Dual-Tower Architecture","Cross-Attention"],"text":"링크: 논문 PDF로 바로 열기 저자: SIIOpenMOSS Team 핵심 연구 목표 기존 비디오 생성 모델에서 간과되던 오디오 요소를 통합하여, 고품질의 동기화된 비디오오디오 콘텐츠를 생성 하는 오픈 소스 모델 MOVA 를 개발하는 것이 목표입니다. 특히, 폐쇄형 시스템인 Veo3 및 Sora 2의 한계를 극복하고, 정확한 립싱크 음성, 환경 인식 사운드"},{"id":"2026-02-10-Modality-Gap-Driven-Subspace-Alignment-Training-Paradigm-For-Multimodal-Large-Language-Models","title":"[논문리뷰] Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models","excerpt":"Hanzhen Zhao이 arXiv에 게시한 'Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Modality-Gap-Driven-Subspace-Alignment-Training-Paradigm-For-Multimodal-Large-Language-Models","tags":["Review","Multimodal Large Language Models (MLLMs)","Modality Gap","Subspace Alignment","Unpaired Data","Representation Learning","Pretraining","Geometric Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaomin Yu, Yi Xin, Wenjie Zhang, Chonghan Liu, Hanzhen Zhao, Xiaoxing Hu, Xinlei Yu, Ziyue Qiao, Hao Tang, Xue Yang, Xiaobin Hu, Chengwei Qin, Hui Xiong, Yu Qiao, Shuicheng YAN "},{"id":"2026-02-10-QuantaAlpha-An-Evolutionary-Framework-for-LLM-Driven-Alpha-Mining","title":"[논문리뷰] QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining","excerpt":"arXiv에 게시된 'QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-QuantaAlpha-An-Evolutionary-Framework-for-LLM-Driven-Alpha-Mining","tags":["Review","Alpha Mining","LLM-Driven Agents","Evolutionary Algorithms","Financial Markets","Factor Generation","Trajectory Optimization","Quantitative Investment"],"text":"링크: 논문 PDF로 바로 열기 저자: Jun Han, Shuo Zhang, Wei Li, Zhi Yang, Yifan Dong, Tu Hu, Jialuo Yuan, Xiaomin Yu, Yumo Zhu, Fangqi Lou, Xin Guo, Zhaowei Liu, Tianyi Jiang, Ruichuan An, Jingping Liu, Biao Wu, R"},{"id":"2026-02-10-Recurrent-Depth-VLA-Implicit-Test-Time-Compute-Scaling-of-Vision-Language-Action-Models-via-Latent-Iterative-Reasoning","title":"[논문리뷰] Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning","excerpt":"arXiv에 게시된 'Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Recurrent-Depth-VLA-Implicit-Test-Time-Compute-Scaling-of-Vision-Language-Action-Models-via-Latent-Iterative-Reasoning","tags":["Review","Vision-Language-Action Models","Latent Iterative Reasoning","Adaptive Compute","Recurrent Neural Networks","Robotics","Transformer","Test-Time Scaling","Continuous Action Space"],"text":"링크: 논문 PDF로 바로 열기 저자: Yalcin Tur, Jalal Naghiyev, Haoquan Fang, WeiChuan Tsai, Jiafei Duan, Dieter Fox, Ranjay Krishna 핵심 연구 목표 기존 VLA 모델의 고정된 연산 깊이로 인한 비효율성과 토큰 기반 추론의 메모리 및 연속적인 액션 공간 한계를 해결합니다. 태스크"},{"id":"2026-02-10-RelayGen-Intra-Generation-Model-Switching-for-Efficient-Reasoning","title":"[논문리뷰] RelayGen: Intra-Generation Model Switching for Efficient Reasoning","excerpt":"arXiv에 게시된 'RelayGen: Intra-Generation Model Switching for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-RelayGen-Intra-Generation-Model-Switching-for-Efficient-Reasoning","tags":["Review","LLM Inference Optimization","Model Switching","Efficient Reasoning","Speculative Decoding","Runtime Adaptation","Discourse-Level Cues","Latency Reduction"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiwon Song, Yoonwon Kim, JaeJoon Kim, SooChet National University 핵심 연구 목표 대규모 추론 모델(LRMs)의 긴 추론 과정에서 발생하는 불균일한 생성 난이도 문제를 해결하여, 상당한 정확도 저하 없이 추론 지연 시간을 줄이는 것 을 목표로 합니다. 특히, 추론 과"},{"id":"2026-02-10-Towards-Bridging-the-Gap-between-Large-Scale-Pretraining-and-Efficient-Finetuning-for-Humanoid-Control","title":"[논문리뷰] Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control","excerpt":"Yao Su이 arXiv에 게시한 'Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Towards-Bridging-the-Gap-between-Large-Scale-Pretraining-and-Efficient-Finetuning-for-Humanoid-Control","tags":["Review","Humanoid Control","Reinforcement Learning","SAC","Model-Based RL","Pretraining","Finetuning","Physics-Informed World Model","Sim-to-Real Transfer"],"text":"링크: 논문 PDF로 바로 열기 저자: Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su, Jingwen Zhang 핵심 연구 목표 대규모 사전 훈련(largescale pretraining)과 효율적인 미세 조정(efficient finetuning) 사이의 간극을 줄여 휴머노이드 로봇 제어의 샘플 효율성"},{"id":"2026-02-10-Weak-Driven-Learning-How-Weak-Agents-make-Strong-Agents-Stronger","title":"[논문리뷰] Weak-Driven Learning: How Weak Agents make Strong Agents Stronger","excerpt":"arXiv에 게시된 'Weak-Driven Learning: How Weak Agents make Strong Agents Stronger' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-Weak-Driven-Learning-How-Weak-Agents-make-Strong-Agents-Stronger","tags":["Review","Weak-Driven Learning","LLM Optimization","Post-training","Gradient Amplification","Curriculum Learning","Knowledge Distillation","Mathematical Reasoning","Code Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zehao Chen, Gongxun Li, Tianxiang Ai, Yifei Li, Zixuan Huang, Wang Zhou, Tao Huang, Fuzhen Zhuang, Xianglong Liu, Jianxin Li, Deqing Wang, Yikun Ban 핵심 연구 목표 이 논문은 대규모 언어 모델(LLM)"},{"id":"2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","title":"[논문리뷰] WorldCompass: Reinforcement Learning for Long-Horizon World Models","excerpt":"arXiv에 게시된 'WorldCompass: Reinforcement Learning for Long-Horizon World Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-10 00:00:00+0900+0900","permalink":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","tags":["Review","Reinforcement Learning","World Models","Video Generation","Autoregressive Generation","Long-Horizon","Post-training","Diffusion Models","Reward Functions"],"text":"링크: 논문 PDF로 바로 열기 저자: Zehan Wang, Tengfei Wang, Haiyu Zhang, Xuhui Zuo, Junta Wu, Haoyuan Wang, Wenqiang Sun, Zhenwei Wang, Chenjie Cao, Hengshuang Zhao, Chunchao Guo, Zhou Zhao 핵심 연구 목표 본 논문은 상호작용적 비"},{"id":"2026-02-11-Agent-Banana-High-Fidelity-Image-Editing-with-Agentic-Thinking-and-Tooling","title":"[논문리뷰] Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling","excerpt":"arXiv에 게시된 'Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Agent-Banana-High-Fidelity-Image-Editing-with-Agentic-Thinking-and-Tooling","tags":["Review","Image Editing","Agentic AI","Multi-turn Interaction","High-Fidelity","Native Resolution","LLM","Context Folding","Layer Decomposition"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruijie Ye, Jiayi Zhang, Zhuoxin Liu, Zihao Zhu, Siyuan Yang, Li Li, Tianfu Fu, Franck Dernoncourt, Yue Zhao, Jiacheng Zhu, Ryan Rossi, Wenhao Chai, Zhengzhong Tu 핵심 연구 목표 본 논문은 기"},{"id":"2026-02-11-Agent-World-Model-Infinity-Synthetic-Environments-for-Agentic-Reinforcement-Learning","title":"[논문리뷰] Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning","excerpt":"arXiv에 게시된 'Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Agent-World-Model-Infinity-Synthetic-Environments-for-Agentic-Reinforcement-Learning","tags":["Review","Agentic AI","Reinforcement Learning","Synthetic Environments","Tool-Use Agents","World Model","Database-Backed Simulation","LLM-powered Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 기반 에이전트 훈련을 위한 다양하고 신뢰할 수 있는 환경의 부족 문제 를 해결하고자 합니다. 특히,"},{"id":"2026-02-11-BagelVLA-Enhancing-Long-Horizon-Manipulation-via-Interleaved-Vision-Language-Action-Generation","title":"[논문리뷰] BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation","excerpt":"Xiaoyu Chen이 arXiv에 게시한 'BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-BagelVLA-Enhancing-Long-Horizon-Manipulation-via-Interleaved-Vision-Language-Action-Generation","tags":["Review","Long-horizon manipulation","Embodied AI","Vision-Language-Action (VLA)","Interleaved planning","Visual forecasting","Residual Flow Guidance","Multimodal learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yucheng Hu, Jianke Zhang, Yuanfei Luo, Yanjiang Guo, Xiaoyu Chen 핵심 연구 목표 본 논문은 복잡하고 장기적인 로봇 조작 작업을 위해 언어적 계획, 시각적 예측, 행동 생성 을 통합하는 통일된 프레임워크를 개발하는 것을 목표로 합니다. 기존 VisionLanguageA"},{"id":"2026-02-11-Chain-of-Mindset-Reasoning-with-Adaptive-Cognitive-Modes","title":"[논문리뷰] Chain of Mindset: Reasoning with Adaptive Cognitive Modes","excerpt":"arXiv에 게시된 'Chain of Mindset: Reasoning with Adaptive Cognitive Modes' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Chain-of-Mindset-Reasoning-with-Adaptive-Cognitive-Modes","tags":["Review","Adaptive Reasoning","Cognitive Modes","Large Language Models (LLMs)","Agentic AI","Multimodal Reasoning","Mindset Orchestration","Contextual Filtering","Training-free Framework"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyi Jiang, Arctanx An, Hengyi Feng, et al. 핵심 연구 목표 기존 LLM(대규모 언어 모델)의 고정된 단일 사고방식 추론 방식이 문제 해결의 여러 단계에서 요구되는 이질적인 인지적 요구를 충족하지 못하는 한계를 해결하고자 합니다. 본 연구는 단계별로 적응적인 사고방식을 유연하게 조"},{"id":"2026-02-11-Code2World-A-GUI-World-Model-via-Renderable-Code-Generation","title":"[논문리뷰] Code2World: A GUI World Model via Renderable Code Generation","excerpt":"arXiv에 게시된 'Code2World: A GUI World Model via Renderable Code Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Code2World-A-GUI-World-Model-via-Renderable-Code-Generation","tags":["Review","GUI World Model","Renderable Code Generation","Vision-Language Model","Reinforcement Learning","HTML Synthesis","UI Prediction","GUI Agents"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuhao Zheng, Lian Zhong, Yi Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu, Linyuan Lü, Philip Torr, Kevin Qinghong Lin 핵심 연구 목표 본 논문은 기존 텍스트 및 픽셀 기반 GUI 월드 모델이 가지는 시각적 충실도와 세밀한 구조적 제"},{"id":"2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss","title":"[논문리뷰] Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss","excerpt":"arXiv에 게시된 'Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss","tags":["Review","Autoregressive Models","Diffusion Models","Image Generation","Condition Refinement","Optimal Transport","Wasserstein Gradient Flow","Score Matching","Patch Denoising"],"text":"링크: 논문 PDF로 바로 열기 저자: Yucheng Zhou, Hao Li, Jianbing Shen 핵심 연구 목표 본 연구는 오토회귀(Autoregressive) 이미지 생성 모델 이 확산 손실(diffusion loss) 과 결합될 때 발생하는 \"조건 불일치(condition inconsistency)\" 문제를 해결하고, 이로 인해 누적되는 extr"},{"id":"2026-02-11-DLLM-Searcher-Adapting-Diffusion-Large-Language-Model-for-Search-Agents","title":"[논문리뷰] DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents","excerpt":"arXiv에 게시된 'DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-DLLM-Searcher-Adapting-Diffusion-Large-Language-Model-for-Search-Agents","tags":["Review","Diffusion Large Language Models","Search Agents","Latency Reduction","P-ReAct","Agentic Post-training","Supervised Fine-Tuning","Preference Optimization","Parallel Decoding"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahao Zhao, Shaoxuan Xu, Zhongxiang Sun, Yuling Shi, Chongxuan Li, Fengqi Zhu, Jingyang Ou, Jun Xu, Xiao Zhang 핵심 연구 목표 본 논문은 기존 Autoregressive 모델(ARM) 기반 검색 에이전트의 직렬 실행 구조로 인한 "},{"id":"2026-02-11-Dr-MAS-Stable-Reinforcement-Learning-for-Multi-Agent-LLM-Systems","title":"[논문리뷰] Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems","excerpt":"arXiv에 게시된 'Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Dr-MAS-Stable-Reinforcement-Learning-for-Multi-Agent-LLM-Systems","tags":["Review","Multi-Agent LLM","Reinforcement Learning","Training Stability","GRPO","Agent-wise Normalization","Gradient Explosion","LLM Orchestration"],"text":"링크: 논문 PDF로 바로 열기 저자: Lang Feng, Longtao Zheng, Shuo He, Fuxiang Zhang, Bo An 핵심 연구 목표 다중 에이전트 LLM 시스템의 강화 학습(RL) 사후 훈련 시 발생하는 불안정성의 핵심 원인을 규명하고, 이를 해결하여 안정적인 훈련을 가능하게 하는 새로운 방법론을 제안하는 것입니다. 특히 그룹 기반 "},{"id":"2026-02-11-Dynamic-Long-Context-Reasoning-over-Compressed-Memory-via-End-to-End-Reinforcement-Learning","title":"[논문리뷰] Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning","excerpt":"arXiv에 게시된 'Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Dynamic-Long-Context-Reasoning-over-Compressed-Memory-via-End-to-End-Reinforcement-Learning","tags":["Review","Long Context Reasoning","Memory Compression","Reinforcement Learning","Large Language Models (LLMs)","Inference Efficiency","Dynamic Recall","KV-Cache","Multi-hop Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang 핵심 연구 목표 대규모 언어 모델(LLMs)이 직면한 긴 컨텍스트 처리의 문제를 해결하는 것이 목표입니다. 특히 연산 비용 증가 , 정보 망각 , 그리고 RAG(RetrievalAugmented Genera"},{"id":"2026-02-11-OPUS-Towards-Efficient-and-Principled-Data-Selection-in-Large-Language-Model-Pre-training-in-Every-Iteration","title":"[논문리뷰] OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration","excerpt":"arXiv에 게시된 'OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-OPUS-Towards-Efficient-and-Principled-Data-Selection-in-Large-Language-Model-Pre-training-in-Every-Iteration","tags":["Review","Data Selection","Large Language Model","Pre-training","Optimizer-Induced Utility","Ghost Technique","CountSketch","Boltzmann Sampling"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaobo Wang, Xuan Ouyang, Tianyi Xu, Yuzheng Hu, Jialin Liu, Guo Chen, Tianyu Zhang, Junhao Zheng, Kexin Yang, Xingzhang Ren, Dayiheng Liu, Linfeng Zhang 핵심 연구 목표 대규모 언어 모델(LLM) "},{"id":"2026-02-11-Olaf-World-Orienting-Latent-Actions-for-Video-World-Modeling","title":"[논문리뷰] Olaf-World: Orienting Latent Actions for Video World Modeling","excerpt":"Mike Zheng Shou이 arXiv에 게시한 'Olaf-World: Orienting Latent Actions for Video World Modeling' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Olaf-World-Orienting-Latent-Actions-for-Video-World-Modeling","tags":["Review","Video World Models","Latent Actions","Cross-context Transfer","Zero-shot Action Transfer","Data-efficient Adaptation","Self-supervised Learning","Representation Alignment"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuxin Jiang, Yuchao Gu, Ivor W. Tsang, Mike Zheng Shou 핵심 연구 목표 본 논문은 액션 레이블의 희소성으로 인해 액션제어 가능한 월드 모델의 확장이 제한되는 문제를 해결하고자 합니다. 특히, 기존 잠재 액션 학습 방식이 컨텍스트(장면, 시점 등)에 따라 액션 의미론이 일관되지"},{"id":"2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads","title":"[논문리뷰] P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads","excerpt":"arXiv에 게시된 'P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads","tags":["Review","Vision-Language Models","Reinforcement Learning","Curriculum Learning","Physics Olympiads","Scientific Reasoning","Agentic AI","Multimodal AI","Physics"],"text":"링크: 논문 PDF로 바로 열기 저자: Yun Sao, Jia Cheng, Fang Chen, Yu Hai, Yuan Wan, Yu Chen, Zhang Sheng, He Zheng, Jun Chi, Yao Qing, Yang Zhang, Hao Nan, He Yun, Luo Yu, Feng Zhao, Fu Ting, Wang Li, Sheng Cheng,"},{"id":"2026-02-11-Prism-Spectral-Aware-Block-Sparse-Attention","title":"[논문리뷰] Prism: Spectral-Aware Block-Sparse Attention","excerpt":"arXiv에 게시된 'Prism: Spectral-Aware Block-Sparse Attention' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Prism-Spectral-Aware-Block-Sparse-Attention","tags":["Review","Block-Sparse Attention","Long-Context LLM","Rotary Positional Embeddings","Spectral Analysis","Attention Efficiency","Pre-filling Acceleration"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinghao Wang, Pengyu Wang, Xiaoran Liu, Fangxu Liu, Jason Chu, Kai Song, Xipeng Qiu 핵심 연구 목표 긴 컨텍스트를 처리하는 LLM의 prefilling 과정 을 가속화하기 위한 블록희소 어텐션(blocksparse attention)의 효율적인 블록 중"},{"id":"2026-02-11-Rethinking-Global-Text-Conditioning-in-Diffusion-Transformers","title":"[논문리뷰] Rethinking Global Text Conditioning in Diffusion Transformers","excerpt":"Yuchen Liu이 arXiv에 게시한 'Rethinking Global Text Conditioning in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-Rethinking-Global-Text-Conditioning-in-Diffusion-Transformers","tags":["Review","Diffusion Transformers","Text Conditioning","CLIP Embedding","Modulation Guidance","Text-to-Image Generation","Image Editing","Training-free"],"text":"링크: 논문 PDF로 바로 열기 저자: Nikita Starodubcev, Daniil Pakhomov, Zongze Wu, Yuchen Liu, Zhonghao Wang, Yuqian Zhou, Zhe Lin, Ilya Drobyshevskiy, Dmitry Baranchuk 핵심 연구 목표 이 논문은 확산 트랜스포머(Diffusion Transforme"},{"id":"2026-02-11-SAGE-Scalable-Agentic-3D-Scene-Generation-for-Embodied-AI","title":"[논문리뷰] SAGE: Scalable Agentic 3D Scene Generation for Embodied AI","excerpt":"arXiv에 게시된 'SAGE: Scalable Agentic 3D Scene Generation for Embodied AI' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-SAGE-Scalable-Agentic-3D-Scene-Generation-for-Embodied-AI","tags":["Review","Embodied AI","3D Scene Generation","Agentic Framework","Simulation-Ready Environments","Robot Policy Learning","Large Language Models (LLM)","Physics Simulation","Data Augmentation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongchi Xia, Xuan Li, Zhaoshuo Li, Qianli Ma, Jiashu Xu, MingYu Liu, Yin Cui, TsungYi Lin, WeiChiu Ma, Shenlong Wang, Shuran Song, Fangyin Wei 핵심 연구 목표 본 논문은 Embodied AI 의 고비용 및 "},{"id":"2026-02-11-SCALE-Self-uncertainty-Conditioned-Adaptive-Looking-and-Execution-for-Vision-Language-Action-Models","title":"[논문리뷰] SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models","excerpt":"arXiv에 게시된 'SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-SCALE-Self-uncertainty-Conditioned-Adaptive-Looking-and-Execution-for-Vision-Language-Action-Models","tags":["Review","Vision-Language-Action Models","Self-Uncertainty Estimation","Adaptive Inference","Active Perception","Action Decoding","Visual Attention","Robotic Manipulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hyeonbeom Choi, Daehul Ahn, Youhan Lee, Taewook Kang, Seongwon Cho, Jonghyun Choi 핵심 연구 목표 VisionLanguageAction (VLA) 모델의 고정된 추론 파이프라인이 지각적 모호성이나 행동의 다중 양상과 같은 불확실한 상황에서 오류를 누적하는"},{"id":"2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training","title":"[논문리뷰] ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training","excerpt":"arXiv에 게시된 'ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training","tags":["Review","Environment Synthesis","Tool-Use Agents","Reinforcement Learning","Generalization","Procedural Generation","LLM Agents","Interactive Environments","Data Scaling"],"text":"링크: 논문 PDF로 바로 열기 저자: Dunwei Tu, Hongyan Hao, Hansi Yang, Yihao Chen, YiKai Zhang, Zhikang Xia, Yu Yang, Yueqing Sun, Xingchen Liu, Furao Shen, Qi Gu, Hui Su, Xunliang Cai 핵심 연구 목표 본 논문은 일반 목적의 도구 사용 "},{"id":"2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning","title":"[논문리뷰] SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning","excerpt":"arXiv에 게시된 'SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning","tags":["Review","LLM Agents","Reinforcement Learning","Skill Discovery","Recursive Evolution","Experience Distillation","Hierarchical Skills","Context Efficiency","Task Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Peng Xia, Jianwen Chen, Hanyang Wang, Jiaqi Liu, Kaide Zeng, Yu Wang, Siwei Han, Yiyang Zhou, Xujiang Zhao, Haifeng Chen, Zeyu Zheng, Cihang Xie, Huaxiu Yao 핵심 연구 목표 LLM(Large La"},{"id":"2026-02-11-TreeCUA-Efficiently-Scaling-GUI-Automation-with-Tree-Structured-Verifiable-Evolution","title":"[논문리뷰] TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution","excerpt":"Liming Zheng이 arXiv에 게시한 'TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-TreeCUA-Efficiently-Scaling-GUI-Automation-with-Tree-Structured-Verifiable-Evolution","tags":["Review","GUI Automation","Computer-Use Agents","Trajectory Synthesis","Tree-Structured Exploration","Multi-Agent Framework","Reinforcement Learning","DPO","Data Efficiency"],"text":"링크: 논문 PDF로 바로 열기 저자: Deyang Jiang, Jing Huang, Xuanle Zhao, Lei Chen, Liming Zheng, Fanfan Liu, Haibo Qiu, Peng Shi, Zhixiong Zeng 핵심 연구 목표 본 연구는 GUI 자동화의 핵심 과제인 GUI 플래닝의 확장성 문제를 해결하는 것을 목표로 합니다. 기존 "},{"id":"2026-02-11-UI-Venus-1-5-Technical-Report","title":"[논문리뷰] UI-Venus-1.5 Technical Report","excerpt":"arXiv에 게시된 'UI-Venus-1.5 Technical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-UI-Venus-1-5-Technical-Report","tags":["Review","GUI Agent","MLLM","Reinforcement Learning","Model Merging","GUI Grounding","Task Navigation","Online-RL","Offline-RL"],"text":"링크: 논문 PDF로 바로 열기 저자: Venus Team, Ant Group 핵심 연구 목표 본 논문은 기존 GUI 에이전트의 일반성 및 일관된 고성능 달성 문제를 해결하기 위해, 강력한 실제 애플리케이션을 위한 통합된 엔드투엔드 GUI 에이전트인 UIVenus1.5 를 제안합니다. 이전 버전의 한계를 극복하고 다양한 GUI 태스크에서 최첨단 성능을 확립"},{"id":"2026-02-11-VLA-JEPA-Enhancing-Vision-Language-Action-Model-with-Latent-World-Model","title":"[논문리뷰] VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model","excerpt":"Zezhi Liu이 arXiv에 게시한 'VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-VLA-JEPA-Enhancing-Vision-Language-Action-Model-with-Latent-World-Model","tags":["Review","Vision-Language-Action (VLA)","Latent World Model","JEPA","Pretraining","Robot Learning","Generalization","Robustness","Human Videos"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingwen Sun, Wenyao Zhang, Zekun Qi, Shaojie Ren, Zezhi Liu, Hanxin Zhu, Guangzhong Sun, Xin Jin, Zhibo Chen 핵심 연구 목표 기존 VLA 정책의 잠재액션 목표가 픽셀 변화에 고착되어 외형 편향, 불필요한 움직임, 정보 누출에 취약한 "},{"id":"2026-02-11-VideoWorld-2-Learning-Transferable-Knowledge-from-Real-world-Videos","title":"[논문리뷰] VideoWorld 2: Learning Transferable Knowledge from Real-world Videos","excerpt":"arXiv에 게시된 'VideoWorld 2: Learning Transferable Knowledge from Real-world Videos' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-11 00:00:00+0900+0900","permalink":"/ai/review/2026-02-11-VideoWorld-2-Learning-Transferable-Knowledge-from-Real-world-Videos","tags":["Review","Transferable Knowledge","Real-world Video Learning","Latent Dynamics Model","Video Diffusion","Robotics Manipulation","Long-horizon Tasks","Unlabeled Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongwei Ren, Yunchao Wei, Xiao Yu, Guixun Luo, Yao Zhao, Bingyi Kang, Jiashi Feng, Xiaojie Jin 핵심 연구 목표 본 연구는 복잡하고 장기적인 태스크를 위해 레이블이 없는 실세계 비디오 데이터 로부터 전이 가능한 지식을 학습하는 것을 목표로 합니"},{"id":"2026-02-12-ASA-Training-Free-Representation-Engineering-for-Tool-Calling-Agents","title":"[논문리뷰] ASA: Training-Free Representation Engineering for Tool-Calling Agents","excerpt":"Hongwei Zeng이 arXiv에 게시한 'ASA: Training-Free Representation Engineering for Tool-Calling Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-ASA-Training-Free-Representation-Engineering-for-Tool-Calling-Agents","tags":["Review","Tool-Calling Agents","LLM Adaptation","Representation Engineering","Activation Steering","Training-Free","Inference-Time Control","Domain Adaptation"],"text":"링크: 논문 PDF로 바로 열기 저자: Youjin Wang, Run Zhou, Rong Fu, Shuaishuai Cao, Hongwei Zeng, Jiaxian Lu, Sicheng Fan, Jiaqiao Zhao, Liangming Pan 핵심 연구 목표 본 논문은 진화하는 인터페이스, 스키마 변화 및 엄격한 파서 조건 하에서 LLM 에이전트의 도구 "},{"id":"2026-02-12-Blockwise-Advantage-Estimation-for-Multi-Objective-RL-with-Verifiable-Rewards","title":"[논문리뷰] Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards","excerpt":"arXiv에 게시된 'Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Blockwise-Advantage-Estimation-for-Multi-Objective-RL-with-Verifiable-Rewards","tags":["Review","Reinforcement Learning","LLMs","Credit Assignment","Multi-Objective Optimization","Advantage Estimation","Calibration","Structured Generation","Group Relative Policy Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Kirill Pavlenko, Alexander Golubev, Simon Karasik, Boris Yangel 핵심 연구 목표 GRPO(Group Relative Policy Optimization) 와 같은 기존 RL 방법론이 단일 스칼라 어드밴티지를 사용하여 구조화된 LLM 생성에서 목적 함수 간 간섭과 잘못된"},{"id":"2026-02-12-CLI-Gym-Scalable-CLI-Task-Generation-via-Agentic-Environment-Inversion","title":"[논문리뷰] CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion","excerpt":"Feiyang Pan이 arXiv에 게시한 'CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-CLI-Gym-Scalable-CLI-Task-Generation-via-Agentic-Environment-Inversion","tags":["Review","Agentic Coding","CLI Automation","Environment Inversion","Task Generation","Large Language Models (LLMs)","Software Engineering","Dockerfile","Terminal-Bench"],"text":"링크: 논문 PDF로 바로 열기 저자: Yusong Lin, Haiyang Wang, Shuzhe Wu, Lue Fan, Feiyang Pan, Sanyuan Zhao, Dandan Tu 핵심 연구 목표 본 논문은 실세계 소프트웨어 개발에 필수적인 CLI(명령줄 인터페이스) 환경과의 상호작용 을 포함하는 환경 집약적 에이전트 작업 의 확장 가능한 데이터 생"},{"id":"2026-02-12-Data-Repetition-Beats-Data-Scaling-in-Long-CoT-Supervised-Fine-Tuning","title":"[논문리뷰] Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning","excerpt":"Yuki M. Asano이 arXiv에 게시한 'Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Data-Repetition-Beats-Data-Scaling-in-Long-CoT-Supervised-Fine-Tuning","tags":["Review","Supervised Fine-tuning (SFT)","Chain-of-Thought (CoT)","Data Repetition","Data Scaling","LLM Training","Generalization","Overfitting","Reasoning Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Dawid J. Kopiczko, Sagar Vaze, Tijmen Blankevoort, Yuki M. Asano 핵심 연구 목표 본 논문은 ChainofThought (CoT) 데이터를 활용한 지도 미세 조정(SFT) 단계에서 제한된 고품질 데이터 를 가장 효과적으로 활용하는 방법을 탐구합니다. 기존의 데이터 확장"},{"id":"2026-02-12-DataChef-Cooking-Up-Optimal-Data-Recipes-for-LLM-Adaptation-via-Reinforcement-Learning","title":"[논문리뷰] DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning","excerpt":"Kai Chen이 arXiv에 게시한 'DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-DataChef-Cooking-Up-Optimal-Data-Recipes-for-LLM-Adaptation-via-Reinforcement-Learning","tags":["Review","LLM Adaptation","Reinforcement Learning","Data Curation","Data Pipelines","Data Recipes","Data Verifier","Data-centric AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Yicheng Chen, Zerun Ma, Xinchen Xie, Yining Li, Kai Chen 핵심 연구 목표 논문은 LLM 적응을 위한 데이터 레시피 설계가 여전히 수작업적이고 노동 집약적이라는 문제에 주목합니다. 이러한 격차를 해소하기 위해, 타겟 벤치마크와 데이터 소스 풀이 주어졌을 때 LLM 적응을 위한"},{"id":"2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies","title":"[논문리뷰] EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies","excerpt":"Yishuo Yuan이 arXiv에 게시한 'EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies","tags":["Review","LLM Evaluation","Long-Horizon Planning","Interactive Economies","Benchmark","Agentic AI","Economic Simulation","Plan-and-Execute"],"text":"링크: 논문 PDF로 바로 열기 저자: Yishuo Yuan, Kangqi Song, Shengze Xu, Jinxiang Xia, Xavier Hu 핵심 연구 목표 이 논문은 LLM 기반 에이전트의 장기적인 계획 및 실행 능력을 평가하는 기존 프레임워크가 단기적이고, 도메인에 특화되어 있으며, 현실적인 경제 역학에 충분히 기반하지 못하는 문제를 해결하는 "},{"id":"2026-02-12-Ex-Omni-Enabling-3D-Facial-Animation-Generation-for-Omni-modal-Large-Language-Models","title":"[논문리뷰] Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models","excerpt":"Tianshu Yu이 arXiv에 게시한 'Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Ex-Omni-Enabling-3D-Facial-Animation-Generation-for-Omni-modal-Large-Language-Models","tags":["Review","Omni-modal LLMs","3D Facial Animation","Speech-to-Face Generation","Token-as-Query Gated Fusion (TQGF)","Discrete Speech Units","ARKit-52 Blendshapes","InstructEx Dataset","Multimodal Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianshu Yu, Yiwen Guo, Zhipeng Li, Haoyu Zhang 핵심 연구 목표 본 논문은 옴니모달 대규모 언어 모델(OLLMs)에 3D 얼굴 애니메이션 생성 기능을 통합하여 텍스트 및 음성 입력에 대한 자연스럽고 표현적인 멀티모달 출력을 가능하게 하는 것을 목표로 합니다. LLM의 이산적인 토큰 "},{"id":"2026-02-12-FeatureBench-Benchmarking-Agentic-Coding-for-Complex-Feature-Development","title":"[논문리뷰] FeatureBench: Benchmarking Agentic Coding for Complex Feature Development","excerpt":"Jiahe Wang이 arXiv에 게시한 'FeatureBench: Benchmarking Agentic Coding for Complex Feature Development' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-FeatureBench-Benchmarking-Agentic-Coding-for-Complex-Feature-Development","tags":["Review","Agentic Coding","Benchmarking","LLMs","Feature Development","Software Engineering","Test-Driven Development","Scalability"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahe Wang, Rui Hao, Haiyang Wang, Jiacheng Zhang, Qixing Zhou 핵심 연구 목표 대규모 언어 모델(LLM) 기반 코드 에이전트의 현재 코딩 능력을 평가하고, 기존 벤치마크의 제한적인 태스크 범위(버그 수정 등)를 넘어 복잡한 기능 개발 시나리오에서의 성능을 측정하기 위한"},{"id":"2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models","title":"[논문리뷰] Free(): Learning to Forget in Malloc-Only Reasoning Models","excerpt":"arXiv에 게시된 'Free(): Learning to Forget in Malloc-Only Reasoning Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models","tags":["Review","Large Language Models","Reasoning Models","Context Management","Memory Pruning","LoRA Adapter","Long-Horizon Reasoning","Self-Forgetting"],"text":"링크: 논문 PDF로 바로 열기 저자: Yilun Zheng, Dongyang Ma, Tian Liang, Jiahao Xu, Xinting Huang, Lihui Chen, Haitao Mi, Yan Wang 핵심 연구 목표 추론 모델이 과도한 \"사고 토큰\"을 축적할 때 성능이 저하되는 문제, 즉 기존 LLM이 쓸모없는 정보를 제거하는 메커니즘 없이 컨텍"},{"id":"2026-02-12-G-LNS-Generative-Large-Neighborhood-Search-for-LLM-Based-Automatic-Heuristic-Design","title":"[논문리뷰] G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design","excerpt":"Liang Zeng이 arXiv에 게시한 'G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-G-LNS-Generative-Large-Neighborhood-Search-for-LLM-Based-Automatic-Heuristic-Design","tags":["Review","Large Language Models (LLMs)","Automated Heuristic Design (AHD)","Large Neighborhood Search (LNS)","Combinatorial Optimization","Evolutionary Algorithm","Destroy Repair Operators","Co-evolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Baoyun Zhao, He Wang, Liang Zeng 핵심 연구 목표 기존 LLM 기반 Automated Heuristic Design (AHD) 방법론이 고정된 휴리스틱 형태(구성 규칙 또는 매개변수화된 지역 탐색) 에 국한되어 탐색 공간을 제한하고 복잡한 조합 최적화 문제(COPs)에서 깊은 지역 최적해 를 "},{"id":"2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite","title":"[논문리뷰] GENIUS: Generative Fluid Intelligence Evaluation Suite","excerpt":"Zijun Shen이 arXiv에 게시한 'GENIUS: Generative Fluid Intelligence Evaluation Suite' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite","tags":["Review","Generative Fluid Intelligence","UMM Evaluation","Visual Generation","Ad-hoc Reasoning","Contextual Adaptation","Benchmark","Attention Intervention"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen 핵심 연구 목표 본 연구는 기존 통합 멀티모달 모델(UMM) 평가 벤치마크가 결정화된 지능(Crystallized Intelligence) 에 치우쳐 있음을 지적하며, 시각 생성 분야에서 생성 유동 지능(Generativ"},{"id":"2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models","title":"[논문리뷰] Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models","excerpt":"Zhen Fang이 arXiv에 게시한 'Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models","tags":["Review","Reinforcement Learning","Large Language Models","Meta-Learning","Error Attribution","Knowledge Internalization","Self-Distillation","Verifiable Rewards"],"text":"링크: 논문 PDF로 바로 열기 저자: Shiting Huang, Zecheng Li, Yu Zeng, Qingnan Ren, Zhen Fang, Qisheng Su, Kou Shi, Lin Chen, Zehui Chen, Feng Zhao 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 능력 강화를 위한 강화 학습(RL) 기법인 RLVR(Re"},{"id":"2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","title":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization","excerpt":"arXiv에 게시된 'Online Causal Kalman Filtering for Stable and Effective Policy Optimization' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","tags":["Review","Reinforcement Learning (RL)","Large Language Models (LLMs)","Policy Optimization","Importance Sampling (IS) Ratio","Kalman Filter","Variance Reduction","Math Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuo He, Lang Feng, Xin Cheng, Lei Feng, Bo An 핵심 연구 목표 대규모 언어 모델(LLM)의 강화 학습(RL)에서 토큰 수준 중요도 샘플링(IS) 비율의 높은 분산이 정책 최적화의 불안정성을 야기하는 문제를 해결하고자 합니다. 기존 방법론들이 토큰 간의 시간적 비정책적 편차를 간과하"},{"id":"2026-02-12-PhyCritic-Multimodal-Critic-Models-for-Physical-AI","title":"[논문리뷰] PhyCritic: Multimodal Critic Models for Physical AI","excerpt":"arXiv에 게시된 'PhyCritic: Multimodal Critic Models for Physical AI' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-PhyCritic-Multimodal-Critic-Models-for-Physical-AI","tags":["Review","Multimodal Critics","Physical AI","Reinforcement Learning","Self-Referential Finetuning","Evaluation Models","Causal Reasoning","Embodied AI","RLVR"],"text":"링크: 논문 PDF로 바로 열기 저자: Tianyi Xiong, Shihao Wang, Guilin Liu, Yi Dong, Ming Li, Heng Huang, Jan Kautz, Zhiding Yu 핵심 연구 목표 본 연구는 물리 AI 태스크 의 평가에 특화된 신뢰성 있는 멀티모달 비평 모델의 부재를 해결하고자 합니다. 기존 비평 모델들이 일반 시각 도"},{"id":"2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search","title":"[논문리뷰] QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search","excerpt":"Hui Zhang이 arXiv에 게시한 'QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search","tags":["Review","Large Language Models (LLMs)","Query Understanding","Multi-Task Learning","Generative AI","Reinforcement Learning (RL)","Social Network Services (SNS)","Xiaohongshu","Search Engines"],"text":"링크: 논문 PDF로 바로 열기 저자: Jianzhao Huang, Xiaorui Huang, Fei Zhao, Yunpeng Liu, Hui Zhang, Fangcheng Shi, Congfeng Li, Zechen Sun, Yi Wu, Yao Hu, Yunhan Bait, Shaosheng Cao 핵심 연구 목표 기존 검색 엔진의 쿼리 처리(QP) 시스"},{"id":"2026-02-12-ROCKET-Rapid-Optimization-via-Calibration-guided-Knapsack-Enhanced-Truncation-for-Efficient-Model-Compression","title":"[논문리뷰] ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression","excerpt":"arXiv에 게시된 'ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-ROCKET-Rapid-Optimization-via-Calibration-guided-Knapsack-Enhanced-Truncation-for-Efficient-Model-Compression","tags":["Review","Model Compression","LLM","Training-Free","Knapsack Problem","Sparse Matrix Factorization","Dictionary Learning","Post-Training Optimization","Weight Sparsification"],"text":"링크: 논문 PDF로 바로 열기 저자: Ammar Ali, Baher Mohammad, Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Stamatios Lefkimmiatis 핵심 연구 목표 ROCKET 논문은 대규모 언어 모델(LLMs)의 과도한 크기로 인한 연산 및 메모리 요구 사항을 해결하기 위해 빠르고 훈"},{"id":"2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters","title":"[논문리뷰] Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters","excerpt":"arXiv에 게시된 'Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters","tags":["Review","Mixture-of-Experts (MoE)","Sparse Models","Inference Efficiency","Hybrid Attention","Multi-Token Prediction (MTP)","Reinforcement Learning (RL)","Agentic AI","Long-Context Understanding"],"text":"링크: 논문 PDF로 바로 열기 저자: StepFun Team 핵심 연구 목표 본 논문은 11B 활성화 파라미터 를 가진 196B MixtureofExperts (MoE) 모델 인 Step 3.5 Flash 를 소개하며, 첨단 에이전트 지능과 컴퓨팅 효율성 간의 격차를 해소하는 것을 목표로 합니다. 특히, 추론 지연 시간을 최소화하여 실제 산업 환경에서 정"},{"id":"2026-02-12-Stroke3D-Lifting-2D-strokes-into-rigged-3D-model-via-latent-diffusion-models","title":"[논문리뷰] Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models","excerpt":"arXiv에 게시된 'Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Stroke3D-Lifting-2D-strokes-into-rigged-3D-model-via-latent-diffusion-models","tags":["Review","2D Strokes","3D Model Generation","Rigged Meshes","Latent Diffusion Models","Skeleton Generation","Text-to-3D","Graph Neural Networks","Preference Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Ruisi Zhao, Haoren Zheng, Zongxin Yang, Hehe Fan, Yi Yang 핵심 연구 목표 애니메이션 가능한 3D 지오메트리 생성의 어려움과 골격 생성에 대한 세밀한 구조적 제어 부족이라는 기존 3D 생성 방법론의 한계를 해결합니다. 사용자 입력인 2D 드로잉 스트로크 와 텍스트 프롬프트 "},{"id":"2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions","title":"[논문리뷰] TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions","excerpt":"arXiv에 게시된 'TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions","tags":["Review","Video Captioning","Multi-Scene Videos","Time-Aware","Structural Captions","Audio-Visual Understanding","Large Language Models","Reinforcement Learning","OmniDCBench"],"text":"링크: 논문 PDF로 바로 열기 저자: Linli Yao, Yuancheng Wei, Yaojie Zhang, Lei Li, Xinlong Chen, Feifan Song, Ziyue Wang, Kun Ouyang, Yuanxin Liu, Lingpeng Kong, Qi Liu, Pengfei Wan, Kun Gai, Yuanxing Zhang, Xu Su"},{"id":"2026-02-12-Towards-Autonomous-Mathematics-Research","title":"[논문리뷰] Towards Autonomous Mathematics Research","excerpt":"arXiv에 게시된 'Towards Autonomous Mathematics Research' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research","tags":["Review","Mathematics Research","Large Language Models","AI Agents","Theorem Proving","Tool Use","Gemini Deep Think","Autonomous Research","Human-AI Collaboration"],"text":"링크: 논문 PDF로 바로 열기 저자: Tony Feng, Trieu H. Trinh, Garrett Bingham, et al. (Google DeepMind Researchers) 핵심 연구 목표 본 논문은 국제 수학 올림피아드(IMO) 수준을 넘어 전문적인 수학 연구 영역으로 AI의 능력을 확장하는 것을 목표로 합니다. 방대한 문헌 탐색과 장기적인 증"},{"id":"2026-02-12-When-the-Prompt-Becomes-Visual-Vision-Centric-Jailbreak-Attacks-for-Large-Image-Editing-Models","title":"[논문리뷰] When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models","excerpt":"arXiv에 게시된 'When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-When-the-Prompt-Becomes-Visual-Vision-Centric-Jailbreak-Attacks-for-Large-Image-Editing-Models","tags":["Review","Vision-Centric Jailbreak Attack","Image Editing Models","Safety Benchmark","IESBench","Multimodal Reasoning","Adversarial Attack","Defense Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiacheng Hou, Yining Sun, Ruochong Jin, Haochen Han, Fangming Liu, Wai Kin Victor Chan, Alex Jinpeng Wang 핵심 연구 목표 본 논문은 대규모 이미지 편집 모델에서 시각적 프롬프트가 사용자 의도를 전달하는 새로운 패러다임이 도입되면서 발생"},{"id":"2026-02-12-When-to-Memorize-and-When-to-Stop-Gated-Recurrent-Memory-for-Long-Context-Reasoning","title":"[논문리뷰] When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning","excerpt":"arXiv에 게시된 'When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-12 00:00:00+0900+0900","permalink":"/ai/review/2026-02-12-When-to-Memorize-and-When-to-Stop-Gated-Recurrent-Memory-for-Long-Context-Reasoning","tags":["Review","Long-Context Reasoning","Large Language Models (LLMs)","Recurrent Memory","Gated Mechanisms","Reinforcement Learning","Memory Efficiency","Early Exit"],"text":"링크: 논문 PDF로 바로 열기 저자: Leheng Sheng, Yongtao Zhang, Wenchang Ma, Yaorui Shi, Ting Huang, Xiang Wang, An Zhang, Ke Shen, TatSeng Chua 핵심 연구 목표 대규모 언어 모델(LLMs)이 장문 컨텍스트 추론에서 겪는 성능 저하, 컨텍스트 길이 증가에 따른 메모리 "},{"id":"2026-02-13-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale","title":"[논문리뷰] Adapting Vision-Language Models for E-commerce Understanding at Scale","excerpt":"arXiv에 게시된 'Adapting Vision-Language Models for E-commerce Understanding at Scale' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale","tags":["Review","E-commerce","Vision-Language Models","Multimodal Understanding","Instruction Tuning","Attribute Extraction","Fine-tuning","Benchmarking","LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Matteo Nulli, Vladimir Orshulevich, Tala Bazazo, Christian Herold, Michael Kozielski, Marcin Mazur, Szymon Tuzel, Cees G. M. Snoek, Seyyed Hadi Hashemi, Omar Javed, Yannick Versl"},{"id":"2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models","title":"[논문리뷰] Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models","excerpt":"arXiv에 게시된 'Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models","tags":["Review","Reinforcement Learning","Large Language Models","Prompt Engineering","Compositional Generalization","Verifiable Rewards","Curriculum Learning","Mathematical Reasoning","Multi-task Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Xin Xu, Clive Bai, Kai Yang, Tianhao Chen, Yangkun Chen, Weijie Liu, Hao Chen, Yang Wang, Saiyong Yang, Can Yang 핵심 연구 목표 (Reinforcement Learning with Verifiable Rewards) 훈련 과정에서"},{"id":"2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing","title":"[논문리뷰] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing","excerpt":"arXiv에 게시된 'DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing","tags":["Review","Multimodal Model","Image Generation","Image Editing","Diffusion Models","VLM-DiT Architecture","Stacked Channel Bridging","Reinforcement Learning","Lightweight Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song, Siyuan Wang, Yibin Wang, Yi Xin, Hongjian Liu, Zhixiong Zhang, Shengyuan Ding, Tianhang Wang, Zhenglin Cheng, Tao Lin, Ch"},{"id":"2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit","title":"[논문리뷰] DeepSight: An All-in-One LM Safety Toolkit","excerpt":"arXiv에 게시된 'DeepSight: An All-in-One LM Safety Toolkit' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit","tags":["Review","LM Safety","Evaluation","Diagnosis","Multimodal AI","Frontier AI Risks","Black-box Analysis","White-box Insight","Open-source Toolkit"],"text":"링크: 논문 PDF로 바로 열기 저자: Bo Zhang, Jiaxuan Guo, Lijun Li, Dongrui Liu, Sujin Chen, Guanxu Chen, Zhijie Zheng, Qihao Lin, Lewen Yan, Chen Qian, Yijin Zhou, Yuyao Wu, Shaoxiong Guo, Tianyi Du, Jingyi Yang,"},{"id":"2026-02-13-EgoHumanoid-Unlocking-In-the-Wild-Loco-Manipulation-with-Robot-Free-Egocentric-Demonstration","title":"[논문리뷰] EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration","excerpt":"Yinghui Li이 arXiv에 게시한 'EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-EgoHumanoid-Unlocking-In-the-Wild-Loco-Manipulation-with-Robot-Free-Egocentric-Demonstration","tags":["Review","Humanoid Robotics","Loco-Manipulation","Egocentric Demonstration","Robot-Free Learning","Cross-Embodiment Transfer","View Alignment","Action Alignment","VLA Co-training"],"text":"링크: 논문 PDF로 바로 열기 저자: Modi Shi, Shijia Peng, Jin Chen, Haoran Jiang, Yinghui Li, Di Huang, Ping Luo, Hongyang Li, Li Chen 핵심 연구 목표 본 논문은 로봇 텔레오퍼레이션의 한계(높은 비용, 복잡성, 환경 제약)로 인해 부족한 휴머노이드 로코조작(locomanipu"},{"id":"2026-02-13-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning","title":"[논문리뷰] GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning","excerpt":"arXiv에 게시된 'GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning","tags":["Review","VLA Models","World Models","Reinforcement Learning","Robotic Manipulation","Long-Horizon Control","Human-in-the-Loop","Continual Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: GigaBrain Team (Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv, Jingyu Liu, Lv Feng, Mingming Yu, Peng Li, Qiuping Deng, Tianze Liu, Xinyu Zhou, Xin"},{"id":"2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments","title":"[논문리뷰] LawThinker: A Deep Research Legal Agent in Dynamic Environments","excerpt":"arXiv에 게시된 'LawThinker: A Deep Research Legal Agent in Dynamic Environments' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments","tags":["Review","Legal Reasoning","AI Agent","Large Language Models","Verification","Knowledge Management","Dynamic Environments","Procedural Compliance","Tool Use"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyu Yang, Chenlong Deng, Tongyu Wen, Binyu Xie, Zhicheng Dou 핵심 연구 목표 법률 추론 태스크에서 정확한 최종 결과뿐만 아니라, 절차적으로도 적합한 추론 과정 을 보장하는 것을 목표로 합니다. 기존 대규모 언어 모델(LLM) 기반 방법론에서 발생하는 중간 추론 단계의"},{"id":"2026-02-13-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation","title":"[논문리뷰] Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation","excerpt":"arXiv에 게시된 'Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation","tags":["Review","On-Policy Distillation","Reward Extrapolation","Large Language Models (LLMs)","Knowledge Distillation","Reinforcement Learning","Math Reasoning","Code Generation","Multi-teacher Distillation"],"text":"링크: 논문 PDF로 바로 열기 저자: Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin 핵심 연구 목표 본 논문은 온폴리시 증류(OPD)의 기계론적 이해 부족 과 잠재력 미활용 문제를 해결하는 것을 목표로 합니다. 표준 OPD를 일반화된 프레임워크로 확장하여 학생 모델이 교사"},{"id":"2026-02-13-MOSS-Audio-Tokenizer-Scaling-Audio-Tokenizers-for-Future-Audio-Foundation-Models","title":"[논문리뷰] MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models","excerpt":"arXiv에 게시된 'MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-MOSS-Audio-Tokenizer-Scaling-Audio-Tokenizers-for-Future-Audio-Foundation-Models","tags":["Review","Audio Tokenizer","Transformer Architecture","End-to-End Learning","Residual Vector Quantization","Speech Synthesis","Audio Foundation Models","Scalability","Autoregressive Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Yitian Gong, Kuangwei Chen, Zhaoye Fei, Xiaogui Yang, Ke Chen, Yang Wang, Kexin Huang, Mingshu Chen, Ruixiao Li, Qinyuan Cheng, Shimin Li (Advisors: Xipeng Qiu) 핵심 연구 목표 기존 오디오 토"},{"id":"2026-02-13-MetaphorStar-Image-Metaphor-Understanding-and-Reasoning-with-End-to-End-Visual-Reinforcement-Learning","title":"[논문리뷰] MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning","excerpt":"Hongsheng Li이 arXiv에 게시한 'MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-MetaphorStar-Image-Metaphor-Understanding-and-Reasoning-with-End-to-End-Visual-Reinforcement-Learning","tags":["Review","Image Metaphor Understanding","Visual Reasoning","Reinforcement Learning","MLLMs","TFQ-GRPO","End-to-End Learning","Cognitive AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenhao Zhang, Yazhe Niu, Hongsheng Li 핵심 연구 목표 본 논문은 최신 Multimodal Large Language Models (MLLMs) 이 기본적인 Visual Question Answering (VQA) 에는 뛰어나지만, 이미지 내에 내재된 미묘한 문화적, 감정적, 상황적 함의"},{"id":"2026-02-13-NarraScore-Bridging-Visual-Narrative-and-Musical-Dynamics-via-Hierarchical-Affective-Control","title":"[논문리뷰] NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control","excerpt":"arXiv에 게시된 'NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-NarraScore-Bridging-Visual-Narrative-and-Musical-Dynamics-via-Hierarchical-Affective-Control","tags":["Review","Video-to-Music Generation","Affective Computing","Vision-Language Models (VLMs)","Hierarchical Control","Soundtrack Generation","Temporal Coherence","Emotion-Driven Music"],"text":"링크: 논문 PDF로 바로 열기 저자: Yufan Wen, Zhaocheng Liu, YeGuo Hua, Ziyi Guo, Lihua Zhang, Chun Yuan, Jian Wu 핵심 연구 목표 본 논문은 긴 길이의 비디오에 대해 계산 효율적이고, 시간적으로 일관되며, 서사적 흐름에 의미론적으로 부합하는 배경 음악을 자동으로 생성하는 것을 목표로 합니다."},{"id":"2026-02-13-Pretraining-A-Large-Language-Model-using-Distributed-GPUs-A-Memory-Efficient-Decentralized-Paradigm","title":"[논문리뷰] Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm","excerpt":"arXiv에 게시된 'Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Pretraining-A-Large-Language-Model-using-Distributed-GPUs-A-Memory-Efficient-Decentralized-Paradigm","tags":["Review","Decentralized Training","Mixture-of-Experts (MoE)","Large Language Models (LLMs)","Memory Efficiency","Sparse Expert Synchronization","Federated Learning","Distributed GPUs"],"text":"링크: 논문 PDF로 바로 열기 저자: Jinrui Zhang, Chaodong Xiao, Aoqi Wu, Xindong Zhang, Lei Zhang 핵심 연구 목표 대규모 언어 모델(LLM) 사전 학습에 필요한 막대한 GPU 메모리 및 통신 대역폭 요구 사항으로 인한 중앙 집중식 학습의 한계를 극복하는 것입니다. 특히 MixtureofExperts (M"},{"id":"2026-02-13-RISE-Self-Improving-Robot-Policy-with-Compositional-World-Model","title":"[논문리뷰] RISE: Self-Improving Robot Policy with Compositional World Model","excerpt":"arXiv에 게시된 'RISE: Self-Improving Robot Policy with Compositional World Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-RISE-Self-Improving-Robot-Policy-with-Compositional-World-Model","tags":["Review","Robot Learning","Reinforcement Learning","World Models","Compositional Models","Robotic Manipulation","Self-Improving","Vision-Language-Action (VLA)"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiazhi Yang, Kunyang Lin, Jinwei Li, Wencong Zhang, Tianwei Lin, Longyan Wu, Zhizhong Su, Hao Zhao, YaQin Zhang, Li Chen, Ping Luo, Xiangyu Yue, Hongyang Li 핵심 연구 목표 본 논문은 VLA(Vi"},{"id":"2026-02-13-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning","title":"[논문리뷰] ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning","excerpt":"arXiv에 게시된 'ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning","tags":["Review","Multimodal Data Selection","Visual Instruction Tuning","Training-Free","Scalability","Subspace Learning","Vision-Language Models","Attention Mechanism"],"text":"링크: 논문 PDF로 바로 열기 저자: Changti Wu, Jiahuai Mao, Yuzhuo Miao, Shijie Lian, Bin Yu, Xiaopeng Lin, Cong Huang, Lei Zhang, Kai Chen 핵심 연구 목표 본 연구는 대규모 Visual Instruction Tuning (VIT) 데이터셋의 높은 중복성으로 인한 비효율적"},{"id":"2026-02-13-Sci-CoE-Co-evolving-Scientific-Reasoning-LLMs-via-Geometric-Consensus-with-Sparse-Supervision","title":"[논문리뷰] Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision","excerpt":"arXiv에 게시된 'Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Sci-CoE-Co-evolving-Scientific-Reasoning-LLMs-via-Geometric-Consensus-with-Sparse-Supervision","tags":["Review","LLM","Scientific Reasoning","Co-evolution","Reinforcement Learning","Sparse Supervision","Geometric Consensus","Self-Play","Verifier"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaohan He, Shiyang Feng, Songtao Huang, Lei Bai, Bin Wang, Bo Zhang 핵심 연구 목표 과학적 추론 태스크에서 대규모 언어 모델(LLM)의 취약한 성능을 개선하는 것을 목표로 합니다. 특히, 신뢰할 수 없는 솔루션 평가와 검증 전략의 다양성 부족 문제, 그리고 제한된"},{"id":"2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation","title":"[논문리뷰] Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation","excerpt":"Yukuan Xu이 arXiv에 게시한 'Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation","tags":["Review","Vision-Language Navigation","Beyond-the-View Navigation","Video Generation Models","Sparse Video Generation","Diffusion Models","Embodied AI","Real-world Navigation","Long-horizon Planning"],"text":"링크: 논문 PDF로 바로 열기 저자: Hai Zhang, Siqi Liang, Li Chen, Yuxian Li, Yukuan Xu, Yichao Zhong, Fu Zhang, Hongyang Li 핵심 연구 목표 본 논문은 실세계 환경에서 BeyondtheView Navigation (BVN) 이 직면한 과제를 해결하는 것을 목표로 합니다. 기존 대규모"},{"id":"2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching","title":"[논문리뷰] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching","excerpt":"arXiv에 게시된 'Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching","tags":["Review","Vector Sketching","Progressive Semantic Illusions","Score Distillation Sampling","Joint Optimization","Visual Anagrams","Bézier Strokes","CLIP-guided Generation","Diffusion Models"],"text":"링크: 논문 PDF로 바로 열기 저자: HuaiHsun Cheng, SiangLing Zhang, YuLun Liu 핵심 연구 목표 본 논문은 단일 벡터 스케치가 스트로크가 순차적으로 추가됨에 따라 극적인 의미 변환을 겪는 새로운 태스크인 \"Progressive Semantic Illusions\" 를 소개합니다. 초기 스트로크가 하나의 객체를 형성함과 동시"},{"id":"2026-02-13-The-Devil-Behind-Moltbook-Anthropic-Safety-is-Always-Vanishing-in-Self-Evolving-AI-Societies","title":"[논문리뷰] The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies","excerpt":"Jinyu Hou이 arXiv에 게시한 'The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-The-Devil-Behind-Moltbook-Anthropic-Safety-is-Always-Vanishing-in-Self-Evolving-AI-Societies","tags":["Review","Multi-agent Systems","Self-evolution","AI Safety","Alignment Drift","Information Theory","Thermodynamics","Entropy Accumulation","Moltbook"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenxu Wang, Chaozhuo Li, Songyang Liu, Zejian Chen, Jinyu Hou, Ji Qi, Rui Li, Litian Zhang, Qiwei Ye, Zheng Liu, Xu Chen, Xi Zhang, Philip S. Yu 핵심 연구 목표 본 논문은 , , 이라는 를 만족하는 AI"},{"id":"2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning","title":"[논문리뷰] Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning","excerpt":"arXiv에 게시된 'Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning","tags":["Review","Large Language Models","In-Context Learning","Reinforcement Learning","Test-Time Scaling","Exploration-Exploitation","State Coverage","Reward Shaping","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Futing Wang, Jianhao Yan, Yun Luo, Ganqu Cui, Zhi Wang, Xiaoye Qu, Yue Zhang, Yu Cheng, Tao Lin 핵심 연구 목표 본 논문은 LLM이 추론 과정에서 다양한 가설을 생성, 검증, 개선하는 'InContext Exploration' 능력을 효과적으로"},{"id":"2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces","title":"[논문리뷰] ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces","excerpt":"Julian McAuley이 arXiv에 게시한 'ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces","tags":["Review","Efficient Reasoning","Latent Space Reasoning","Discrete Space Reasoning","LLM Confidence","Routing Mechanism","Inference-Time Optimization","Chain-of-Thought"],"text":"링크: 논문 PDF로 바로 열기 저자: Julian McAuley, Haoliang Wang, Xiang Chen, Tong Yu, Xin Xu 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 추론 효율성을 향상시키는 것을 목표로 합니다. 특히, 기존의 명시적 추론 궤적(CoT) 및 잠재 공간 추론 방식의 한계를 극복하고, 추론 정확도를 높이면서 생"},{"id":"2026-02-13-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction","title":"[논문리뷰] Thinking with Drafting: Optical Decompression via Logical Reconstruction","excerpt":"arXiv에 게시된 'Thinking with Drafting: Optical Decompression via Logical Reconstruction' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction","tags":["Review","Multimodal Reasoning","Visual Algebra","Domain-Specific Language","Optical Decompression","Logical Reconstruction","Bar Model","MLLMs","Verification"],"text":"링크: 논문 PDF로 바로 열기 저자: Jingxuan Wei, Honghao He, Caijun Jia, Siyuan Li, Zheng Sun, Yuhang Xu, Yuanyuan Lin, Linzhuang Sun, Yuchen Wu, Bihui Yu, Xiangxiang Zhang, Cheng Tan 핵심 연구 목표 본 논문은 멀티모달 대규모 언어 모델"},{"id":"2026-02-13-Unveiling-Implicit-Advantage-Symmetry-Why-GRPO-Struggles-with-Exploration-and-Difficulty-Adaptation","title":"[논문리뷰] Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation","excerpt":"arXiv에 게시된 'Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-Unveiling-Implicit-Advantage-Symmetry-Why-GRPO-Struggles-with-Exploration-and-Difficulty-Adaptation","tags":["Review","Reinforcement Learning","LLM Reasoning","Group Relative Policy Optimization","Advantage Estimation","Exploration-Exploitation","Curriculum Learning","Multi-modal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhiqi Yu, Zhangquan Chen, Mengting Liu, Heye Zhang, Liangqiong Qu 핵심 연구 목표 본 논문은 Group Relative Policy Optimization (GRPO) 가 탐색 및 난이도 적응에서 겪는 어려움의 근본 원인을 규명하는 것을 목표로 합니다. 특히, Gro"},{"id":"2026-02-13-dVoting-Fast-Voting-for-dLLMs","title":"[논문리뷰] dVoting: Fast Voting for dLLMs","excerpt":"arXiv에 게시된 'dVoting: Fast Voting for dLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-dVoting-Fast-Voting-for-dLLMs","tags":["Review","dLLMs","Diffusion Models","Test-Time Scaling","Voting","Reasoning","Masked Language Models","Parallel Decoding","Remasking"],"text":"링크: 논문 PDF로 바로 열기 저자: Sicheng Feng, Zigeng Chen, Xinyin Ma, Gongfan Fang, Xinchao Wang 핵심 연구 목표 본 논문은 확산 대규모 언어 모델( dLLMs )의 추론 능력을 훈련 없이 향상시키면서 기존 테스트 시간 스케일링 기법의 비효율성으로 인한 높은 추론 비용 문제를 해결하는 것을 목표로 합"},{"id":"2026-02-13-χ_0-Resource-Aware-Robust-Manipulation-via-Taming-Distributional-Inconsistencies","title":"[논문리뷰] χ_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies","excerpt":"arXiv에 게시된 'χ_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-13 00:00:00+0900+0900","permalink":"/ai/review/2026-02-13-χ_0-Resource-Aware-Robust-Manipulation-via-Taming-Distributional-Inconsistencies","tags":["Review","Robotic Manipulation","Distributional Shift","Imitation Learning","Model Arithmetic","Stage Advantage","Train-Deploy Alignment","Resource-Efficient AI","Long-Horizon Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai, Hongyang Li, Huijie Wang, Jin Chen, Kaiyang Wu, Li Chen, Lirui Zhao, Modi Shi, Ping Luo, Qingwen Bu, Shijia "},{"id":"2026-02-16-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning","title":"[논문리뷰] ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning","excerpt":"arXiv에 게시된 'ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning","tags":["Review","Robotic Manipulation","Vision-Language-Action (VLA)","Foundation Models","Action Manifold Learning","Diffusion Transformers","Data Curation","Embodied AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Xinyuan Chang, Feng Xiong, Mu Xu, Zhiheng Ma, Xing Wei, et al. 핵심 연구 목표 본 논문은 파편화된 데이터, 불일치하는 표현, 그리고 학습 목표의 불균형으로 인해 다형성 로봇 하드웨어에 걸쳐 범용적인 임베디드 에이전트를 구축하는 데 따르는 근본적인 문제를 해결하고자 합니"},{"id":"2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models","title":"[논문리뷰] BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models","excerpt":"arXiv에 게시된 'BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models","tags":["Review","Quantization","Large Language Models","Post-Training Quantization","Bit-Plane Decomposition","Variable Quantization Grid","Low-Bit Quantization","Model Compression","Hessian-Induced Geometry"],"text":"링크: 논문 PDF로 바로 열기 저자: Junyu Chen, Jungang Li, Taiqiang Wu, Mengzhao Chen, Jing Xiong, Wenjie Wang, Qingyao Yang, He Xiao, Zhen Li, Zhen Peng, Chaofan Tao, Long Shi, Hongxia Yang, Ngai Wong 핵심 연구 목표 본 "},{"id":"2026-02-16-CoPE-VideoLM-Codec-Primitives-For-Efficient-Video-Language-Models","title":"[논문리뷰] CoPE-VideoLM: Codec Primitives For Efficient Video Language Models","excerpt":"arXiv에 게시된 'CoPE-VideoLM: Codec Primitives For Efficient Video Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-CoPE-VideoLM-Codec-Primitives-For-Efficient-Video-Language-Models","tags":["Review","Video Language Models","Codec Primitives","Efficient Tokenization","Motion Vectors","Residuals","Temporal Reasoning","Long-Context Understanding","Video Compression"],"text":"링크: 논문 PDF로 바로 열기 저자: Sayan Deb Sarkar, Rémi Pautrat, Ondrej Miksik, Marc Pollefeys, Iro Armeni, Mahdi Rad, Mihai Dusmanu 핵심 연구 목표 기존 Video Language Models (VideoLMs)의 밀집 RGB 프레임 인코딩으로 인한 높은 계산 오버헤드 및"},{"id":"2026-02-16-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels","title":"[논문리뷰] DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels","excerpt":"Zhiqiang Tao이 arXiv에 게시한 'DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels","tags":["Review","Diffusion LLM","CUDA Kernel Generation","Reinforcement Learning","Code Generation","High-Performance Computing","Bi-phase Curated RL","CuKe Dataset"],"text":"링크: 논문 PDF로 바로 열기 저자: Haolei Bai, Lingcheng Kong, Xueyi Chen, Jianmian Wang, Zhiqiang Tao, Huan Wang 핵심 연구 목표 본 연구는 고도로 전문화된 CUDA 커널 생성 태스크에서 diffusion large language models (dLLMs) 의 잠재력을 탐색하고, 이 분야의"},{"id":"2026-02-16-FLAC-Maximum-Entropy-RL-via-Kinetic-Energy-Regularized-Bridge-Matching","title":"[논문리뷰] FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching","excerpt":"Xiao Ma이 arXiv에 게시한 'FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-FLAC-Maximum-Entropy-RL-via-Kinetic-Energy-Regularized-Bridge-Matching","tags":["Review","Reinforcement Learning","Maximum Entropy RL","Kinetic Energy Regularization","Schrödinger Bridge","Generative Policies","Flow Matching","Actor-Critic"],"text":"링크: 논문 PDF로 바로 열기 저자: Lei Lv, Yunfei Li, Yu Luo, Fuchun Sun, Xiao Ma 핵심 연구 목표 본 논문은 Diffusion Models 및 Flow Matching 과 같은 반복적인 생성 정책(iterative generative policies)을 Maximum Entropy Reinforcement Learn"},{"id":"2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics","title":"[논문리뷰] GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics","excerpt":"MingMing Cheng이 arXiv에 게시한 'GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics","tags":["Review","Geolocation","Reinforcement Learning","Vision-Language Models","Chain-of-Thought","Geospatial AI","Dataset","Reward Function"],"text":"링크: 논문 PDF로 바로 열기 GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics 저자: Modi Jin, Yiming Zhang, Boyuan Sun, Dingwen Zhang, MingMing Cheng, Qibin Hou 핵심 연구 목표 기존 VLL"},{"id":"2026-02-16-Intelligent-AI-Delegation","title":"[논문리뷰] Intelligent AI Delegation","excerpt":"arXiv에 게시된 'Intelligent AI Delegation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-Intelligent-AI-Delegation","tags":["Review","AI Delegation","Multi-agent Systems","Task Decomposition","Agentic AI","Trust & Safety","LLM","Adaptive Coordination"],"text":"링크: 논문 PDF로 바로 열기 저자: Nenad Tomašev, Matija Franklin, Simon Osindero 핵심 연구 목표 본 논문은 기존 AI 태스크 분해 및 위임 방식의 한계(단순한 휴리스틱, 환경 변화에 대한 취약성)를 극복하고자 합니다. AI 에이전트가 복잡한 목표를 의미 있게 분해하고, 다른 AI 에이전트 및 사람에게 안전하게 위임"},{"id":"2026-02-16-Learning-Image-based-Tree-Crown-Segmentation-from-Enhanced-Lidar-based-Pseudo-labels","title":"[논문리뷰] Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels","excerpt":"Xiaowei Yu이 arXiv에 게시한 'Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-Learning-Image-based-Tree-Crown-Segmentation-from-Enhanced-Lidar-based-Pseudo-labels","tags":["Review","Instance Segmentation","Tree Crown Delineation","Remote Sensing","Lidar Data","Multispectral Imagery","Pseudo-labeling","Segment Anything Model (SAM)","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Julius Pesonen, Stefan Rua, Josef Taher, Niko Koivumäki, Xiaowei Yu, Eija Honkavaara 핵심 연구 목표 본 연구는 항공 이미지에서 나무 수관을 자동으로 분할하고 구분하는 데 있어 텍스처 및 부분적 겹침으로 인한 어려움을 해결하고자 합니다. 특히, 수동 라"},{"id":"2026-02-16-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs","title":"[논문리뷰] Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs","excerpt":"Ninghao Liu이 arXiv에 게시한 'Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs","tags":["Review","Data Synthesis","LLMs","Feature Space","Sparse Autoencoders","Diversity Metrics","Post-Training","Instruction Tuning","Feature Activation Coverage"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhongzhi Li, Xuansheng Wu, Yijiang Li, Lijie Hu, Ninghao Liu 핵심 연구 목표 대규모 언어 모델(LLM)의 후처리 훈련에서 데이터 다양성이 중요함에도 불구하고, 기존 텍스트 기반 또는 일반 임베딩 기반 다양성 지표는 태스크 관련 특징을 제대로 포착하지 못하는 문제를 해결하"},{"id":"2026-02-16-MedXIAOHE-A-Comprehensive-Recipe-for-Building-Medical-MLLMs","title":"[논문리뷰] MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs","excerpt":"arXiv에 게시된 'MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-MedXIAOHE-A-Comprehensive-Recipe-for-Building-Medical-MLLMs","tags":["Review","Medical LLMs","Multimodal Foundation Models","Continual Pre-training","Entity-Aware Learning","Reinforcement Learning","Medical Diagnosis","Instruction Following","Unified Benchmarking"],"text":"링크: 논문 PDF로 바로 열기 저자: ByteDance XiaoHe Medical AI 핵심 연구 목표 본 논문은 실세계 임상 애플리케이션에서 일반 목적의 의료 이해 및 추론을 발전시키기 위한 MedXIAOHE 라는 의료 비전언어 파운데이션 모델을 제안합니다. 기존 의료 VLM의 한계인 지식 범위, 장문 생성에서의 환각, 복잡한 추론 능력 및 불충분한 평"},{"id":"2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs","title":"[논문리뷰] On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs","excerpt":"arXiv에 게시된 'On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs","tags":["Review","VLM","RL Fine-tuning","Chain-of-Thought","Robustness","Faithfulness","Textual Perturbations","Visual Grounding","Uncertainty Calibration"],"text":"링크: 논문 PDF로 바로 열기 저자: Rosie Zhao, Anshul Shah, Xiaoyu Zhu, Xinke Deng, Zhongyu Jiang, Yang Yang, Joerg Liebelt, Arnab Mondal 핵심 연구 목표 본 논문은 강화 학습(RL)으로 파인튜닝된 비전 언어 모델(VLM) 의 강건성(robustness) 및 사고 과정(Ch"},{"id":"2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","title":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence","excerpt":"arXiv에 게시된 'OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","tags":["Review","Multimodal AI","Video Understanding","Sparse Attention","Vision Transformer","Codec-Aligned Processing","Self-Supervised Learning","Predictive Coding","Efficient AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Feilong Tang, Xiang An, Yunyao Yan, Yin Xie, Bin Qin, Kaicheng Yang, Yifei Shen, Yuanhan Zhang, Chunyuan Li, Shikun Feng, Changrui Chen, Huajie Tan, Ming Hu, Manyuan Zhang, Bo Li"},{"id":"2026-02-16-RLinf-Co-Reinforcement-Learning-Based-Sim-Real-Co-Training-for-VLA-Models","title":"[논문리뷰] RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models","excerpt":"arXiv에 게시된 'RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-RLinf-Co-Reinforcement-Learning-Based-Sim-Real-Co-Training-for-VLA-Models","tags":["Review","Reinforcement Learning","Sim-to-Real","Co-training","VLA Models","Robotic Manipulation","Supervised Fine-tuning","Catastrophic Forgetting"],"text":"링크: 논문 PDF로 바로 열기 저자: Liangzhi Shi, Shuaihang Chen, Feng Gao, Yinuo Chen, Kang Chen, Tonghe Zhang, Hongzhi Zhang, Weinan Zhang, Chao Yu, and Yu Wang 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA) 모델 훈련 시, "},{"id":"2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents","title":"[논문리뷰] SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents","excerpt":"Huayu Sha이 arXiv에 게시한 'SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents","tags":["Review","LLM Agents","Tool-use","Scientific Reasoning","Benchmarking","Interactive Environment","Data Synthesis","Error Recovery","Multi-step Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Yujiong Shen, Yajie Yang, Zhiheng Xi, Binze Hu, Huayu Sha, Jiazheng Zhang, Qiyuan Peng, Junlin Shang, Jixuan Huang, Yutao Fan, Jingqi Tong, Shihan Dou, Ming Zhang, Lei Bai, Zhenf"},{"id":"2026-02-16-Self-EvolveRec-Self-Evolving-Recommender-Systems-with-LLM-based-Directional-Feedback","title":"[논문리뷰] Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback","excerpt":"Jimin Seo이 arXiv에 게시한 'Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-Self-EvolveRec-Self-Evolving-Recommender-Systems-with-LLM-based-Directional-Feedback","tags":["Review","Recommender System","LLM-based Code Evolution","Directional Feedback","User Simulator","Model Diagnosis Tool","Agentic AI","AutoML"],"text":"링크: 논문 PDF로 바로 열기 저자: Jimin Seo, Wonjoong Kim, Hongseok Kang, Sangwu Park, Sein Kim, Kanghoon Yoon, Chanyoung Park 핵심 연구 목표 기존 추천 시스템 코드 진화 프레임워크들이 스칼라 지표(NDCG, Hit Ratio)에만 의존하여 진단적 통찰력을 제공하지 못하고, 고정"},{"id":"2026-02-16-Towards-Universal-Video-MLLMs-with-Attribute-Structured-and-Quality-Verified-Instructions","title":"[논문리뷰] Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions","excerpt":"arXiv에 게시된 'Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-Towards-Universal-Video-MLLMs-with-Attribute-Structured-and-Quality-Verified-Instructions","tags":["Review","Video Understanding","Multimodal Large Language Models (MLLMs)","Instruction Tuning","Data Curation","Attribute-Structured Data","Quality Verification","Temporal Grounding","Video Captioning"],"text":"링크: 논문 PDF로 바로 열기 저자: Yunheng Li, Hengrui Zhang, MengHao Guo, Wenzhao Gao, Shaoyong Jia, Shaohui Jiao, Qibin Hou, MingMing Cheng 핵심 연구 목표 이 연구는 기존 비디오명령어 데이터가 불완전하고 세분화된 정보 및 신뢰성 있는 주석이 부족하여 범용적인 비디오 "},{"id":"2026-02-16-What-does-RL-improve-for-Visual-Reasoning-A-Frankenstein-Style-Analysis","title":"[논문리뷰] What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis","excerpt":"arXiv에 게시된 'What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-What-does-RL-improve-for-Visual-Reasoning-A-Frankenstein-Style-Analysis","tags":["Review","Reinforcement Learning","Visual Reasoning","Vision-Language Models","Causal Probing","Model Merging","Parameter Analysis","Transformer Layers","Functional Localization"],"text":"링크: 논문 PDF로 바로 열기 저자: Xirui Li, Ming Li, Tianyi Zhou 핵심 연구 목표 본 논문은 시각적 추론을 위한 VisionLanguage Model (VLM)에서 강화 학습(RL)이 실제로 어떤 능력을 향상시키는지에 대한 모호함을 해결하고자 합니다. 특히, 기존의 지도 학습 기반 파인튜닝(IN)과 비교하여 RL이 제공하는 개선"},{"id":"2026-02-16-Xiaomi-Robotics-0-An-Open-Sourced-Vision-Language-Action-Model-with-Real-Time-Execution","title":"[논문리뷰] Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution","excerpt":"arXiv에 게시된 'Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-Xiaomi-Robotics-0-An-Open-Sourced-Vision-Language-Action-Model-with-Real-Time-Execution","tags":["Review","Vision-Language-Action (VLA)","Real-Time Robotics","Diffusion Transformer","Flow Matching","Asynchronous Execution","Robot Manipulation","Pre-training","Catastrophic Forgetting"],"text":"링크: 논문 PDF로 바로 열기 저자: Xiaomi Robotics 핵심 연구 목표 본 논문은 대규모 VLA 모델의 높은 추론 지연 시간으로 인한 실시간 로봇 제어의 어려움과, 사전 학습된 VLM의 시각의미론적 지식 손실(catastrophic forgetting) 문제를 해결하는 것을 목표로 합니다. 궁극적으로 고성능이며 빠르고 부드러운 실시간 로봇 구동"},{"id":"2026-02-16-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception","title":"[논문리뷰] Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception","excerpt":"arXiv에 게시된 'Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-16 00:00:00+0900+0900","permalink":"/ai/review/2026-02-16-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception","tags":["Review","Multimodal Perception","Fine-Grained Analysis","Knowledge Distillation","Region-to-Image","MLLMs","ZoomBench","Reinforcement Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Lai Wei, Liangbo He, Jun Lan, Lingzhong Dong, Yutong Cai, Siyuan Li, Huijia Zhu, Weiqiang Wang, Linghe Kong, Yue Wang, Zhuosheng Zhang, Weiran Huang 핵심 연구 목표 논문은 멀티모달 대규모 언어 모델(M"},{"id":"2026-02-17-A-Critical-Look-at-Targeted-Instruction-Selection-Disentangling-What-Matters-and-What-Doesnt","title":"[논문리뷰] A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)","excerpt":"arXiv에 게시된 'A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-A-Critical-Look-at-Targeted-Instruction-Selection-Disentangling-What-Matters-and-What-Doesnt","tags":["Review","Instruction Tuning","Data Selection","Large Language Models (LLMs)","Gradient-based Representations","Optimal Transport","Generalization Bounds","Data Representation"],"text":"링크: 논문 PDF로 바로 열기 저자: Nihal V. Nayak, Paula RodriguezDiaz, Neha Hulkund, Sara Beery, David AlvarezMelis 핵심 연구 목표 대규모 언어 모델(LLMs)의 표적 명령어 선택(targeted instruction selection) 연구 분야가 파편화되어 있고 명확한 지침이 부족하다"},{"id":"2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub","title":"[논문리뷰] AIDev: Studying AI Coding Agents on GitHub","excerpt":"Ahmed E. Hassan이 arXiv에 게시한 'AIDev: Studying AI Coding Agents on GitHub' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub","tags":["Review","AI Coding Agents","GitHub Data","Software Engineering","Pull Request Analysis","Human-AI Collaboration","Developer Productivity","Large Language Models"],"text":"링크: 논문 PDF로 바로 열기 저자: Hao Li, Haoxiang Zhang, Ahmed E. Hassan 핵심 연구 목표 AI 코딩 에이전트가 실제 소프트웨어 프로젝트에서 어떻게 활용되는지에 대한 포괄적인 데이터셋의 부재를 해결하는 것이 이 연구의 핵심 목표입니다. AIDev 라는 대규모 데이터셋을 구축하여 AI 도입, 개발자 생산성, 그리고 인간AI"},{"id":"2026-02-17-Acoustivision-Pro-An-Open-Source-Interactive-Platform-for-Room-Impulse-Response-Analysis-and-Acoustic-Characterization","title":"[논문리뷰] Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization","excerpt":"Mandip Goswami이 arXiv에 게시한 'Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Acoustivision-Pro-An-Open-Source-Interactive-Platform-for-Room-Impulse-Response-Analysis-and-Acoustic-Characterization","tags":["Review","Room Acoustics","Room Impulse Response","Acoustic Analysis","Open-Source Platform","Web Application","Data Visualization","Acoustic Metrics","Standards Compliance"],"text":"링크: 논문 PDF로 바로 열기 저자: Mandip Goswami 핵심 연구 목표 이 논문은 전문적인 룸 음향 분석(Room Acoustics Analysis) 도구의 높은 비용과 기술적 복잡성을 해결하고자 합니다. 연구의 핵심 목표는 룸 임펄스 응답(RIR) 분석 을 위한 접근성 높은 오픈소스 웹 기반 플랫폼 AcoustiVision Pro 를 제공하여,"},{"id":"2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation","title":"[논문리뷰] Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation","excerpt":"Ryan Rossi이 arXiv에 게시한 'Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation","tags":["Review","RAG Security","Knowledge Extraction Attack","Benchmarking","Privacy Leakage","Defense Mechanisms","Large Language Models","Retrieval Augmented Generation"],"text":"링크: 논문 PDF로 바로 열기 저자: Zhisheng Qi, Utkarsh Sahu, Li Ma, Haoyu Han, Ryan Rossi, Franck Dernoncourt, Mahantesh Halappanavar, Nesreen Ahmed, Yushun Dong, Yue Zhao, Yu Zhang, Yu Wang 핵심 연구 목표 이 연구는 Retrie"},{"id":"2026-02-17-BitDance-Scaling-Autoregressive-Generative-Models-with-Binary-Tokens","title":"[논문리뷰] BitDance: Scaling Autoregressive Generative Models with Binary Tokens","excerpt":"Xuefeng Hu이 arXiv에 게시한 'BitDance: Scaling Autoregressive Generative Models with Binary Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-BitDance-Scaling-Autoregressive-Generative-Models-with-Binary-Tokens","tags":["Review","Autoregressive Models","Binary Tokens","Diffusion Head","Image Generation","Tokenizer","Parallel Prediction","High-Resolution"],"text":"링크: 논문 PDF로 바로 열기 저자: Yuang Ai, Jiaming Han, Shaobin Zhuang, Weijia Mao, Xuefeng Hu, Ziyan Yang, Zhenheng Yang, Huaibo Huang, Xiangyu Yue, Hao Chen 핵심 연구 목표 본 논문은 기존 Autoregressive (AR) 모델의 제한된 토큰 표현력"},{"id":"2026-02-17-Blind-to-the-Human-Touch-Overlap-Bias-in-LLM-Based-Summary-Evaluation","title":"[논문리뷰] Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation","excerpt":"Puneet Mathur이 arXiv에 게시한 'Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Blind-to-the-Human-Touch-Overlap-Bias-in-LLM-Based-Summary-Evaluation","tags":["Review","LLM-as-a-judge","Summarization Evaluation","Overlap Bias","Position Bias","N-gram Metrics","Gemma","Llama","Evaluation Bias"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiangnan Fang, ChengTse Liu, Hanieh Deilamsalehy, Nesreen K. Ahmed, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)을 요약 평가 심사관으로 활용할 때"},{"id":"2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents","title":"[논문리뷰] BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents","excerpt":"Yanzhe Dan이 arXiv에 게시한 'BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents","tags":["Review","Multimodal LLMs","Web Browsing Agents","Deep Search","Benchmark","Tool Use","Process Evaluation","Multimodal Reasoning","Open-world QA"],"text":"링크: 논문 PDF로 바로 열기 저자: Huanyao Zhang, Jiepeng Zhou, Bo Li, Bowen Zhou, Yanzhe Dan, Haishan Lu, Zhiyong Cao, Jiaoyang Chen, Yuqian Han, Zinan Sheng, Zhengwei Tao, Hao Liang, Jialong Wu, Yang Shi, Yuanpe"},{"id":"2026-02-17-Data-Darwinism-Part-I-Unlocking-the-Value-of-Scientific-Data-for-Pre-training","title":"[논문리뷰] Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training","excerpt":"arXiv에 게시된 'Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Data-Darwinism-Part-I-Unlocking-the-Value-of-Scientific-Data-for-Pre-training","tags":["Review","Data Darwinism","Scientific Data","Pre-training","Foundation Models","Data Processing Hierarchy","Generative Refinement","Cognitive Completion","Learnability Gap"],"text":"링크: 논문 PDF로 바로 열기 저자: Yiwei Qin, Zhen Huang, Tiantian Mi, Weiye Si, Chenyang Zhou, Qipeng Guo, Siyuan Feng, Pengfei Liu 핵심 연구 목표 본 논문은 파운데이션 모델 학습 데이터 처리의 체계적인 프레임워크 부재 문제를 해결하고자 합니다. 데이터를 모델과 공진화(coe"},{"id":"2026-02-17-DeepImageSearch-Benchmarking-Multimodal-Agents-for-Context-Aware-Image-Retrieval-in-Visual-Histories","title":"[논문리뷰] DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories","excerpt":"arXiv에 게시된 'DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-DeepImageSearch-Benchmarking-Multimodal-Agents-for-Context-Aware-Image-Retrieval-in-Visual-Histories","tags":["Review","Multimodal Agents","Image Retrieval","Context-Aware","Visual Histories","Benchmarking","Vision-Language Models","Agentic AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Chenlong Deng, Mengjie Deng, Junjie Wu, Dun Zeng, Teng Wang, Qingsong Xie, Jiadeng Huang, Shengjie Ma, Changwang Zhang, Zhaoxiang Wang, Jun Wang, Yutao Zhu, Zhicheng Dou 핵심 연구 목표"},{"id":"2026-02-17-Experiential-Reinforcement-Learning","title":"[논문리뷰] Experiential Reinforcement Learning","excerpt":"arXiv에 게시된 'Experiential Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Experiential-Reinforcement-Learning","tags":["Review","Reinforcement Learning","Language Models","Self-Reflection","Experiential Learning","Policy Optimization","Distillation","Agentic Reasoning"],"text":"링크: 논문 PDF로 바로 열기 저자: Taiwei Shi, Sihao Chen, Bowen Jiang, Linxin Song, Longqi Yang, Jieyu Zhao 핵심 연구 목표 언어 모델(LMs)이 희소하고 지연된 환경 피드백으로부터 학습하는 과정에서 발생하는 비효율성과 불안정성을 해결하는 것이 주요 목표입니다. 모델이 관찰된 실패를 미래 행동 "},{"id":"2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks","title":"[논문리뷰] Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks","excerpt":"arXiv에 게시된 'Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks","tags":["Review","Large Language Models","Prefill Attacks","AI Safety","Red Teaming","Vulnerability","Open-Weight Models","Jailbreaking","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Lukas Struppek, Adam Gleave, Kellin Pelrine, FAR.AI 핵심 연구 목표 본 논문은 오픈웨이트 대규모 언어 모델(LLM)이 프리필(prefill) 공격 에 체계적으로 취약하다는 점을 폭로하는 것을 목표로 합니다. 공격자가 모델의 초기 응답 토큰을 미리 정의하여 유해한 출력을 유도할 "},{"id":"2026-02-17-FireRed-Image-Edit-1-0-Techinical-Report","title":"[논문리뷰] FireRed-Image-Edit-1.0 Techinical Report","excerpt":"Cunzheng Wang이 arXiv에 게시한 'FireRed-Image-Edit-1.0 Techinical Report' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-FireRed-Image-Edit-1-0-Techinical-Report","tags":["Review","Image Editing","Diffusion Transformer","Instruction-based Editing","Data Curation","Reinforcement Learning","Multimodal Models","REDEdit-Bench","Generative AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Cunzheng Wang, Chen Li, Chao Hui, Changhao Qiao, Super Intelligence Team 핵심 연구 목표 본 논문은 텍스트 지시 기반 이미지 편집(instructionbased image editing) 분야에서 CNN 의존성을 넘어선 새로운 접근 방식 을 제시하며, 데이터 큐"},{"id":"2026-02-17-InnoEval-On-Research-Idea-Evaluation-as-a-Knowledge-Grounded-Multi-Perspective-Reasoning-Problem","title":"[논문리뷰] InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem","excerpt":"arXiv에 게시된 'InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-InnoEval-On-Research-Idea-Evaluation-as-a-Knowledge-Grounded-Multi-Perspective-Reasoning-Problem","tags":["Review","Research Idea Evaluation","Large Language Models (LLMs)","Knowledge Grounding","Multi-Perspective Reasoning","Agent-based Systems","Scientific Discovery","Peer Review Simulation","Automated Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shuofei Qiao, Yunxiang Wei, Xuehai Wang, Bin Wu, Boyang Xue, Ningyu Zhang, Hossein A. Rahmani, Yanshan Wang, Qiang Zhang, Keyan Ding, Jeff Z. Pan, Huajun Chen, Emine Yilmaz 핵심 연구"},{"id":"2026-02-17-LaViDa-R1-Advancing-Reasoning-for-Unified-Multimodal-Diffusion-Language-Models","title":"[논문리뷰] LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models","excerpt":"arXiv에 게시된 'LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-LaViDa-R1-Advancing-Reasoning-for-Unified-Multimodal-Diffusion-Language-Models","tags":["Review","Multimodal Diffusion Models","Reasoning","Reinforcement Learning","Supervised Finetuning","Visual Question Answering","Image Editing","Object Grounding","Policy Gradient"],"text":"링크: 논문 PDF로 바로 열기 저자: Shufan Li, Yuchen Zhu, Jiuxiang Gu, Kangning Liu, Zhe Lin, Yongxin Chen, Molei Tao, Aditya Grover, Jason Kuen 핵심 연구 목표 본 논문은 기존 확산 언어 모델(dLLMs) 기반 추론 시스템이 겪는 태스크 특이성, RL 학습 불안정성,"},{"id":"2026-02-17-MoRL-Reinforced-Reasoning-for-Unified-Motion-Understanding-and-Generation","title":"[논문리뷰] MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation","excerpt":"arXiv에 게시된 'MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-MoRL-Reinforced-Reasoning-for-Unified-Motion-Understanding-and-Generation","tags":["Review","Motion Understanding","Motion Generation","Reinforcement Learning","Chain-of-Motion","Multimodal LLM","Human Motion Synthesis","Text-to-Motion"],"text":"링크: 논문 PDF로 바로 열기 저자: Hongpeng Wang, Zeyu Zhang, Wenhao Li, Hao Tang 핵심 연구 목표 인간 모션 이해 및 생성 분야에서 제한적인 추론 능력 과 테스트 시간 계획의 한계 를 극복하는 것을 목표로 합니다. 이를 위해, 모션 이해와 생성을 통합하는 단일 멀티모달 모션 모델 을 제안하여, 논리적 추론과 지각적 "},{"id":"2026-02-17-Nanbeige4-1-3B-A-Small-General-Model-that-Reasons-Aligns-and-Acts","title":"[논문리뷰] Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts","excerpt":"arXiv에 게시된 'Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Nanbeige4-1-3B-A-Small-General-Model-that-Reasons-Aligns-and-Acts","tags":["Review","Small Language Model","Generalist AI","Reasoning","Code Generation","Agentic Behavior","Reinforcement Learning","Tool Use","Deep Search"],"text":"링크: 논문 PDF로 바로 열기 저자: Nanbeige LLM Lab, Boss Zhipin 핵심 연구 목표 컴팩트한 30억(3B) 파라미터 규모의 모델인 Nanbeige4.13B 를 개발하여 강력한 에이전트 행동, 코드 생성 및 일반적인 추론 능력을 동시에 달성하는 것을 목표로 합니다. 단일 소규모 언어 모델(SLM) 내에서 이러한 다재다능성을 입증하고,"},{"id":"2026-02-17-Preliminary-sonification-of-ENSO-using-traditional-Javanese-gamelan-scales","title":"[논문리뷰] Preliminary sonification of ENSO using traditional Javanese gamelan scales","excerpt":"arXiv에 게시된 'Preliminary sonification of ENSO using traditional Javanese gamelan scales' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Preliminary-sonification-of-ENSO-using-traditional-Javanese-gamelan-scales","tags":["Review","Sonification","ENSO","Gamelan Scales","Complex Systems","Phase Space Analysis","Recurrence Quantification","Parameter Mapping"],"text":"링크: 논문 PDF로 바로 열기 저자: Sandy H. S. Herho, Rusmawan Suwarman, Nurjanna J. Trilaksono, Iwan P. Anwar, and Faiz R. Fajary 핵심 연구 목표 이 연구는 복잡한 동역학 시스템인 엘니뇨남방 진동(ENSO)의 데이터를 비서구권 음악적 프레임워크(자바 가믈란 음계)를 사용하여 소"},{"id":"2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model","title":"[논문리뷰] Query as Anchor: Scenario-Adaptive User Representation via Large Language Model","excerpt":"arXiv에 게시된 'Query as Anchor: Scenario-Adaptive User Representation via Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model","tags":["Review","User Representation Learning","Large Language Models","Scenario-Adaptive","Query-Conditioned","Multi-modal","Prompt Tuning","KV-Cache","Industrial AI"],"text":"링크: 논문 PDF로 바로 열기 저자: Jiahao Yuan, Yike Xu, Jinyong Wen, Baokun Wang, Ziyi Gao, Xiaotong Lin, Yun Liu, Xing Fu, Yu Cheng, Yongchao Liu, Weiqiang Wang, Zhongle Xie 핵심 연구 목표 본 논문은 정적이고 태스크에 독립적인 사용자 임베딩"},{"id":"2026-02-17-Qute-Towards-Quantum-Native-Database","title":"[논문리뷰] Qute: Towards Quantum-Native Database","excerpt":"Surui Tang이 arXiv에 게시한 'Qute: Towards Quantum-Native Database' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-Qute-Towards-Quantum-Native-Database","tags":["Review","Quantum Database","Quantum Computing","SQL Compilation","Hybrid Optimizer","Quantum Indexing","Fidelity-Preserving Storage","Grover's Algorithm"],"text":"링크: 논문 PDF로 바로 열기 저자: Muzhi Chen, Xuanhe Zhou, Wei Zhou, Bangrui Xu, Surui Tang 핵심 연구 목표 논문은 고전적인 컴퓨터로는 처리하기 점점 어려워지는 워크로드를 가속화하기 위해 양자 컴퓨터를 활용하는 양자 데이터베이스(Qute) 를 제안합니다. 기존 시뮬레이션 기반 접근법의 한계를 극복하고, 양자"},{"id":"2026-02-17-REDSearcher-A-Scalable-and-Cost-Efficient-Framework-for-Long-Horizon-Search-Agents","title":"[논문리뷰] REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents","excerpt":"arXiv에 게시된 'REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-REDSearcher-A-Scalable-and-Cost-Efficient-Framework-for-Long-Horizon-Search-Agents","tags":["Review","Long-Horizon Search","Multimodal LLM","Task Synthesis","Agentic Mid-Training","Reinforcement Learning","Tool-Augmented Agents","Web Search"],"text":"링크: 논문 PDF로 바로 열기 저자: Zheng Chu, Xiao Wang, Jack Hong 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)이 긴 탐색 경로와 많은 상호작용이 필요한 심층 검색 태스크를 수행할 때 겪는 어려움, 특히 고품질 훈련 데이터 부족과 높은 상호작용 비용 문제를 해결하는 것을 목표로 합니다. 텍스트 전용 및 멀티모달 환경 모"},{"id":"2026-02-17-UniWeTok-An-Unified-Binary-Tokenizer-with-Codebook-Size-2128-for-Unified-Multimodal-Large-Language-Model","title":"[논문리뷰] UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model","excerpt":"arXiv에 게시된 'UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-17 00:00:00+0900+0900","permalink":"/ai/review/2026-02-17-UniWeTok-An-Unified-Binary-Tokenizer-with-Codebook-Size-2128-for-Unified-Multimodal-Large-Language-Model","tags":["Review","Multimodal LLM","Visual Tokenizer","Binary Codebook","Image Generation","Semantic Extraction","Pre-Post Distillation","Hybrid Architecture"],"text":"링크: 논문 PDF로 바로 열기 저자: Shaobin Zhuang, Yuang Ai, Jiaming Han, Weijia Mao, Xiaohui Li, Fangyikang Wang, Xiao Wang, Yan Li, Shanchuan Lin, Kun Xu, Zhenheng Yang, Huaibo Huang, Xiangyu Yue, Hao Chen, Yali"},{"id":"2026-02-18-COMPOT-Calibration-Optimized-Matrix-Procrustes-Orthogonalization-for-Transformers-Compression","title":"[논문리뷰] COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression","excerpt":"arXiv에 게시된 'COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-COMPOT-Calibration-Optimized-Matrix-Procrustes-Orthogonalization-for-Transformers-Compression","tags":["Review","Transformer Compression","Matrix Factorization","Sparse Dictionary Learning","Post-Training Quantization","Procrustes Analysis","Orthogonal Dictionary","Dynamic Allocation"],"text":"링크: 논문 PDF로 바로 열기 저자: Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Ammar Ali, Baher Mohammad, Stamatios Lefkimmiatis 핵심 연구 목표 본 논문은 Transformer 모델의 사후 학습 압축에서 발생하는 정확도 저하 문제를 해결하고자 합니다. 특히, 단일 공"},{"id":"2026-02-18-Causal-JEPA-Learning-World-Models-through-Object-Level-Latent-Interventions","title":"[논문리뷰] Causal-JEPA: Learning World Models through Object-Level Latent Interventions","excerpt":"arXiv에 게시된 'Causal-JEPA: Learning World Models through Object-Level Latent Interventions' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Causal-JEPA-Learning-World-Models-through-Object-Level-Latent-Interventions","tags":["Review","World Models","Object-Centric Representations","Latent Interventions","Masked Prediction","Causal Inductive Bias","Joint Embedding Predictive Architecture (JEPA)","Visual Question Answering (VQA)","Model Predictive Control (MPC)"],"text":"링크: 논문 PDF로 바로 열기 저자: Heejeong Nam, Quentin Le Lidec, Lucas Maes, Yann LeCun, Randall Balestriero 핵심 연구 목표 기존 객체 중심(objectcentric) 월드 모델이 상호작용 의존적 다이내믹스를 포착하지 못하고 자가 다이내믹스나 우발적 상관관계에 의존하는 한계를 해결하고자 합니"},{"id":"2026-02-18-ClinAlign-Scaling-Healthcare-Alignment-from-Clinician-Preference","title":"[논문리뷰] ClinAlign: Scaling Healthcare Alignment from Clinician Preference","excerpt":"Chaohe Zhang이 arXiv에 게시한 'ClinAlign: Scaling Healthcare Alignment from Clinician Preference' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-ClinAlign-Scaling-Healthcare-Alignment-from-Clinician-Preference","tags":["Review","Healthcare AI","LLM Alignment","Clinician Preference","Rubric-based RLHF","Medical LLMs","Data Curation","HealthBench","Principle-based Supervision"],"text":"링크: 논문 PDF로 바로 열기 저자: Shiwei Lyu, Xidong Wang, Lei Liu, Hao Zhu, Chaohe Zhang, Jian Wang, Jinjie Gu, Benyou Wang, Yue Shen 핵심 연구 목표 대규모 언어 모델(LLM)을 의료 분야에서 의사의 세밀한 선호도 및 전문 표준에 맞춰 정렬하는 문제를 해결하는 것이 목표입"},{"id":"2026-02-18-Does-Socialization-Emerge-in-AI-Agent-Society-A-Case-Study-of-Moltbook","title":"[논문리뷰] Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook","excerpt":"Ming Li이 arXiv에 게시한 'Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Does-Socialization-Emerge-in-AI-Agent-Society-A-Case-Study-of-Moltbook","tags":["Review","AI Agent Societies","Socialization","Large Language Models (LLMs)","Collective Dynamics","Semantic Analysis","Network Analysis","Moltbook"],"text":"링크: 논문 PDF로 바로 열기 저자: Ming Li, Xirui Li, Tianyi Zhou 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM) 에이전트 사회에서 인간 사회와 유사한 사회화(socialization) 현상이 발생하는지 탐구합니다. 특히, Moltbook 이라는 대규모 AI 전용 소셜 플랫폼을 사례 연구로 삼아, 에이전트 간의 지속적인 상"},{"id":"2026-02-18-GLM-5-from-Vibe-Coding-to-Agentic-Engineering","title":"[논문리뷰] GLM-5: from Vibe Coding to Agentic Engineering","excerpt":"GLM-5 Team이 arXiv에 게시한 'GLM-5: from Vibe Coding to Agentic Engineering' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-GLM-5-from-Vibe-Coding-to-Agentic-Engineering","tags":["Review","Foundation Model","Agentic AI","Reinforcement Learning","Sparse Attention","Software Engineering","Long-Context Models","GPU Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: GLM5 Team, wangcunxiang, yitianlian, Stanislas, zxdu20 핵심 연구 목표 본 논문은 AI 모델이 인간의 지시(vibe coding)에 의존하는 것을 넘어 자율적인 계획, 구현 및 반복 이 가능한 Agentic Engineering 패러다임으로 전환하는 것을 목표로 합니다. 기존"},{"id":"2026-02-18-Geometry-Aware-Rotary-Position-Embedding-for-Consistent-Video-World-Model","title":"[논문리뷰] Geometry-Aware Rotary Position Embedding for Consistent Video World Model","excerpt":"arXiv에 게시된 'Geometry-Aware Rotary Position Embedding for Consistent Video World Model' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Geometry-Aware-Rotary-Position-Embedding-for-Consistent-Video-World-Model","tags":["Review","Video World Model","Generative AI","Transformer","Positional Encoding","3D Consistency","View Synthesis","Sparse Attention","Loop Closure"],"text":"링크: 논문 PDF로 바로 열기 저자: Chendong Xiang, Jiajun Liu, Jintao Zhang, Xiao Yang, Zhengwei Fang, Shizun Wang, Zijun Wang, Yingtian Zou, Hang Su, Jun Zhu 핵심 연구 목표 본 논문은 카메라 제어가 가능한 시각적 월드 모델(predictive visual"},{"id":"2026-02-18-Learning-Native-Continuation-for-Action-Chunking-Flow-Policies","title":"[논문리뷰] Learning Native Continuation for Action Chunking Flow Policies","excerpt":"Di Zhang이 arXiv에 게시한 'Learning Native Continuation for Action Chunking Flow Policies' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Learning-Native-Continuation-for-Action-Chunking-Flow-Policies","tags":["Review","Action Chunking","Flow-based Policies","Trajectory Continuation","Robotics","Vision-Language-Action (VLA)","Denoising Dynamics","Schedule-shaped Guidance","Real-time Control"],"text":"링크: 논문 PDF로 바로 열기 저자: Di Zhang, Bocheng Li, Juntu Zhao, Hang Yu, lyfeng001 핵심 연구 목표 본 논문은 VisionLanguageAction (VLA) 모델에서 액션 청킹(action chunking) 시 발생하는 청크 경계의 불연속성 문제를 해결하고자 합니다. 특히, 기존 실시간 청킹 (RTC) 과"},{"id":"2026-02-18-On-Surprising-Effectiveness-of-Masking-Updates-in-Adaptive-Optimizers","title":"[논문리뷰] On Surprising Effectiveness of Masking Updates in Adaptive Optimizers","excerpt":"arXiv에 게시된 'On Surprising Effectiveness of Masking Updates in Adaptive Optimizers' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-On-Surprising-Effectiveness-of-Masking-Updates-in-Adaptive-Optimizers","tags":["Review","Adaptive Optimizers","Gradient Masking","LLM Training","Geometric Regularization","Momentum Alignment","RMSProp","Perplexity","Deep Learning"],"text":"링크: 논문 PDF로 바로 열기 저자: Taejong Joo, Wenhan Xia, Cheolmin Kim, Ming Zhang, Eugene Ie 핵심 연구 목표 대규모 언어 모델(LLM) 학습에 주로 사용되는 밀집형 적응적 옵티마이저의 한계에 도전하고, 무작위 업데이트 마스킹이 최적화 성능을 향상시킬 수 있음을 입증하는 것이 목표입니다. 특히, 모멘텀그"},{"id":"2026-02-18-Prescriptive-Scaling-Reveals-the-Evolution-of-Language-Model-Capabilities","title":"[논문리뷰] Prescriptive Scaling Reveals the Evolution of Language Model Capabilities","excerpt":"Sham Kakade이 arXiv에 게시한 'Prescriptive Scaling Reveals the Evolution of Language Model Capabilities' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Prescriptive-Scaling-Reveals-the-Evolution-of-Language-Model-Capabilities","tags":["Review","Prescriptive Scaling","Language Models","Capability Boundaries","Quantile Regression","Scaling Laws","Temporal Stability","I-Optimal Design","Benchmark Saturation"],"text":"링크: 논문 PDF로 바로 열기 저자: Hanlin Zhang, Jikai Jin, Vasilis Syrgkanis, Sham Kakade 핵심 연구 목표 언어 모델의 실제 배포 시점에 다양한 후처리(posttraining) 절차와 시간적 영향으로 인해 발생하는 예측 불가능성을 해결하고자 합니다. 사전 훈련(pretraining) 컴퓨팅 예산을 바탕으로 달"},{"id":"2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research","title":"[논문리뷰] ResearchGym: Evaluating Language Model Agents on Real-World AI Research","excerpt":"Arman Cohan이 arXiv에 게시한 'ResearchGym: Evaluating Language Model Agents on Real-World AI Research' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research","tags":["Review","LLM Agents","AI Research","Benchmark","Closed-loop Research","Agent Evaluation","Reproducibility","Real-world Tasks"],"text":"링크: 논문 PDF로 바로 열기 저자: Arman Cohan, Manasi Patwardhan, Aniketh Garikaparthi 핵심 연구 목표 AI 시스템이 가설 제시, 실험 설계, 결과 검증, 신념 업데이트를 포함하는 폐쇄 루프(closedloop) 연구 를 자율적으로 수행할 수 있는지 객관적으로 평가하는 벤치마크를 제시하는 것을 목표로 합니다. "},{"id":"2026-02-18-Revisiting-the-Platonic-Representation-Hypothesis-An-Aristotelian-View","title":"[논문리뷰] Revisiting the Platonic Representation Hypothesis: An Aristotelian View","excerpt":"Maria Brbić이 arXiv에 게시한 'Revisiting the Platonic Representation Hypothesis: An Aristotelian View' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Revisiting-the-Platonic-Representation-Hypothesis-An-Aristotelian-View","tags":["Review","Representational Similarity","Null Calibration","Permutation Testing","Confounder","Neural Network Representation","Platonic Representation Hypothesis","Aristotelian Representation Hypothesis"],"text":"링크: 논문 PDF로 바로 열기 저자: Fabian Gröger, Shuo Wen, Maria Brbić 핵심 연구 목표 본 논문은 신경망 표현의 유사성을 측정하는 기존 지표들이 모델의 폭(width) 과 깊이(depth) 에 의해 체계적으로 왜곡된다는 문제를 제기하며, Platonic Representation Hypothesis 의 타당성을 재검토하는 "},{"id":"2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens","title":"[논문리뷰] STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens","excerpt":"Zhilong Zheng이 arXiv에 게시한 'STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens","tags":["Review","Reinforcement Learning","Large Language Models","Training Stability","Policy Optimization","Spurious Tokens","Entropy Regularization","Gradient Modulation"],"text":"링크: 논문 PDF로 바로 열기 저자: Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng 핵심 연구 목표 대규모 언어 모델(LLM)의 강화 학습(RL) 미세 조정 과정에서 발생하는 훈련 불안정성, 특히 후반부 성능 저하 문제를 해결하는 것을 목표로 합니다. 기존 RL 미세 조정 방식이 엔트로피 정규화"},{"id":"2026-02-18-Sanity-Checks-for-Sparse-Autoencoders-Do-SAEs-Beat-Random-Baselines","title":"[논문리뷰] Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?","excerpt":"Ivan Oseledets이 arXiv에 게시한 'Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Sanity-Checks-for-Sparse-Autoencoders-Do-SAEs-Beat-Random-Baselines","tags":["Review","Sparse Autoencoders","Interpretability","Neural Network Internals","Evaluation Baselines","Feature Decomposition","LLMs","Mechanistic Interpretability"],"text":"링크: 논문 PDF로 바로 열기 저자: Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina 핵심 연구 목표 본 논문은 Sparse Autoencoders (SAEs)가 신경망의 활성화를 해석 가능한 희소 특징으로 분해하는 데 있어 실제"},{"id":"2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models","title":"[논문리뷰] Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models","excerpt":"Liwei Wang이 arXiv에 게시한 'Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models","tags":["Review","Multimodal Models","Generative AI","Understanding","Reason-Reflect-Refine (R3)","Reinforcement Learning (RL)","Text-to-Image Generation","Optimization Dilemma","Image Editing"],"text":"링크: 논문 PDF로 바로 열기 저자: Sen Ye, Mengde Xu, Shuyang Gu, Di He, Liwei Wang, Han Hu 핵심 연구 목표 멀티모달 모델에서 생성 능력과 이해 능력 향상이 서로 상충되는 \"최적화 딜레마\"를 해결하는 것을 목표로 합니다. 생성과 이해가 경쟁적 목표가 아닌 시너지를 발휘하도록 하여, 강력한 생성 성능과 개선된 "},{"id":"2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling","title":"[논문리뷰] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling","excerpt":"Animesh Sinha이 arXiv에 게시한 'UniT: Unified Multimodal Chain-of-Thought Test-time Scaling' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling","tags":["Review","Multimodal AI","Chain-of-Thought","Test-time Scaling","Unified Models","Iterative Reasoning","Image Generation","Visual Reasoning","Self-Correction"],"text":"링크: 논문 PDF로 바로 열기 저자: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha 핵심 연구 목표 본 논문은 기존 통합 멀티모달 모델들이 단일 패스로만 작동하여 반복적인 개선 없이 출력을 생성하는 한계를 지적합니다. 복잡한 공간 구성, 다중 객체 상호작용, 진화하는 지침 등 다단"},{"id":"2026-02-18-Visual-Persuasion-What-Influences-Decisions-of-Vision-Language-Models","title":"[논문리뷰] Visual Persuasion: What Influences Decisions of Vision-Language Models?","excerpt":"Nikhil Singh이 arXiv에 게시한 'Visual Persuasion: What Influences Decisions of Vision-Language Models?' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-18 00:00:00+0900+0900","permalink":"/ai/review/2026-02-18-Visual-Persuasion-What-Influences-Decisions-of-Vision-Language-Models","tags":["Review","Vision-Language Models","Visual Persuasion","Prompt Optimization","Image Generation","AI Agent Behavior","Interpretability","Behavioral Evaluation"],"text":"링크: 논문 PDF로 바로 열기 저자: Manuel Cherep, Pranav M R, Pattie Maes, Nikhil Singh 핵심 연구 목표 본 연구는 VisionLanguage Model (VLM) 이 시각적 요인에 의해 의사결정에 어떻게 영향을 받는지 체계적으로 이해하는 것을 목표로 합니다. 특히, VLM의 시각적 선호도 구조를 밝히고, 이미지"},{"id":"2026-02-19-BiManiBench-A-Hierarchical-Benchmark-for-Evaluating-Bimanual-Coordination-of-Multimodal-Large-Language-Models","title":"[논문리뷰] BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models","excerpt":"arXiv에 게시된 'BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-BiManiBench-A-Hierarchical-Benchmark-for-Evaluating-Bimanual-Coordination-of-Multimodal-Large-Language-Models","tags":["Review","Bimanual Manipulation","MLLMs","Robotics Benchmark","Spatial Reasoning","Action Planning","End-Effector Control","Embodied AI","Multimodal LLMs"],"text":"링크: 논문 PDF로 바로 열기 저자: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li 핵심 연구 목표 기존 로봇 조작 벤치마크가 주로 단일 팔 조작에 국한되어 양팔 조작에 필수적인 공간시간적 조정, 동적 역할 할당, 자가 충돌 방지 등의 복잡성을 포착하지 못하는 문제를 해결하는 것이 목표입"},{"id":"2026-02-19-Empty-Shelves-or-Lost-Keys-Recall-Is-the-Bottleneck-for-Parametric-Factuality","title":"[논문리뷰] Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality","excerpt":"arXiv에 게시된 'Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-Empty-Shelves-or-Lost-Keys-Recall-Is-the-Bottleneck-for-Parametric-Factuality","tags":["Review","LLM Factuality","Knowledge Profiling","Encoding vs. Recall","WikiProfile Benchmark","Inference-time Computation","Reversal Curse","Long-tail Knowledge","Parametric Knowledge"],"text":"링크: 논문 PDF로 바로 열기 저자: Nitay Calderon, Eyal BenDavid, Zorik Gekhman, Eran Ofek, Gal Yona 핵심 연구 목표 본 논문은 대규모 언어 모델(LLM)의 사실성(factuality) 오류 원인을 '지식 누락(encoding failure, empty shelves)'과 '인코딩된 사실 접근 제한(r"},{"id":"2026-02-19-Learning-Humanoid-End-Effector-Control-for-Open-Vocabulary-Visual-Loco-Manipulation","title":"[논문리뷰] Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation","excerpt":"arXiv에 게시된 'Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-Learning-Humanoid-End-Effector-Control-for-Open-Vocabulary-Visual-Loco-Manipulation","tags":["Review","Humanoid Robotics","End-Effector Control","Loco-Manipulation","Open-Vocabulary Perception","Visual Generalization","Sim2Real Transfer","Residual Learning","Robot Grasping"],"text":"링크: 논문 PDF로 바로 열기 저자: Runpei Dong†, Ziyan Li†, Xialin He, Saurabh Gupta 핵심 연구 목표 본 연구는 인간형 로봇이 온보드 센서만을 사용하여 새로운 객체를 새로운 환경에서 자율적으로 로코조작(locomanipulate) 하는 능력을 개발하는 것을 목표로 합니다. 특히, 정확한 엔드이펙터(EE) 제어 와 "},{"id":"2026-02-19-Learning-Situated-Awareness-in-the-Real-World","title":"[논문리뷰] Learning Situated Awareness in the Real World","excerpt":"Rajiv Dhawan이 arXiv에 게시한 'Learning Situated Awareness in the Real World' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World","tags":["Review","Situated Awareness","Egocentric Vision","Spatial Reasoning","Multimodal Foundation Models","Video Understanding","Benchmark","Real-world Data"],"text":"링크: 논문 PDF로 바로 열기 저자: Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan, Jiajun Wu, MingHsuan Yang, Xin Eric Wang 핵심 연구 목표 본 논문은 기존의 멀티모달 파운데이션 모델(MFM) 벤치마크들이 환경 중심의 공간 관계에만 초점을 맞추고, 에이전트의 "},{"id":"2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark","title":"[논문리뷰] MAEB: Massive Audio Embedding Benchmark","excerpt":"arXiv에 게시된 'MAEB: Massive Audio Embedding Benchmark' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark","tags":["Review","Audio Embedding","Benchmark","Multimodal","Zero-shot Classification","Clustering","Representation Learning","MTEB Ecosystem","Cross-modal Audio-Text","Multilingual Audio"],"text":"링크: 논문 PDF로 바로 열기 저자: Adnan El Assadi, Isaac Chung, Chenghao Xiao, Roman Solomatin, Animesh Jha, Rahul Chand, Silky Singh, Kaitlyn Wang, Ali Sartaz Khan, Marc Moussa Nasser, Sufen Fong, Pengfei He, Al"},{"id":"2026-02-19-MMA-Multimodal-Memory-Agent","title":"[논문리뷰] MMA: Multimodal Memory Agent","excerpt":"arXiv에 게시된 'MMA: Multimodal Memory Agent' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-MMA-Multimodal-Memory-Agent","tags":["Review","Multimodal AI","Memory-Augmented Agents","Reliability Assessment","Epistemic Prudence","RAG Systems","Confidence Scoring","Belief Dynamics","Multimodal Conflict"],"text":"링크: 논문 PDF로 바로 열기 저자: Yihao Lu, Wanru Cheng, Zeyu Zhang, Hao Tang 핵심 연구 목표 롱호라이즌 멀티모달 에이전트의 메모리 검색 시 발생하는 오래되거나, 신뢰도가 낮거나, 상충되는 정보로 인한 과신 오류 및 안전 문제를 해결하는 것이 목표입니다. 특히 에이전트가 노이즈가 많고, 정보가 불안정하며, 모순적인 기"},{"id":"2026-02-19-Multi-agent-cooperation-through-in-context-co-player-inference","title":"[논문리뷰] Multi-agent cooperation through in-context co-player inference","excerpt":"arXiv에 게시된 'Multi-agent cooperation through in-context co-player inference' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-Multi-agent-cooperation-through-in-context-co-player-inference","tags":["Review","Multi-Agent Reinforcement Learning","In-Context Learning","Cooperation","Sequence Models","Opponent Shaping","Iterated Prisoner's Dilemma","Predictive Policy Improvement"],"text":"링크: 논문 PDF로 바로 열기 저자: Marissa A. Weis, Maciej Wołczyk, Rajai Nasser, Rif A. Saurous, Blaise Agüera y Arcas, João Sacramento, Alexander Meulemans 핵심 연구 목표 다중 에이전트 강화 학습(MARL)에서 자기 이익을 추구하는 에이전트 간의 협력을 "},{"id":"2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation","title":"[논문리뷰] Optimizing Few-Step Generation with Adaptive Matching Distillation","excerpt":"arXiv에 게시된 'Optimizing Few-Step Generation with Adaptive Matching Distillation' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation","tags":["Review","Diffusion Models","Knowledge Distillation","Few-Step Generation","Adaptive Matching","Forbidden Zones","Generative Models","Sample Quality","Training Stability"],"text":"링크: 논문 PDF로 바로 열기 저자: Lichen Bai, Zikai Zhou, Shitong Shao, Wenliang Zhong, Shuo Yang, Shuo Chen, Bojun Chen, Zeke Xie 핵심 연구 목표 본 논문은 Distribution Matching Distillation (DMD) 과정에서 발생하는 \"Forbidden Zone"},{"id":"2026-02-19-SAM-3D-Body-Robust-Full-Body-Human-Mesh-Recovery","title":"[논문리뷰] SAM 3D Body: Robust Full-Body Human Mesh Recovery","excerpt":"Taosha Fan이 arXiv에 게시한 'SAM 3D Body: Robust Full-Body Human Mesh Recovery' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-SAM-3D-Body-Robust-Full-Body-Human-Mesh-Recovery","tags":["Review","Human Mesh Recovery (HMR)","Full-Body Pose Estimation","Promptable Models","Momentum Human Rig (MHR)","Data Engine","Encoder-Decoder","Robustness","3D Vision"],"text":"링크: 논문 PDF로 바로 열기 저자: Xitong Yang, Devansh Kukreja, Don Pinkus, Anushka Sagar, Taosha Fan, Jinhyung Park, Soyong Shin, Jinkun Cao, Jiawei Liu, Nicolas Ugrinovic, Matt Feiszli, Jitendra Malik, Piotr Do"},{"id":"2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","title":"[논문리뷰] SLA2: Sparse-Linear Attention with Learnable Routing and QAT","excerpt":"arXiv에 게시된 'SLA2: Sparse-Linear Attention with Learnable Routing and QAT' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","tags":["Review","Sparse-Linear Attention","Diffusion Models","Video Generation","Learnable Routing","Quantization-Aware Training","Attention Acceleration","Model Optimization"],"text":"링크: 논문 PDF로 바로 열기 저자: Jintao Zhang¹, Haoxu Wang¹, Kai Jiang¹, Kaiwen Zheng¹, Youhe Jiang¹, Ion Stoica², Jianfei Chen¹, Jun Zhu¹, Joseph E. Gonzalez² 핵심 연구 목표 본 논문은 기존 SparseLinear Attention (SLA)의 한계,"},{"id":"2026-02-19-Towards-a-Science-of-AI-Agent-Reliability","title":"[논문리뷰] Towards a Science of AI Agent Reliability","excerpt":"arXiv에 게시된 'Towards a Science of AI Agent Reliability' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-Towards-a-Science-of-AI-Agent-Reliability","tags":["Review","AI Agents","Reliability","Evaluation Metrics","Consistency","Robustness","Predictability","Safety","Benchmarks"],"text":"링크: 논문 PDF로 바로 열기 저자: Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan 핵심 연구 목표 AI 에이전트의 높은 벤치마크 정확도와 실제 배포 시의 잦은 실패 간의 격차를 해소하는 것이 이 연구의 주요 목표입니다. 기존의 단일 "},{"id":"2026-02-19-Visual-Memory-Injection-Attacks-for-Multi-Turn-Conversations","title":"[논문리뷰] Visual Memory Injection Attacks for Multi-Turn Conversations","excerpt":"Matthias Hein이 arXiv에 게시한 'Visual Memory Injection Attacks for Multi-Turn Conversations' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-Visual-Memory-Injection-Attacks-for-Multi-Turn-Conversations","tags":["Review","LVLM","Adversarial Attacks","Multi-Turn Conversations","Visual Memory Injection","Stealthy Attacks","Benign Anchoring","Context-Cycling"],"text":"링크: 논문 PDF로 바로 열기 저자: Christian Schlarmann, Matthias Hein 핵심 연구 목표 본 논문은 대규모 시각언어 모델(LVLM)의 다중 턴 대화 환경에서의 보안 취약점을 해결하고자 합니다. 기존 단일 턴 공격의 한계를 넘어, 사용자의 의심을 사지 않고 수많은 무관한 대화 턴 이후에도 특정 트리거 프롬프트에 의해 미리 정의된"},{"id":"2026-02-19-World-Action-Models-are-Zero-shot-Policies","title":"[논문리뷰] World Action Models are Zero-shot Policies","excerpt":"arXiv에 게시된 'World Action Models are Zero-shot Policies' 논문에 대한 자세한 리뷰입니다.","date":"2026-02-19 00:00:00+0900+0900","permalink":"/ai/review/2026-02-19-World-Action-Models-are-Zero-shot-Policies","tags":["Review","World Action Models","Video Diffusion Models","Zero-shot Generalization","Cross-embodiment Transfer","Real-time Control","Robotics","Foundation Models","Flow Matching"],"text":"링크: 논문 PDF로 바로 열기 저자: Seonghyeon Ye†, Yunhao Ge\\, Kaiyuan Zheng\\, Shenyuan Gao\\, Sihyun Yu\\, George Kurian\\, Suneel Indupuru, You Liang Tan, Chuning Zhu, Jiannan Xiang, Ayaan Malik, Kyungmin Lee, Will"}]