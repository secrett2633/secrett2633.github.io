[
  {
    "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18866",
    "pdf_path": "temp_pdfs/2510.18866.pdf"
  },
  {
    "title": "World-in-World: World Models in a Closed-Loop World",
    "authors": "Arda Uzunoglu, Taiming Lu, Nanru Dai, Muqing Jiang, Jiahan Zhang",
    "link": "https://arxiv.org/abs/2510.18135",
    "pdf_path": "temp_pdfs/2510.18135.pdf"
  },
  {
    "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation",
    "authors": "Yujie Zhou, Jiazi Bu, Yuhang Zang, Zhimin Li, Yibin Wang",
    "link": "https://arxiv.org/abs/2510.18701",
    "pdf_path": "temp_pdfs/2510.18701.pdf"
  },
  {
    "title": "Chem-R: Learning to Reason as a Chemist",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.16880",
    "pdf_path": "temp_pdfs/2510.16880.pdf"
  },
  {
    "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18692",
    "pdf_path": "temp_pdfs/2510.18692.pdf"
  },
  {
    "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18876",
    "pdf_path": "temp_pdfs/2510.18876.pdf"
  },
  {
    "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18726",
    "pdf_path": "temp_pdfs/2510.18726.pdf"
  },
  {
    "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning",
    "authors": "Yuchen Eleanor Jiang, Dongyi Ding, Tiannan Wang, Meiling Tao, Chenghao Zhu",
    "link": "https://arxiv.org/abs/2510.18849",
    "pdf_path": "temp_pdfs/2510.18849.pdf"
  },
  {
    "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18855",
    "pdf_path": "temp_pdfs/2510.18855.pdf"
  },
  {
    "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.17722",
    "pdf_path": "temp_pdfs/2510.17722.pdf"
  },
  {
    "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM\n  Fine-tuning",
    "authors": "Xiangdong Zhang, Cancheng Zhang, Ning Liao, Xiaoxing Wang, Xiaohan Qin",
    "link": "https://arxiv.org/abs/2510.18250",
    "pdf_path": "temp_pdfs/2510.18250.pdf"
  },
  {
    "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.17519",
    "pdf_path": "temp_pdfs/2510.17519.pdf"
  },
  {
    "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
    "authors": "Zonghao Guo, Qi Ming, Ziyong Feng, Kaicheng Yang, Xiaoxing Hu",
    "link": "https://arxiv.org/abs/2510.18795",
    "pdf_path": "temp_pdfs/2510.18795.pdf"
  },
  {
    "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
    "authors": "Ran Yi, Zihan Su, Jiangning Zhang, Teng Hu",
    "link": "https://arxiv.org/abs/2510.18775",
    "pdf_path": "temp_pdfs/2510.18775.pdf"
  },
  {
    "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18873",
    "pdf_path": "temp_pdfs/2510.18873.pdf"
  },
  {
    "title": "Is Multilingual LLM Watermarking Truly Multilingual? A Simple\n  Back-Translation Solution",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18019",
    "pdf_path": "temp_pdfs/2510.18019.pdf"
  },
  {
    "title": "Video Reasoning without Training",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.17045",
    "pdf_path": "temp_pdfs/2510.17045.pdf"
  },
  {
    "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies",
    "authors": "James Glass, Sivan Doveh, M. Jehanzeb Mirza, Yufang Hou, Lukas Selch",
    "link": "https://arxiv.org/abs/2510.16505",
    "pdf_path": "temp_pdfs/2510.16505.pdf"
  },
  {
    "title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement\n  Learning Framework for Stock Trading",
    "authors": "Jiashu Wang, Zheye Deng",
    "link": "https://arxiv.org/abs/2510.14264",
    "pdf_path": "temp_pdfs/2510.14264.pdf"
  },
  {
    "title": "Extracting alignment data in open models",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.18554",
    "pdf_path": "temp_pdfs/2510.18554.pdf"
  },
  {
    "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from\n  Alternating-exposure Monocular Videos",
    "authors": "Dan Xu, Jinwen Chen, Mi Zhou, Lingtong Kong, Jinfeng Liu",
    "link": "https://arxiv.org/abs/2510.18489",
    "pdf_path": "temp_pdfs/2510.18489.pdf"
  },
  {
    "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation\n  via Structured Component-based Reward Mechanism",
    "authors": "Shuang Gu, Yaning Pan, Zhenyu Tang, Yankai Jiang, Haoran Sun",
    "link": "https://arxiv.org/abs/2510.15600",
    "pdf_path": "temp_pdfs/2510.15600.pdf"
  },
  {
    "title": "EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable\n  Learning",
    "authors": "Qipeng Guo, Siyang He, Aijun Yang, Bowen Li, He Du",
    "link": "https://arxiv.org/abs/2510.17928",
    "pdf_path": "temp_pdfs/2510.17928.pdf"
  },
  {
    "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.15862",
    "pdf_path": "temp_pdfs/2510.15862.pdf"
  }
]