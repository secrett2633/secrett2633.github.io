[
  {
    "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.22115",
    "pdf_path": "temp_pdfs\\2510.22115.pdf"
  },
  {
    "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
    "authors": "Yunxin Liu, Alexi Gladstone, Yiqi Huang, liuhuxian, travisddavies",
    "link": "https://arxiv.org/abs/2510.27545",
    "pdf_path": "temp_pdfs\\2510.27545.pdf"
  },
  {
    "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.00086",
    "pdf_path": "temp_pdfs\\2511.00086.pdf"
  },
  {
    "title": "World Simulation with Video Foundation Models for Physical AI",
    "authors": "Junjie Bai, Arslan Ali, NVIDIA, daerduomkch, richardaecn",
    "link": "https://arxiv.org/abs/2511.00062",
    "pdf_path": "temp_pdfs\\2511.00062.pdf"
  },
  {
    "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.01295",
    "pdf_path": "temp_pdfs\\2511.01295.pdf"
  },
  {
    "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
    "authors": "Lei Zhang, Zhongkai Xue, HideOnBush, weipang142857, Xinjiansz",
    "link": "https://arxiv.org/abs/2510.24788",
    "pdf_path": "temp_pdfs\\2510.24788.pdf"
  },
  {
    "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.01678",
    "pdf_path": "temp_pdfs\\2511.01678.pdf"
  },
  {
    "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
    "authors": "Feng Li, Wei Chow, Yongyuan Liang, russwang, marstin",
    "link": "https://arxiv.org/abs/2511.01163",
    "pdf_path": "temp_pdfs\\2511.01163.pdf"
  },
  {
    "title": "MR-Align: Meta-Reasoning Informed Factuality Alignment for Large\n  Reasoning Models",
    "authors": "Bin Yu, Jian Xu, Banjiuyufen, hongzhuyi, PeterKKQ",
    "link": "https://arxiv.org/abs/2510.24794",
    "pdf_path": "temp_pdfs\\2510.24794.pdf"
  },
  {
    "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.26236",
    "pdf_path": "temp_pdfs\\2510.26236.pdf"
  },
  {
    "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.01266",
    "pdf_path": "temp_pdfs\\2511.01266.pdf"
  },
  {
    "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
    "authors": "Guanting Dong, douzc, MengjieDeng",
    "link": "https://arxiv.org/abs/2510.27363",
    "pdf_path": "temp_pdfs\\2510.27363.pdf"
  },
  {
    "title": "OpenSIR: Open-Ended Self-Improving Reasoner",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.00602",
    "pdf_path": "temp_pdfs\\2511.00602.pdf"
  },
  {
    "title": "LongCat-Flash-Omni Technical Report",
    "authors": "Bin Xiao, Bayan, Bairui Wang, Meituan LongCat Team, Fengjiao",
    "link": "https://arxiv.org/abs/2511.00279",
    "pdf_path": "temp_pdfs\\2511.00279.pdf"
  },
  {
    "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.27571",
    "pdf_path": "temp_pdfs\\2510.27571.pdf"
  },
  {
    "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning",
    "authors": "Shaoheng Lin, Jike Zhong, Ming Li, haoquan03, stzhao",
    "link": "https://arxiv.org/abs/2511.01833",
    "pdf_path": "temp_pdfs\\2511.01833.pdf"
  },
  {
    "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.26909",
    "pdf_path": "temp_pdfs\\2510.26909.pdf"
  },
  {
    "title": "left|,circlearrowright,text{BUS},right|: A Large and\n  Diverse Multimodal Benchmark for evaluating the ability of Vision-Language\n  Models to understand Rebus Puzzles",
    "authors": "Deepiha S, Khush Bajaj, Abhilash Nandy, Trishanu Das",
    "link": "https://arxiv.org/abs/2511.01340",
    "pdf_path": "temp_pdfs\\2511.01340.pdf"
  },
  {
    "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench",
    "authors": "",
    "link": "https://arxiv.org/abs/2510.26865",
    "pdf_path": "temp_pdfs\\2510.26865.pdf"
  },
  {
    "title": "Trove: A Flexible Toolkit for Dense Retrieval",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.01857",
    "pdf_path": "temp_pdfs\\2511.01857.pdf"
  },
  {
    "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models",
    "authors": "Changfeng Ma, Xinyu Fu, Hao Sun, Xiaoyu Zhan, Osilly",
    "link": "https://arxiv.org/abs/2511.01618",
    "pdf_path": "temp_pdfs\\2511.01618.pdf"
  },
  {
    "title": "Towards Robust Mathematical Reasoning",
    "authors": "Yuri Chervonyi, Golnaz Ghiasi, Hoang H. Nguyen, Dawsen Hwang, Thang Luong",
    "link": "https://arxiv.org/abs/2511.01846",
    "pdf_path": "temp_pdfs\\2511.01846.pdf"
  },
  {
    "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
    "authors": "Jiale Cheng, Xujun Li, Dazhi Jiang, traveler2333, lez3f",
    "link": "https://arxiv.org/abs/2510.26491",
    "pdf_path": "temp_pdfs\\2510.26491.pdf"
  },
  {
    "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment",
    "authors": "Yuhao Zhai, Biao Yang, Jinlin Wu, Qing Xu, FrancisChen",
    "link": "https://arxiv.org/abs/2511.01775",
    "pdf_path": "temp_pdfs\\2511.01775.pdf"
  },
  {
    "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.01718",
    "pdf_path": "temp_pdfs\\2511.01718.pdf"
  },
  {
    "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
    "authors": "Jinsong Su, Jie Zhou, Fandong Meng, Liqiang Niu, Zhibin Lan",
    "link": "https://arxiv.org/abs/2511.00405",
    "pdf_path": "temp_pdfs\\2511.00405.pdf"
  },
  {
    "title": "AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat\n  Intelligence",
    "authors": "Peter Worth, Nidhi Rastogi, Salman Ahmad, Dipkamal Bhusal, Md Tanvirul Alam",
    "link": "https://arxiv.org/abs/2511.01144",
    "pdf_path": "temp_pdfs\\2511.01144.pdf"
  },
  {
    "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor\n  for GUI Grounding",
    "authors": "Wanrong Zhu, Jihyung Kil, Hao Tan, Viet Dac Lai, Shijie Zhou",
    "link": "https://arxiv.org/abs/2511.00810",
    "pdf_path": "temp_pdfs\\2511.00810.pdf"
  },
  {
    "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers",
    "authors": "",
    "link": "https://arxiv.org/abs/2511.01617",
    "pdf_path": "temp_pdfs\\2511.01617.pdf"
  },
  {
    "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace\n  Disentanglement",
    "authors": "Isabelle Augenstein, Pepa Atanasova, sekhcopenlu",
    "link": "https://arxiv.org/abs/2511.01706",
    "pdf_path": "temp_pdfs\\2511.01706.pdf"
  }
]