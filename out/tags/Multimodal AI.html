<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ddc331716d5e47a2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-0395f68b8cc78a20.js" async=""></script><script src="/_next/static/chunks/23-7d3f7f0b78aa2fd3.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-467e37449c5a68fc.js" async=""></script><script src="/_next/static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js" async=""></script><script src="/_next/static/chunks/app/layout-b0a450f8e4964582.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><title>#Multimodal AI - secrett2633&#x27;s blog</title><meta name="description" content="Multimodal AI 태그가 포함된 포스트 목록"/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/tags/Multimodal%20AI"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="#Multimodal AI - secrett2633&#x27;s blog"/><meta property="og:description" content="Multimodal AI 태그가 포함된 포스트 목록"/><meta property="og:url" content="https://blog.secrett2633.cloud/tags/Multimodal%20AI"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="#Multimodal AI - secrett2633&#x27;s blog"/><meta name="twitter:description" content="Multimodal AI 태그가 포함된 포스트 목록"/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"CollectionPage","name":"#Multimodal AI - secrett2633's blog","description":"Multimodal AI 태그가 포함된 포스트 목록","url":"https://blog.secrett2633.cloud/tags/Multimodal%20AI","isPartOf":{"@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud"},"inLanguage":"ko"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"#Multimodal AI","item":"https://blog.secrett2633.cloud/tags/Multimodal%20AI"}]}</script><div class="space-y-6"><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2728<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">#Multimodal AI</span></li></ol></nav><h1 class="page__title mb-6">#<!-- -->Multimodal AI</h1><p class="text-gray-500 mb-6">104<!-- -->개의 포스트</p><div class="entries-list"><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling">[논문리뷰] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling">Animesh Sinha이 [arXiv]에 게시한 &#x27;UniT: Unified Multimodal Chain-of-Thought Test-time Scaling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-18 00:00:00+0900+0900">2026년 2월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence">[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence">이 [arXiv]에 게시한 &#x27;OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-16 00:00:00+0900+0900">2026년 2월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit">[논문리뷰] DeepSight: An All-in-One LM Safety Toolkit</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit">이 [arXiv]에 게시한 &#x27;DeepSight: An All-in-One LM Safety Toolkit&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-13 00:00:00+0900+0900">2026년 2월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads">[논문리뷰] P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads">이 [arXiv]에 게시한 &#x27;P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-11 00:00:00+0900+0900">2026년 2월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation">[논문리뷰] MOVA: Towards Scalable and Synchronized Video-Audio Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation">이 [arXiv]에 게시한 &#x27;MOVA: Towards Scalable and Synchronized Video-Audio Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-10 00:00:00+0900+0900">2026년 2월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark">[논문리뷰] BABE: Biology Arena BEnchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark">이 [arXiv]에 게시한 &#x27;BABE: Biology Arena BEnchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks">[논문리뷰] Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks">이 [arXiv]에 게시한 &#x27;Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-04 00:00:00+0900+0900">2026년 2월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence">[논문리뷰] Kimi K2.5: Visual Agentic Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence">이 [arXiv]에 게시한 &#x27;Kimi K2.5: Visual Agentic Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-03 00:00:00+0900+0900">2026년 2월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution">[논문리뷰] OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution">Yusai Zhao이 [arXiv]에 게시한 &#x27;OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-29 00:00:00+0900+0900">2026년 1월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models">[논문리뷰] Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models">이 [arXiv]에 게시한 &#x27;Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-28 00:00:00+0900+0900">2026년 1월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-27-SkyReels-V3-Technique-Report">[논문리뷰] SkyReels-V3 Technique Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-27-SkyReels-V3-Technique-Report">이 [arXiv]에 게시한 &#x27;SkyReels-V3 Technique Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-27 00:00:00+0900+0900">2026년 1월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation">[논문리뷰] AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation">이 [arXiv]에 게시한 &#x27;AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-27 00:00:00+0900+0900">2026년 1월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation">[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation">이 [arXiv]에 게시한 &#x27;FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-21 00:00:00+0900+0900">2026년 1월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature">[논문리뷰] SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature">이 [arXiv]에 게시한 &#x27;SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-20 00:00:00+0900+0900">2026년 1월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding">[논문리뷰] Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding">Mohammadreza Salehi이 [arXiv]에 게시한 &#x27;Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-16 00:00:00+0900+0900">2026년 1월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-15-TranslateGemma-Technical-Report">[논문리뷰] TranslateGemma Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-15-TranslateGemma-Technical-Report">이 [arXiv]에 게시한 &#x27;TranslateGemma Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-15 00:00:00+0900+0900">2026년 1월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model">[논문리뷰] LTX-2: Efficient Joint Audio-Visual Foundation Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model">Andrew Kvochko이 [arXiv]에 게시한 &#x27;LTX-2: Efficient Joint Audio-Visual Foundation Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-07 00:00:00+0900+0900">2026년 1월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation">[논문리뷰] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation">이 [arXiv]에 게시한 &#x27;NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-06 00:00:00+0900+0900">2026년 1월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation">[논문리뷰] DreamOmni3: Scribble-based Editing and Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation">이 [arXiv]에 게시한 &#x27;DreamOmni3: Scribble-based Editing and Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-31 00:00:00+0900+0900">2025년 12월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone">[논문리뷰] Dream-VL &amp; Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone">이 [arXiv]에 게시한 &#x27;Dream-VL &amp; Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-30 00:00:00+0900+0900">2025년 12월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers">[논문리뷰] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers">이 [arXiv]에 게시한 &#x27;SlideTailor: Personalized Presentation Slide Generation for Scientific Papers&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-29 00:00:00+0900+0900">2025년 12월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding">[논문리뷰] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding">Yixin Chen이 [arXiv]에 게시한 &#x27;Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-29 00:00:00+0900+0900">2025년 12월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search">[논문리뷰] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search">Jierun Chen이 [arXiv]에 게시한 &#x27;InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-29 00:00:00+0900+0900">2025년 12월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models">[논문리뷰] Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models">이 [arXiv]에 게시한 &#x27;Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-25 00:00:00+0900+0900">2025년 12월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model">[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model">이 [arXiv]에 게시한 &#x27;Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-19 00:00:00+0900+0900">2025년 12월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models">[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models">이 [arXiv]에 게시한 &#x27;N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-19 00:00:00+0900+0900">2025년 12월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression">[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression">이 [arXiv]에 게시한 &#x27;VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-18 00:00:00+0900+0900">2025년 12월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models">[논문리뷰] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models">이 [arXiv]에 게시한 &#x27;DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-18 00:00:00+0900+0900">2025년 12월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality">[논문리뷰] The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality">이 [arXiv]에 게시한 &#x27;The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-12 00:00:00+0900+0900">2025년 12월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks">[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks">Yang Li이 [arXiv]에 게시한 &#x27;From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-08 00:00:00+0900+0900">2025년 12월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation">[논문리뷰] TV2TV: A Unified Framework for Interleaved Language and Video Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation">이 [arXiv]에 게시한 &#x27;TV2TV: A Unified Framework for Interleaved Language and Video Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-05 00:00:00+0900+0900">2025년 12월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation">[논문리뷰] ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation">이 [arXiv]에 게시한 &#x27;ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch">[논문리뷰] Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch">이 [arXiv]에 게시한 &#x27;Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing">[논문리뷰] WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing">Wendong Bu이 [arXiv]에 게시한 &#x27;WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-LFM2-Technical-Report">[논문리뷰] LFM2 Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-LFM2-Technical-Report">이 [arXiv]에 게시한 &#x27;LFM2 Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence">[논문리뷰] From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence">이 [arXiv]에 게시한 &#x27;From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights">[논문리뷰] Envision: Benchmarking Unified Understanding &amp; Generation for Causal World Process Insights</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights">이 [arXiv]에 게시한 &#x27;Envision: Benchmarking Unified Understanding &amp; Generation for Causal World Process Insights&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing">[논문리뷰] MIRA: Multimodal Iterative Reasoning Agent for Image Editing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing">Jiebo Luo이 [arXiv]에 게시한 &#x27;MIRA: Multimodal Iterative Reasoning Agent for Image Editing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-28 00:00:00+0900+0900">2025년 11월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning">[논문리뷰] SPHINX: A Synthetic Environment for Visual Perception and Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning">Nidhi Rastogi이 [arXiv]에 게시한 &#x27;SPHINX: A Synthetic Environment for Visual Perception and Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-27 00:00:00+0900+0900">2025년 11월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy">[논문리뷰] Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy">이 [arXiv]에 게시한 &#x27;Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-27 00:00:00+0900+0900">2025년 11월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning">[논문리뷰] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning">이 [arXiv]에 게시한 &#x27;Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-26 00:00:00+0900+0900">2025년 11월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO">[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO">이 [arXiv]에 게시한 &#x27;Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-21 00:00:00+0900+0900">2025년 11월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models">[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models">Baijiong Lin이 [arXiv]에 게시한 &#x27;V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-21 00:00:00+0900+0900">2025년 11월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding">[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding">이 [arXiv]에 게시한 &#x27;TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-21 00:00:00+0900+0900">2025년 11월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models">[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models">Rong Zhao이 [arXiv]에 게시한 &#x27;TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-19 00:00:00+0900+0900">2025년 11월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs">[논문리뷰] MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs">Kaijie Chen이 [arXiv]에 게시한 &#x27;MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-19 00:00:00+0900+0900">2025년 11월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant">[논문리뷰] HI-TransPA: Hearing Impairments Translation Personal Assistant</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant">이 [arXiv]에 게시한 &#x27;HI-TransPA: Hearing Impairments Translation Personal Assistant&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-17 00:00:00+0900+0900">2025년 11월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models">[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models">Siyuan Li이 [arXiv]에 게시한 &#x27;GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-17 00:00:00+0900+0900">2025년 11월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models">[논문리뷰] Music Flamingo: Scaling Music Understanding in Audio Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models">이 [arXiv]에 게시한 &#x27;Music Flamingo: Scaling Music Understanding in Audio Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-14 00:00:00+0900+0900">2025년 11월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images">[논문리뷰] V-Thinker: Interactive Thinking with Images</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images">Peiqing Yang이 [arXiv]에 게시한 &#x27;V-Thinker: Interactive Thinking with Images&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 22:08:24+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions">[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions">이 [arXiv]에 게시한 &#x27;UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 21:54:30+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought">[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought">이 [arXiv]에 게시한 &#x27;When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation">[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation">이 [arXiv]에 게시한 &#x27;VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation">[논문리뷰] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation">Feng Li이 [arXiv]에 게시한 &#x27;ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:22:42+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models">[논문리뷰] A Survey on Efficient Vision-Language-Action Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models">이 [arXiv]에 게시한 &#x27;A Survey on Efficient Vision-Language-Action Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:01:31+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games">[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games">Justin Cui이 [arXiv]에 게시한 &#x27;Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation">[논문리뷰] Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation">이 [arXiv]에 게시한 &#x27;Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences">[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences">이 [arXiv]에 게시한 &#x27;Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS">[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS">이 [arXiv]에 게시한 &#x27;Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-27-A-Definition-of-AGI">[논문리뷰] A Definition of AGI</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-27-A-Definition-of-AGI">Yarin Gal이 [arXiv]에 게시한 &#x27;A Definition of AGI&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-27 13:07:36+0900">2025년 10월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering">[논문리뷰] Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering">이 [arXiv]에 게시한 &#x27;Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-21 13:08:30+0900">2025년 10월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression">[논문리뷰] Glyph: Scaling Context Windows via Visual-Text Compression</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression">Wenyi Hong이 [arXiv]에 게시한 &#x27;Glyph: Scaling Context Windows via Visual-Text Compression&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-21 13:08:30+0900">2025년 10월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning">[논문리뷰] MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning">이 [arXiv]에 게시한 &#x27;MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-20 13:04:24+0900">2025년 10월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation">[논문리뷰] BLIP3o-NEXT: Next Frontier of Native Image Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation">이 [arXiv]에 게시한 &#x27;BLIP3o-NEXT: Next Frontier of Native Image Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-20 13:04:24+0900">2025년 10월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE">[논문리뷰] UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE">이 [arXiv]에 게시한 &#x27;UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark">[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark">이 [arXiv]에 게시한 &#x27;Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models">[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models">이 [arXiv]에 게시한 &#x27;LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner">[논문리뷰] Generative Universal Verifier as Multimodal Meta-Reasoner</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner">이 [arXiv]에 게시한 &#x27;Generative Universal Verifier as Multimodal Meta-Reasoner&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs">[논문리뷰] Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs">이 [arXiv]에 게시한 &#x27;Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos">[논문리뷰] UniVideo: Unified Understanding, Generation, and Editing for Videos</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos">Xintao Wang이 [arXiv]에 게시한 &#x27;UniVideo: Unified Understanding, Generation, and Editing for Videos&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models">[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models">Mohit Bansal이 [arXiv]에 게시한 &#x27;SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance">[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance">Xinghui Li이 [arXiv]에 게시한 &#x27;InstructX: Towards Unified Visual Editing with MLLM Guidance&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer">[논문리뷰] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer">이 [arXiv]에 게시한 &#x27;Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-09 13:45:06+0900">2025년 10월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition">[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition">이 [arXiv]에 게시한 &#x27;MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs">[논문리뷰] Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs">Zeyi Liao이 [arXiv]에 게시한 &#x27;Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents">[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents">Neil Zhenqiang Gong이 [arXiv]에 게시한 &#x27;WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-06 13:29:11+0900">2025년 10월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play">[논문리뷰] Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play">Jing Shi이 [arXiv]에 게시한 &#x27;Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics">[논문리뷰] TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics">Szu-Chi Chen이 [arXiv]에 게시한 &#x27;TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark">[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark">Yuran Wang이 [arXiv]에 게시한 &#x27;RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-30 13:52:24+0900">2025년 9월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing">[논문리뷰] OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing">Huanyu Zhang이 [arXiv]에 게시한 &#x27;OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-30 13:52:24+0900">2025년 9월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction">[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction">Guoxian Song이 [arXiv]에 게시한 &#x27;X-Streamer: Unified Human World Modeling with Audiovisual Interaction&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-29 13:47:46+0900">2025년 9월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation">[논문리뷰] Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation">Zhe Lin이 [arXiv]에 게시한 &#x27;Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-25 13:08:16+0900">2025년 9월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation">[논문리뷰] Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation">Jianbin Zheng이 [arXiv]에 게시한 &#x27;Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-24 13:14:19+0900">2025년 9월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions">[논문리뷰] FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions">tengdai722이 [arXiv]에 게시한 &#x27;FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents">[논문리뷰] Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents">Chao Zhang이 [arXiv]에 게시한 &#x27;Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-22 13:11:29+0900">2025년 9월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence">[논문리뷰] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence">Qinghua Huang이 [arXiv]에 게시한 &#x27;EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-19 13:12:21+0900">2025년 9월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision">[논문리뷰] AToken: A Unified Tokenizer for Vision</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision">Mingze Xu이 [arXiv]에 게시한 &#x27;AToken: A Unified Tokenizer for Vision&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-19 13:12:21+0900">2025년 9월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits">[논문리뷰] PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits">Zhenhao Chen이 [arXiv]에 게시한 &#x27;PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-16 13:16:41+0900">2025년 9월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models">[논문리뷰] Lost in Embeddings: Information Loss in Vision-Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models">Ivan Vulić이 [arXiv]에 게시한 &#x27;Lost in Embeddings: Information Loss in Vision-Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-16 13:16:41+0900">2025년 9월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding">[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding">Li Zheng이 [arXiv]에 게시한 &#x27;Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-16 13:16:41+0900">2025년 9월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding">[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding">Ethan Chern이 [arXiv]에 게시한 &#x27;Visual Programmability: A Guide for Code-as-Thought in Chart Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-12 13:12:46+0900">2025년 9월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models">[논문리뷰] A Survey of Reinforcement Learning for Large Reasoning Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models">Runze Liu이 [arXiv]에 게시한 &#x27;A Survey of Reinforcement Learning for Large Reasoning Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-11 13:02:36+0900">2025년 9월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey">[논문리뷰] Reinforcement Learning Foundations for Deep Research Systems: A Survey</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey">Wei Han이 [arXiv]에 게시한 &#x27;Reinforcement Learning Foundations for Deep Research Systems: A Survey&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-09 13:19:09+0900">2025년 9월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery">[논문리뷰] Mimicking the Physicist&#x27;s Eye:A VLM-centric Approach for Physics Formula Discovery</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery">Wenjie Zhou이 [arXiv]에 게시한 &#x27;Mimicking the Physicist&#x27;s Eye:A VLM-centric Approach for Physics Formula Discovery&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-01 13:14:34+0900">2025년 9월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification">[논문리뷰] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification">Liqiang Nie이 [arXiv]에 게시한 &#x27;CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-29 13:14:44+0900">2025년 8월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models">[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models">Yixiao Ge이 [arXiv]에 게시한 &#x27;AudioStory: Generating Long-Form Narrative Audio with Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-28 13:10:39+0900">2025년 8월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning">[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning">Xin Zheng이 [arXiv]에 게시한 &#x27;Explain Before You Answer: A Survey on Compositional Visual Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-26 13:21:57+0900">2025년 8월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding">[논문리뷰] When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding">Rui Guo이 [arXiv]에 게시한 &#x27;When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-22 13:10:52+0900">2025년 8월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions">[논문리뷰] ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions">Daeyoung Kim이 [arXiv]에 게시한 &#x27;ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-21 13:15:28+0900">2025년 8월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence">[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence">Fernando López이 [arXiv]에 게시한 &#x27;MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-20 13:26:54+0900">2025년 8월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models">[논문리뷰] A Survey on Diffusion Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models">Zhiqiang Shen이 [arXiv]에 게시한 &#x27;A Survey on Diffusion Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-15 13:09:31+0900">2025년 8월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences">[논문리뷰] Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences">Matvey Skripkin이 [arXiv]에 게시한 &#x27;Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-12 13:29:09+0900">2025년 8월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh">[논문리뷰] MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh">Yi Yang이 [arXiv]에 게시한 &#x27;MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-11 13:13:28+0900">2025년 8월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation">[논문리뷰] Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation">Tianyidan Xie이 [arXiv]에 게시한 &#x27;Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-06 13:46:36+0900">2025년 8월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article></div></div></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ddc331716d5e47a2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[9038,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js\"],\"default\"]\n6:I[231,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js\"],\"\"]\n7:I[227,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js\"],\"default\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nb:I[9157,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"default\"]\nc:I[4080,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"\"]\ne:I[6130,[],\"\"]\n9:[\"tag\",\"Multimodal%20AI\",\"d\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ddc331716d5e47a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"WcxaIiCPz9cbpnkGvOjOK\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/tags/Multimodal%20AI\",\"initialTree\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Multimodal%20AI\",\"d\"],{\"children\":[\"__PAGE__?{\\\"tag\\\":\\\"Multimodal AI\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Multimodal%20AI\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"$L5\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"CollectionPage\\\",\\\"name\\\":\\\"#Multimodal AI - secrett2633's blog\\\",\\\"description\\\":\\\"Multimodal AI 태그가 포함된 포스트 목록\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud/tags/Multimodal%20AI\\\",\\\"isPartOf\\\":{\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"inLanguage\\\":\\\"ko\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"#Multimodal AI\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/tags/Multimodal%20AI\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2728,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/tags/Multimodal%20AI\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"#Multimodal AI\"}]]}]]]}]}],[\"$\",\"h1\",null,{\"className\":\"page__title mb-6\",\"children\":[\"#\",\"Multimodal AI\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-500 mb-6\",\"children\":[104,\"개의 포스트\"]}],[\"$\",\"div\",null,{\"className\":\"entries-list\",\"children\":[[\"$\",\"article\",\"2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling\",\"children\":\"[논문리뷰] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling\",\"children\":\"Animesh Sinha이 [arXiv]에 게시한 'UniT: Unified Multimodal Chain-of-Thought Test-time Scaling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-18 00:00:00+0900+0900\",\"children\":\"2026년 2월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling\"}]]}]]}],[\"$\",\"article\",\"2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence\",\"children\":\"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence\",\"children\":\"이 [arXiv]에 게시한 'OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-16 00:00:00+0900+0900\",\"children\":\"2026년 2월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit\",\"children\":\"[논문리뷰] DeepSight: An All-in-One LM Safety Toolkit\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit\",\"children\":\"이 [arXiv]에 게시한 'DeepSight: An All-in-One LM Safety Toolkit' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-13 00:00:00+0900+0900\",\"children\":\"2026년 2월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit\"}]]}]]}],[\"$\",\"article\",\"2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads\",\"children\":\"[논문리뷰] P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads\",\"children\":\"이 [arXiv]에 게시한 'P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-11 00:00:00+0900+0900\",\"children\":\"2026년 2월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads\"}]]}]]}],[\"$\",\"article\",\"2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation\",\"children\":\"[논문리뷰] MOVA: Towards Scalable and Synchronized Video-Audio Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation\",\"children\":\"이 [arXiv]에 게시한 'MOVA: Towards Scalable and Synchronized Video-Audio Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-10 00:00:00+0900+0900\",\"children\":\"2026년 2월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation\"}]]}]]}],[\"$\",\"article\",\"2026-02-06-BABE-Biology-Arena-BEnchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark\",\"children\":\"[논문리뷰] BABE: Biology Arena BEnchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark\",\"children\":\"이 [arXiv]에 게시한 'BABE: Biology Arena BEnchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark\"}]]}]]}],[\"$\",\"article\",\"2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks\",\"children\":\"[논문리뷰] Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks\",\"children\":\"이 [arXiv]에 게시한 'Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-04 00:00:00+0900+0900\",\"children\":\"2026년 2월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks\"}]]}]]}],[\"$\",\"article\",\"2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence\",\"children\":\"[논문리뷰] Kimi K2.5: Visual Agentic Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence\",\"children\":\"이 [arXiv]에 게시한 'Kimi K2.5: Visual Agentic Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-03 00:00:00+0900+0900\",\"children\":\"2026년 2월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution\",\"children\":\"[논문리뷰] OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution\",\"children\":\"Yusai Zhao이 [arXiv]에 게시한 'OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-29 00:00:00+0900+0900\",\"children\":\"2026년 1월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution\"}]]}]]}],[\"$\",\"article\",\"2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models\",\"children\":\"[논문리뷰] Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models\",\"children\":\"이 [arXiv]에 게시한 'Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-28 00:00:00+0900+0900\",\"children\":\"2026년 1월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models\"}]]}]]}],[\"$\",\"article\",\"2026-01-27-SkyReels-V3-Technique-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-SkyReels-V3-Technique-Report\",\"children\":\"[논문리뷰] SkyReels-V3 Technique Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-SkyReels-V3-Technique-Report\",\"children\":\"이 [arXiv]에 게시한 'SkyReels-V3 Technique Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-27 00:00:00+0900+0900\",\"children\":\"2026년 1월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-27-SkyReels-V3-Technique-Report\"}]]}]]}],[\"$\",\"article\",\"2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation\",\"children\":\"[논문리뷰] AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation\",\"children\":\"이 [arXiv]에 게시한 'AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-27 00:00:00+0900+0900\",\"children\":\"2026년 1월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation\"}]]}]]}],[\"$\",\"article\",\"2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation\",\"children\":\"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation\",\"children\":\"이 [arXiv]에 게시한 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-21 00:00:00+0900+0900\",\"children\":\"2026년 1월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation\"}]]}]]}],[\"$\",\"article\",\"2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature\",\"children\":\"[논문리뷰] SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature\",\"children\":\"이 [arXiv]에 게시한 'SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-20 00:00:00+0900+0900\",\"children\":\"2026년 1월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature\"}]]}]]}],[\"$\",\"article\",\"2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding\",\"children\":\"[논문리뷰] Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding\",\"children\":\"Mohammadreza Salehi이 [arXiv]에 게시한 'Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-16 00:00:00+0900+0900\",\"children\":\"2026년 1월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding\"}]]}]]}],[\"$\",\"article\",\"2026-01-15-TranslateGemma-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-TranslateGemma-Technical-Report\",\"children\":\"[논문리뷰] TranslateGemma Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-TranslateGemma-Technical-Report\",\"children\":\"이 [arXiv]에 게시한 'TranslateGemma Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-15 00:00:00+0900+0900\",\"children\":\"2026년 1월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-15-TranslateGemma-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model\",\"children\":\"[논문리뷰] LTX-2: Efficient Joint Audio-Visual Foundation Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model\",\"children\":\"Andrew Kvochko이 [arXiv]에 게시한 'LTX-2: Efficient Joint Audio-Visual Foundation Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-07 00:00:00+0900+0900\",\"children\":\"2026년 1월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model\"}]]}]]}],[\"$\",\"article\",\"2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation\",\"children\":\"[논문리뷰] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation\",\"children\":\"이 [arXiv]에 게시한 'NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-06 00:00:00+0900+0900\",\"children\":\"2026년 1월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation\",\"children\":\"[논문리뷰] DreamOmni3: Scribble-based Editing and Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation\",\"children\":\"이 [arXiv]에 게시한 'DreamOmni3: Scribble-based Editing and Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-31 00:00:00+0900+0900\",\"children\":\"2025년 12월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone\",\"children\":\"[논문리뷰] Dream-VL \u0026 Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone\",\"children\":\"이 [arXiv]에 게시한 'Dream-VL \u0026 Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-30 00:00:00+0900+0900\",\"children\":\"2025년 12월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone\"}]]}]]}],[\"$\",\"article\",\"2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\",\"children\":\"[논문리뷰] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\",\"children\":\"이 [arXiv]에 게시한 'SlideTailor: Personalized Presentation Slide Generation for Scientific Papers' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-29 00:00:00+0900+0900\",\"children\":\"2025년 12월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\"}]]}]]}],[\"$\",\"article\",\"2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding\",\"children\":\"[논문리뷰] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding\",\"children\":\"Yixin Chen이 [arXiv]에 게시한 'Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-29 00:00:00+0900+0900\",\"children\":\"2025년 12월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search\",\"children\":\"[논문리뷰] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search\",\"children\":\"Jierun Chen이 [arXiv]에 게시한 'InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-29 00:00:00+0900+0900\",\"children\":\"2025년 12월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search\"}]]}]]}],[\"$\",\"article\",\"2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models\",\"children\":\"[논문리뷰] Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models\",\"children\":\"이 [arXiv]에 게시한 'Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-25 00:00:00+0900+0900\",\"children\":\"2025년 12월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model\",\"children\":\"[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model\",\"children\":\"이 [arXiv]에 게시한 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-19 00:00:00+0900+0900\",\"children\":\"2025년 12월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model\"}]]}]]}],[\"$\",\"article\",\"2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models\",\"children\":\"[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models\",\"children\":\"이 [arXiv]에 게시한 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-19 00:00:00+0900+0900\",\"children\":\"2025년 12월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\",\"children\":\"[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\",\"children\":\"이 [arXiv]에 게시한 'VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-18 00:00:00+0900+0900\",\"children\":\"2025년 12월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\"}]]}]]}],[\"$\",\"article\",\"2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models\",\"children\":\"[논문리뷰] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models\",\"children\":\"이 [arXiv]에 게시한 'DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-18 00:00:00+0900+0900\",\"children\":\"2025년 12월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality\",\"children\":\"[논문리뷰] The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality\",\"children\":\"이 [arXiv]에 게시한 'The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-12 00:00:00+0900+0900\",\"children\":\"2025년 12월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality\"}]]}]]}],[\"$\",\"article\",\"2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\",\"children\":\"[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\",\"children\":\"Yang Li이 [arXiv]에 게시한 'From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-08 00:00:00+0900+0900\",\"children\":\"2025년 12월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation\",\"children\":\"[논문리뷰] TV2TV: A Unified Framework for Interleaved Language and Video Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation\",\"children\":\"이 [arXiv]에 게시한 'TV2TV: A Unified Framework for Interleaved Language and Video Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-05 00:00:00+0900+0900\",\"children\":\"2025년 12월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation\",\"children\":\"[논문리뷰] ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation\",\"children\":\"이 [arXiv]에 게시한 'ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch\",\"children\":\"[논문리뷰] Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch\",\"children\":\"이 [arXiv]에 게시한 'Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing\",\"children\":\"[논문리뷰] WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing\",\"children\":\"Wendong Bu이 [arXiv]에 게시한 'WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-LFM2-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-LFM2-Technical-Report\",\"children\":\"[논문리뷰] LFM2 Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-LFM2-Technical-Report\",\"children\":\"이 [arXiv]에 게시한 'LFM2 Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-LFM2-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence\",\"children\":\"[논문리뷰] From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence\",\"children\":\"이 [arXiv]에 게시한 'From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights\",\"children\":\"[논문리뷰] Envision: Benchmarking Unified Understanding \u0026 Generation for Causal World Process Insights\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights\",\"children\":\"이 [arXiv]에 게시한 'Envision: Benchmarking Unified Understanding \u0026 Generation for Causal World Process Insights' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights\"}]]}]]}],[\"$\",\"article\",\"2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing\",\"children\":\"[논문리뷰] MIRA: Multimodal Iterative Reasoning Agent for Image Editing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing\",\"children\":\"Jiebo Luo이 [arXiv]에 게시한 'MIRA: Multimodal Iterative Reasoning Agent for Image Editing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-28 00:00:00+0900+0900\",\"children\":\"2025년 11월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing\"}]]}]]}],[\"$\",\"article\",\"2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning\",\"children\":\"[논문리뷰] SPHINX: A Synthetic Environment for Visual Perception and Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning\",\"children\":\"Nidhi Rastogi이 [arXiv]에 게시한 'SPHINX: A Synthetic Environment for Visual Perception and Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-27 00:00:00+0900+0900\",\"children\":\"2025년 11월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy\",\"children\":\"[논문리뷰] Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy\",\"children\":\"이 [arXiv]에 게시한 'Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-27 00:00:00+0900+0900\",\"children\":\"2025년 11월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy\"}]]}]]}],[\"$\",\"article\",\"2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning\",\"children\":\"[논문리뷰] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning\",\"children\":\"이 [arXiv]에 게시한 'Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-26 00:00:00+0900+0900\",\"children\":\"2025년 11월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO\",\"children\":\"[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO\",\"children\":\"이 [arXiv]에 게시한 'Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-21 00:00:00+0900+0900\",\"children\":\"2025년 11월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO\"}]]}]]}],[\"$\",\"article\",\"2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models\",\"children\":\"[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models\",\"children\":\"Baijiong Lin이 [arXiv]에 게시한 'V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-21 00:00:00+0900+0900\",\"children\":\"2025년 11월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding\",\"children\":\"[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding\",\"children\":\"이 [arXiv]에 게시한 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-21 00:00:00+0900+0900\",\"children\":\"2025년 11월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models\",\"children\":\"[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models\",\"children\":\"Rong Zhao이 [arXiv]에 게시한 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-19 00:00:00+0900+0900\",\"children\":\"2025년 11월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs\",\"children\":\"[논문리뷰] MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs\",\"children\":\"Kaijie Chen이 [arXiv]에 게시한 'MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-19 00:00:00+0900+0900\",\"children\":\"2025년 11월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs\"}]]}]]}],[\"$\",\"article\",\"2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant\",\"children\":\"[논문리뷰] HI-TransPA: Hearing Impairments Translation Personal Assistant\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant\",\"children\":\"이 [arXiv]에 게시한 'HI-TransPA: Hearing Impairments Translation Personal Assistant' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-17 00:00:00+0900+0900\",\"children\":\"2025년 11월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant\"}]]}]]}],[\"$\",\"article\",\"2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\",\"children\":\"[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\",\"children\":\"Siyuan Li이 [arXiv]에 게시한 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-17 00:00:00+0900+0900\",\"children\":\"2025년 11월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models\",\"children\":\"[논문리뷰] Music Flamingo: Scaling Music Understanding in Audio Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models\",\"children\":\"이 [arXiv]에 게시한 'Music Flamingo: Scaling Music Understanding in Audio Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-14 00:00:00+0900+0900\",\"children\":\"2025년 11월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-7-V-Thinker-Interactive-Thinking-with-Images\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images\",\"children\":\"[논문리뷰] V-Thinker: Interactive Thinking with Images\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images\",\"children\":\"Peiqing Yang이 [arXiv]에 게시한 'V-Thinker: Interactive Thinking with Images' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 22:08:24+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images\"}]]}]]}],[\"$\",\"article\",\"2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions\",\"children\":\"[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions\",\"children\":\"이 [arXiv]에 게시한 'UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 21:54:30+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\",\"children\":\"[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\",\"children\":\"이 [arXiv]에 게시한 'When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\",\"children\":\"[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\",\"children\":\"이 [arXiv]에 게시한 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\"}]]}]]}],[\"$\",\"article\",\"2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation\",\"children\":\"[논문리뷰] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation\",\"children\":\"Feng Li이 [arXiv]에 게시한 'ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:22:42+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models\",\"children\":\"[논문리뷰] A Survey on Efficient Vision-Language-Action Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models\",\"children\":\"이 [arXiv]에 게시한 'A Survey on Efficient Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:01:31+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\",\"children\":\"[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\",\"children\":\"Justin Cui이 [arXiv]에 게시한 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation\",\"children\":\"[논문리뷰] Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation\",\"children\":\"이 [arXiv]에 게시한 'Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\",\"children\":\"[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\",\"children\":\"이 [arXiv]에 게시한 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\",\"children\":\"[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\",\"children\":\"이 [arXiv]에 게시한 'Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\"}]]}]]}],[\"$\",\"article\",\"2025-10-27-A-Definition-of-AGI\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-A-Definition-of-AGI\",\"children\":\"[논문리뷰] A Definition of AGI\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-A-Definition-of-AGI\",\"children\":\"Yarin Gal이 [arXiv]에 게시한 'A Definition of AGI' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-27 13:07:36+0900\",\"children\":\"2025년 10월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-27-A-Definition-of-AGI\"}]]}]]}],[\"$\",\"article\",\"2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering\",\"children\":\"[논문리뷰] Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering\",\"children\":\"이 [arXiv]에 게시한 'Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-21 13:08:30+0900\",\"children\":\"2025년 10월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering\"}]]}]]}],[\"$\",\"article\",\"2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression\",\"children\":\"[논문리뷰] Glyph: Scaling Context Windows via Visual-Text Compression\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression\",\"children\":\"Wenyi Hong이 [arXiv]에 게시한 'Glyph: Scaling Context Windows via Visual-Text Compression' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-21 13:08:30+0900\",\"children\":\"2025년 10월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression\"}]]}]]}],[\"$\",\"article\",\"2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning\",\"children\":\"[논문리뷰] MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning\",\"children\":\"이 [arXiv]에 게시한 'MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-20 13:04:24+0900\",\"children\":\"2025년 10월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation\",\"children\":\"[논문리뷰] BLIP3o-NEXT: Next Frontier of Native Image Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation\",\"children\":\"이 [arXiv]에 게시한 'BLIP3o-NEXT: Next Frontier of Native Image Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-20 13:04:24+0900\",\"children\":\"2025년 10월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE\",\"children\":\"[논문리뷰] UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE\",\"children\":\"이 [arXiv]에 게시한 'UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\",\"children\":\"[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\",\"children\":\"이 [arXiv]에 게시한 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\",\"children\":\"[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\",\"children\":\"이 [arXiv]에 게시한 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner\",\"children\":\"[논문리뷰] Generative Universal Verifier as Multimodal Meta-Reasoner\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner\",\"children\":\"이 [arXiv]에 게시한 'Generative Universal Verifier as Multimodal Meta-Reasoner' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs\",\"children\":\"[논문리뷰] Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs\",\"children\":\"이 [arXiv]에 게시한 'Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos\",\"children\":\"[논문리뷰] UniVideo: Unified Understanding, Generation, and Editing for Videos\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos\",\"children\":\"Xintao Wang이 [arXiv]에 게시한 'UniVideo: Unified Understanding, Generation, and Editing for Videos' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\",\"children\":\"[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\",\"children\":\"Mohit Bansal이 [arXiv]에 게시한 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance\",\"children\":\"[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance\",\"children\":\"Xinghui Li이 [arXiv]에 게시한 'InstructX: Towards Unified Visual Editing with MLLM Guidance' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance\"}]]}]]}],[\"$\",\"article\",\"2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer\",\"children\":\"[논문리뷰] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer\",\"children\":\"이 [arXiv]에 게시한 'Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-09 13:45:06+0900\",\"children\":\"2025년 10월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\",\"children\":\"[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\",\"children\":\"이 [arXiv]에 게시한 'MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs\",\"children\":\"[논문리뷰] Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs\",\"children\":\"Zeyi Liao이 [arXiv]에 게시한 'Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs\"}]]}]]}],[\"$\",\"article\",\"2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\",\"children\":\"[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\",\"children\":\"Neil Zhenqiang Gong이 [arXiv]에 게시한 'WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-06 13:29:11+0900\",\"children\":\"2025년 10월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play\",\"children\":\"[논문리뷰] Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play\",\"children\":\"Jing Shi이 [arXiv]에 게시한 'Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics\",\"children\":\"[논문리뷰] TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics\",\"children\":\"Szu-Chi Chen이 [arXiv]에 게시한 'TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics\"}]]}]]}],[\"$\",\"article\",\"2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\",\"children\":\"[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\",\"children\":\"Yuran Wang이 [arXiv]에 게시한 'RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-30 13:52:24+0900\",\"children\":\"2025년 9월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing\",\"children\":\"[논문리뷰] OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing\",\"children\":\"Huanyu Zhang이 [arXiv]에 게시한 'OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-30 13:52:24+0900\",\"children\":\"2025년 9월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing\"}]]}]]}],[\"$\",\"article\",\"2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction\",\"children\":\"[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction\",\"children\":\"Guoxian Song이 [arXiv]에 게시한 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-29 13:47:46+0900\",\"children\":\"2025년 9월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction\"}]]}]]}],[\"$\",\"article\",\"2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation\",\"children\":\"[논문리뷰] Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation\",\"children\":\"Zhe Lin이 [arXiv]에 게시한 'Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-25 13:08:16+0900\",\"children\":\"2025년 9월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation\",\"children\":\"[논문리뷰] Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation\",\"children\":\"Jianbin Zheng이 [arXiv]에 게시한 'Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-24 13:14:19+0900\",\"children\":\"2025년 9월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions\",\"children\":\"[논문리뷰] FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions\",\"children\":\"tengdai722이 [arXiv]에 게시한 'FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions\"}]]}]]}],[\"$\",\"article\",\"2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents\",\"children\":\"[논문리뷰] Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents\",\"children\":\"Chao Zhang이 [arXiv]에 게시한 'Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-22 13:11:29+0900\",\"children\":\"2025년 9월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence\",\"children\":\"[논문리뷰] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence\",\"children\":\"Qinghua Huang이 [arXiv]에 게시한 'EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-19 13:12:21+0900\",\"children\":\"2025년 9월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-9-19-AToken-A-Unified-Tokenizer-for-Vision\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision\",\"children\":\"[논문리뷰] AToken: A Unified Tokenizer for Vision\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision\",\"children\":\"Mingze Xu이 [arXiv]에 게시한 'AToken: A Unified Tokenizer for Vision' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-19 13:12:21+0900\",\"children\":\"2025년 9월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision\"}]]}]]}],[\"$\",\"article\",\"2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits\",\"children\":\"[논문리뷰] PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits\",\"children\":\"Zhenhao Chen이 [arXiv]에 게시한 'PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-16 13:16:41+0900\",\"children\":\"2025년 9월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits\"}]]}]]}],[\"$\",\"article\",\"2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models\",\"children\":\"[논문리뷰] Lost in Embeddings: Information Loss in Vision-Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models\",\"children\":\"Ivan Vulić이 [arXiv]에 게시한 'Lost in Embeddings: Information Loss in Vision-Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-16 13:16:41+0900\",\"children\":\"2025년 9월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding\",\"children\":\"[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding\",\"children\":\"Li Zheng이 [arXiv]에 게시한 'Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-16 13:16:41+0900\",\"children\":\"2025년 9월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding\"}]]}]]}],[\"$\",\"article\",\"2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\",\"children\":\"[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\",\"children\":\"Ethan Chern이 [arXiv]에 게시한 'Visual Programmability: A Guide for Code-as-Thought in Chart Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-12 13:12:46+0900\",\"children\":\"2025년 9월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models\",\"children\":\"[논문리뷰] A Survey of Reinforcement Learning for Large Reasoning Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models\",\"children\":\"Runze Liu이 [arXiv]에 게시한 'A Survey of Reinforcement Learning for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-11 13:02:36+0900\",\"children\":\"2025년 9월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey\",\"children\":\"[논문리뷰] Reinforcement Learning Foundations for Deep Research Systems: A Survey\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey\",\"children\":\"Wei Han이 [arXiv]에 게시한 'Reinforcement Learning Foundations for Deep Research Systems: A Survey' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-09 13:19:09+0900\",\"children\":\"2025년 9월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey\"}]]}]]}],[\"$\",\"article\",\"2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery\",\"children\":\"[논문리뷰] Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery\",\"children\":\"Wenjie Zhou이 [arXiv]에 게시한 'Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-01 13:14:34+0900\",\"children\":\"2025년 9월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery\"}]]}]]}],[\"$\",\"article\",\"2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification\",\"children\":\"[논문리뷰] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing \u0026 Sparsification\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification\",\"children\":\"Liqiang Nie이 [arXiv]에 게시한 'CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing \u0026 Sparsification' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-29 13:14:44+0900\",\"children\":\"2025년 8월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification\"}]]}]]}],[\"$\",\"article\",\"2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\",\"children\":\"[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\",\"children\":\"Yixiao Ge이 [arXiv]에 게시한 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-28 13:10:39+0900\",\"children\":\"2025년 8월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\",\"children\":\"[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\",\"children\":\"Xin Zheng이 [arXiv]에 게시한 'Explain Before You Answer: A Survey on Compositional Visual Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-26 13:21:57+0900\",\"children\":\"2025년 8월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding\",\"children\":\"[논문리뷰] When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding\",\"children\":\"Rui Guo이 [arXiv]에 게시한 'When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-22 13:10:52+0900\",\"children\":\"2025년 8월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions\",\"children\":\"[논문리뷰] ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions\",\"children\":\"Daeyoung Kim이 [arXiv]에 게시한 'ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-21 13:15:28+0900\",\"children\":\"2025년 8월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions\"}]]}]]}],[\"$\",\"article\",\"2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\",\"children\":\"[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\",\"children\":\"Fernando López이 [arXiv]에 게시한 'MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-20 13:26:54+0900\",\"children\":\"2025년 8월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-8-15-A-Survey-on-Diffusion-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models\",\"children\":\"[논문리뷰] A Survey on Diffusion Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models\",\"children\":\"Zhiqiang Shen이 [arXiv]에 게시한 'A Survey on Diffusion Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-15 13:09:31+0900\",\"children\":\"2025년 8월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences\",\"children\":\"[논문리뷰] Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences\",\"children\":\"Matvey Skripkin이 [arXiv]에 게시한 'Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-12 13:29:09+0900\",\"children\":\"2025년 8월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences\"}]]}]]}],[\"$\",\"article\",\"2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh\",\"children\":\"[논문리뷰] MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh\",\"children\":\"Yi Yang이 [arXiv]에 게시한 'MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-11 13:13:28+0900\",\"children\":\"2025년 8월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh\"}]]}]]}],[\"$\",\"article\",\"2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation\",\"children\":\"[논문리뷰] Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation\",\"children\":\"Tianyidan Xie이 [arXiv]에 게시한 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-06 13:46:36+0900\",\"children\":\"2025년 8월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation\"}]]}]]}]]}]]}]]}]}]]}]],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lc\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lc\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"#Multimodal AI - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Multimodal AI 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/tags/Multimodal%20AI\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"#Multimodal AI - secrett2633's blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"Multimodal AI 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/tags/Multimodal%20AI\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"#Multimodal AI - secrett2633's blog\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"Multimodal AI 태그가 포함된 포스트 목록\"}],[\"$\",\"link\",\"20\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"21\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>