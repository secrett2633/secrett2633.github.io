2:I[9038,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
3:I[231,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],""]
4:I[227,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
5:I[9275,[],""]
7:I[1343,[],""]
8:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
9:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
6:["tag","Diffusion%20Models","d"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":["tags",{"children":[["tag","Diffusion%20Models","d"],{"children":["__PAGE__?{\"tag\":\"Diffusion Models\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["tags",{"children":[["tag","Diffusion%20Models","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"CollectionPage\",\"name\":\"#Diffusion Models - secrett2633's blog\",\"description\":\"Diffusion Models 태그가 포함된 포스트 목록\",\"url\":\"https://blog.secrett2633.cloud/tags/Diffusion%20Models\",\"isPartOf\":{\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\"},\"inLanguage\":\"ko\"}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"#Diffusion Models\",\"item\":\"https://blog.secrett2633.cloud/tags/Diffusion%20Models\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L3",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L3",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L3",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L3",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L3",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L3",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L3",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L3",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L3",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L3",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L3",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L3",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L3",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L3",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/tags/Diffusion%20Models",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"#Diffusion Models"}]]}]]]}]}],["$","h1",null,{"className":"page__title mb-6","children":["#","Diffusion Models"]}],["$","p",null,{"className":"text-gray-500 mb-6","children":[301,"개의 포스트"]}],["$","div",null,{"className":"entries-list","children":[["$","article","2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","children":"[논문리뷰] SLA2: Sparse-Linear Attention with Learnable Routing and QAT"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","children":"arXiv에 게시된 'SLA2: Sparse-Linear Attention with Learnable Routing and QAT' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT"}]]}]]}],["$","article","2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation","children":"[논문리뷰] Optimizing Few-Step Generation with Adaptive Matching Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation","children":"arXiv에 게시된 'Optimizing Few-Step Generation with Adaptive Matching Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation"}]]}]]}],["$","article","2026-02-13-dVoting-Fast-Voting-for-dLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-dVoting-Fast-Voting-for-dLLMs","children":"[논문리뷰] dVoting: Fast Voting for dLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-dVoting-Fast-Voting-for-dLLMs","children":"arXiv에 게시된 'dVoting: Fast Voting for dLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-dVoting-Fast-Voting-for-dLLMs"}]]}]]}],["$","article","2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching","children":"[논문리뷰] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching","children":"arXiv에 게시된 'Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching"}]]}]]}],["$","article","2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation","children":"[논문리뷰] Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation","children":"Yukuan Xu이 arXiv에 게시한 'Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation"}]]}]]}],["$","article","2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing","children":"[논문리뷰] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing","children":"arXiv에 게시된 'DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing"}]]}]]}],["$","article","2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss","children":"[논문리뷰] Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss","children":"arXiv에 게시된 'Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss"}]]}]]}],["$","article","2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","children":"[논문리뷰] WorldCompass: Reinforcement Learning for Long-Horizon World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","children":"arXiv에 게시된 'WorldCompass: Reinforcement Learning for Long-Horizon World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models"}]]}]]}],["$","article","2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","children":"[논문리뷰] Context Forcing: Consistent Autoregressive Video Generation with Long Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","children":"arXiv에 게시된 'Context Forcing: Consistent Autoregressive Video Generation with Long Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context"}]]}]]}],["$","article","2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models","children":"[논문리뷰] VLS: Steering Pretrained Robot Policies via Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models","children":"arXiv에 게시된 'VLS: Steering Pretrained Robot Policies via Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models"}]]}]]}],["$","article","2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers","children":"[논문리뷰] Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers","children":"arXiv에 게시된 'Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers"}]]}]]}],["$","article","2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis","children":"[논문리뷰] Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis","children":"arXiv에 게시된 'Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis"}]]}]]}],["$","article","2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation","children":"[논문리뷰] 3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation","children":"arXiv에 게시된 '3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation"}]]}]]}],["$","article","2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards","children":"[논문리뷰] PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards","children":"arXiv에 게시된 'PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards"}]]}]]}],["$","article","2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars","children":"[논문리뷰] Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars","children":"Teng Hu이 arXiv에 게시한 'Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars"}]]}]]}],["$","article","2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation","children":"[논문리뷰] Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation","children":"arXiv에 게시된 'Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation"}]]}]]}],["$","article","2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality","children":"[논문리뷰] Revisiting Diffusion Model Predictions Through Dimensionality"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality","children":"Chaoyang Wang이 arXiv에 게시한 'Revisiting Diffusion Model Predictions Through Dimensionality' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality"}]]}]]}],["$","article","2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","children":"[논문리뷰] DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","children":"arXiv에 게시된 'DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning"}]]}]]}],["$","article","2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation","children":"[논문리뷰] DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation","children":"Jong Chul Ye이 arXiv에 게시한 'DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation"}]]}]]}],["$","article","2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code","children":"[논문리뷰] iFSQ: Improving FSQ for Image Generation with 1 Line of Code"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code","children":"arXiv에 게시된 'iFSQ: Improving FSQ for Image Generation with 1 Line of Code' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code"}]]}]]}],["$","article","2026-01-27-SkyReels-V3-Technique-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report","children":"[논문리뷰] SkyReels-V3 Technique Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report","children":"arXiv에 게시된 'SkyReels-V3 Technique Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report"}]]}]]}],["$","article","2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior","children":"[논문리뷰] VideoMaMa: Mask-Guided Video Matting via Generative Prior"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior","children":"arXiv에 게시된 'VideoMaMa: Mask-Guided Video Matting via Generative Prior' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior"}]]}]]}],["$","article","2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders","children":"[논문리뷰] Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders","children":"arXiv에 게시된 'Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders"}]]}]]}],["$","article","2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning","children":"[논문리뷰] Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning","children":"arXiv에 게시된 'Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning"}]]}]]}],["$","article","2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","children":"[논문리뷰] OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","children":"arXiv에 게시된 'OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer"}]]}]]}],["$","article","2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","children":"[논문리뷰] CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","children":"Hengshuang이 arXiv에 게시한 'CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation"}]]}]]}],["$","article","2026-01-16-VIBE-Visual-Instruction-Based-Editor",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-VIBE-Visual-Instruction-Based-Editor","children":"[논문리뷰] VIBE: Visual Instruction Based Editor"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-VIBE-Visual-Instruction-Based-Editor","children":"Bulat Suleimanov이 arXiv에 게시한 'VIBE: Visual Instruction Based Editor' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-VIBE-Visual-Instruction-Based-Editor"}]]}]]}],["$","article","2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","children":"[논문리뷰] Transition Matching Distillation for Fast Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","children":"arXiv에 게시된 'Transition Matching Distillation for Fast Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation"}]]}]]}],["$","article","2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders","children":"[논문리뷰] Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders","children":"arXiv에 게시된 'Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders"}]]}]]}],["$","article","2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation","children":"[논문리뷰] CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation","children":"arXiv에 게시된 'CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation"}]]}]]}],["$","article","2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images","children":"[논문리뷰] Alterbute: Editing Intrinsic Attributes of Objects in Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images","children":"arXiv에 게시된 'Alterbute: Editing Intrinsic Attributes of Objects in Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images"}]]}]]}],["$","article","2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","children":"[논문리뷰] Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","children":"Ayush Tewari이 arXiv에 게시한 'Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering"}]]}]]}],["$","article","2026-01-14-Motion-Attribution-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation","children":"[논문리뷰] Motion Attribution for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation","children":"arXiv에 게시된 'Motion Attribution for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation"}]]}]]}],["$","article","2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance","children":"[논문리뷰] End-to-End Video Character Replacement without Structural Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance","children":"arXiv에 게시된 'End-to-End Video Character Replacement without Structural Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance"}]]}]]}],["$","article","2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","children":"[논문리뷰] Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","children":"Arjan Chakravarthy이 arXiv에 게시한 'Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals"}]]}]]}],["$","article","2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","children":"[논문리뷰] VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","children":"Ying Shan이 arXiv에 게시한 'VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control"}]]}]]}],["$","article","2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","children":"[논문리뷰] RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","children":"Mingda Jia이 arXiv에 게시한 'RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation"}]]}]]}],["$","article","2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","children":"[논문리뷰] Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","children":"Yu Xu이 arXiv에 게시한 'Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing"}]]}]]}],["$","article","2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study","children":"[논문리뷰] Memorization in 3D Shape Generation: An Empirical Study"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study","children":"arXiv에 게시된 'Memorization in 3D Shape Generation: An Empirical Study' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study"}]]}]]}],["$","article","2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","children":"[논문리뷰] DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","children":"Jing Ma이 arXiv에 게시한 'DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs"}]]}]]}],["$","article","2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization","children":"[논문리뷰] DreamStyle: A Unified Framework for Video Stylization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization","children":"arXiv에 게시된 'DreamStyle: A Unified Framework for Video Stylization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization"}]]}]]}],["$","article","2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes","children":"[논문리뷰] Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes","children":"Shuo Yang이 arXiv에 게시한 'Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes"}]]}]]}],["$","article","2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models","children":"[논문리뷰] M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models","children":"Jun-Cheng Chen이 arXiv에 게시한 'M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models"}]]}]]}],["$","article","2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking","children":"[논문리뷰] GARDO: Reinforcing Diffusion Models without Reward Hacking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking","children":"Zhiyong Wang이 arXiv에 게시한 'GARDO: Reinforcing Diffusion Models without Reward Hacking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking"}]]}]]}],["$","article","2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation","children":"[논문리뷰] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation","children":"arXiv에 게시된 'Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation"}]]}]]}],["$","article","2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation","children":"[논문리뷰] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation","children":"Sung Ju Hwang이 arXiv에 게시한 'Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation"}]]}]]}],["$","article","2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs","children":"[논문리뷰] On the Role of Discreteness in Diffusion LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs","children":"arXiv에 게시된 'On the Role of Discreteness in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-02 00:00:00+0900+0900","children":"2026년 1월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs"}]]}]]}],["$","article","2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models","children":"[논문리뷰] DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models","children":"Siyuan Huang이 arXiv에 게시한 'DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-02 00:00:00+0900+0900","children":"2026년 1월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models"}]]}]]}],["$","article","2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","children":"[논문리뷰] Pretraining Frame Preservation in Autoregressive Video Memory Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","children":"Beijia Lu이 arXiv에 게시한 'Pretraining Frame Preservation in Autoregressive Video Memory Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression"}]]}]]}],["$","article","2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself","children":"[논문리뷰] Guiding a Diffusion Transformer with the Internal Dynamics of Itself"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself","children":"arXiv에 게시된 'Guiding a Diffusion Transformer with the Internal Dynamics of Itself' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself"}]]}]]}],["$","article","2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction","children":"[논문리뷰] GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction","children":"Yu-Lun Liu이 arXiv에 게시한 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction"}]]}]]}],["$","article","2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement","children":"[논문리뷰] UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement","children":"Kaiyi Zhang이 arXiv에 게시한 'UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement"}]]}]]}],["$","article","2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation","children":"[논문리뷰] DreamOmni3: Scribble-based Editing and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation","children":"arXiv에 게시된 'DreamOmni3: Scribble-based Editing and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation"}]]}]]}],["$","article","2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion","children":"[논문리뷰] Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion","children":"Po-Fan Yu이 arXiv에 게시한 'Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion"}]]}]]}],["$","article","2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models","children":"[논문리뷰] GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models","children":"arXiv에 게시된 'GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models"}]]}]]}],["$","article","2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone","children":"[논문리뷰] Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone","children":"arXiv에 게시된 'Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone"}]]}]]}],["$","article","2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right","children":"[논문리뷰] ProEdit: Inversion-based Editing From Prompts Done Right"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right","children":"Kun-Yu Lin이 arXiv에 게시한 'ProEdit: Inversion-based Editing From Prompts Done Right' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right"}]]}]]}],["$","article","2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion","children":"[논문리뷰] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion","children":"arXiv에 게시된 'InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion"}]]}]]}],["$","article","2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","children":"[논문리뷰] Spatia: Video Generation with Updatable Spatial Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","children":"arXiv에 게시된 'Spatia: Video Generation with Updatable Spatial Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory"}]]}]]}],["$","article","2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode","children":"[논문리뷰] How Much 3D Do Video Foundation Models Encode?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode","children":"arXiv에 게시된 'How Much 3D Do Video Foundation Models Encode?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode"}]]}]]}],["$","article","2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","children":"[논문리뷰] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","children":"arXiv에 게시된 'TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times"}]]}]]}],["$","article","2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming","children":"[논문리뷰] HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming","children":"arXiv에 게시된 'HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming"}]]}]]}],["$","article","2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","children":"[논문리뷰] SemanticGen: Video Generation in Semantic Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","children":"arXiv에 게시된 'SemanticGen: Video Generation in Semantic Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space"}]]}]]}],["$","article","2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory","children":"[논문리뷰] StoryMem: Multi-shot Long Video Storytelling with Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory","children":"arXiv에 게시된 'StoryMem: Multi-shot Long Video Storytelling with Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory"}]]}]]}],["$","article","2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing","children":"[논문리뷰] Region-Constraint In-Context Generation for Instructional Video Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing","children":"arXiv에 게시된 'Region-Constraint In-Context Generation for Instructional Video Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing"}]]}]]}],["$","article","2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry","children":"[논문리뷰] MatSpray: Fusing 2D Material World Knowledge on 3D Geometry"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry","children":"arXiv에 게시된 'MatSpray: Fusing 2D Material World Knowledge on 3D Geometry' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry"}]]}]]}],["$","article","2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding","children":"[논문리뷰] LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding","children":"arXiv에 게시된 'LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding"}]]}]]}],["$","article","2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","children":"[논문리뷰] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","children":"arXiv에 게시된 'Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation"}]]}]]}],["$","article","2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras","children":"[논문리뷰] RadarGen: Automotive Radar Point Cloud Generation from Cameras"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras","children":"Or Litany이 arXiv에 게시한 'RadarGen: Automotive Radar Point Cloud Generation from Cameras' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras"}]]}]]}],["$","article","2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","children":"[논문리뷰] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","children":"arXiv에 게시된 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text"}]]}]]}],["$","article","2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","children":"[논문리뷰] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","children":"arXiv에 게시된 'StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors"}]]}]]}],["$","article","2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing","children":"[논문리뷰] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing","children":"Yuqi Liu이 arXiv에 게시한 'RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing"}]]}]]}],["$","article","2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image","children":"[논문리뷰] Generative Refocusing: Flexible Defocus Control from a Single Image"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image","children":"Yu-Lun Liu이 arXiv에 게시한 'Generative Refocusing: Flexible Defocus Control from a Single Image' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image"}]]}]]}],["$","article","2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering","children":"[논문리뷰] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering","children":"Hendrik P. A. Lensch이 arXiv에 게시한 'FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering"}]]}]]}],["$","article","2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","children":"[논문리뷰] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","children":"arXiv에 게시된 'FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction"}]]}]]}],["$","article","2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content","children":"[논문리뷰] Robust and Calibrated Detection of Authentic Multimedia Content"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content","children":"arXiv에 게시된 'Robust and Calibrated Detection of Authentic Multimedia Content' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content"}]]}]]}],["$","article","2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition","children":"[논문리뷰] Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition","children":"Xiao Xu이 arXiv에 게시한 'Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition"}]]}]]}],["$","article","2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models","children":"[논문리뷰] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models","children":"arXiv에 게시된 'DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models"}]]}]]}],["$","article","2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement","children":"[논문리뷰] ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement","children":"Zhaohe Liao이 arXiv에 게시한 'ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement"}]]}]]}],["$","article","2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans","children":"[논문리뷰] Towards Interactive Intelligence for Digital Humans"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans","children":"Yifei Huang이 arXiv에 게시한 'Towards Interactive Intelligence for Digital Humans' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans"}]]}]]}],["$","article","2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver","children":"[논문리뷰] Image Diffusion Preview with Consistency Solver"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver","children":"arXiv에 게시된 'Image Diffusion Preview with Consistency Solver' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver"}]]}]]}],["$","article","2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","children":"[논문리뷰] V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","children":"arXiv에 게시된 'V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties"}]]}]]}],["$","article","2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","children":"[논문리뷰] Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","children":"Qifeng Chen이 arXiv에 게시한 'Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation"}]]}]]}],["$","article","2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming","children":"[논문리뷰] PersonaLive! Expressive Portrait Image Animation for Live Streaming"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming","children":"Jue Wang이 arXiv에 게시한 'PersonaLive! Expressive Portrait Image Animation for Live Streaming' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming"}]]}]]}],["$","article","2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","children":"[논문리뷰] Exploring MLLM-Diffusion Information Transfer with MetaCanvas"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","children":"arXiv에 게시된 'Exploring MLLM-Diffusion Information Transfer with MetaCanvas' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas"}]]}]]}],["$","article","2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning","children":"[논문리뷰] ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning","children":"Yujin Han이 arXiv에 게시한 'ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning"}]]}]]}],["$","article","2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos","children":"[논문리뷰] H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos","children":"Mike Zheng Shou이 arXiv에 게시한 'H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos"}]]}]]}],["$","article","2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory","children":"[논문리뷰] VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory","children":"arXiv에 게시된 'VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory"}]]}]]}],["$","article","2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","children":"[논문리뷰] StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","children":"Guixun Luo이 arXiv에 게시한 'StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation"}]]}]]}],["$","article","2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding","children":"[논문리뷰] Composing Concepts from Images and Videos via Concept-prompt Binding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding","children":"arXiv에 게시된 'Composing Concepts from Images and Videos via Concept-prompt Binding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding"}]]}]]}],["$","article","2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","children":"[논문리뷰] Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","children":"arXiv에 게시된 'Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance"}]]}]]}],["$","article","2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models","children":"[논문리뷰] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models","children":"Weirui Ye이 arXiv에 게시한 'TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models"}]]}]]}],["$","article","2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality","children":"[논문리뷰] Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality","children":"arXiv에 게시된 'Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality"}]]}]]}],["$","article","2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory","children":"[논문리뷰] OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory","children":"arXiv에 게시된 'OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory"}]]}]]}],["$","article","2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","children":"[논문리뷰] MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","children":"arXiv에 게시된 'MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment"}]]}]]}],["$","article","2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","children":"[논문리뷰] UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","children":"arXiv에 게시된 'UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation"}]]}]]}],["$","article","2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","children":"[논문리뷰] Unified Video Editing with Temporal Reasoner"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","children":"arXiv에 게시된 'Unified Video Editing with Temporal Reasoner' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner"}]]}]]}],["$","article","2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation","children":"[논문리뷰] Scaling Zero-Shot Reference-to-Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation","children":"arXiv에 게시된 'Scaling Zero-Shot Reference-to-Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation"}]]}]]}],["$","article","2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","children":"[논문리뷰] ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","children":"Taojun Ding이 arXiv에 게시한 'ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation"}]]}]]}],["$","article","2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","children":"[논문리뷰] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","children":"arXiv에 게시된 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing"}]]}]]}],["$","article","2025-12-09-Distribution-Matching-Variational-AutoEncoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder","children":"[논문리뷰] Distribution Matching Variational AutoEncoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder","children":"arXiv에 게시된 'Distribution Matching Variational AutoEncoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder"}]]}]]}],["$","article","2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows","children":"[논문리뷰] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows","children":"arXiv에 게시된 'TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows"}]]}]]}],["$","article","2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards","children":"[논문리뷰] RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards","children":"Zilong Huang이 arXiv에 게시한 'RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards"}]]}]]}],["$","article","2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation","children":"[논문리뷰] NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation","children":"Vitor Guizilini이 arXiv에 게시한 'NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation"}]]}]]}],["$","article","2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length","children":"[논문리뷰] Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length","children":"Shifeng Zhang이 arXiv에 게시한 'Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length"}]]}]]}],["$","article","2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale","children":"[논문리뷰] LATTICE: Democratize High-Fidelity 3D Generation at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale","children":"Qingxiang Lin이 arXiv에 게시한 'LATTICE: Democratize High-Fidelity 3D Generation at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale"}]]}]]}],["$","article","2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior","children":"[논문리뷰] Generative Neural Video Compression via Video Diffusion Prior"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior","children":"arXiv에 게시된 'Generative Neural Video Compression via Video Diffusion Prior' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior"}]]}]]}],["$","article","2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","children":"[논문리뷰] BulletTime: Decoupled Control of Time and Camera Pose for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","children":"Jan Ackermann이 arXiv에 게시한 'BulletTime: Decoupled Control of Time and Camera Pose for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation"}]]}]]}],["$","article","2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","children":"[논문리뷰] RELIC: Interactive Video World Model with Long-Horizon Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","children":"Chongjian Ge이 arXiv에 게시한 'RELIC: Interactive Video World Model with Long-Horizon Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory"}]]}]]}],["$","article","2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation","children":"[논문리뷰] CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation","children":"Yi Yao이 arXiv에 게시한 'CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation"}]]}]]}],["$","article","2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation","children":"[논문리뷰] YingVideo-MV: Music-Driven Multi-Stage Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation","children":"Chaofan Ding이 arXiv에 게시한 'YingVideo-MV: Music-Driven Multi-Stage Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation"}]]}]]}],["$","article","2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","children":"[논문리뷰] Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","children":"Yu Ning이 arXiv에 게시한 'Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation"}]]}]]}],["$","article","2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework","children":"[논문리뷰] MultiShotMaster: A Controllable Multi-Shot Video Generation Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework","children":"arXiv에 게시된 'MultiShotMaster: A Controllable Multi-Shot Video Generation Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework"}]]}]]}],["$","article","2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample","children":"[논문리뷰] Glance: Accelerating Diffusion Models with 1 Sample"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample","children":"Linjie Li이 arXiv에 게시한 'Glance: Accelerating Diffusion Models with 1 Sample' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample"}]]}]]}],["$","article","2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","children":"[논문리뷰] DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","children":"Zixin Zhang이 arXiv에 게시한 'DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation"}]]}]]}],["$","article","2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","children":"[논문리뷰] Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","children":"arXiv에 게시된 'Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation"}]]}]]}],["$","article","2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models","children":"[논문리뷰] C^2DLM: Causal Concept-Guided Diffusion Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models","children":"Xinpeng Dong이 arXiv에 게시한 'C^2DLM: Causal Concept-Guided Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models"}]]}]]}],["$","article","2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing","children":"[논문리뷰] WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing","children":"Wendong Bu이 arXiv에 게시한 'WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing"}]]}]]}],["$","article","2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation","children":"[논문리뷰] Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation","children":"Wenhua Wu이 arXiv에 게시한 'Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation"}]]}]]}],["$","article","2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","children":"[논문리뷰] What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","children":"arXiv에 게시된 'What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards"}]]}]]}],["$","article","2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment","children":"[논문리뷰] The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment","children":"arXiv에 게시된 'The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment"}]]}]]}],["$","article","2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model","children":"[논문리뷰] Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model","children":"Ying-Cong Chen이 arXiv에 게시한 'Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model"}]]}]]}],["$","article","2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning","children":"[논문리뷰] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning","children":"arXiv에 게시된 'Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-01-Vision-Bridge-Transformer-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale","children":"[논문리뷰] Vision Bridge Transformer at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale","children":"Xinchao Wang이 arXiv에 게시한 'Vision Bridge Transformer at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale"}]]}]]}],["$","article","2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps","children":"[논문리뷰] Test-time scaling of diffusions with flow maps"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps","children":"Sanja Fidler이 arXiv에 게시한 'Test-time scaling of diffusions with flow maps' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps"}]]}]]}],["$","article","2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement","children":"[논문리뷰] OmniRefiner: Reinforcement-Guided Local Diffusion Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement","children":"Yiren Song이 arXiv에 게시한 'OmniRefiner: Reinforcement-Guided Local Diffusion Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement"}]]}]]}],["$","article","2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge","children":"[논문리뷰] Layer-Aware Video Composition via Split-then-Merge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge","children":"Wen-Sheng Chu이 arXiv에 게시한 'Layer-Aware Video Composition via Split-then-Merge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge"}]]}]]}],["$","article","2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration","children":"[논문리뷰] Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration","children":"arXiv에 게시된 'Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration"}]]}]]}],["$","article","2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space","children":"[논문리뷰] DiP: Taming Diffusion Models in Pixel Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space","children":"Xu Chen이 arXiv에 게시한 'DiP: Taming Diffusion Models in Pixel Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space"}]]}]]}],["$","article","2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield","children":"[논문리뷰] Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield","children":"arXiv에 게시된 'Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield"}]]}]]}],["$","article","2025-12-01-Captain-Safari-A-World-Engine",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine","children":"[논문리뷰] Captain Safari: A World Engine"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine","children":"Yitong Li이 arXiv에 게시한 'Captain Safari: A World Engine' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine"}]]}]]}],["$","article","2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement","children":"[논문리뷰] AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement","children":"Yicheng Ji이 arXiv에 게시한 'AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement"}]]}]]}],["$","article","2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","children":"[논문리뷰] Video Generation Models Are Good Latent Reward Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","children":"arXiv에 게시된 'Video Generation Models Are Good Latent Reward Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models"}]]}]]}],["$","article","2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing","children":"[논문리뷰] MIRA: Multimodal Iterative Reasoning Agent for Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing","children":"Jiebo Luo이 arXiv에 게시한 'MIRA: Multimodal Iterative Reasoning Agent for Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing"}]]}]]}],["$","article","2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls","children":"[논문리뷰] Canvas-to-Image: Compositional Image Generation with Multimodal Controls"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls","children":"Kfir Aberman이 arXiv에 게시한 'Canvas-to-Image: Compositional Image Generation with Multimodal Controls' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls"}]]}]]}],["$","article","2025-11-27-Terminal-Velocity-Matching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Terminal-Velocity-Matching","children":"[논문리뷰] Terminal Velocity Matching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Terminal-Velocity-Matching","children":"Jiaming Song이 arXiv에 게시한 'Terminal Velocity Matching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Terminal-Velocity-Matching"}]]}]]}],["$","article","2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs","children":"[논문리뷰] Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs","children":"Xin Yang이 arXiv에 게시한 'Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs"}]]}]]}],["$","article","2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy","children":"[논문리뷰] Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy","children":"arXiv에 게시된 'Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy"}]]}]]}],["$","article","2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","children":"[논문리뷰] Block Cascading: Training Free Acceleration of Block-Causal Video Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","children":"arXiv에 게시된 'Block Cascading: Training Free Acceleration of Block-Causal Video Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models"}]]}]]}],["$","article","2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation","children":"[논문리뷰] iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation","children":"arXiv에 게시된 'iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation"}]]}]]}],["$","article","2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","children":"[논문리뷰] PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","children":"Hongzhi Zhang이 arXiv에 게시한 'PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding"}]]}]]}],["$","article","2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts","children":"[논문리뷰] MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts","children":"arXiv에 게시된 'MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts"}]]}]]}],["$","article","2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection","children":"[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection","children":"Mike Zheng Shou이 arXiv에 게시한 'DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection"}]]}]]}],["$","article","2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios","children":"[논문리뷰] UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios","children":"arXiv에 게시된 'UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios"}]]}]]}],["$","article","2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis","children":"[논문리뷰] SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis","children":"Hongwen Zhang이 arXiv에 게시한 'SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis"}]]}]]}],["$","article","2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation","children":"[논문리뷰] Controllable Layer Decomposition for Reversible Multi-Layer Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation","children":"arXiv에 게시된 'Controllable Layer Decomposition for Reversible Multi-Layer Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation"}]]}]]}],["$","article","2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection","children":"[논문리뷰] Taming Generative Synthetic Data for X-ray Prohibited Item Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection","children":"Renshuai Tao이 arXiv에 게시한 'Taming Generative Synthetic Data for X-ray Prohibited Item Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection"}]]}]]}],["$","article","2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","children":"[논문리뷰] Planning with Sketch-Guided Verification for Physics-Aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","children":"Shayegan Omidshafiei이 arXiv에 게시한 'Planning with Sketch-Guided Verification for Physics-Aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation"}]]}]]}],["$","article","2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","children":"[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","children":"Vladimir Arkhipkin이 arXiv에 게시한 'Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation"}]]}]]}],["$","article","2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space","children":"[논문리뷰] A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space","children":"arXiv에 게시된 'A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space"}]]}]]}],["$","article","2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","children":"[논문리뷰] EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","children":"Zeyu Wang이 arXiv에 게시한 'EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation"}]]}]]}],["$","article","2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance","children":"[논문리뷰] Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance","children":"Kwanyoung Kim이 arXiv에 게시한 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance"}]]}]]}],["$","article","2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions","children":"[논문리뷰] Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions","children":"arXiv에 게시된 'Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions"}]]}]]}],["$","article","2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models","children":"[논문리뷰] Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models","children":"Zhao Xu이 arXiv에 게시한 'Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models"}]]}]]}],["$","article","2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects","children":"[논문리뷰] DIMO: Diverse 3D Motion Generation for Arbitrary Objects"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects","children":"Kostas Daniilidis이 arXiv에 게시한 'DIMO: Diverse 3D Motion Generation for Arbitrary Objects' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects"}]]}]]}],["$","article","2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference","children":"[논문리뷰] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference","children":"arXiv에 게시된 'EVTAR: End-to-End Try on with Additional Unpaired Visual Reference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference"}]]}]]}],["$","article","2025-11-5-iFlyBot-VLA-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report","children":"[논문리뷰] iFlyBot-VLA Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report","children":"Jiajia wu이 arXiv에 게시한 'iFlyBot-VLA Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report"}]]}]]}],["$","article","2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","children":"[논문리뷰] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","children":"arXiv에 게시된 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation"}]]}]]}],["$","article","2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer","children":"[논문리뷰] Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer","children":"arXiv에 게시된 'Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer"}]]}]]}],["$","article","2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process","children":"[논문리뷰] Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process","children":"arXiv에 게시된 'Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process"}]]}]]}],["$","article","2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback","children":"[논문리뷰] UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback","children":"arXiv에 게시된 'UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback"}]]}]]}],["$","article","2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls","children":"[논문리뷰] MotionStream: Real-Time Video Generation with Interactive Motion Controls"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls","children":"arXiv에 게시된 'MotionStream: Real-Time Video Generation with Interactive Motion Controls' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls"}]]}]]}],["$","article","2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model","children":"[논문리뷰] Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model","children":"Jinwoo Shin이 arXiv에 게시한 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model"}]]}]]}],["$","article","2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification","children":"[논문리뷰] Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification","children":"Olga Russakovsky이 arXiv에 게시한 'Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification"}]]}]]}],["$","article","2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","children":"[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","children":"arXiv에 게시된 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation"}]]}]]}],["$","article","2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution","children":"[논문리뷰] FullPart: Generating each 3D Part at Full Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution","children":"Chenjian Gao이 arXiv에 게시한 'FullPart: Generating each 3D Part at Full Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution"}]]}]]}],["$","article","2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control","children":"[논문리뷰] Exploring Conditions for Diffusion models in Robotic Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control","children":"arXiv에 게시된 'Exploring Conditions for Diffusion models in Robotic Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control"}]]}]]}],["$","article","2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","children":"[논문리뷰] VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","children":"Xiaoyu Shi이 arXiv에 게시한 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning"}]]}]]}],["$","article","2025-10-30-The-Principles-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models","children":"[논문리뷰] The Principles of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models","children":"Stefano Ermon이 arXiv에 게시한 'The Principles of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models"}]]}]]}],["$","article","2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks","children":"[논문리뷰] Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks","children":"arXiv에 게시된 'Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks"}]]}]]}],["$","article","2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing","children":"[논문리뷰] RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing","children":"Peng Ye이 arXiv에 게시한 'RegionE: Adaptive Region-Aware Generation for Efficient Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing"}]]}]]}],["$","article","2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design","children":"[논문리뷰] ODesign: A World Model for Biomolecular Interaction Design"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design","children":"Qinghan Wang이 arXiv에 게시한 'ODesign: A World Model for Biomolecular Interaction Design' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design"}]]}]]}],["$","article","2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset","children":"[논문리뷰] UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset","children":"arXiv에 게시된 'UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset"}]]}]]}],["$","article","2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization","children":"[논문리뷰] EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization","children":"Yaxing Wang이 arXiv에 게시한 'EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization"}]]}]]}],["$","article","2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers","children":"[논문리뷰] Visual Diffusion Models are Geometric Solvers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers","children":"Or Patashnik이 arXiv에 게시한 'Visual Diffusion Models are Geometric Solvers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers"}]]}]]}],["$","article","2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation","children":"[논문리뷰] Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation","children":"arXiv에 게시된 'Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation"}]]}]]}],["$","article","2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling","children":"[논문리뷰] RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling","children":"arXiv에 게시된 'RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling"}]]}]]}],["$","article","2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video","children":"[논문리뷰] Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video","children":"arXiv에 게시된 'Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video"}]]}]]}],["$","article","2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets","children":"[논문리뷰] Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets","children":"arXiv에 게시된 'Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets"}]]}]]}],["$","article","2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas","children":"[논문리뷰] LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas","children":"arXiv에 게시된 'LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas"}]]}]]}],["$","article","2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives","children":"[논문리뷰] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives","children":"arXiv에 게시된 'HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives"}]]}]]}],["$","article","2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion","children":"[논문리뷰] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion","children":"arXiv에 게시된 'DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion"}]]}]]}],["$","article","2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models","children":"[논문리뷰] DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models","children":"Roi Reichart이 arXiv에 게시한 'DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models"}]]}]]}],["$","article","2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling","children":"[논문리뷰] Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling","children":"Dim P. Papadopoulos이 arXiv에 게시한 'Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling"}]]}]]}],["$","article","2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback","children":"[논문리뷰] Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback","children":"arXiv에 게시된 'Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback"}]]}]]}],["$","article","2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing","children":"[논문리뷰] PICABench: How Far Are We from Physically Realistic Image Editing?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing","children":"Kaiwen Zhu이 arXiv에 게시한 'PICABench: How Far Are We from Physically Realistic Image Editing?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing"}]]}]]}],["$","article","2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery","children":"[논문리뷰] Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery","children":"Chung-Ho Wu이 arXiv에 게시한 'Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery"}]]}]]}],["$","article","2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset","children":"[논문리뷰] Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset","children":"Hao Ouyang이 arXiv에 게시한 'Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset"}]]}]]}],["$","article","2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal","children":"[논문리뷰] LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal","children":"arXiv에 게시된 'LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal"}]]}]]}],["$","article","2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation","children":"[논문리뷰] Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation","children":"Junsheng Yu이 arXiv에 게시한 'Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation"}]]}]]}],["$","article","2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation","children":"[논문리뷰] pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation","children":"arXiv에 게시된 'pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation"}]]}]]}],["$","article","2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","children":"[논문리뷰] RealDPO: Real or Not Real, that is the Preference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","children":"Chenyang Si이 arXiv에 게시한 'RealDPO: Real or Not Real, that is the Preference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference"}]]}]]}],["$","article","2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation","children":"[논문리뷰] Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation","children":"arXiv에 게시된 'Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation"}]]}]]}],["$","article","2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs","children":"[논문리뷰] Learning an Image Editing Model without Image Editing Pairs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs","children":"arXiv에 게시된 'Learning an Image Editing Model without Image Editing Pairs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs"}]]}]]}],["$","article","2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","children":"[논문리뷰] ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","children":"arXiv에 게시된 'ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints"}]]}]]}],["$","article","2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation","children":"[논문리뷰] DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation","children":"arXiv에 게시된 'DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation"}]]}]]}],["$","article","2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","children":"[논문리뷰] PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","children":"Hengshuang Zhao이 arXiv에 게시한 'PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy","children":"[논문리뷰] InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy","children":"Yilun Chen이 arXiv에 게시한 'InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy"}]]}]]}],["$","article","2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds","children":"[논문리뷰] FlashWorld: High-quality 3D Scene Generation within Seconds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds","children":"Chunchao Guo이 arXiv에 게시한 'FlashWorld: High-quality 3D Scene Generation within Seconds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds"}]]}]]}],["$","article","2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","children":"[논문리뷰] CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","children":"Jingcheng Ni이 arXiv에 게시한 'CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving"}]]}]]}],["$","article","2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation","children":"[논문리뷰] UniFusion: Vision-Language Model as Unified Encoder in Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation","children":"arXiv에 게시된 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation"}]]}]]}],["$","article","2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models","children":"[논문리뷰] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models","children":"arXiv에 게시된 'Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models"}]]}]]}],["$","article","2025-10-15-Robot-Learning-A-Tutorial",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial","children":"[논문리뷰] Robot Learning: A Tutorial"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial","children":"arXiv에 게시된 'Robot Learning: A Tutorial' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial"}]]}]]}],["$","article","2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution","children":"[논문리뷰] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution","children":"Yihao Liu이 arXiv에 게시한 'FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution"}]]}]]}],["$","article","2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training","children":"[논문리뷰] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training","children":"arXiv에 게시된 'Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training"}]]}]]}],["$","article","2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control","children":"[논문리뷰] TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control","children":"Adityan Jothi이 arXiv에 게시한 'TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control"}]]}]]}],["$","article","2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections","children":"[논문리뷰] UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections","children":"Boqian Li이 arXiv에 게시한 'UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections"}]]}]]}],["$","article","2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction","children":"[논문리뷰] Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction","children":"arXiv에 게시된 'Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction"}]]}]]}],["$","article","2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization","children":"[논문리뷰] Reinforcing Diffusion Models by Direct Group Preference Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization","children":"Jing Tang이 arXiv에 게시한 'Reinforcing Diffusion Models by Direct Group Preference Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization"}]]}]]}],["$","article","2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance","children":"[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance","children":"Xinghui Li이 arXiv에 게시한 'InstructX: Towards Unified Visual Editing with MLLM Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance"}]]}]]}],["$","article","2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization","children":"[논문리뷰] Fidelity-Aware Data Composition for Robust Robot Generalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization","children":"Liliang Chen이 arXiv에 게시한 'Fidelity-Aware Data Composition for Robust Robot Generalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization"}]]}]]}],["$","article","2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","children":"[논문리뷰] WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","children":"arXiv에 게시된 'WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation"}]]}]]}],["$","article","2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation","children":"[논문리뷰] StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation","children":"arXiv에 게시된 'StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation"}]]}]]}],["$","article","2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot","children":"[논문리뷰] OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot","children":"arXiv에 게시된 'OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot"}]]}]]}],["$","article","2025-10-9-Bridging-Text-and-Video-Generation-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey","children":"[논문리뷰] Bridging Text and Video Generation: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey","children":"G. Maragatham이 arXiv에 게시한 'Bridging Text and Video Generation: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey"}]]}]]}],["$","article","2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","children":"[논문리뷰] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","children":"Zheng Zhan이 arXiv에 게시한 'LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation"}]]}]]}],["$","article","2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models","children":"[논문리뷰] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models","children":"arXiv에 게시된 'Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models"}]]}]]}],["$","article","2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","children":"[논문리뷰] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","children":"Chengzu Li이 arXiv에 게시한 'Deforming Videos to Masks: Flow Matching for Referring Video Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation"}]]}]]}],["$","article","2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"[논문리뷰] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"Paul Debevec이 arXiv에 게시한 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation"}]]}]]}],["$","article","2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder","children":"[논문리뷰] SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder","children":"Or Patashnik이 arXiv에 게시한 'SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder"}]]}]]}],["$","article","2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","children":"[논문리뷰] Factuality Matters: When Image Generation and Editing Meet Structured Visuals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","children":"Boxiang Qiu이 arXiv에 게시한 'Factuality Matters: When Image Generation and Editing Meet Structured Visuals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals"}]]}]]}],["$","article","2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","children":"[논문리뷰] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","children":"arXiv에 게시된 'ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation"}]]}]]}],["$","article","2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs","children":"[논문리뷰] Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs","children":"arXiv에 게시된 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs"}]]}]]}],["$","article","2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition","children":"[논문리뷰] Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition","children":"arXiv에 게시된 'Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition"}]]}]]}],["$","article","2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents","children":"[논문리뷰] Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents","children":"Jong Chul Ye이 arXiv에 게시한 'Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents"}]]}]]}],["$","article","2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","children":"[논문리뷰] BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","children":"Xiangyang Xia이 arXiv에 게시한 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration"}]]}]]}],["$","article","2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching","children":"[논문리뷰] d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching","children":"Jiarui Wang이 arXiv에 게시한 'd^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching"}]]}]]}],["$","article","2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation","children":"[논문리뷰] MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation","children":"Limin Wang이 arXiv에 게시한 'MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation"}]]}]]}],["$","article","2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification","children":"[논문리뷰] MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification","children":"Zhiming Luo이 arXiv에 게시한 'MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification"}]]}]]}],["$","article","2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance","children":"[논문리뷰] IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance","children":"arXiv에 게시된 'IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance"}]]}]]}],["$","article","2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","children":"[논문리뷰] DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","children":"arXiv에 게시된 'DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder"}]]}]]}],["$","article","2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","children":"[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","children":"Guoxian Song이 arXiv에 게시한 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction"}]]}]]}],["$","article","2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction","children":"[논문리뷰] WoW: Towards a World omniscient World model Through Embodied Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction","children":"Weishi Mi이 arXiv에 게시한 'WoW: Towards a World omniscient World model Through Embodied Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction"}]]}]]}],["$","article","2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation","children":"[논문리뷰] Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation","children":"Peter Wonka이 arXiv에 게시한 'Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation"}]]}]]}],["$","article","2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation","children":"[논문리뷰] LongLive: Real-time Interactive Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation","children":"arXiv에 게시된 'LongLive: Real-time Interactive Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation"}]]}]]}],["$","article","2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models","children":"[논문리뷰] HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models","children":"Romann M. Weber이 arXiv에 게시한 'HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models"}]]}]]}],["$","article","2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing","children":"[논문리뷰] FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing","children":"Linghe Kong이 arXiv에 게시한 'FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing"}]]}]]}],["$","article","2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows","children":"[논문리뷰] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows","children":"Yi-Zhe Song이 arXiv에 게시한 'SD3.5-Flash: Distribution-Guided Distillation of Generative Flows' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows"}]]}]]}],["$","article","2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets","children":"[논문리뷰] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets","children":"Bowen Zhang이 arXiv에 게시한 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets"}]]}]]}],["$","article","2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition","children":"[논문리뷰] Does FLUX Already Know How to Perform Physically Plausible Image Composition?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition","children":"Chen Zhao이 arXiv에 게시한 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition"}]]}]]}],["$","article","2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","children":"[논문리뷰] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","children":"Yiming Huang이 arXiv에 게시한 'PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation"}]]}]]}],["$","article","2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching","children":"[논문리뷰] CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching","children":"Rui Qian이 arXiv에 게시한 'CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching"}]]}]]}],["$","article","2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","children":"[논문리뷰] OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","children":"Pengze Zhang이 arXiv에 게시한 'OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models"}]]}]]}],["$","article","2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process","children":"[논문리뷰] DiffusionNFT: Online Diffusion Reinforcement with Forward Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process","children":"Qinsheng Zhang이 arXiv에 게시한 'DiffusionNFT: Online Diffusion Reinforcement with Forward Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process"}]]}]]}],["$","article","2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation","children":"[논문리뷰] SPATIALGEN: Layout-guided 3D Indoor Scene Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation","children":"Yongsen Mao이 arXiv에 게시한 'SPATIALGEN: Layout-guided 3D Indoor Scene Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation"}]]}]]}],["$","article","2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue","children":"[논문리뷰] Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue","children":"Hui Zhang이 arXiv에 게시한 'Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue"}]]}]]}],["$","article","2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication","children":"[논문리뷰] Wan-Animate: Unified Character Animation and Replacement with Holistic Replication"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication","children":"Mingyang Huang이 arXiv에 게시한 'Wan-Animate: Unified Character Animation and Replacement with Holistic Replication' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication"}]]}]]}],["$","article","2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation","children":"[논문리뷰] Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation","children":"Lixin Xu이 arXiv에 게시한 'Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation"}]]}]]}],["$","article","2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics","children":"[논문리뷰] Locality in Image Diffusion Models Emerges from Data Statistics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics","children":"Vincent Sitzmann이 arXiv에 게시한 'Locality in Image Diffusion Models Emerges from Data Statistics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics"}]]}]]}],["$","article","2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence","children":"[논문리뷰] LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence","children":"Lionel M. Ni이 arXiv에 게시한 'LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence"}]]}]]}],["$","article","2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition","children":"[논문리뷰] X-Part: high fidelity and structure coherent shape decomposition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition","children":"Yunhan Yang이 arXiv에 게시한 'X-Part: high fidelity and structure coherent shape decomposition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition"}]]}]]}],["$","article","2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis","children":"[논문리뷰] InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis","children":"Song Guo이 arXiv에 게시한 'InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis"}]]}]]}],["$","article","2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies","children":"[논문리뷰] FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies","children":"Fabian Otto이 arXiv에 게시한 'FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies"}]]}]]}],["$","article","2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning","children":"[논문리뷰] HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning","children":"Zhuowei Chen이 arXiv에 게시한 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning"}]]}]]}],["$","article","2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward","children":"[논문리뷰] UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward","children":"Fei Ding이 arXiv에 게시한 'UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward"}]]}]]}],["$","article","2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling","children":"[논문리뷰] Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling","children":"Diana Marculescu이 arXiv에 게시한 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling"}]]}]]}],["$","article","2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference","children":"[논문리뷰] Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference","children":"Yingfang Zhang이 arXiv에 게시한 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference"}]]}]]}],["$","article","2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation","children":"[논문리뷰] Interleaving Reasoning for Better Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation","children":"Shixiang Tang이 arXiv에 게시한 'Interleaving Reasoning for Better Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation"}]]}]]}],["$","article","2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator","children":"[논문리뷰] Set Block Decoding is a Language Model Inference Accelerator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator","children":"Jeremy Reizenstein이 arXiv에 게시한 'Set Block Decoding is a Language Model Inference Accelerator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator"}]]}]]}],["$","article","2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer","children":"[논문리뷰] LuxDiT: Lighting Estimation with Video Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer","children":"Sanja Fidler이 arXiv에 게시한 'LuxDiT: Lighting Estimation with Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer"}]]}]]}],["$","article","2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective","children":"[논문리뷰] Transition Models: Rethinking the Generative Learning Objective"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective","children":"Yangguang Li이 arXiv에 게시한 'Transition Models: Rethinking the Generative Learning Objective' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective"}]]}]]}],["$","article","2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer","children":"[논문리뷰] Durian: Dual Reference-guided Portrait Animation with Attribute Transfer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer","children":"Hanbyul Joo이 arXiv에 게시한 'Durian: Dual Reference-guided Portrait Animation with Attribute Transfer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer"}]]}]]}],["$","article","2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement","children":"[논문리뷰] MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement","children":"Hualiang Wang이 arXiv에 게시한 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-04 12:56:15+0900","children":"2025년 9월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement"}]]}]]}],["$","article","2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer","children":"[논문리뷰] GenCompositor: Generative Video Compositing with Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer","children":"Lingen Li이 arXiv에 게시한 'GenCompositor: Generative Video Compositing with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer"}]]}]]}],["$","article","2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models","children":"[논문리뷰] FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models","children":"Zhen Wang이 arXiv에 게시한 'FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models"}]]}]]}],["$","article","2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning","children":"[논문리뷰] USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning","children":"Jiahe Tian이 arXiv에 게시한 'USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning"}]]}]]}],["$","article","2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation","children":"[논문리뷰] Collaborative Multi-Modal Coding for High-Quality 3D Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation","children":"Ziwei Liu이 arXiv에 게시한 'Collaborative Multi-Modal Coding for High-Quality 3D Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation"}]]}]]}],["$","article","2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation","children":"[논문리뷰] MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation","children":"Yan Zhou이 arXiv에 게시한 'MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation"}]]}]]}],["$","article","2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models","children":"[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models","children":"Yixiao Ge이 arXiv에 게시한 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models"}]]}]]}],["$","article","2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation","children":"[논문리뷰] Wan-S2V: Audio-Driven Cinematic Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation","children":"Chaonan Ji이 arXiv에 게시한 'Wan-S2V: Audio-Driven Cinematic Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation"}]]}]]}],["$","article","2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space","children":"[논문리뷰] VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space","children":"Rui Chen이 arXiv에 게시한 'VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space"}]]}]]}],["$","article","2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","children":"[논문리뷰] CineScale: Free Lunch in High-Resolution Cinematic Visual Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","children":"Ziwei Liu이 arXiv에 게시한 'CineScale: Free Lunch in High-Resolution Cinematic Visual Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation"}]]}]]}],["$","article","2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods","children":"[논문리뷰] SpotEdit: Evaluating Visually-Guided Image Editing Methods"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods","children":"Ersin Yumer이 arXiv에 게시한 'SpotEdit: Evaluating Visually-Guided Image Editing Methods' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods"}]]}]]}],["$","article","2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion","children":"[논문리뷰] MV-RAG: Retrieval Augmented Multiview Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion","children":"sagiebenaim이 arXiv에 게시한 'MV-RAG: Retrieval Augmented Multiview Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion"}]]}]]}],["$","article","2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass","children":"[논문리뷰] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass","children":"Ya Zhang이 arXiv에 게시한 'SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass"}]]}]]}],["$","article","2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization","children":"[논문리뷰] Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization","children":"Hao Chen이 arXiv에 게시한 'Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization"}]]}]]}],["$","article","2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models","children":"[논문리뷰] S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models","children":"Meiqi Wu이 arXiv에 게시한 'S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models"}]]}]]}],["$","article","2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts","children":"[논문리뷰] Precise Action-to-Video Generation Through Visual Action Prompts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts","children":"Minghan Qin이 arXiv에 게시한 'Precise Action-to-Video Generation Through Visual Action Prompts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts"}]]}]]}],["$","article","2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model","children":"[논문리뷰] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model","children":"Yifan Zhang이 arXiv에 게시한 'Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model"}]]}]]}],["$","article","2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models","children":"[논문리뷰] Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models","children":"Zixiang Gao이 arXiv에 게시한 'Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models"}]]}]]}],["$","article","2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy","children":"[논문리뷰] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy","children":"Zeng Tao이 arXiv에 게시한 '4DNeX: Feed-Forward 4D Generative Modeling Made Easy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy"}]]}]]}],["$","article","2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation","children":"[논문리뷰] FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation","children":"Mu Xu이 arXiv에 게시한 'FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation"}]]}]]}],["$","article","2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation","children":"[논문리뷰] Story2Board: A Training-Free Approach for Expressive Storyboard Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation","children":"Dani Lischinski이 arXiv에 게시한 'Story2Board: A Training-Free Approach for Expressive Storyboard Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation"}]]}]]}],["$","article","2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","children":"[논문리뷰] Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","children":"Chen Li이 arXiv에 게시한 'Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation"}]]}]]}],["$","article","2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models","children":"[논문리뷰] Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models","children":"Zeynep Akata이 arXiv에 게시한 'Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models"}]]}]]}],["$","article","2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation","children":"[논문리뷰] Matrix-3D: Omnidirectional Explorable 3D World Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation","children":"Yuqi Li이 arXiv에 게시한 'Matrix-3D: Omnidirectional Explorable 3D World Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation"}]]}]]}],["$","article","2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation","children":"[논문리뷰] CharacterShot: Controllable and Consistent 4D Character Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation","children":"Fei Shen이 arXiv에 게시한 'CharacterShot: Controllable and Consistent 4D Character Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation"}]]}]]}],["$","article","2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","children":"[논문리뷰] Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","children":"Xiaokun Feng이 arXiv에 게시한 'Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation"}]]}]]}],["$","article","2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control","children":"[논문리뷰] Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control","children":"Hongyu Liu이 arXiv에 게시한 'Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control"}]]}]]}],["$","article","2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion","children":"[논문리뷰] LightSwitch: Multi-view Relighting with Material-guided Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion","children":"Shubham Tulsiani이 arXiv에 게시한 'LightSwitch: Multi-view Relighting with Material-guided Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion"}]]}]]}],["$","article","2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance","children":"[논문리뷰] StrandDesigner: Towards Practical Strand Generation with Sketch Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance","children":"Xiaobin Hu이 arXiv에 게시한 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance"}]]}]]}],["$","article","2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression","children":"[논문리뷰] Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression","children":"Yifei Ji이 arXiv에 게시한 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression"}]]}]]}],["$","article","2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models","children":"[논문리뷰] The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models","children":"Elisabetta Rocchetti이 arXiv에 게시한 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models"}]]}]]}],["$","article","2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis","children":"[논문리뷰] Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis","children":"Feng Zhao이 arXiv에 게시한 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis"}]]}]]}],["$","article","2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference","children":"[논문리뷰] Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference","children":"Fan Xia이 arXiv에 게시한 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference"}]]}]]}],["$","article","2025-8-6-Multi-human-Interactive-Talking-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset","children":"[논문리뷰] Multi-human Interactive Talking Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset","children":"Mike Zheng Shou이 arXiv에 게시한 'Multi-human Interactive Talking Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset"}]]}]]}],["$","article","2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation","children":"[논문리뷰] LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation","children":"Chenyang Si이 arXiv에 게시한 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation"}]]}]]}],["$","article","2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer","children":"[논문리뷰] LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer","children":"Shunyu Yao이 arXiv에 게시한 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer"}]]}]]}],["$","article","2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation","children":"[논문리뷰] SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation","children":"Long Chen이 arXiv에 게시한 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation"}]]}]]}],["$","article","2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion","children":"[논문리뷰] PixNerd: Pixel Neural Field Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion","children":"Limin Wang이 arXiv에 게시한 'PixNerd: Pixel Neural Field Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion"}]]}]]}],["$","article","2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models","children":"[논문리뷰] villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models","children":"Kaixin Wang이 arXiv에 게시한 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models"}]]}]]}]]}]]}]]}]}]]}]],null],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children","$6","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L8",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L3",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L9",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L9",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$La"]]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"#Diffusion Models - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"Diffusion Models 태그가 포함된 포스트 목록"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/tags/Diffusion%20Models"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"#Diffusion Models - secrett2633's blog"}],["$","meta","14",{"property":"og:description","content":"Diffusion Models 태그가 포함된 포스트 목록"}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/tags/Diffusion%20Models"}],["$","meta","16",{"property":"og:type","content":"website"}],["$","meta","17",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","18",{"name":"twitter:title","content":"#Diffusion Models - secrett2633's blog"}],["$","meta","19",{"name":"twitter:description","content":"Diffusion Models 태그가 포함된 포스트 목록"}],["$","link","20",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
