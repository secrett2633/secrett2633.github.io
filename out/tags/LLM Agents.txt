2:I[9038,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
3:I[231,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],""]
4:I[227,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
5:I[9275,[],""]
7:I[1343,[],""]
8:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
9:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
6:["tag","LLM%20Agents","d"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":["tags",{"children":[["tag","LLM%20Agents","d"],{"children":["__PAGE__?{\"tag\":\"LLM Agents\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["tags",{"children":[["tag","LLM%20Agents","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"CollectionPage\",\"name\":\"#LLM Agents - secrett2633's blog\",\"description\":\"LLM Agents 태그가 포함된 포스트 목록\",\"url\":\"https://blog.secrett2633.cloud/tags/LLM%20Agents\",\"isPartOf\":{\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\"},\"inLanguage\":\"ko\"}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"#LLM Agents\",\"item\":\"https://blog.secrett2633.cloud/tags/LLM%20Agents\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L3",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L3",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L3",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L3",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L3",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L3",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L3",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L3",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L3",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L3",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L3",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L3",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L3",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L3",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/tags/LLM%20Agents",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"#LLM Agents"}]]}]]]}]}],["$","h1",null,{"className":"page__title mb-6","children":["#","LLM Agents"]}],["$","p",null,{"className":"text-gray-500 mb-6","children":[112,"개의 포스트"]}],["$","div",null,{"className":"entries-list","children":[["$","article","2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research","children":"[논문리뷰] ResearchGym: Evaluating Language Model Agents on Real-World AI Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research","children":"Arman Cohan이 arXiv에 게시한 'ResearchGym: Evaluating Language Model Agents on Real-World AI Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research"}]]}]]}],["$","article","2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents","children":"[논문리뷰] SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents","children":"Huayu Sha이 arXiv에 게시한 'SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents"}]]}]]}],["$","article","2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning","children":"[논문리뷰] SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning","children":"arXiv에 게시된 'SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training","children":"[논문리뷰] ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training","children":"arXiv에 게시된 'ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training"}]]}]]}],["$","article","2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory","children":"[논문리뷰] Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory","children":"arXiv에 게시된 'Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory"}]]}]]}],["$","article","2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents","children":"[논문리뷰] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents","children":"arXiv에 게시된 'AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents"}]]}]]}],["$","article","2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions","children":"[논문리뷰] OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions","children":"heroding77이 arXiv에 게시한 'OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions"}]]}]]}],["$","article","2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening","children":"[논문리뷰] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening","children":"arXiv에 게시된 'Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening"}]]}]]}],["$","article","2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty","children":"[논문리뷰] CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty","children":"arXiv에 게시된 'CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty"}]]}]]}],["$","article","2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents","children":"[논문리뷰] TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents","children":"Qiushi Sun이 arXiv에 게시한 'TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents"}]]}]]}],["$","article","2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning","children":"[논문리뷰] Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning","children":"arXiv에 게시된 'Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling","children":"[논문리뷰] WideSeek: Advancing Wide Research via Multi-Agent Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling","children":"Zhongtao Jiang이 arXiv에 게시한 'WideSeek: Advancing Wide Research via Multi-Agent Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling"}]]}]]}],["$","article","2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions","children":"[논문리뷰] SWE-Universe: Scale Real-World Verifiable Environments to Millions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions","children":"arXiv에 게시된 'SWE-Universe: Scale Real-World Verifiable Environments to Millions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions"}]]}]]}],["$","article","2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents","children":"[논문리뷰] FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents","children":"arXiv에 게시된 'FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents"}]]}]]}],["$","article","2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives","children":"[논문리뷰] Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives","children":"arXiv에 게시된 'Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives"}]]}]]}],["$","article","2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning","children":"[논문리뷰] Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning","children":"Shuai Zhang이 arXiv에 게시한 'Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning"}]]}]]}],["$","article","2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents","children":"[논문리뷰] Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents","children":"arXiv에 게시된 'Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents"}]]}]]}],["$","article","2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints","children":"[논문리뷰] DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints","children":"arXiv에 게시된 'DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints"}]]}]]}],["$","article","2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal","children":"[논문리뷰] DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal","children":"Jiaxuan You이 arXiv에 게시한 'DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal"}]]}]]}],["$","article","2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance","children":"[논문리뷰] Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance","children":"arXiv에 게시된 'Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance"}]]}]]}],["$","article","2026-01-22-Agentic-Reasoning-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Agentic-Reasoning-for-Large-Language-Models","children":"[논문리뷰] Agentic Reasoning for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Agentic-Reasoning-for-Large-Language-Models","children":"arXiv에 게시된 'Agentic Reasoning for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Agentic-Reasoning-for-Large-Language-Models"}]]}]]}],["$","article","2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization","children":"[논문리뷰] AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization","children":"arXiv에 게시된 'AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization"}]]}]]}],["$","article","2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning","children":"[논문리뷰] Toward Efficient Agents: Memory, Tool learning, and Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning","children":"arXiv에 게시된 'Toward Efficient Agents: Memory, Tool learning, and Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning"}]]}]]}],["$","article","2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development","children":"[논문리뷰] ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development","children":"arXiv에 게시된 'ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development"}]]}]]}],["$","article","2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems","children":"[논문리뷰] AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems","children":"Xipeng Qiu이 arXiv에 게시한 'AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-19 00:00:00+0900+0900","children":"2026년 1월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems"}]]}]]}],["$","article","2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering","children":"[논문리뷰] Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering","children":"arXiv에 게시된 'Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering"}]]}]]}],["$","article","2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback","children":"[논문리뷰] ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback","children":"Shikun Zhang이 arXiv에 게시한 'ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback"}]]}]]}],["$","article","2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models","children":"[논문리뷰] Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models","children":"Wenjie Li이 arXiv에 게시한 'Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models"}]]}]]}],["$","article","2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines","children":"[논문리뷰] EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines","children":"arXiv에 게시된 'EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines"}]]}]]}],["$","article","2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents","children":"[논문리뷰] The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents","children":"Junjue Wang이 arXiv에 게시한 'The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents"}]]}]]}],["$","article","2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning","children":"[논문리뷰] MemoBrain: Executive Memory as an Agentic Brain for Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning","children":"Zheng Liu이 arXiv에 게시한 'MemoBrain: Executive Memory as an Agentic Brain for Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning"}]]}]]}],["$","article","2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","children":"[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","children":"arXiv에 게시된 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking"}]]}]]}],["$","article","2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","children":"[논문리뷰] TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","children":"Hao Wang이 arXiv에 게시한 'TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning"}]]}]]}],["$","article","2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning","children":"[논문리뷰] OpenTinker: Separating Concerns in Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning","children":"Jiaxuan You이 arXiv에 게시한 'OpenTinker: Separating Concerns in Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning","children":"[논문리뷰] Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning","children":"Zhicheng Dou이 arXiv에 게시한 'Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning"}]]}]]}],["$","article","2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering","children":"[논문리뷰] AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering","children":"Di Zhang이 arXiv에 게시한 'AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering"}]]}]]}],["$","article","2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search","children":"[논문리뷰] AT^2PO: Agentic Turn-based Policy Optimization via Tree Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search","children":"arXiv에 게시된 'AT^2PO: Agentic Turn-based Policy Optimization via Tree Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search"}]]}]]}],["$","article","2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts","children":"[논문리뷰] Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts","children":"arXiv에 게시된 'Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts"}]]}]]}],["$","article","2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents","children":"[논문리뷰] Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents","children":"arXiv에 게시된 'Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents"}]]}]]}],["$","article","2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization","children":"[논문리뷰] Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization","children":"arXiv에 게시된 'Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization"}]]}]]}],["$","article","2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking","children":"[논문리뷰] Nested Browser-Use Learning for Agentic Information Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking","children":"arXiv에 게시된 'Nested Browser-Use Learning for Agentic Information Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking"}]]}]]}],["$","article","2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library","children":"[논문리뷰] Reinforcement Learning for Self-Improving Agent with Skill Library"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library","children":"Soumya Smruti Mishra이 arXiv에 게시한 'Reinforcement Learning for Self-Improving Agent with Skill Library' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library"}]]}]]}],["$","article","2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems","children":"[논문리뷰] MemEvolve: Meta-Evolution of Agent Memory Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems","children":"Junhao Wang이 arXiv에 게시한 'MemEvolve: Meta-Evolution of Agent Memory Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems"}]]}]]}],["$","article","2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators","children":"[논문리뷰] GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators","children":"arXiv에 게시된 'GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators"}]]}]]}],["$","article","2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs","children":"[논문리뷰] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs","children":"Lihong Li이 arXiv에 게시한 'Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs"}]]}]]}],["$","article","2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents","children":"[논문리뷰] Meta-RL Induces Exploration in Language Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents","children":"Maria Brbic이 arXiv에 게시한 'Meta-RL Induces Exploration in Language Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents"}]]}]]}],["$","article","2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness","children":"[논문리뷰] SCOPE: Prompt Evolution for Enhancing Agent Effectiveness"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness","children":"Yunhe Wang이 arXiv에 게시한 'SCOPE: Prompt Evolution for Enhancing Agent Effectiveness' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness"}]]}]]}],["$","article","2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents","children":"[논문리뷰] Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents","children":"Xiaodong Gu이 arXiv에 게시한 'Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents"}]]}]]}],["$","article","2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning","children":"[논문리뷰] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning","children":"arXiv에 게시된 'Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication","children":"[논문리뷰] Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication","children":"Hengshu Zhu이 arXiv에 게시한 'Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication"}]]}]]}],["$","article","2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce","children":"[논문리뷰] EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce","children":"arXiv에 게시된 'EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce"}]]}]]}],["$","article","2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing","children":"[논문리뷰] PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing","children":"arXiv에 게시된 'PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing"}]]}]]}],["$","article","2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research","children":"[논문리뷰] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research","children":"arXiv에 게시된 'DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research"}]]}]]}],["$","article","2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling","children":"[논문리뷰] Budget-Aware Tool-Use Enables Effective Agent Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling","children":"arXiv에 게시된 'Budget-Aware Tool-Use Enables Effective Agent Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling"}]]}]]}],["$","article","2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents","children":"[논문리뷰] O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents","children":"arXiv에 게시된 'O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents"}]]}]]}],["$","article","2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning","children":"[논문리뷰] Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning","children":"Yucong Luo이 arXiv에 게시한 'Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning"}]]}]]}],["$","article","2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering","children":"[논문리뷰] LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering","children":"arXiv에 게시된 'LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering"}]]}]]}],["$","article","2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly","children":"[논문리뷰] Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly","children":"Lingming Zhang이 arXiv에 게시한 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly"}]]}]]}],["$","article","2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction","children":"[논문리뷰] IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction","children":"Haotian Xu이 arXiv에 게시한 'IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction"}]]}]]}],["$","article","2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience","children":"[논문리뷰] FLEX: Continuous Agent Evolution via Forward Learning from Experience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience","children":"Jiangjie Chen이 arXiv에 게시한 'FLEX: Continuous Agent Evolution via Forward Learning from Experience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience"}]]}]]}],["$","article","2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments","children":"[논문리뷰] Real-Time Reasoning Agents in Evolving Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments","children":"arXiv에 게시된 'Real-Time Reasoning Agents in Evolving Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments"}]]}]]}],["$","article","2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis","children":"[논문리뷰] Scaling Agent Learning via Experience Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis","children":"arXiv에 게시된 'Scaling Agent Learning via Experience Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis"}]]}]]}],["$","article","2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents","children":"[논문리뷰] CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents","children":"Shijue Huang이 arXiv에 게시한 'CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents"}]]}]]}],["$","article","2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model","children":"[논문리뷰] MASPRM: Multi-Agent System Process Reward Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model","children":"Ying Xiong이 arXiv에 게시한 'MASPRM: Multi-Agent System Process Reward Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model"}]]}]]}],["$","article","2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking","children":"[논문리뷰] ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking","children":"arXiv에 게시된 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking"}]]}]]}],["$","article","2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis","children":"[논문리뷰] AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis","children":"arXiv에 게시된 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis"}]]}]]}],["$","article","2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control","children":"[논문리뷰] ReCode: Unify Plan and Action for Universal Granularity Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control","children":"Yifan Wu이 arXiv에 게시한 'ReCode: Unify Plan and Action for Universal Granularity Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control"}]]}]]}],["$","article","2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision","children":"[논문리뷰] Search Self-play: Pushing the Frontier of Agent Capability without Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision","children":"arXiv에 게시된 'Search Self-play: Pushing the Frontier of Agent Capability without Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision"}]]}]]}],["$","article","2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading","children":"[논문리뷰] AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading","children":"Jiashu Wang이 arXiv에 게시한 'AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading"}]]}]]}],["$","article","2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics","children":"[논문리뷰] Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics","children":"arXiv에 게시된 'Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics"}]]}]]}],["$","article","2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents","children":"[논문리뷰] Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents","children":"arXiv에 게시된 'Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents"}]]}]]}],["$","article","2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search","children":"[논문리뷰] GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search","children":"Zijian Zhang이 arXiv에 게시한 'GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search"}]]}]]}],["$","article","2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review","children":"[논문리뷰] ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review","children":"Christopher Pal이 arXiv에 게시한 'ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review"}]]}]]}],["$","article","2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks","children":"[논문리뷰] A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks","children":"Fanchao Qi이 arXiv에 게시한 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks"}]]}]]}],["$","article","2025-10-10-Training-Free-Group-Relative-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization","children":"[논문리뷰] Training-Free Group Relative Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization","children":"arXiv에 게시된 'Training-Free Group Relative Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization"}]]}]]}],["$","article","2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents","children":"[논문리뷰] NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents","children":"Baixuan Xu이 arXiv에 게시한 'NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents"}]]}]]}],["$","article","2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks","children":"[논문리뷰] Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks","children":"arXiv에 게시된 'Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks"}]]}]]}],["$","article","2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards","children":"[논문리뷰] CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards","children":"Yijiang Li이 arXiv에 게시한 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards"}]]}]]}],["$","article","2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline","children":"[논문리뷰] MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline","children":"arXiv에 게시된 'MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline"}]]}]]}],["$","article","2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation","children":"[논문리뷰] VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation","children":"arXiv에 게시된 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation"}]]}]]}],["$","article","2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions","children":"[논문리뷰] BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions","children":"Shipei Lin이 arXiv에 게시한 'BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions"}]]}]]}],["$","article","2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails","children":"[논문리뷰] Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails","children":"Xinyuan Liu이 arXiv에 게시한 'Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails"}]]}]]}],["$","article","2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys","children":"[논문리뷰] SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys","children":"Shuo Wang이 arXiv에 게시한 'SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys"}]]}]]}],["$","article","2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning","children":"[논문리뷰] A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning","children":"arXiv에 게시된 'A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution","children":"[논문리뷰] Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution","children":"arXiv에 게시된 'Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution"}]]}]]}],["$","article","2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents","children":"[논문리뷰] ACON: Optimizing Context Compression for Long-horizon LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents","children":"arXiv에 게시된 'ACON: Optimizing Context Compression for Long-horizon LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents"}]]}]]}],["$","article","2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications","children":"[논문리뷰] VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications","children":"arXiv에 게시된 'VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications"}]]}]]}],["$","article","2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning","children":"[논문리뷰] Mem-α: Learning Memory Construction via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning","children":"Yuzhen Mao이 arXiv에 게시한 'Mem-α: Learning Memory Construction via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use","children":"[논문리뷰] MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use","children":"arXiv에 게시된 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use"}]]}]]}],["$","article","2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents","children":"[논문리뷰] InfoAgent: Advancing Autonomous Information-Seeking Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents","children":"arXiv에 게시된 'InfoAgent: Advancing Autonomous Information-Seeking Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents"}]]}]]}],["$","article","2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software","children":"[논문리뷰] BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software","children":"arXiv에 게시된 'BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software"}]]}]]}],["$","article","2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios","children":"[논문리뷰] UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios","children":"Zeyu Qin이 arXiv에 게시한 'UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios"}]]}]]}],["$","article","2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning","children":"[논문리뷰] Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning","children":"Gang Li이 arXiv에 게시한 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning","children":"[논문리뷰] EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning","children":"Li Yu-Jhe이 arXiv에 게시한 'EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning","children":"[논문리뷰] Tree Search for LLM Agent Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning","children":"Xiangxiang Chu이 arXiv에 게시한 'Tree Search for LLM Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations","children":"[논문리뷰] ARE: Scaling Up Agent Environments and Evaluations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations","children":"Matteo Bettini이 arXiv에 게시한 'ARE: Scaling Up Agent Environments and Evaluations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations"}]]}]]}],["$","article","2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research","children":"[논문리뷰] WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research","children":"Houquan Zhou이 arXiv에 게시한 'WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research"}]]}]]}],["$","article","2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization","children":"[논문리뷰] ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization","children":"Litu Ou이 arXiv에 게시한 'ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization"}]]}]]}],["$","article","2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents","children":"[논문리뷰] Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents","children":"Xintao Wang이 arXiv에 게시한 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents"}]]}]]}],["$","article","2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","children":"[논문리뷰] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","children":"Honglin Guo이 arXiv에 게시한 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey","children":"[논문리뷰] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey","children":"Hejia Geng이 arXiv에 게시한 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey"}]]}]]}],["$","article","2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench","children":"[논문리뷰] How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench","children":"Jayanth Srinivasa이 arXiv에 게시한 'How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-02 13:01:41+0900","children":"2025년 9월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench"}]]}]]}],["$","article","2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers","children":"[논문리뷰] MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers","children":"Shashank Biju이 arXiv에 게시한 'MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers"}]]}]]}],["$","article","2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo","children":"[논문리뷰] Training Language Model Agents to Find Vulnerabilities with CTF-Dojo"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo","children":"Zijian Wang이 arXiv에 게시한 'Training Language Model Agents to Find Vulnerabilities with CTF-Dojo' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo"}]]}]]}],["$","article","2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications","children":"[논문리뷰] AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications","children":"Liuyi Yao이 arXiv에 게시한 'AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications"}]]}]]}],["$","article","2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction","children":"[논문리뷰] FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction","children":"tianlecai이 arXiv에 게시한 'FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction"}]]}]]}],["$","article","2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents","children":"[논문리뷰] ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents","children":"Flora D. Salim이 arXiv에 게시한 'ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents"}]]}]]}],["$","article","2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL","children":"[논문리뷰] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL","children":"Chuyi He이 arXiv에 게시한 'Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL"}]]}]]}],["$","article","2025-8-11-Memp-Exploring-Agent-Procedural-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory","children":"[논문리뷰] Memp: Exploring Agent Procedural Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory","children":"Shuofei Qiao이 arXiv에 게시한 'Memp: Exploring Agent Procedural Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory"}]]}]]}],["$","article","2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost","children":"[논문리뷰] Efficient Agents: Building Effective Agents While Reducing Cost"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost","children":"Yue Hou이 arXiv에 게시한 'Efficient Agents: Building Effective Agents While Reducing Cost' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost"}]]}]]}],["$","article","2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks","children":"[논문리뷰] AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks","children":"Zhiwei Zhang이 arXiv에 게시한 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks"}]]}]]}],["$","article","2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution","children":"[논문리뷰] SWE-Exp: Experience-Driven Software Issue Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution","children":"Heng Lian이 arXiv에 게시한 'SWE-Exp: Experience-Driven Software Issue Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution"}]]}]]}]]}]]}]]}]}]]}]],null],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children","$6","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L8",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L3",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L9",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L9",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$La"]]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"#LLM Agents - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"LLM Agents 태그가 포함된 포스트 목록"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/tags/LLM%20Agents"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"#LLM Agents - secrett2633's blog"}],["$","meta","14",{"property":"og:description","content":"LLM Agents 태그가 포함된 포스트 목록"}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/tags/LLM%20Agents"}],["$","meta","16",{"property":"og:type","content":"website"}],["$","meta","17",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","18",{"name":"twitter:title","content":"#LLM Agents - secrett2633's blog"}],["$","meta","19",{"name":"twitter:description","content":"LLM Agents 태그가 포함된 포스트 목록"}],["$","link","20",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
