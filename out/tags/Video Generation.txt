2:I[9038,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
3:I[231,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],""]
4:I[227,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
5:I[9275,[],""]
7:I[1343,[],""]
8:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
9:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
6:["tag","Video%20Generation","d"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":["tags",{"children":[["tag","Video%20Generation","d"],{"children":["__PAGE__?{\"tag\":\"Video Generation\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["tags",{"children":[["tag","Video%20Generation","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"CollectionPage\",\"name\":\"#Video Generation - secrett2633's blog\",\"description\":\"Video Generation 태그가 포함된 포스트 목록\",\"url\":\"https://blog.secrett2633.cloud/tags/Video%20Generation\",\"isPartOf\":{\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\"},\"inLanguage\":\"ko\"}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"#Video Generation\",\"item\":\"https://blog.secrett2633.cloud/tags/Video%20Generation\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L3",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L3",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L3",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L3",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L3",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L3",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L3",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L3",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L3",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L3",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L3",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L3",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L3",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L3",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/tags/Video%20Generation",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"#Video Generation"}]]}]]]}]}],["$","h1",null,{"className":"page__title mb-6","children":["#","Video Generation"]}],["$","p",null,{"className":"text-gray-500 mb-6","children":[122,"개의 포스트"]}],["$","div",null,{"className":"entries-list","children":[["$","article","2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","children":"[논문리뷰] SLA2: Sparse-Linear Attention with Learnable Routing and QAT"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","children":"arXiv에 게시된 'SLA2: Sparse-Linear Attention with Learnable Routing and QAT' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT"}]]}]]}],["$","article","2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","children":"[논문리뷰] WorldCompass: Reinforcement Learning for Long-Horizon World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","children":"arXiv에 게시된 'WorldCompass: Reinforcement Learning for Long-Horizon World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models"}]]}]]}],["$","article","2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning","children":"[논문리뷰] Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning","children":"arXiv에 게시된 'Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning"}]]}]]}],["$","article","2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules","children":"[논문리뷰] RISE-Video: Can Video Generators Decode Implicit World Rules?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules","children":"Zicheng Zhang이 arXiv에 게시한 'RISE-Video: Can Video Generators Decode Implicit World Rules?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules"}]]}]]}],["$","article","2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","children":"[논문리뷰] Context Forcing: Consistent Autoregressive Video Generation with Long Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","children":"arXiv에 게시된 'Context Forcing: Consistent Autoregressive Video Generation with Long Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context"}]]}]]}],["$","article","2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","children":"[논문리뷰] DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","children":"arXiv에 게시된 'DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning"}]]}]]}],["$","article","2026-01-29-Advancing-Open-source-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Advancing-Open-source-World-Models","children":"[논문리뷰] Advancing Open-source World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Advancing-Open-source-World-Models","children":"arXiv에 게시된 'Advancing Open-source World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Advancing-Open-source-World-Models"}]]}]]}],["$","article","2026-01-27-SkyReels-V3-Technique-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report","children":"[논문리뷰] SkyReels-V3 Technique Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report","children":"arXiv에 게시된 'SkyReels-V3 Technique Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report"}]]}]]}],["$","article","2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer","children":"[논문리뷰] SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer","children":"arXiv에 게시된 'SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer"}]]}]]}],["$","article","2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World","children":"[논문리뷰] Rethinking Video Generation Model for the Embodied World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World","children":"arXiv에 게시된 'Rethinking Video Generation Model for the Embodied World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World"}]]}]]}],["$","article","2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","children":"[논문리뷰] OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","children":"arXiv에 게시된 'OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer"}]]}]]}],["$","article","2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","children":"[논문리뷰] CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","children":"Hengshuang이 arXiv에 게시한 'CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation"}]]}]]}],["$","article","2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","children":"[논문리뷰] Transition Matching Distillation for Fast Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","children":"arXiv에 게시된 'Transition Matching Distillation for Fast Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation"}]]}]]}],["$","article","2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","children":"[논문리뷰] Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","children":"Ayush Tewari이 arXiv에 게시한 'Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering"}]]}]]}],["$","article","2026-01-14-Motion-Attribution-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation","children":"[논문리뷰] Motion Attribution for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation","children":"arXiv에 게시된 'Motion Attribution for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation"}]]}]]}],["$","article","2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head","children":"[논문리뷰] MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head","children":"arXiv에 게시된 'MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head"}]]}]]}],["$","article","2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving","children":"[논문리뷰] DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving","children":"arXiv에 게시된 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving"}]]}]]}],["$","article","2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction","children":"[논문리뷰] VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction","children":"Yu Sun이 arXiv에 게시한 'VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction"}]]}]]}],["$","article","2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","children":"[논문리뷰] Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","children":"Arjan Chakravarthy이 arXiv에 게시한 'Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals"}]]}]]}],["$","article","2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","children":"[논문리뷰] VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","children":"Ying Shan이 arXiv에 게시한 'VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control"}]]}]]}],["$","article","2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","children":"[논문리뷰] RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","children":"Mingda Jia이 arXiv에 게시한 'RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation"}]]}]]}],["$","article","2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos","children":"[논문리뷰] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos","children":"Feng Wang이 arXiv에 게시한 'NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos"}]]}]]}],["$","article","2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","children":"[논문리뷰] Pretraining Frame Preservation in Autoregressive Video Memory Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","children":"Beijia Lu이 arXiv에 게시한 'Pretraining Frame Preservation in Autoregressive Video Memory Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression"}]]}]]}],["$","article","2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation","children":"[논문리뷰] JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation","children":"arXiv에 게시된 'JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation"}]]}]]}],["$","article","2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling","children":"[논문리뷰] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling","children":"arXiv에 게시된 'SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling"}]]}]]}],["$","article","2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning","children":"[논문리뷰] SVBench: Evaluation of Video Generation Models on Social Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning","children":"Xiaojie Xu이 arXiv에 게시한 'SVBench: Evaluation of Video Generation Models on Social Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning"}]]}]]}],["$","article","2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","children":"[논문리뷰] Spatia: Video Generation with Updatable Spatial Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","children":"arXiv에 게시된 'Spatia: Video Generation with Updatable Spatial Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory"}]]}]]}],["$","article","2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","children":"[논문리뷰] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","children":"arXiv에 게시된 'TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times"}]]}]]}],["$","article","2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation","children":"[논문리뷰] DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation","children":"arXiv에 게시된 'DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation"}]]}]]}],["$","article","2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","children":"[논문리뷰] SemanticGen: Video Generation in Semantic Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","children":"arXiv에 게시된 'SemanticGen: Video Generation in Semantic Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space"}]]}]]}],["$","article","2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface","children":"[논문리뷰] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface","children":"Liliang Chen이 arXiv에 게시한 'Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface"}]]}]]}],["$","article","2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","children":"[논문리뷰] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","children":"arXiv에 게시된 'Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation"}]]}]]}],["$","article","2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","children":"[논문리뷰] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","children":"arXiv에 게시된 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text"}]]}]]}],["$","article","2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","children":"[논문리뷰] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","children":"arXiv에 게시된 'StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors"}]]}]]}],["$","article","2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model","children":"[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model","children":"arXiv에 게시된 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model"}]]}]]}],["$","article","2025-12-19-Kling-Omni-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Kling-Omni-Technical-Report","children":"[논문리뷰] Kling-Omni Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Kling-Omni-Technical-Report","children":"arXiv에 게시된 'Kling-Omni Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Kling-Omni-Technical-Report"}]]}]]}],["$","article","2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","children":"[논문리뷰] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","children":"arXiv에 게시된 'FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction"}]]}]]}],["$","article","2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","children":"[논문리뷰] V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","children":"arXiv에 게시된 'V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties"}]]}]]}],["$","article","2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","children":"[논문리뷰] Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","children":"Qifeng Chen이 arXiv에 게시한 'Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation"}]]}]]}],["$","article","2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","children":"[논문리뷰] Exploring MLLM-Diffusion Information Transfer with MetaCanvas"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","children":"arXiv에 게시된 'Exploring MLLM-Diffusion Information Transfer with MetaCanvas' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas"}]]}]]}],["$","article","2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator","children":"[논문리뷰] Evaluating Gemini Robotics Policies in a Veo World Simulator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator","children":"arXiv에 게시된 'Evaluating Gemini Robotics Policies in a Veo World Simulator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator"}]]}]]}],["$","article","2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","children":"[논문리뷰] UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","children":"arXiv에 게시된 'UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving"}]]}]]}],["$","article","2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","children":"[논문리뷰] StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","children":"Guixun Luo이 arXiv에 게시한 'StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation"}]]}]]}],["$","article","2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","children":"[논문리뷰] Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","children":"arXiv에 게시된 'Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance"}]]}]]}],["$","article","2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","children":"[논문리뷰] MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","children":"arXiv에 게시된 'MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment"}]]}]]}],["$","article","2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","children":"[논문리뷰] UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","children":"arXiv에 게시된 'UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation"}]]}]]}],["$","article","2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","children":"[논문리뷰] ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","children":"Taojun Ding이 arXiv에 게시한 'ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation"}]]}]]}],["$","article","2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","children":"[논문리뷰] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","children":"arXiv에 게시된 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing"}]]}]]}],["$","article","2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations","children":"[논문리뷰] SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations","children":"arXiv에 게시된 'SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations"}]]}]]}],["$","article","2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation","children":"[논문리뷰] ProPhy: Progressive Physical Alignment for Dynamic World Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation","children":"Yuhao Cheng이 arXiv에 게시한 'ProPhy: Progressive Physical Alignment for Dynamic World Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation"}]]}]]}],["$","article","2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation","children":"[논문리뷰] TV2TV: A Unified Framework for Interleaved Language and Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation","children":"arXiv에 게시된 'TV2TV: A Unified Framework for Interleaved Language and Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation"}]]}]]}],["$","article","2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","children":"[논문리뷰] BulletTime: Decoupled Control of Time and Camera Pose for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","children":"Jan Ackermann이 arXiv에 게시한 'BulletTime: Decoupled Control of Time and Camera Pose for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation"}]]}]]}],["$","article","2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","children":"[논문리뷰] RELIC: Interactive Video World Model with Long-Horizon Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","children":"Chongjian Ge이 arXiv에 게시한 'RELIC: Interactive Video World Model with Long-Horizon Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory"}]]}]]}],["$","article","2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","children":"[논문리뷰] Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","children":"Yu Ning이 arXiv에 게시한 'Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation"}]]}]]}],["$","article","2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI","children":"[논문리뷰] PAI-Bench: A Comprehensive Benchmark For Physical AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI","children":"Humphrey Shi이 arXiv에 게시한 'PAI-Bench: A Comprehensive Benchmark For Physical AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI"}]]}]]}],["$","article","2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","children":"[논문리뷰] DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","children":"Zixin Zhang이 arXiv에 게시한 'DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation"}]]}]]}],["$","article","2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","children":"[논문리뷰] Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","children":"arXiv에 게시된 'Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation"}]]}]]}],["$","article","2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation","children":"[논문리뷰] BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation","children":"arXiv에 게시된 'BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation"}]]}]]}],["$","article","2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","children":"[논문리뷰] What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","children":"arXiv에 게시된 'What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards"}]]}]]}],["$","article","2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf","children":"[논문리뷰] Seeing the Wind from a Falling Leaf"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf","children":"Emily Yue-Ting Jia이 arXiv에 게시한 'Seeing the Wind from a Falling Leaf' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf"}]]}]]}],["$","article","2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","children":"[논문리뷰] Video Generation Models Are Good Latent Reward Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","children":"arXiv에 게시된 'Video Generation Models Are Good Latent Reward Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models"}]]}]]}],["$","article","2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation","children":"[논문리뷰] Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation","children":"Jiahao He이 arXiv에 게시한 'Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation"}]]}]]}],["$","article","2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","children":"[논문리뷰] Block Cascading: Training Free Acceleration of Block-Causal Video Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","children":"arXiv에 게시된 'Block Cascading: Training Free Acceleration of Block-Causal Video Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models"}]]}]]}],["$","article","2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","children":"[논문리뷰] PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","children":"Hongzhi Zhang이 arXiv에 게시한 'PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding"}]]}]]}],["$","article","2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI","children":"[논문리뷰] GigaWorld-0: World Models as Data Engine to Empower Embodied AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI","children":"Chaojun Ni이 arXiv에 게시한 'GigaWorld-0: World Models as Data Engine to Empower Embodied AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI"}]]}]]}],["$","article","2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning","children":"[논문리뷰] Plan-X: Instruct Video Generation via Semantic Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning","children":"Chenxu Zhang이 arXiv에 게시한 'Plan-X: Instruct Video Generation via Semantic Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning"}]]}]]}],["$","article","2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control","children":"[논문리뷰] In-Video Instructions: Visual Signals as Generative Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control","children":"arXiv에 게시된 'In-Video Instructions: Visual Signals as Generative Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control"}]]}]]}],["$","article","2025-11-25-HunyuanVideo-1-5-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report","children":"[논문리뷰] HunyuanVideo 1.5 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report","children":"Fang Yang이 arXiv에 게시한 'HunyuanVideo 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report"}]]}]]}],["$","article","2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","children":"[논문리뷰] Planning with Sketch-Guided Verification for Physics-Aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","children":"Shayegan Omidshafiei이 arXiv에 게시한 'Planning with Sketch-Guided Verification for Physics-Aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation"}]]}]]}],["$","article","2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO","children":"[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO","children":"arXiv에 게시된 'Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO"}]]}]]}],["$","article","2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models","children":"[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models","children":"Baijiong Lin이 arXiv에 게시한 'V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models"}]]}]]}],["$","article","2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization","children":"[논문리뷰] First Frame Is the Place to Go for Video Content Customization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization","children":"arXiv에 게시된 'First Frame Is the Place to Go for Video Content Customization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization"}]]}]]}],["$","article","2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks","children":"[논문리뷰] Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks","children":"Yiran Peng이 arXiv에 게시한 'Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks"}]]}]]}],["$","article","2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","children":"[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","children":"Vladimir Arkhipkin이 arXiv에 게시한 'Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation"}]]}]]}],["$","article","2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap","children":"[논문리뷰] Simulating the Visual World with Artificial Intelligence: A Roadmap"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap","children":"Pengfei Wan이 arXiv에 게시한 'Simulating the Visual World with Artificial Intelligence: A Roadmap' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap"}]]}]]}],["$","article","2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers","children":"[논문리뷰] LiteAttention: A Temporal Sparse Attention for Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers","children":"arXiv에 게시된 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers"}]]}]]}],["$","article","2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","children":"[논문리뷰] EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","children":"Zeyu Wang이 arXiv에 게시한 'EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation"}]]}]]}],["$","article","2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist","children":"[논문리뷰] UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist","children":"arXiv에 게시된 'UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist"}]]}]]}],["$","article","2025-11-11-Robot-Learning-from-a-Physical-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model","children":"[논문리뷰] Robot Learning from a Physical World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model","children":"arXiv에 게시된 'Robot Learning from a Physical World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model"}]]}]]}],["$","article","2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm","children":"[논문리뷰] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm","children":"arXiv에 게시된 'Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm"}]]}]]}],["$","article","2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","children":"[논문리뷰] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","children":"arXiv에 게시된 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation"}]]}]]}],["$","article","2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment","children":"[논문리뷰] How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment","children":"Yuhao Zhai이 arXiv에 게시한 'How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment"}]]}]]}],["$","article","2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals","children":"[논문리뷰] Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals","children":"arXiv에 게시된 'Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals"}]]}]]}],["$","article","2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","children":"[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","children":"arXiv에 게시된 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation"}]]}]]}],["$","article","2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","children":"[논문리뷰] VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","children":"Xiaoyu Shi이 arXiv에 게시한 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning"}]]}]]}],["$","article","2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation","children":"[논문리뷰] Uniform Discrete Diffusion with Metric Path for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation","children":"arXiv에 게시된 'Uniform Discrete Diffusion with Metric Path for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation"}]]}]]}],["$","article","2025-10-28-LongCat-Video-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LongCat-Video-Technical-Report","children":"[논문리뷰] LongCat-Video Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LongCat-Video-Technical-Report","children":"Hongyu Li이 arXiv에 게시한 'LongCat-Video Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-LongCat-Video-Technical-Report"}]]}]]}],["$","article","2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation","children":"[논문리뷰] Video-As-Prompt: Unified Semantic Control for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation","children":"arXiv에 게시된 'Video-As-Prompt: Unified Semantic Control for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation"}]]}]]}],["$","article","2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention","children":"[논문리뷰] UltraGen: High-Resolution Video Generation with Hierarchical Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention","children":"Ran Yi이 arXiv에 게시한 'UltraGen: High-Resolution Video Generation with Hierarchical Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention"}]]}]]}],["$","article","2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models","children":"[논문리뷰] MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models","children":"arXiv에 게시된 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models"}]]}]]}],["$","article","2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator","children":"[논문리뷰] VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator","children":"Federico Tombari이 arXiv에 게시한 'VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator"}]]}]]}],["$","article","2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","children":"[논문리뷰] RealDPO: Real or Not Real, that is the Preference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","children":"Chenyang Si이 arXiv에 게시한 'RealDPO: Real or Not Real, that is the Preference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference"}]]}]]}],["$","article","2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","children":"[논문리뷰] ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","children":"arXiv에 게시된 'ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints"}]]}]]}],["$","article","2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","children":"[논문리뷰] PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","children":"Hengshuang Zhao이 arXiv에 게시한 'PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","children":"[논문리뷰] CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","children":"Jingcheng Ni이 arXiv에 게시한 'CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving"}]]}]]}],["$","article","2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning","children":"[논문리뷰] VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning","children":"Quande Liu이 arXiv에 게시한 'VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning"}]]}]]}],["$","article","2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos","children":"[논문리뷰] UniVideo: Unified Understanding, Generation, and Editing for Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos","children":"Xintao Wang이 arXiv에 게시한 'UniVideo: Unified Understanding, Generation, and Editing for Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos"}]]}]]}],["$","article","2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","children":"[논문리뷰] WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","children":"arXiv에 게시된 'WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation"}]]}]]}],["$","article","2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation","children":"[논문리뷰] MATRIX: Mask Track Alignment for Interaction-aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation","children":"Hyunwook Choi이 arXiv에 게시한 'MATRIX: Mask Track Alignment for Interaction-aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation"}]]}]]}],["$","article","2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","children":"[논문리뷰] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","children":"Zheng Zhan이 arXiv에 게시한 'LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation"}]]}]]}],["$","article","2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"[논문리뷰] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"Paul Debevec이 arXiv에 게시한 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation"}]]}]]}],["$","article","2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","children":"[논문리뷰] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","children":"arXiv에 게시된 'ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation"}]]}]]}],["$","article","2025-10-7-Character-Mixing-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation","children":"[논문리뷰] Character Mixing for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation","children":"arXiv에 게시된 'Character Mixing for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation"}]]}]]}],["$","article","2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty","children":"[논문리뷰] How Confident are Video Models? Empowering Video Models to Express their Uncertainty"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty","children":"Anirudha Majumdar이 arXiv에 게시한 'How Confident are Video Models? Empowering Video Models to Express their Uncertainty' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty"}]]}]]}],["$","article","2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","children":"[논문리뷰] BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","children":"Xiangyang Xia이 arXiv에 게시한 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration"}]]}]]}],["$","article","2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation","children":"[논문리뷰] Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation","children":"arXiv에 게시된 'Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation"}]]}]]}],["$","article","2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","children":"[논문리뷰] DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","children":"arXiv에 게시된 'DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder"}]]}]]}],["$","article","2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention","children":"[논문리뷰] SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention","children":"arXiv에 게시된 'SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention"}]]}]]}],["$","article","2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer","children":"[논문리뷰] SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer","children":"arXiv에 게시된 'SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer"}]]}]]}],["$","article","2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","children":"[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","children":"Guoxian Song이 arXiv에 게시한 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction"}]]}]]}],["$","article","2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models","children":"[논문리뷰] UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models","children":"Yuchao Gu이 arXiv에 게시한 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models"}]]}]]}],["$","article","2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","children":"[논문리뷰] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","children":"Yiming Huang이 arXiv에 게시한 'PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation"}]]}]]}],["$","article","2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning","children":"[논문리뷰] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning","children":"Tianyu Wang이 arXiv에 게시한 'EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning"}]]}]]}],["$","article","2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","children":"[논문리뷰] OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","children":"Pengze Zhang이 arXiv에 게시한 'OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models"}]]}]]}],["$","article","2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling","children":"[논문리뷰] OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling","children":"Yang Zhou이 arXiv에 게시한 'OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling"}]]}]]}],["$","article","2025-9-11-3D-and-4D-World-Modeling-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey","children":"[논문리뷰] 3D and 4D World Modeling: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey","children":"Ao Liang이 arXiv에 게시한 '3D and 4D World Modeling: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey"}]]}]]}],["$","article","2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","children":"[논문리뷰] CineScale: Free Lunch in High-Resolution Cinematic Visual Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","children":"Ziwei Liu이 arXiv에 게시한 'CineScale: Free Lunch in High-Resolution Cinematic Visual Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation"}]]}]]}],["$","article","2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation","children":"[논문리뷰] Waver: Wave Your Way to Lifelike Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation","children":"Yifu Zhang이 arXiv에 게시한 'Waver: Wave Your Way to Lifelike Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation"}]]}]]}],["$","article","2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","children":"[논문리뷰] Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","children":"Chen Li이 arXiv에 게시한 'Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation"}]]}]]}],["$","article","2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning","children":"[논문리뷰] Cut2Next: Generating Next Shot via In-Context Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning","children":"Yu Qiao이 arXiv에 게시한 'Cut2Next: Generating Next Shot via In-Context Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning"}]]}]]}],["$","article","2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","children":"[논문리뷰] Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","children":"Xiaokun Feng이 arXiv에 게시한 'Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation"}]]}]]}],["$","article","2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation","children":"[논문리뷰] Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation","children":"Shengcong Chen이 arXiv에 게시한 'Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation"}]]}]]}]]}]]}]]}]}]]}]],null],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children","$6","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L8",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L3",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L9",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L9",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$La"]]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"#Video Generation - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"Video Generation 태그가 포함된 포스트 목록"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/tags/Video%20Generation"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"#Video Generation - secrett2633's blog"}],["$","meta","14",{"property":"og:description","content":"Video Generation 태그가 포함된 포스트 목록"}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/tags/Video%20Generation"}],["$","meta","16",{"property":"og:type","content":"website"}],["$","meta","17",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","18",{"name":"twitter:title","content":"#Video Generation - secrett2633's blog"}],["$","meta","19",{"name":"twitter:description","content":"Video Generation 태그가 포함된 포스트 목록"}],["$","link","20",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
