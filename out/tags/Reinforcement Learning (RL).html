<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ddc331716d5e47a2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-0395f68b8cc78a20.js" async=""></script><script src="/_next/static/chunks/23-7d3f7f0b78aa2fd3.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-467e37449c5a68fc.js" async=""></script><script src="/_next/static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js" async=""></script><script src="/_next/static/chunks/app/layout-b0a450f8e4964582.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><title>#Reinforcement Learning (RL) - secrett2633&#x27;s blog</title><meta name="description" content="Reinforcement Learning (RL) 태그가 포함된 포스트 목록"/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="#Reinforcement Learning (RL) - secrett2633&#x27;s blog"/><meta property="og:description" content="Reinforcement Learning (RL) 태그가 포함된 포스트 목록"/><meta property="og:url" content="https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="#Reinforcement Learning (RL) - secrett2633&#x27;s blog"/><meta name="twitter:description" content="Reinforcement Learning (RL) 태그가 포함된 포스트 목록"/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"CollectionPage","name":"#Reinforcement Learning (RL) - secrett2633's blog","description":"Reinforcement Learning (RL) 태그가 포함된 포스트 목록","url":"https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)","isPartOf":{"@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud"},"inLanguage":"ko"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"#Reinforcement Learning (RL)","item":"https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)"}]}</script><div class="space-y-6"><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2728<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">#Reinforcement Learning (RL)</span></li></ol></nav><h1 class="page__title mb-6">#<!-- -->Reinforcement Learning (RL)</h1><p class="text-gray-500 mb-6">79<!-- -->개의 포스트</p><div class="entries-list"><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models">[논문리뷰] Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models">Liwei Wang이 [arXiv]에 게시한 &#x27;Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-18 00:00:00+0900+0900">2026년 2월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters">[논문리뷰] Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters">이 [arXiv]에 게시한 &#x27;Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search">[논문리뷰] QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search">Hui Zhang이 [arXiv]에 게시한 &#x27;QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization">[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization">이 [arXiv]에 게시한 &#x27;Online Causal Kalman Filtering for Stable and Effective Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research">[논문리뷰] AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research">이 [arXiv]에 게시한 &#x27;AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-10 00:00:00+0900+0900">2026년 2월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey">[논문리뷰] POINTS-GUI-G: GUI-Grounding Journey</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey">Le Tian이 [arXiv]에 게시한 &#x27;POINTS-GUI-G: GUI-Grounding Journey&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-09 00:00:00+0900+0900">2026년 2월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation">[논문리뷰] Unified Personalized Reward Model for Vision Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation">이 [arXiv]에 게시한 &#x27;Unified Personalized Reward Model for Vision Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-04 00:00:00+0900+0900">2026년 2월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models">[논문리뷰] Self-Improving Pretraining: using post-trained models to pretrain better models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models">이 [arXiv]에 게시한 &#x27;Self-Improving Pretraining: using post-trained models to pretrain better models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning">[논문리뷰] Beyond Imitation: Reinforcement Learning for Active Latent Planning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning">Wee Sun Lee이 [arXiv]에 게시한 &#x27;Beyond Imitation: Reinforcement Learning for Active Latent Planning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report">[논문리뷰] LongCat-Flash-Thinking-2601 Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report">이 [arXiv]에 게시한 &#x27;LongCat-Flash-Thinking-2601 Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-26 00:00:00+0900+0900">2026년 1월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs">[논문리뷰] Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs">이 [arXiv]에 게시한 &#x27;Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-16 00:00:00+0900+0900">2026년 1월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests">[논문리뷰] X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests">Jane Luo이 [arXiv]에 게시한 &#x27;X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-13 00:00:00+0900+0900">2026년 1월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration">[논문리뷰] ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration">이 [arXiv]에 게시한 &#x27;ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-13 00:00:00+0900+0900">2026년 1월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data">[논문리뷰] Dr. Zero: Self-Evolving Search Agents without Training Data</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data">Shaoliang Nie이 [arXiv]에 게시한 &#x27;Dr. Zero: Self-Evolving Search Agents without Training Data&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-13 00:00:00+0900+0900">2026년 1월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction">[논문리뷰] Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction">Zhengkang Guo이 [arXiv]에 게시한 &#x27;Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-13 00:00:00+0900+0900">2026년 1월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice">[논문리뷰] VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice">이 [arXiv]에 게시한 &#x27;VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-09 00:00:00+0900+0900">2026년 1월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting">[논문리뷰] Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting">이 [arXiv]에 게시한 &#x27;Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-08 00:00:00+0900+0900">2026년 1월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-06-K-EXAONE-Technical-Report">[논문리뷰] K-EXAONE Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-06-K-EXAONE-Technical-Report">이 [arXiv]에 게시한 &#x27;K-EXAONE Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-06 00:00:00+0900+0900">2026년 1월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling">[논문리뷰] Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling">이 [arXiv]에 게시한 &#x27;Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-06 00:00:00+0900+0900">2026년 1월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future">[논문리뷰] Scaling Open-Ended Reasoning to Predict the Future</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future">이 [arXiv]에 게시한 &#x27;Scaling Open-Ended Reasoning to Predict the Future&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-01 00:00:00+0900+0900">2026년 1월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards">[논문리뷰] Training AI Co-Scientists Using Rubric Rewards</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards">이 [arXiv]에 게시한 &#x27;Training AI Co-Scientists Using Rubric Rewards&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-30 00:00:00+0900+0900">2025년 12월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning">[논문리뷰] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning">이 [arXiv]에 게시한 &#x27;See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-29 00:00:00+0900+0900">2025년 12월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment">[논문리뷰] Multi-hop Reasoning via Early Knowledge Alignment</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment">Xuanjing Huang이 [arXiv]에 게시한 &#x27;Multi-hop Reasoning via Early Knowledge Alignment&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-25 00:00:00+0900+0900">2025년 12월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library">[논문리뷰] Reinforcement Learning for Self-Improving Agent with Skill Library</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library">Soumya Smruti Mishra이 [arXiv]에 게시한 &#x27;Reinforcement Learning for Self-Improving Agent with Skill Library&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-24 00:00:00+0900+0900">2025년 12월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs">[논문리뷰] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs">이 [arXiv]에 게시한 &#x27;Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-23 00:00:00+0900+0900">2025년 12월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding">[논문리뷰] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding">Runtao Liu이 [arXiv]에 게시한 &#x27;Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-22 00:00:00+0900+0900">2025년 12월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning">[논문리뷰] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning">이 [arXiv]에 게시한 &#x27;Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-18 00:00:00+0900+0900">2025년 12월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection">[논문리뷰] Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection">이 [arXiv]에 게시한 &#x27;Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-16 00:00:00+0900+0900">2025년 12월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models">[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models">이 [arXiv]에 게시한 &#x27;On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-09 00:00:00+0900+0900">2025년 12월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor">[논문리뷰] EditThinker: Unlocking Iterative Reasoning for Any Image Editor</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor">Ziyu Guo이 [arXiv]에 게시한 &#x27;EditThinker: Unlocking Iterative Reasoning for Any Image Editor&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-08 00:00:00+0900+0900">2025년 12월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral">[논문리뷰] On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral">Christos Thrampoulidis이 [arXiv]에 게시한 &#x27;On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-05 00:00:00+0900+0900">2025년 12월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization">[논문리뷰] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization">이 [arXiv]에 게시한 &#x27;Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning">[논문리뷰] Artemis: Structured Visual Reasoning for Perception Policy Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning">Piotr Koniusz이 [arXiv]에 게시한 &#x27;Artemis: Structured Visual Reasoning for Perception Policy Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices">[논문리뷰] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices">이 [arXiv]에 게시한 &#x27;Stabilizing Reinforcement Learning with LLMs: Formulation and Practices&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning">[논문리뷰] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning">이 [arXiv]에 게시한 &#x27;DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-01 00:00:00+0900+0900">2025년 12월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language">[논문리뷰] Monet: Reasoning in Latent Visual Space Beyond Images and Language</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language">Pengfei Wan이 [arXiv]에 게시한 &#x27;Monet: Reasoning in Latent Visual Space Beyond Images and Language&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-27 00:00:00+0900+0900">2025년 11월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots">[논문리뷰] MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots">Rui Yang이 [arXiv]에 게시한 &#x27;MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-27 00:00:00+0900+0900">2025년 11월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs">[논문리뷰] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs">이 [arXiv]에 게시한 &#x27;Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-26 00:00:00+0900+0900">2025년 11월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation">[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation">Xinyan Chen이 [arXiv]에 게시한 &#x27;Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-21 00:00:00+0900+0900">2025년 11월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models">[논문리뷰] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models">이 [arXiv]에 게시한 &#x27;WMPO: World Model-based Policy Optimization for Vision-Language-Action Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-13 00:00:00+0900+0900">2025년 11월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls">[논문리뷰] LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls">이 [arXiv]에 게시한 &#x27;LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-13 00:00:00+0900+0900">2025년 11월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B">[논문리뷰] Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B">이 [arXiv]에 게시한 &#x27;Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries">[논문리뷰] Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries">Zhengzhong Tu이 [arXiv]에 게시한 &#x27;Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-11 00:00:00+0900+0900">2025년 11월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models">[논문리뷰] π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models">이 [arXiv]에 게시한 &#x27;π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:01:31+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree">[논문리뷰] PORTool: Tool-Use LLM Training with Rewarded Tree</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree">이 [arXiv]에 게시한 &#x27;PORTool: Tool-Use LLM Training with Rewarded Tree&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs">[논문리뷰] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs">이 [arXiv]에 게시한 &#x27;MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment">[논문리뷰] Evolving Diagnostic Agents in a Virtual Clinical Environment</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment">이 [arXiv]에 게시한 &#x27;Evolving Diagnostic Agents in a Virtual Clinical Environment&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning">[논문리뷰] UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning">이 [arXiv]에 게시한 &#x27;UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-27 13:07:36+0900">2025년 10월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think">[논문리뷰] Reasoning with Sampling: Your Base Model is Smarter Than You Think</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think">이 [arXiv]에 게시한 &#x27;Reasoning with Sampling: Your Base Model is Smarter Than You Think&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-27 13:07:36+0900">2025년 10월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense">[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense">이 [arXiv]에 게시한 &#x27;Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-21 13:08:30+0900">2025년 10월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs">[논문리뷰] Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs">Hao Zhang이 [arXiv]에 게시한 &#x27;Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model">[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model">Wieland Brendel이 [arXiv]에 게시한 &#x27;MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning">[논문리뷰] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning">이 [arXiv]에 게시한 &#x27;ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-15 13:01:40+0900">2025년 10월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression">[논문리뷰] Which Heads Matter for Reasoning? RL-Guided KV Cache Compression</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression">Huan Wang이 [arXiv]에 게시한 &#x27;Which Heads Matter for Reasoning? RL-Guided KV Cache Compression&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels">[논문리뷰] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels">이 [arXiv]에 게시한 &#x27;Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models">[논문리뷰] TTRV: Test-Time Reinforcement Learning for Vision Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models">Serena Yeung-Levy이 [arXiv]에 게시한 &#x27;TTRV: Test-Time Reinforcement Learning for Vision Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-09 13:45:06+0900">2025년 10월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use">[논문리뷰] In-the-Flow Agentic System Optimization for Effective Planning and Tool Use</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use">이 [arXiv]에 게시한 &#x27;In-the-Flow Agentic System Optimization for Effective Planning and Tool Use&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-08 13:48:12+0900">2025년 10월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models">[논문리뷰] Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models">zeliang0426이 [arXiv]에 게시한 &#x27;Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training">[논문리뷰] Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training">이 [arXiv]에 게시한 &#x27;Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation">[논문리뷰] Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation">이 [arXiv]에 게시한 &#x27;Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-02 13:30:22+0900">2025년 10월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training">[논문리뷰] Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training">이 [arXiv]에 게시한 &#x27;Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies">[논문리뷰] Residual Off-Policy RL for Finetuning Behavior Cloning Policies</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies">Pieter Abbeel이 [arXiv]에 게시한 &#x27;Residual Off-Policy RL for Finetuning Behavior Cloning Policies&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-25-Logics-Parsing-Technical-Report">[논문리뷰] Logics-Parsing Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-25-Logics-Parsing-Technical-Report">Fan Yang이 [arXiv]에 게시한 &#x27;Logics-Parsing Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-25 13:08:16+0900">2025년 9월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning">[논문리뷰] GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning">Hou Pong Chan이 [arXiv]에 게시한 &#x27;GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning">[논문리뷰] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning">Jiangmiao이 [arXiv]에 게시한 &#x27;A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-22 13:11:29+0900">2025년 9월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning">[논문리뷰] Improving Context Fidelity via Native Retrieval-Augmented Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning">Xiangru Tang이 [arXiv]에 게시한 &#x27;Improving Context Fidelity via Native Retrieval-Augmented Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-18 13:07:00+0900">2025년 9월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding">[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding">Ethan Chern이 [arXiv]에 게시한 &#x27;Visual Programmability: A Guide for Code-as-Thought in Chart Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-12 13:12:46+0900">2025년 9월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning">[논문리뷰] SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning">Zhaohui Yang이 [arXiv]에 게시한 &#x27;SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-12 13:12:46+0900">2025년 9월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents">[논문리뷰] WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents">Aili Chen이 [arXiv]에 게시한 &#x27;WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-09 13:19:09+0900">2025년 9월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers">[논문리뷰] Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers">Xia Xiao이 [arXiv]에 게시한 &#x27;Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-09 13:19:09+0900">2025년 9월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement">[논문리뷰] Bootstrapping Task Spaces for Self-Improvement</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement">Yoram Bachrach이 [arXiv]에 게시한 &#x27;Bootstrapping Task Spaces for Self-Improvement&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-08 13:10:18+0900">2025년 9월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training">[논문리뷰] Towards a Unified View of Large Language Model Post-Training</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training">Hongyi Liu이 [arXiv]에 게시한 &#x27;Towards a Unified View of Large Language Model Post-Training&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-05 13:07:20+0900">2025년 9월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning">[논문리뷰] Robix: A Unified Model for Robot Interaction, Reasoning and Planning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning">Zixuan Wang이 [arXiv]에 게시한 &#x27;Robix: A Unified Model for Robot Interaction, Reasoning and Planning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-04 12:56:15+0900">2025년 9월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model">[논문리뷰] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model">Jianwei Yang이 [arXiv]에 게시한 &#x27;LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning">[논문리뷰] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning">Han Hu이 [arXiv]에 게시한 &#x27;R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-01 13:14:34+0900">2025년 9월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments">[논문리뷰] Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments">Xuesong Yao이 [arXiv]에 게시한 &#x27;Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-13 13:29:23+0900">2025년 8월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey">[논문리뷰] Reinforcement Learning in Vision: A Survey</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey">Qingwei Meng이 [arXiv]에 게시한 &#x27;Reinforcement Learning in Vision: A Survey&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-12 13:29:09+0900">2025년 8월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification">[논문리뷰] On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification">Xinyu Ye이 [arXiv]에 게시한 &#x27;On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search">[논문리뷰] Tool-integrated Reinforcement Learning for Repo Deep Search</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search">Yanzhen Zou이 [arXiv]에 게시한 &#x27;Tool-integrated Reinforcement Learning for Repo Deep Search&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-06 13:46:36+0900">2025년 8월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article></div></div></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ddc331716d5e47a2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[9038,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js\"],\"default\"]\n6:I[231,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js\"],\"\"]\n7:I[227,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-d13158d00abd62a4.js\"],\"default\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nb:I[9157,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"default\"]\nc:I[4080,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"\"]\ne:I[6130,[],\"\"]\n9:[\"tag\",\"Reinforcement%20Learning%20(RL)\",\"d\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ddc331716d5e47a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"WcxaIiCPz9cbpnkGvOjOK\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/tags/Reinforcement%20Learning%20(RL)\",\"initialTree\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Reinforcement%20Learning%20(RL)\",\"d\"],{\"children\":[\"__PAGE__?{\\\"tag\\\":\\\"Reinforcement Learning (RL)\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Reinforcement%20Learning%20(RL)\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"$L5\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"CollectionPage\\\",\\\"name\\\":\\\"#Reinforcement Learning (RL) - secrett2633's blog\\\",\\\"description\\\":\\\"Reinforcement Learning (RL) 태그가 포함된 포스트 목록\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)\\\",\\\"isPartOf\\\":{\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"inLanguage\\\":\\\"ko\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"#Reinforcement Learning (RL)\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2728,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/tags/Reinforcement%20Learning%20(RL)\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"#Reinforcement Learning (RL)\"}]]}]]]}]}],[\"$\",\"h1\",null,{\"className\":\"page__title mb-6\",\"children\":[\"#\",\"Reinforcement Learning (RL)\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-500 mb-6\",\"children\":[79,\"개의 포스트\"]}],[\"$\",\"div\",null,{\"className\":\"entries-list\",\"children\":[[\"$\",\"article\",\"2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models\",\"children\":\"[논문리뷰] Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models\",\"children\":\"Liwei Wang이 [arXiv]에 게시한 'Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-18 00:00:00+0900+0900\",\"children\":\"2026년 2월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters\",\"children\":\"[논문리뷰] Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters\",\"children\":\"이 [arXiv]에 게시한 'Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search\",\"children\":\"[논문리뷰] QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search\",\"children\":\"Hui Zhang이 [arXiv]에 게시한 'QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization\",\"children\":\"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization\",\"children\":\"이 [arXiv]에 게시한 'Online Causal Kalman Filtering for Stable and Effective Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization\"}]]}]]}],[\"$\",\"article\",\"2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research\",\"children\":\"[논문리뷰] AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research\",\"children\":\"이 [arXiv]에 게시한 'AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-10 00:00:00+0900+0900\",\"children\":\"2026년 2월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research\"}]]}]]}],[\"$\",\"article\",\"2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey\",\"children\":\"[논문리뷰] POINTS-GUI-G: GUI-Grounding Journey\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey\",\"children\":\"Le Tian이 [arXiv]에 게시한 'POINTS-GUI-G: GUI-Grounding Journey' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-09 00:00:00+0900+0900\",\"children\":\"2026년 2월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey\"}]]}]]}],[\"$\",\"article\",\"2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation\",\"children\":\"[논문리뷰] Unified Personalized Reward Model for Vision Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation\",\"children\":\"이 [arXiv]에 게시한 'Unified Personalized Reward Model for Vision Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-04 00:00:00+0900+0900\",\"children\":\"2026년 2월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models\",\"children\":\"[논문리뷰] Self-Improving Pretraining: using post-trained models to pretrain better models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models\",\"children\":\"이 [arXiv]에 게시한 'Self-Improving Pretraining: using post-trained models to pretrain better models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning\",\"children\":\"[논문리뷰] Beyond Imitation: Reinforcement Learning for Active Latent Planning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning\",\"children\":\"Wee Sun Lee이 [arXiv]에 게시한 'Beyond Imitation: Reinforcement Learning for Active Latent Planning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning\"}]]}]]}],[\"$\",\"article\",\"2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report\",\"children\":\"[논문리뷰] LongCat-Flash-Thinking-2601 Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report\",\"children\":\"이 [arXiv]에 게시한 'LongCat-Flash-Thinking-2601 Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-26 00:00:00+0900+0900\",\"children\":\"2026년 1월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs\",\"children\":\"[논문리뷰] Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs\",\"children\":\"이 [arXiv]에 게시한 'Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-16 00:00:00+0900+0900\",\"children\":\"2026년 1월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs\"}]]}]]}],[\"$\",\"article\",\"2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests\",\"children\":\"[논문리뷰] X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests\",\"children\":\"Jane Luo이 [arXiv]에 게시한 'X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-13 00:00:00+0900+0900\",\"children\":\"2026년 1월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests\"}]]}]]}],[\"$\",\"article\",\"2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration\",\"children\":\"[논문리뷰] ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration\",\"children\":\"이 [arXiv]에 게시한 'ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-13 00:00:00+0900+0900\",\"children\":\"2026년 1월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration\"}]]}]]}],[\"$\",\"article\",\"2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data\",\"children\":\"[논문리뷰] Dr. Zero: Self-Evolving Search Agents without Training Data\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data\",\"children\":\"Shaoliang Nie이 [arXiv]에 게시한 'Dr. Zero: Self-Evolving Search Agents without Training Data' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-13 00:00:00+0900+0900\",\"children\":\"2026년 1월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data\"}]]}]]}],[\"$\",\"article\",\"2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction\",\"children\":\"[논문리뷰] Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction\",\"children\":\"Zhengkang Guo이 [arXiv]에 게시한 'Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-13 00:00:00+0900+0900\",\"children\":\"2026년 1월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction\"}]]}]]}],[\"$\",\"article\",\"2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice\",\"children\":\"[논문리뷰] VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice\",\"children\":\"이 [arXiv]에 게시한 'VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-09 00:00:00+0900+0900\",\"children\":\"2026년 1월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice\"}]]}]]}],[\"$\",\"article\",\"2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting\",\"children\":\"[논문리뷰] Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting\",\"children\":\"이 [arXiv]에 게시한 'Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-08 00:00:00+0900+0900\",\"children\":\"2026년 1월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting\"}]]}]]}],[\"$\",\"article\",\"2026-01-06-K-EXAONE-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-K-EXAONE-Technical-Report\",\"children\":\"[논문리뷰] K-EXAONE Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-K-EXAONE-Technical-Report\",\"children\":\"이 [arXiv]에 게시한 'K-EXAONE Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-06 00:00:00+0900+0900\",\"children\":\"2026년 1월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-06-K-EXAONE-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling\",\"children\":\"[논문리뷰] Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling\",\"children\":\"이 [arXiv]에 게시한 'Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-06 00:00:00+0900+0900\",\"children\":\"2026년 1월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling\"}]]}]]}],[\"$\",\"article\",\"2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future\",\"children\":\"[논문리뷰] Scaling Open-Ended Reasoning to Predict the Future\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future\",\"children\":\"이 [arXiv]에 게시한 'Scaling Open-Ended Reasoning to Predict the Future' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-01 00:00:00+0900+0900\",\"children\":\"2026년 1월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future\"}]]}]]}],[\"$\",\"article\",\"2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards\",\"children\":\"[논문리뷰] Training AI Co-Scientists Using Rubric Rewards\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards\",\"children\":\"이 [arXiv]에 게시한 'Training AI Co-Scientists Using Rubric Rewards' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-30 00:00:00+0900+0900\",\"children\":\"2025년 12월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards\"}]]}]]}],[\"$\",\"article\",\"2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning\",\"children\":\"[논문리뷰] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning\",\"children\":\"이 [arXiv]에 게시한 'See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-29 00:00:00+0900+0900\",\"children\":\"2025년 12월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment\",\"children\":\"[논문리뷰] Multi-hop Reasoning via Early Knowledge Alignment\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment\",\"children\":\"Xuanjing Huang이 [arXiv]에 게시한 'Multi-hop Reasoning via Early Knowledge Alignment' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-25 00:00:00+0900+0900\",\"children\":\"2025년 12월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment\"}]]}]]}],[\"$\",\"article\",\"2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library\",\"children\":\"[논문리뷰] Reinforcement Learning for Self-Improving Agent with Skill Library\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library\",\"children\":\"Soumya Smruti Mishra이 [arXiv]에 게시한 'Reinforcement Learning for Self-Improving Agent with Skill Library' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-24 00:00:00+0900+0900\",\"children\":\"2025년 12월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library\"}]]}]]}],[\"$\",\"article\",\"2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs\",\"children\":\"[논문리뷰] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs\",\"children\":\"이 [arXiv]에 게시한 'Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-23 00:00:00+0900+0900\",\"children\":\"2025년 12월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs\"}]]}]]}],[\"$\",\"article\",\"2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding\",\"children\":\"[논문리뷰] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding\",\"children\":\"Runtao Liu이 [arXiv]에 게시한 'Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-22 00:00:00+0900+0900\",\"children\":\"2025년 12월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning\",\"children\":\"[논문리뷰] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning\",\"children\":\"이 [arXiv]에 게시한 'Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-18 00:00:00+0900+0900\",\"children\":\"2025년 12월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection\",\"children\":\"[논문리뷰] Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection\",\"children\":\"이 [arXiv]에 게시한 'Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-16 00:00:00+0900+0900\",\"children\":\"2025년 12월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection\"}]]}]]}],[\"$\",\"article\",\"2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models\",\"children\":\"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models\",\"children\":\"이 [arXiv]에 게시한 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-09 00:00:00+0900+0900\",\"children\":\"2025년 12월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor\",\"children\":\"[논문리뷰] EditThinker: Unlocking Iterative Reasoning for Any Image Editor\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor\",\"children\":\"Ziyu Guo이 [arXiv]에 게시한 'EditThinker: Unlocking Iterative Reasoning for Any Image Editor' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-08 00:00:00+0900+0900\",\"children\":\"2025년 12월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor\"}]]}]]}],[\"$\",\"article\",\"2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral\",\"children\":\"[논문리뷰] On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral\",\"children\":\"Christos Thrampoulidis이 [arXiv]에 게시한 'On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-05 00:00:00+0900+0900\",\"children\":\"2025년 12월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization\",\"children\":\"[논문리뷰] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization\",\"children\":\"이 [arXiv]에 게시한 'Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning\",\"children\":\"[논문리뷰] Artemis: Structured Visual Reasoning for Perception Policy Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning\",\"children\":\"Piotr Koniusz이 [arXiv]에 게시한 'Artemis: Structured Visual Reasoning for Perception Policy Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices\",\"children\":\"[논문리뷰] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices\",\"children\":\"이 [arXiv]에 게시한 'Stabilizing Reinforcement Learning with LLMs: Formulation and Practices' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices\"}]]}]]}],[\"$\",\"article\",\"2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning\",\"children\":\"[논문리뷰] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning\",\"children\":\"이 [arXiv]에 게시한 'DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-01 00:00:00+0900+0900\",\"children\":\"2025년 12월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language\",\"children\":\"[논문리뷰] Monet: Reasoning in Latent Visual Space Beyond Images and Language\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language\",\"children\":\"Pengfei Wan이 [arXiv]에 게시한 'Monet: Reasoning in Latent Visual Space Beyond Images and Language' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-27 00:00:00+0900+0900\",\"children\":\"2025년 11월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language\"}]]}]]}],[\"$\",\"article\",\"2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots\",\"children\":\"[논문리뷰] MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots\",\"children\":\"Rui Yang이 [arXiv]에 게시한 'MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-27 00:00:00+0900+0900\",\"children\":\"2025년 11월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots\"}]]}]]}],[\"$\",\"article\",\"2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs\",\"children\":\"[논문리뷰] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs\",\"children\":\"이 [arXiv]에 게시한 'Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-26 00:00:00+0900+0900\",\"children\":\"2025년 11월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs\"}]]}]]}],[\"$\",\"article\",\"2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation\",\"children\":\"[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation\",\"children\":\"Xinyan Chen이 [arXiv]에 게시한 'Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-21 00:00:00+0900+0900\",\"children\":\"2025년 11월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models\",\"children\":\"[논문리뷰] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models\",\"children\":\"이 [arXiv]에 게시한 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-13 00:00:00+0900+0900\",\"children\":\"2025년 11월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls\",\"children\":\"[논문리뷰] LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls\",\"children\":\"이 [arXiv]에 게시한 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-13 00:00:00+0900+0900\",\"children\":\"2025년 11월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B\",\"children\":\"[논문리뷰] Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B\",\"children\":\"이 [arXiv]에 게시한 'Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B\"}]]}]]}],[\"$\",\"article\",\"2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries\",\"children\":\"[논문리뷰] Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries\",\"children\":\"Zhengzhong Tu이 [arXiv]에 게시한 'Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-11 00:00:00+0900+0900\",\"children\":\"2025년 11월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries\"}]]}]]}],[\"$\",\"article\",\"2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models\",\"children\":\"[논문리뷰] π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models\",\"children\":\"이 [arXiv]에 게시한 'π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:01:31+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree\",\"children\":\"[논문리뷰] PORTool: Tool-Use LLM Training with Rewarded Tree\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree\",\"children\":\"이 [arXiv]에 게시한 'PORTool: Tool-Use LLM Training with Rewarded Tree' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs\",\"children\":\"[논문리뷰] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs\",\"children\":\"이 [arXiv]에 게시한 'MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment\",\"children\":\"[논문리뷰] Evolving Diagnostic Agents in a Virtual Clinical Environment\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment\",\"children\":\"이 [arXiv]에 게시한 'Evolving Diagnostic Agents in a Virtual Clinical Environment' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment\"}]]}]]}],[\"$\",\"article\",\"2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning\",\"children\":\"[논문리뷰] UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning\",\"children\":\"이 [arXiv]에 게시한 'UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-27 13:07:36+0900\",\"children\":\"2025년 10월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think\",\"children\":\"[논문리뷰] Reasoning with Sampling: Your Base Model is Smarter Than You Think\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think\",\"children\":\"이 [arXiv]에 게시한 'Reasoning with Sampling: Your Base Model is Smarter Than You Think' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-27 13:07:36+0900\",\"children\":\"2025년 10월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think\"}]]}]]}],[\"$\",\"article\",\"2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense\",\"children\":\"[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense\",\"children\":\"이 [arXiv]에 게시한 'Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-21 13:08:30+0900\",\"children\":\"2025년 10월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs\",\"children\":\"[논문리뷰] Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs\",\"children\":\"Hao Zhang이 [arXiv]에 게시한 'Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\",\"children\":\"[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\",\"children\":\"Wieland Brendel이 [arXiv]에 게시한 'MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\"}]]}]]}],[\"$\",\"article\",\"2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning\",\"children\":\"[논문리뷰] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning\",\"children\":\"이 [arXiv]에 게시한 'ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-15 13:01:40+0900\",\"children\":\"2025년 10월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression\",\"children\":\"[논문리뷰] Which Heads Matter for Reasoning? RL-Guided KV Cache Compression\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression\",\"children\":\"Huan Wang이 [arXiv]에 게시한 'Which Heads Matter for Reasoning? RL-Guided KV Cache Compression' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels\",\"children\":\"[논문리뷰] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels\",\"children\":\"이 [arXiv]에 게시한 'Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels\"}]]}]]}],[\"$\",\"article\",\"2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models\",\"children\":\"[논문리뷰] TTRV: Test-Time Reinforcement Learning for Vision Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models\",\"children\":\"Serena Yeung-Levy이 [arXiv]에 게시한 'TTRV: Test-Time Reinforcement Learning for Vision Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-09 13:45:06+0900\",\"children\":\"2025년 10월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use\",\"children\":\"[논문리뷰] In-the-Flow Agentic System Optimization for Effective Planning and Tool Use\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use\",\"children\":\"이 [arXiv]에 게시한 'In-the-Flow Agentic System Optimization for Effective Planning and Tool Use' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-08 13:48:12+0900\",\"children\":\"2025년 10월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models\",\"children\":\"[논문리뷰] Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models\",\"children\":\"zeliang0426이 [arXiv]에 게시한 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training\",\"children\":\"[논문리뷰] Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training\",\"children\":\"이 [arXiv]에 게시한 'Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training\"}]]}]]}],[\"$\",\"article\",\"2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation\",\"children\":\"[논문리뷰] Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation\",\"children\":\"이 [arXiv]에 게시한 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-02 13:30:22+0900\",\"children\":\"2025년 10월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training\",\"children\":\"[논문리뷰] Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training\",\"children\":\"이 [arXiv]에 게시한 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies\",\"children\":\"[논문리뷰] Residual Off-Policy RL for Finetuning Behavior Cloning Policies\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies\",\"children\":\"Pieter Abbeel이 [arXiv]에 게시한 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies\"}]]}]]}],[\"$\",\"article\",\"2025-9-25-Logics-Parsing-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-Logics-Parsing-Technical-Report\",\"children\":\"[논문리뷰] Logics-Parsing Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-Logics-Parsing-Technical-Report\",\"children\":\"Fan Yang이 [arXiv]에 게시한 'Logics-Parsing Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-25 13:08:16+0900\",\"children\":\"2025년 9월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-25-Logics-Parsing-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning\",\"children\":\"[논문리뷰] GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning\",\"children\":\"Hou Pong Chan이 [arXiv]에 게시한 'GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning\",\"children\":\"[논문리뷰] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning\",\"children\":\"Jiangmiao이 [arXiv]에 게시한 'A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-22 13:11:29+0900\",\"children\":\"2025년 9월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning\",\"children\":\"[논문리뷰] Improving Context Fidelity via Native Retrieval-Augmented Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning\",\"children\":\"Xiangru Tang이 [arXiv]에 게시한 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-18 13:07:00+0900\",\"children\":\"2025년 9월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\",\"children\":\"[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\",\"children\":\"Ethan Chern이 [arXiv]에 게시한 'Visual Programmability: A Guide for Code-as-Thought in Chart Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-12 13:12:46+0900\",\"children\":\"2025년 9월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning\",\"children\":\"[논문리뷰] SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning\",\"children\":\"Zhaohui Yang이 [arXiv]에 게시한 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-12 13:12:46+0900\",\"children\":\"2025년 9월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents\",\"children\":\"[논문리뷰] WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents\",\"children\":\"Aili Chen이 [arXiv]에 게시한 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-09 13:19:09+0900\",\"children\":\"2025년 9월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers\",\"children\":\"[논문리뷰] Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers\",\"children\":\"Xia Xiao이 [arXiv]에 게시한 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-09 13:19:09+0900\",\"children\":\"2025년 9월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers\"}]]}]]}],[\"$\",\"article\",\"2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement\",\"children\":\"[논문리뷰] Bootstrapping Task Spaces for Self-Improvement\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement\",\"children\":\"Yoram Bachrach이 [arXiv]에 게시한 'Bootstrapping Task Spaces for Self-Improvement' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-08 13:10:18+0900\",\"children\":\"2025년 9월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement\"}]]}]]}],[\"$\",\"article\",\"2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training\",\"children\":\"[논문리뷰] Towards a Unified View of Large Language Model Post-Training\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training\",\"children\":\"Hongyi Liu이 [arXiv]에 게시한 'Towards a Unified View of Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-05 13:07:20+0900\",\"children\":\"2025년 9월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training\"}]]}]]}],[\"$\",\"article\",\"2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning\",\"children\":\"[논문리뷰] Robix: A Unified Model for Robot Interaction, Reasoning and Planning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning\",\"children\":\"Zixuan Wang이 [arXiv]에 게시한 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-04 12:56:15+0900\",\"children\":\"2025년 9월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model\",\"children\":\"[논문리뷰] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model\",\"children\":\"Jianwei Yang이 [arXiv]에 게시한 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model\"}]]}]]}],[\"$\",\"article\",\"2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning\",\"children\":\"[논문리뷰] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning\",\"children\":\"Han Hu이 [arXiv]에 게시한 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-01 13:14:34+0900\",\"children\":\"2025년 9월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments\",\"children\":\"[논문리뷰] Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments\",\"children\":\"Xuesong Yao이 [arXiv]에 게시한 'Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-13 13:29:23+0900\",\"children\":\"2025년 8월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments\"}]]}]]}],[\"$\",\"article\",\"2025-8-12-Reinforcement-Learning-in-Vision-A-Survey\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey\",\"children\":\"[논문리뷰] Reinforcement Learning in Vision: A Survey\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey\",\"children\":\"Qingwei Meng이 [arXiv]에 게시한 'Reinforcement Learning in Vision: A Survey' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-12 13:29:09+0900\",\"children\":\"2025년 8월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification\",\"children\":\"[논문리뷰] On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification\",\"children\":\"Xinyu Ye이 [arXiv]에 게시한 'On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification\"}]]}]]}],[\"$\",\"article\",\"2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search\",\"children\":\"[논문리뷰] Tool-integrated Reinforcement Learning for Repo Deep Search\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search\",\"children\":\"Yanzhen Zou이 [arXiv]에 게시한 'Tool-integrated Reinforcement Learning for Repo Deep Search' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-06 13:46:36+0900\",\"children\":\"2025년 8월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search\"}]]}]]}]]}]]}]]}]}]]}]],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lc\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lc\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"#Reinforcement Learning (RL) - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Reinforcement Learning (RL) 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"#Reinforcement Learning (RL) - secrett2633's blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"Reinforcement Learning (RL) 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/tags/Reinforcement%20Learning%20(RL)\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"#Reinforcement Learning (RL) - secrett2633's blog\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"Reinforcement Learning (RL) 태그가 포함된 포스트 목록\"}],[\"$\",\"link\",\"20\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"21\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>