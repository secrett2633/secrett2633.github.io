<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/edb8d4ad4fe2f3b0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-4b0d66bdf1ba1813.js" async=""></script><script src="/_next/static/chunks/23-41c976638cd1a58c.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-ee5764c1002761f9.js" async=""></script><script src="/_next/static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js" async=""></script><script src="/_next/static/chunks/132-273e49420772df1e.js" async=""></script><script src="/_next/static/chunks/app/layout-d443cbc354279241.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><title>#Benchmark - secrett2633&#x27;s blog</title><meta name="description" content="Benchmark 태그가 포함된 포스트 목록"/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/tags/Benchmark"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="#Benchmark - secrett2633&#x27;s blog"/><meta property="og:description" content="Benchmark 태그가 포함된 포스트 목록"/><meta property="og:url" content="https://blog.secrett2633.cloud/tags/Benchmark"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="#Benchmark - secrett2633&#x27;s blog"/><meta name="twitter:description" content="Benchmark 태그가 포함된 포스트 목록"/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"CollectionPage","name":"#Benchmark - secrett2633's blog","description":"Benchmark 태그가 포함된 포스트 목록","url":"https://blog.secrett2633.cloud/tags/Benchmark","isPartOf":{"@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud"},"inLanguage":"ko"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"#Benchmark","item":"https://blog.secrett2633.cloud/tags/Benchmark"}]}</script><div class="space-y-6"><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2741<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">#Benchmark</span></li></ol></nav><h1 class="page__title mb-6">#<!-- -->Benchmark</h1><p class="text-gray-500 mb-6">158<!-- -->개의 포스트</p><div class="entries-list"><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark">[논문리뷰] MAEB: Massive Audio Embedding Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark">arXiv에 게시된 &#x27;MAEB: Massive Audio Embedding Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-19 00:00:00+0900+0900">2026년 2월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World">[논문리뷰] Learning Situated Awareness in the Real World</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World">Rajiv Dhawan이 arXiv에 게시한 &#x27;Learning Situated Awareness in the Real World&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-19 00:00:00+0900+0900">2026년 2월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research">[논문리뷰] ResearchGym: Evaluating Language Model Agents on Real-World AI Research</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research">Arman Cohan이 arXiv에 게시한 &#x27;ResearchGym: Evaluating Language Model Agents on Real-World AI Research&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-18 00:00:00+0900+0900">2026년 2월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents">[논문리뷰] BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents">Yanzhe Dan이 arXiv에 게시한 &#x27;BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-17 00:00:00+0900+0900">2026년 2월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite">[논문리뷰] GENIUS: Generative Fluid Intelligence Evaluation Suite</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite">Zijun Shen이 arXiv에 게시한 &#x27;GENIUS: Generative Fluid Intelligence Evaluation Suite&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies">[논문리뷰] EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies">Yishuo Yuan이 arXiv에 게시한 &#x27;EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant">[논문리뷰] GISA: A Benchmark for General Information-Seeking Assistant</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant">arXiv에 게시된 &#x27;GISA: A Benchmark for General Information-Seeking Assistant&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-10 00:00:00+0900+0900">2026년 2월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments">[논문리뷰] GEBench: Benchmarking Image Generation Models as GUI Environments</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments">arXiv에 게시된 &#x27;GEBench: Benchmarking Image Generation Models as GUI Environments&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-10 00:00:00+0900+0900">2026년 2월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition">[논문리뷰] Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition">arXiv에 게시된 &#x27;Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-10 00:00:00+0900+0900">2026년 2월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks">[논문리뷰] PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks">Zhixin Wang이 arXiv에 게시한 &#x27;PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-09 00:00:00+0900+0900">2026년 2월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities">[논문리뷰] Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities">arXiv에 게시된 &#x27;Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules">[논문리뷰] RISE-Video: Can Video Generators Decode Implicit World Rules?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules">Zicheng Zhang이 arXiv에 게시한 &#x27;RISE-Video: Can Video Generators Decode Implicit World Rules?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets">[논문리뷰] HY3D-Bench: Generation of 3D Assets</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets">arXiv에 게시된 &#x27;HY3D-Bench: Generation of 3D Assets&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-05 00:00:00+0900+0900">2026년 2월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles">[논문리뷰] Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles">arXiv에 게시된 &#x27;Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-03 00:00:00+0900+0900">2026년 2월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models">[논문리뷰] Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models">Shuang Chen이 arXiv에 게시한 &#x27;Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-03 00:00:00+0900+0900">2026년 2월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model">[논문리뷰] Toward Cognitive Supersensing in Multimodal Large Language Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model">Yifan Xu이 arXiv에 게시한 &#x27;Toward Cognitive Supersensing in Multimodal Large Language Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-03 00:00:00+0900+0900">2026년 2월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance">[논문리뷰] TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance">Daniil Grebenkin이 arXiv에 게시한 &#x27;TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-02 00:00:00+0900+0900">2026년 2월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models">[논문리뷰] Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models">arXiv에 게시된 &#x27;Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents">[논문리뷰] DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents">arXiv에 게시된 &#x27;DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts">[논문리뷰] AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts">arXiv에 게시된 &#x27;AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking">[논문리뷰] AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs&#x27; Contextual and Cultural Knowledge and Thinking</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking">arXiv에 게시된 &#x27;AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs&#x27; Contextual and Cultural Knowledge and Thinking&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-28 00:00:00+0900+0900">2026년 1월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents">[논문리뷰] VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents">arXiv에 게시된 &#x27;VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-26 00:00:00+0900+0900">2026년 1월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing">[논문리뷰] Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing">Dingkun Long이 arXiv에 게시한 &#x27;Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-23 00:00:00+0900+0900">2026년 1월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents">[논문리뷰] MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents">Samiul Alam이 arXiv에 게시한 &#x27;MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-22 00:00:00+0900+0900">2026년 1월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents">[논문리뷰] ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents">arXiv에 게시된 &#x27;ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-21 00:00:00+0900+0900">2026년 1월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs">[논문리뷰] FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs">arXiv에 게시된 &#x27;FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-21 00:00:00+0900+0900">2026년 1월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems">[논문리뷰] AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems">Xipeng Qiu이 arXiv에 게시한 &#x27;AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-19 00:00:00+0900+0900">2026년 1월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning">[논문리뷰] Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning">Shuo Zhang이 arXiv에 게시한 &#x27;Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-13 00:00:00+0900+0900">2026년 1월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving">[논문리뷰] DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving">arXiv에 게시된 &#x27;DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-13 00:00:00+0900+0900">2026년 1월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language">[논문리뷰] BabyVision: Visual Reasoning Beyond Language</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language">Yiyan Liang이 arXiv에 게시한 &#x27;BabyVision: Visual Reasoning Beyond Language&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-13 00:00:00+0900+0900">2026년 1월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning">[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning">Guanchen Wu이 arXiv에 게시한 &#x27;EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-08 00:00:00+0900+0900">2026년 1월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web">[논문리뷰] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web">Kaixin Liang이 arXiv에 게시한 &#x27;Video-BrowseComp: Benchmarking Agentic Video Research on Open Web&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-30 00:00:00+0900+0900">2025년 12월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs">[논문리뷰] VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs">Xihui Liu이 arXiv에 게시한 &#x27;VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-30 00:00:00+0900+0900">2025년 12월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning">[논문리뷰] SVBench: Evaluation of Video Generation Models on Social Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning">Xiaojie Xu이 arXiv에 게시한 &#x27;SVBench: Evaluation of Video Generation Models on Social Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-29 00:00:00+0900+0900">2025년 12월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior">[논문리뷰] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior">arXiv에 게시된 &#x27;TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-25 00:00:00+0900+0900">2025년 12월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation">[논문리뷰] T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation">arXiv에 게시된 &#x27;T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-25 00:00:00+0900+0900">2025년 12월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs">[논문리뷰] SpatialTree: How Spatial Abilities Branch Out in MLLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs">arXiv에 게시된 &#x27;SpatialTree: How Spatial Abilities Branch Out in MLLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-24 00:00:00+0900+0900">2025년 12월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering">[논문리뷰] HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering">arXiv에 게시된 &#x27;HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-22 00:00:00+0900+0900">2025년 12월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation">[논문리뷰] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation">arXiv에 게시된 &#x27;GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-22 00:00:00+0900+0900">2025년 12월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks">[논문리뷰] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks">arXiv에 게시된 &#x27;VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-19 00:00:00+0900+0900">2025년 12월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image">[논문리뷰] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image">arXiv에 게시된 &#x27;Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-19 00:00:00+0900+0900">2025년 12월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression">[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression">arXiv에 게시된 &#x27;VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-18 00:00:00+0900+0900">2025년 12월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents">[논문리뷰] NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents">chongyang09이 arXiv에 게시한 &#x27;NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-16 00:00:00+0900+0900">2025년 12월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce">[논문리뷰] EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce">arXiv에 게시된 &#x27;EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-10 00:00:00+0900+0900">2025년 12월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation">[논문리뷰] OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation">Simeng Qin이 arXiv에 게시한 &#x27;OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-09 00:00:00+0900+0900">2025년 12월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing">[논문리뷰] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing">arXiv에 게시된 &#x27;EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-09 00:00:00+0900+0900">2025년 12월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI">[논문리뷰] PAI-Bench: A Comprehensive Benchmark For Physical AI</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI">Humphrey Shi이 arXiv에 게시한 &#x27;PAI-Bench: A Comprehensive Benchmark For Physical AI&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos">[논문리뷰] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos">arXiv에 게시된 &#x27;StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages">[논문리뷰] IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages">arXiv에 게시된 &#x27;IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists">[논문리뷰] RefineBench: Evaluating Refinement Capability of Language Models via Checklists</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists">arXiv에 게시된 &#x27;RefineBench: Evaluating Refinement Capability of Language Models via Checklists&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-01 00:00:00+0900+0900">2025년 12월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model">[논문리뷰] OralGPT-Omni: A Versatile Dental Multimodal Large Language Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model">arXiv에 게시된 &#x27;OralGPT-Omni: A Versatile Dental Multimodal Large Language Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-01 00:00:00+0900+0900">2025년 12월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following">[논문리뷰] Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following">arXiv에 게시된 &#x27;Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-28 00:00:00+0900+0900">2025년 11월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering">[논문리뷰] VQ-VA World: Towards High-Quality Visual Question-Visual Answering</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering">Feng Li이 arXiv에 게시한 &#x27;VQ-VA World: Towards High-Quality Visual Question-Visual Answering&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-26 00:00:00+0900+0900">2025년 11월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection">[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection">Mike Zheng Shou이 arXiv에 게시한 &#x27;DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-26 00:00:00+0900+0900">2025년 11월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets">[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets">Zhaowei Lu이 arXiv에 게시한 &#x27;Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-25 00:00:00+0900+0900">2025년 11월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning">[논문리뷰] AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning">Alphamasterliu이 arXiv에 게시한 &#x27;AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-25 00:00:00+0900+0900">2025년 11월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs">[논문리뷰] Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs">arXiv에 게시된 &#x27;Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-24 00:00:00+0900+0900">2025년 11월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks">[논문리뷰] Reasoning via Video: The First Evaluation of Video Models&#x27; Reasoning Abilities through Maze-Solving Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks">Yiran Peng이 arXiv에 게시한 &#x27;Reasoning via Video: The First Evaluation of Video Models&#x27; Reasoning Abilities through Maze-Solving Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-20 00:00:00+0900+0900">2025년 11월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning">[논문리뷰] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning">Yuqiang Li이 arXiv에 게시한 &#x27;ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-19 00:00:00+0900+0900">2025년 11월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models">[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models">Siyuan Li이 arXiv에 게시한 &#x27;GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-17 00:00:00+0900+0900">2025년 11월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents">[논문리뷰] ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents">arXiv에 게시된 &#x27;ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-14 00:00:00+0900+0900">2025년 11월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique">[논문리뷰] MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique">arXiv에 게시된 &#x27;MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-14 00:00:00+0900+0900">2025년 11월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains">[논문리뷰] Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains">arXiv에 게시된 &#x27;Too Good to be Bad: On the Failure of LLMs to Role-Play Villains&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-10 00:00:00+0900+0900">2025년 11월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents">[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents">arXiv에 게시된 &#x27;GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 22:08:24+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity">[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity">arXiv에 게시된 &#x27;MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 21:54:30+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation">[논문리뷰] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation">Soohyun Oh이 arXiv에 게시한 &#x27;LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 21:54:30+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought">[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought">arXiv에 게시된 &#x27;When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation">[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation">arXiv에 게시된 &#x27;VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs">[논문리뷰] RiddleBench: A New Generative Reasoning Benchmark for LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs">arXiv에 게시된 &#x27;RiddleBench: A New Generative Reasoning Benchmark for LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw">[논문리뷰] LTD-Bench: Evaluating Large Language Models by Letting Them Draw</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw">arXiv에 게시된 &#x27;LTD-Bench: Evaluating Large Language Models by Letting Them Draw&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models">[논문리뷰] Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models">Shijie Dong이 arXiv에 게시한 &#x27;Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark">[논문리뷰] UniREditBench: A Unified Reasoning-based Image Editing Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark">arXiv에 게시된 &#x27;UniREditBench: A Unified Reasoning-based Image Editing Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:22:42+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation">[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation">arXiv에 게시된 &#x27;The Quest for Generalizable Motion Generation: Data, Model, and Evaluation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment">[논문리뷰] ChartAB: A Benchmark for Chart Grounding &amp; Dense Alignment</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment">arXiv에 게시된 &#x27;ChartAB: A Benchmark for Chart Grounding &amp; Dense Alignment&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark">[논문리뷰] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark">arXiv에 게시된 &#x27;CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions">[논문리뷰] AMO-Bench: Large Language Models Still Struggle in High School Math Competitions</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions">arXiv에 게시된 &#x27;AMO-Bench: Large Language Models Still Struggle in High School Math Competitions&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains">[논문리뷰] BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains">arXiv에 게시된 &#x27;BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations">[논문리뷰] VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations">Jiayi Zhang이 arXiv에 게시한 &#x27;VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-29 13:11:02+0900">2025년 10월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence">[논문리뷰] STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence">arXiv에 게시된 &#x27;STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-29 13:11:02+0900">2025년 10월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding">[논문리뷰] PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding">Denis Cavallucci이 arXiv에 게시한 &#x27;PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-29 13:11:02+0900">2025년 10월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents">[논문리뷰] OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents">arXiv에 게시된 &#x27;OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-29 13:11:02+0900">2025년 10월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences">[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences">arXiv에 게시된 &#x27;Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models">[논문리뷰] SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models">arXiv에 게시된 &#x27;SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-24 13:04:16+0900">2025년 10월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding">[논문리뷰] Diff-XYZ: A Benchmark for Evaluating Diff Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding">arXiv에 게시된 &#x27;Diff-XYZ: A Benchmark for Evaluating Diff Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-24 13:04:16+0900">2025년 10월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents">[논문리뷰] DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents">arXiv에 게시된 &#x27;DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-23 13:08:59+0900">2025년 10월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation">[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation">Yujie Zhou이 arXiv에 게시한 &#x27;UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies">[논문리뷰] PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies">James Glass이 arXiv에 게시한 &#x27;PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues">[논문리뷰] MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues">arXiv에 게시된 &#x27;MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions">[논문리뷰] IF-VidCap: Can Video Caption Models Follow Instructions?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions">arXiv에 게시된 &#x27;IF-VidCap: Can Video Caption Models Follow Instructions?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence">[논문리뷰] DSI-Bench: A Benchmark for Dynamic Spatial Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence">arXiv에 게시된 &#x27;DSI-Bench: A Benchmark for Dynamic Spatial Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing">[논문리뷰] PICABench: How Far Are We from Physically Realistic Image Editing?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing">Kaiwen Zhu이 arXiv에 게시한 &#x27;PICABench: How Far Are We from Physically Realistic Image Editing?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-21 13:08:30+0900">2025년 10월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models">[논문리뷰] MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models">arXiv에 게시된 &#x27;MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-21 13:08:30+0900">2025년 10월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain">[논문리뷰] FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain">Arman Cohan이 arXiv에 게시한 &#x27;FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-20 13:04:24+0900">2025년 10월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning">[논문리뷰] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning">Ke Wang이 arXiv에 게시한 &#x27;MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-17 13:09:57+0900">2025년 10월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark">[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark">arXiv에 게시된 &#x27;Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs">[논문리뷰] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs">arXiv에 게시된 &#x27;ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model">[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model">Wieland Brendel이 arXiv에 게시한 &#x27;MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models">[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models">arXiv에 게시된 &#x27;LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math">[논문리뷰] Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math">arXiv에 게시된 &#x27;Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning">[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning">arXiv에 게시된 &#x27;ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-15 13:01:40+0900">2025년 10월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports">[논문리뷰] Understanding DeepResearch via Reports</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports">Chengen Huang이 arXiv에 게시한 &#x27;Understanding DeepResearch via Reports&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval">[논문리뷰] MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval">Tingyu Song이 arXiv에 게시한 &#x27;MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion">[논문리뷰] AutoPR: Let&#x27;s Automate Your Academic Promotion!</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion">Yixin Yuan이 arXiv에 게시한 &#x27;AutoPR: Let&#x27;s Automate Your Academic Promotion!&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG">[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG">arXiv에 게시된 &#x27;UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models">[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models">Mohit Bansal이 arXiv에 게시한 &#x27;SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization">[논문리뷰] MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization">vanilla1116이 arXiv에 게시한 &#x27;MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline">[논문리뷰] MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline">arXiv에 게시된 &#x27;MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-09 13:45:06+0900">2025년 10월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark">[논문리뷰] EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark">Tianwen Qian이 arXiv에 게시한 &#x27;EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-08 13:48:12+0900">2025년 10월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL">[논문리뷰] LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL">arXiv에 게시된 &#x27;LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys">[논문리뷰] SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys">Shuo Wang이 arXiv에 게시한 &#x27;SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-06 13:29:11+0900">2025년 10월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus">[논문리뷰] SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus">Zhonghao Zhang이 arXiv에 게시한 &#x27;SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-06 13:29:11+0900">2025년 10월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses">[논문리뷰] BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses">Julian McAuley이 arXiv에 게시한 &#x27;BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-02 13:30:22+0900">2025년 10월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap">[논문리뷰] Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap">Hengfan Zhang이 arXiv에 게시한 &#x27;Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes">[논문리뷰] VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes">Muhammad Huzaifa이 arXiv에 게시한 &#x27;VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use">[논문리뷰] MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use">arXiv에 게시된 &#x27;MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark">[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark">Yuran Wang이 arXiv에 게시한 &#x27;RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-30 13:52:24+0900">2025년 9월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling">[논문리뷰] EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling">arXiv에 게시된 &#x27;EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-30 13:52:24+0900">2025년 9월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models">[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models">Shawn Guo이 arXiv에 게시한 &#x27;V-GameGym: Visual Game Generation for Code Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models">[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models">Javad Lavaei이 arXiv에 게시한 &#x27;StyleBench: Evaluating thinking styles in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition">[논문리뷰] Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition">Chen Zhao이 arXiv에 게시한 &#x27;Does FLUX Already Know How to Perform Physically Plausible Image Composition?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback">[논문리뷰] BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback">Dongha Lee이 arXiv에 게시한 &#x27;BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction">[논문리뷰] VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction">So Fukuda이 arXiv에 게시한 &#x27;VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-24 13:14:19+0900">2025년 9월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation">[논문리뷰] OpenGVL - Benchmarking Visual Temporal Progress for Data Curation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation">Viktor Petrenko이 arXiv에 게시한 &#x27;OpenGVL - Benchmarking Visual Temporal Progress for Data Curation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-24 13:14:19+0900">2025년 9월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR">[논문리뷰] Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR">Zeina Aldallal이 arXiv에 게시한 &#x27;Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-24 13:14:19+0900">2025년 9월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery">[논문리뷰] VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery">Shiya Huang이 arXiv에 게시한 &#x27;VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks">[논문리뷰] SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks">Yannis Yiming He이 arXiv에 게시한 &#x27;SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects">[논문리뷰] CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects">Hang Yu이 arXiv에 게시한 &#x27;CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing">[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing">Jaeho Lee이 arXiv에 게시한 &#x27;AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations">[논문리뷰] ARE: Scaling Up Agent Environments and Evaluations</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations">Matteo Bettini이 arXiv에 게시한 &#x27;ARE: Scaling Up Agent Environments and Evaluations&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs">[논문리뷰] SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs">Zhun Wang이 arXiv에 게시한 &#x27;SteeringControl: Holistic Evaluation of Alignment Steering in LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-18 13:07:00+0900">2025년 9월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam">[논문리뷰] GenExam: A Multidisciplinary Text-to-Image Exam</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam">Yu Qiao이 arXiv에 게시한 &#x27;GenExam: A Multidisciplinary Text-to-Image Exam&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-18 13:07:00+0900">2025년 9월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models">[논문리뷰] Measuring Epistemic Humility in Multimodal Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models">Kaiyang Zhou이 arXiv에 게시한 &#x27;Measuring Epistemic Humility in Multimodal Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-16 13:16:41+0900">2025년 9월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions">[논문리뷰] VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions">Dong Zhang이 arXiv에 게시한 &#x27;VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-15 13:12:08+0900">2025년 9월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China">[논문리뷰] CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China">XU Han이 arXiv에 게시한 &#x27;CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-15 13:12:08+0900">2025년 9월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering">[논문리뷰] LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering">Jianguo Zhang이 arXiv에 게시한 &#x27;LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-12 13:12:46+0900">2025년 9월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark">[논문리뷰] FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark">Shuai Bai이 arXiv에 게시한 &#x27;FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-12 13:12:46+0900">2025년 9월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants">[논문리뷰] HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants">Jacy Reese Anthis이 arXiv에 게시한 &#x27;HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-11 13:02:36+0900">2025년 9월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge">[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge">Dipanjan Das이 arXiv에 게시한 &#x27;SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-10 13:11:01+0900">2025년 9월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents">[논문리뷰] MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents">Zhengxi Lu이 arXiv에 게시한 &#x27;MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-09 13:19:09+0900">2025년 9월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions">[논문리뷰] Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions">Yu Fu이 arXiv에 게시한 &#x27;Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-05 13:07:20+0900">2025년 9월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks">[논문리뷰] DeepResearch Arena: The First Exam of LLMs&#x27; Research Abilities via Seminar-Grounded Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks">Jiaxuan Lu이 arXiv에 게시한 &#x27;DeepResearch Arena: The First Exam of LLMs&#x27; Research Abilities via Seminar-Grounded Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-05 13:07:20+0900">2025년 9월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games">[논문리뷰] FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games">Dongmin Park이 arXiv에 게시한 &#x27;FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding">[논문리뷰] ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding">Xuanyu Zheng이 arXiv에 게시한 &#x27;ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks">[논문리뷰] ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks">Kai Jia이 arXiv에 게시한 &#x27;ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics">[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics">Dongchen Huang이 arXiv에 게시한 &#x27;CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods">[논문리뷰] SpotEdit: Evaluating Visually-Guided Image Editing Methods</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods">Ersin Yumer이 arXiv에 게시한 &#x27;SpotEdit: Evaluating Visually-Guided Image Editing Methods&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-26 13:21:57+0900">2025년 8월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions">[논문리뷰] AetherCode: Evaluating LLMs&#x27; Ability to Win In Premier Programming Competitions</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions">Yidi Du이 arXiv에 게시한 &#x27;AetherCode: Evaluating LLMs&#x27; Ability to Win In Premier Programming Competitions&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-25 13:13:07+0900">2025년 8월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior">[논문리뷰] INTIMA: A Benchmark for Human-AI Companionship Behavior</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior">Yacine Jernite이 arXiv에 게시한 &#x27;INTIMA: A Benchmark for Human-AI Companionship Behavior&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-22 13:10:52+0900">2025년 8월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References">[논문리뷰] MultiRef: Controllable Image Generation with Multiple Visual References</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References">Shiyun Lang이 arXiv에 게시한 &#x27;MultiRef: Controllable Image Generation with Multiple Visual References&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-20 13:26:54+0900">2025년 8월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence">[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence">Fernando López이 arXiv에 게시한 &#x27;MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-20 13:26:54+0900">2025년 8월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents">[논문리뷰] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents">Jun Dong이 arXiv에 게시한 &#x27;MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-20 13:26:54+0900">2025년 8월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds">[논문리뷰] HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds">Artyom Sorokin이 arXiv에 게시한 &#x27;HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-19 13:15:01+0900">2025년 8월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models">[논문리뷰] VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models">Dongdong Zhang이 arXiv에 게시한 &#x27;VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-14 13:19:02+0900">2025년 8월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking">[논문리뷰] WideSearch: Benchmarking Agentic Broad Info-Seeking</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking">Yan Gao이 arXiv에 게시한 &#x27;WideSearch: Benchmarking Agentic Broad Info-Seeking&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-12 13:29:09+0900">2025년 8월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding">[논문리뷰] VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding">Tong Yu이 arXiv에 게시한 &#x27;VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-12 13:29:09+0900">2025년 8월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes">[논문리뷰] MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes">Xudong Jiang이 arXiv에 게시한 &#x27;MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning">[논문리뷰] DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning">Ziming Wang이 arXiv에 게시한 &#x27;DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools">[논문리뷰] LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools">Yaojie Lu이 arXiv에 게시한 &#x27;LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-06 13:46:36+0900">2025년 8월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article></div></div></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[9038,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js\"],\"default\"]\n6:I[231,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js\"],\"\"]\n7:I[227,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js\"],\"default\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nb:I[9157,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"default\"]\nc:I[4080,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"\"]\ne:I[6130,[],\"\"]\n9:[\"tag\",\"Benchmark\",\"d\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"mJI0q5Z-SQWBtT83kG_N7\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/tags/Benchmark\",\"initialTree\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Benchmark\",\"d\"],{\"children\":[\"__PAGE__?{\\\"tag\\\":\\\"Benchmark\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Benchmark\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"$L5\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"CollectionPage\\\",\\\"name\\\":\\\"#Benchmark - secrett2633's blog\\\",\\\"description\\\":\\\"Benchmark 태그가 포함된 포스트 목록\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud/tags/Benchmark\\\",\\\"isPartOf\\\":{\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"inLanguage\\\":\\\"ko\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"#Benchmark\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/tags/Benchmark\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2741,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/tags/Benchmark\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"#Benchmark\"}]]}]]]}]}],[\"$\",\"h1\",null,{\"className\":\"page__title mb-6\",\"children\":[\"#\",\"Benchmark\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-500 mb-6\",\"children\":[158,\"개의 포스트\"]}],[\"$\",\"div\",null,{\"className\":\"entries-list\",\"children\":[[\"$\",\"article\",\"2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark\",\"children\":\"[논문리뷰] MAEB: Massive Audio Embedding Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark\",\"children\":\"arXiv에 게시된 'MAEB: Massive Audio Embedding Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-19 00:00:00+0900+0900\",\"children\":\"2026년 2월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2026-02-19-Learning-Situated-Awareness-in-the-Real-World\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World\",\"children\":\"[논문리뷰] Learning Situated Awareness in the Real World\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World\",\"children\":\"Rajiv Dhawan이 arXiv에 게시한 'Learning Situated Awareness in the Real World' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-19 00:00:00+0900+0900\",\"children\":\"2026년 2월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World\"}]]}]]}],[\"$\",\"article\",\"2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research\",\"children\":\"[논문리뷰] ResearchGym: Evaluating Language Model Agents on Real-World AI Research\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research\",\"children\":\"Arman Cohan이 arXiv에 게시한 'ResearchGym: Evaluating Language Model Agents on Real-World AI Research' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-18 00:00:00+0900+0900\",\"children\":\"2026년 2월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research\"}]]}]]}],[\"$\",\"article\",\"2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents\",\"children\":\"[논문리뷰] BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents\",\"children\":\"Yanzhe Dan이 arXiv에 게시한 'BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-17 00:00:00+0900+0900\",\"children\":\"2026년 2월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite\",\"children\":\"[논문리뷰] GENIUS: Generative Fluid Intelligence Evaluation Suite\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite\",\"children\":\"Zijun Shen이 arXiv에 게시한 'GENIUS: Generative Fluid Intelligence Evaluation Suite' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies\",\"children\":\"[논문리뷰] EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies\",\"children\":\"Yishuo Yuan이 arXiv에 게시한 'EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies\"}]]}]]}],[\"$\",\"article\",\"2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant\",\"children\":\"[논문리뷰] GISA: A Benchmark for General Information-Seeking Assistant\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant\",\"children\":\"arXiv에 게시된 'GISA: A Benchmark for General Information-Seeking Assistant' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-10 00:00:00+0900+0900\",\"children\":\"2026년 2월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant\"}]]}]]}],[\"$\",\"article\",\"2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments\",\"children\":\"[논문리뷰] GEBench: Benchmarking Image Generation Models as GUI Environments\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments\",\"children\":\"arXiv에 게시된 'GEBench: Benchmarking Image Generation Models as GUI Environments' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-10 00:00:00+0900+0900\",\"children\":\"2026년 2월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments\"}]]}]]}],[\"$\",\"article\",\"2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition\",\"children\":\"[논문리뷰] Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition\",\"children\":\"arXiv에 게시된 'Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-10 00:00:00+0900+0900\",\"children\":\"2026년 2월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition\"}]]}]]}],[\"$\",\"article\",\"2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks\",\"children\":\"[논문리뷰] PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks\",\"children\":\"Zhixin Wang이 arXiv에 게시한 'PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-09 00:00:00+0900+0900\",\"children\":\"2026년 2월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks\"}]]}]]}],[\"$\",\"article\",\"2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\",\"children\":\"[논문리뷰] Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\",\"children\":\"arXiv에 게시된 'Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\"}]]}]]}],[\"$\",\"article\",\"2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules\",\"children\":\"[논문리뷰] RISE-Video: Can Video Generators Decode Implicit World Rules?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules\",\"children\":\"Zicheng Zhang이 arXiv에 게시한 'RISE-Video: Can Video Generators Decode Implicit World Rules?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules\"}]]}]]}],[\"$\",\"article\",\"2026-02-05-HY3D-Bench-Generation-of-3D-Assets\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets\",\"children\":\"[논문리뷰] HY3D-Bench: Generation of 3D Assets\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets\",\"children\":\"arXiv에 게시된 'HY3D-Bench: Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-05 00:00:00+0900+0900\",\"children\":\"2026년 2월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets\"}]]}]]}],[\"$\",\"article\",\"2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles\",\"children\":\"[논문리뷰] Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles\",\"children\":\"arXiv에 게시된 'Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-03 00:00:00+0900+0900\",\"children\":\"2026년 2월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles\"}]]}]]}],[\"$\",\"article\",\"2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models\",\"children\":\"[논문리뷰] Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models\",\"children\":\"Shuang Chen이 arXiv에 게시한 'Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-03 00:00:00+0900+0900\",\"children\":\"2026년 2월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model\",\"children\":\"[논문리뷰] Toward Cognitive Supersensing in Multimodal Large Language Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model\",\"children\":\"Yifan Xu이 arXiv에 게시한 'Toward Cognitive Supersensing in Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-03 00:00:00+0900+0900\",\"children\":\"2026년 2월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model\"}]]}]]}],[\"$\",\"article\",\"2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance\",\"children\":\"[논문리뷰] TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance\",\"children\":\"Daniil Grebenkin이 arXiv에 게시한 'TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-02 00:00:00+0900+0900\",\"children\":\"2026년 2월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models\",\"children\":\"[논문리뷰] Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models\",\"children\":\"arXiv에 게시된 'Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents\",\"children\":\"[논문리뷰] DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents\",\"children\":\"arXiv에 게시된 'DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts\",\"children\":\"[논문리뷰] AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts\",\"children\":\"arXiv에 게시된 'AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts\"}]]}]]}],[\"$\",\"article\",\"2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking\",\"children\":\"[논문리뷰] AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking\",\"children\":\"arXiv에 게시된 'AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-28 00:00:00+0900+0900\",\"children\":\"2026년 1월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking\"}]]}]]}],[\"$\",\"article\",\"2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents\",\"children\":\"[논문리뷰] VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents\",\"children\":\"arXiv에 게시된 'VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-26 00:00:00+0900+0900\",\"children\":\"2026년 1월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing\",\"children\":\"[논문리뷰] Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing\",\"children\":\"Dingkun Long이 arXiv에 게시한 'Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-23 00:00:00+0900+0900\",\"children\":\"2026년 1월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing\"}]]}]]}],[\"$\",\"article\",\"2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents\",\"children\":\"[논문리뷰] MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents\",\"children\":\"Samiul Alam이 arXiv에 게시한 'MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-22 00:00:00+0900+0900\",\"children\":\"2026년 1월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\",\"children\":\"[논문리뷰] ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\",\"children\":\"arXiv에 게시된 'ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-21 00:00:00+0900+0900\",\"children\":\"2026년 1월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs\",\"children\":\"[논문리뷰] FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs\",\"children\":\"arXiv에 게시된 'FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-21 00:00:00+0900+0900\",\"children\":\"2026년 1월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs\"}]]}]]}],[\"$\",\"article\",\"2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems\",\"children\":\"[논문리뷰] AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems\",\"children\":\"Xipeng Qiu이 arXiv에 게시한 'AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-19 00:00:00+0900+0900\",\"children\":\"2026년 1월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems\"}]]}]]}],[\"$\",\"article\",\"2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning\",\"children\":\"[논문리뷰] Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning\",\"children\":\"Shuo Zhang이 arXiv에 게시한 'Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-13 00:00:00+0900+0900\",\"children\":\"2026년 1월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving\",\"children\":\"[논문리뷰] DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving\",\"children\":\"arXiv에 게시된 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-13 00:00:00+0900+0900\",\"children\":\"2026년 1월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving\"}]]}]]}],[\"$\",\"article\",\"2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language\",\"children\":\"[논문리뷰] BabyVision: Visual Reasoning Beyond Language\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language\",\"children\":\"Yiyan Liang이 arXiv에 게시한 'BabyVision: Visual Reasoning Beyond Language' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-13 00:00:00+0900+0900\",\"children\":\"2026년 1월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language\"}]]}]]}],[\"$\",\"article\",\"2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\",\"children\":\"[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\",\"children\":\"Guanchen Wu이 arXiv에 게시한 'EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-08 00:00:00+0900+0900\",\"children\":\"2026년 1월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web\",\"children\":\"[논문리뷰] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web\",\"children\":\"Kaixin Liang이 arXiv에 게시한 'Video-BrowseComp: Benchmarking Agentic Video Research on Open Web' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-30 00:00:00+0900+0900\",\"children\":\"2025년 12월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web\"}]]}]]}],[\"$\",\"article\",\"2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs\",\"children\":\"[논문리뷰] VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs\",\"children\":\"Xihui Liu이 arXiv에 게시한 'VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-30 00:00:00+0900+0900\",\"children\":\"2025년 12월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs\"}]]}]]}],[\"$\",\"article\",\"2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning\",\"children\":\"[논문리뷰] SVBench: Evaluation of Video Generation Models on Social Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning\",\"children\":\"Xiaojie Xu이 arXiv에 게시한 'SVBench: Evaluation of Video Generation Models on Social Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-29 00:00:00+0900+0900\",\"children\":\"2025년 12월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior\",\"children\":\"[논문리뷰] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior\",\"children\":\"arXiv에 게시된 'TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-25 00:00:00+0900+0900\",\"children\":\"2025년 12월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior\"}]]}]]}],[\"$\",\"article\",\"2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation\",\"children\":\"[논문리뷰] T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation\",\"children\":\"arXiv에 게시된 'T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-25 00:00:00+0900+0900\",\"children\":\"2025년 12월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs\",\"children\":\"[논문리뷰] SpatialTree: How Spatial Abilities Branch Out in MLLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs\",\"children\":\"arXiv에 게시된 'SpatialTree: How Spatial Abilities Branch Out in MLLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-24 00:00:00+0900+0900\",\"children\":\"2025년 12월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs\"}]]}]]}],[\"$\",\"article\",\"2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering\",\"children\":\"[논문리뷰] HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering\",\"children\":\"arXiv에 게시된 'HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-22 00:00:00+0900+0900\",\"children\":\"2025년 12월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering\"}]]}]]}],[\"$\",\"article\",\"2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation\",\"children\":\"[논문리뷰] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation\",\"children\":\"arXiv에 게시된 'GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-22 00:00:00+0900+0900\",\"children\":\"2025년 12월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation\"}]]}]]}],[\"$\",\"article\",\"2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks\",\"children\":\"[논문리뷰] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks\",\"children\":\"arXiv에 게시된 'VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-19 00:00:00+0900+0900\",\"children\":\"2025년 12월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image\",\"children\":\"[논문리뷰] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image\",\"children\":\"arXiv에 게시된 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-19 00:00:00+0900+0900\",\"children\":\"2025년 12월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image\"}]]}]]}],[\"$\",\"article\",\"2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\",\"children\":\"[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\",\"children\":\"arXiv에 게시된 'VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-18 00:00:00+0900+0900\",\"children\":\"2025년 12월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression\"}]]}]]}],[\"$\",\"article\",\"2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents\",\"children\":\"[논문리뷰] NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents\",\"children\":\"chongyang09이 arXiv에 게시한 'NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-16 00:00:00+0900+0900\",\"children\":\"2025년 12월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce\",\"children\":\"[논문리뷰] EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce\",\"children\":\"arXiv에 게시된 'EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-10 00:00:00+0900+0900\",\"children\":\"2025년 12월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce\"}]]}]]}],[\"$\",\"article\",\"2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation\",\"children\":\"[논문리뷰] OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation\",\"children\":\"Simeng Qin이 arXiv에 게시한 'OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-09 00:00:00+0900+0900\",\"children\":\"2025년 12월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation\"}]]}]]}],[\"$\",\"article\",\"2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing\",\"children\":\"[논문리뷰] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing\",\"children\":\"arXiv에 게시된 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-09 00:00:00+0900+0900\",\"children\":\"2025년 12월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI\",\"children\":\"[논문리뷰] PAI-Bench: A Comprehensive Benchmark For Physical AI\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI\",\"children\":\"Humphrey Shi이 arXiv에 게시한 'PAI-Bench: A Comprehensive Benchmark For Physical AI' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos\",\"children\":\"[논문리뷰] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos\",\"children\":\"arXiv에 게시된 'StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages\",\"children\":\"[논문리뷰] IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages\",\"children\":\"arXiv에 게시된 'IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages\"}]]}]]}],[\"$\",\"article\",\"2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists\",\"children\":\"[논문리뷰] RefineBench: Evaluating Refinement Capability of Language Models via Checklists\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists\",\"children\":\"arXiv에 게시된 'RefineBench: Evaluating Refinement Capability of Language Models via Checklists' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-01 00:00:00+0900+0900\",\"children\":\"2025년 12월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists\"}]]}]]}],[\"$\",\"article\",\"2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model\",\"children\":\"[논문리뷰] OralGPT-Omni: A Versatile Dental Multimodal Large Language Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model\",\"children\":\"arXiv에 게시된 'OralGPT-Omni: A Versatile Dental Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-01 00:00:00+0900+0900\",\"children\":\"2025년 12월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model\"}]]}]]}],[\"$\",\"article\",\"2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following\",\"children\":\"[논문리뷰] Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following\",\"children\":\"arXiv에 게시된 'Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-28 00:00:00+0900+0900\",\"children\":\"2025년 11월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following\"}]]}]]}],[\"$\",\"article\",\"2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering\",\"children\":\"[논문리뷰] VQ-VA World: Towards High-Quality Visual Question-Visual Answering\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering\",\"children\":\"Feng Li이 arXiv에 게시한 'VQ-VA World: Towards High-Quality Visual Question-Visual Answering' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-26 00:00:00+0900+0900\",\"children\":\"2025년 11월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering\"}]]}]]}],[\"$\",\"article\",\"2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection\",\"children\":\"[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection\",\"children\":\"Mike Zheng Shou이 arXiv에 게시한 'DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-26 00:00:00+0900+0900\",\"children\":\"2025년 11월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection\"}]]}]]}],[\"$\",\"article\",\"2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets\",\"children\":\"[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets\",\"children\":\"Zhaowei Lu이 arXiv에 게시한 'Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-25 00:00:00+0900+0900\",\"children\":\"2025년 11월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets\"}]]}]]}],[\"$\",\"article\",\"2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning\",\"children\":\"[논문리뷰] AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning\",\"children\":\"Alphamasterliu이 arXiv에 게시한 'AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-25 00:00:00+0900+0900\",\"children\":\"2025년 11월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs\",\"children\":\"[논문리뷰] Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs\",\"children\":\"arXiv에 게시된 'Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-24 00:00:00+0900+0900\",\"children\":\"2025년 11월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks\",\"children\":\"[논문리뷰] Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks\",\"children\":\"Yiran Peng이 arXiv에 게시한 'Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-20 00:00:00+0900+0900\",\"children\":\"2025년 11월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning\",\"children\":\"[논문리뷰] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning\",\"children\":\"Yuqiang Li이 arXiv에 게시한 'ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-19 00:00:00+0900+0900\",\"children\":\"2025년 11월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\",\"children\":\"[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\",\"children\":\"Siyuan Li이 arXiv에 게시한 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-17 00:00:00+0900+0900\",\"children\":\"2025년 11월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents\",\"children\":\"[논문리뷰] ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents\",\"children\":\"arXiv에 게시된 'ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-14 00:00:00+0900+0900\",\"children\":\"2025년 11월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique\",\"children\":\"[논문리뷰] MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique\",\"children\":\"arXiv에 게시된 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-14 00:00:00+0900+0900\",\"children\":\"2025년 11월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique\"}]]}]]}],[\"$\",\"article\",\"2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains\",\"children\":\"[논문리뷰] Too Good to be Bad: On the Failure of LLMs to Role-Play Villains\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains\",\"children\":\"arXiv에 게시된 'Too Good to be Bad: On the Failure of LLMs to Role-Play Villains' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-10 00:00:00+0900+0900\",\"children\":\"2025년 11월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains\"}]]}]]}],[\"$\",\"article\",\"2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents\",\"children\":\"[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents\",\"children\":\"arXiv에 게시된 'GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 22:08:24+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity\",\"children\":\"[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity\",\"children\":\"arXiv에 게시된 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 21:54:30+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity\"}]]}]]}],[\"$\",\"article\",\"2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation\",\"children\":\"[논문리뷰] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation\",\"children\":\"Soohyun Oh이 arXiv에 게시한 'LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 21:54:30+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\",\"children\":\"[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\",\"children\":\"arXiv에 게시된 'When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\",\"children\":\"[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\",\"children\":\"arXiv에 게시된 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs\",\"children\":\"[논문리뷰] RiddleBench: A New Generative Reasoning Benchmark for LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs\",\"children\":\"arXiv에 게시된 'RiddleBench: A New Generative Reasoning Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw\",\"children\":\"[논문리뷰] LTD-Bench: Evaluating Large Language Models by Letting Them Draw\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw\",\"children\":\"arXiv에 게시된 'LTD-Bench: Evaluating Large Language Models by Letting Them Draw' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models\",\"children\":\"[논문리뷰] Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models\",\"children\":\"Shijie Dong이 arXiv에 게시한 'Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark\",\"children\":\"[논문리뷰] UniREditBench: A Unified Reasoning-based Image Editing Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark\",\"children\":\"arXiv에 게시된 'UniREditBench: A Unified Reasoning-based Image Editing Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:22:42+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation\",\"children\":\"[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation\",\"children\":\"arXiv에 게시된 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment\",\"children\":\"[논문리뷰] ChartAB: A Benchmark for Chart Grounding \u0026 Dense Alignment\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment\",\"children\":\"arXiv에 게시된 'ChartAB: A Benchmark for Chart Grounding \u0026 Dense Alignment' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark\",\"children\":\"[논문리뷰] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark\",\"children\":\"arXiv에 게시된 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions\",\"children\":\"[논문리뷰] AMO-Bench: Large Language Models Still Struggle in High School Math Competitions\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions\",\"children\":\"arXiv에 게시된 'AMO-Bench: Large Language Models Still Struggle in High School Math Competitions' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains\",\"children\":\"[논문리뷰] BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains\",\"children\":\"arXiv에 게시된 'BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains\"}]]}]]}],[\"$\",\"article\",\"2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations\",\"children\":\"[논문리뷰] VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations\",\"children\":\"Jiayi Zhang이 arXiv에 게시한 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-29 13:11:02+0900\",\"children\":\"2025년 10월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations\"}]]}]]}],[\"$\",\"article\",\"2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence\",\"children\":\"[논문리뷰] STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence\",\"children\":\"arXiv에 게시된 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-29 13:11:02+0900\",\"children\":\"2025년 10월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding\",\"children\":\"[논문리뷰] PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding\",\"children\":\"Denis Cavallucci이 arXiv에 게시한 'PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-29 13:11:02+0900\",\"children\":\"2025년 10월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding\"}]]}]]}],[\"$\",\"article\",\"2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents\",\"children\":\"[논문리뷰] OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents\",\"children\":\"arXiv에 게시된 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-29 13:11:02+0900\",\"children\":\"2025년 10월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\",\"children\":\"[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\",\"children\":\"arXiv에 게시된 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences\"}]]}]]}],[\"$\",\"article\",\"2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models\",\"children\":\"[논문리뷰] SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models\",\"children\":\"arXiv에 게시된 'SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-24 13:04:16+0900\",\"children\":\"2025년 10월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding\",\"children\":\"[논문리뷰] Diff-XYZ: A Benchmark for Evaluating Diff Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding\",\"children\":\"arXiv에 게시된 'Diff-XYZ: A Benchmark for Evaluating Diff Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-24 13:04:16+0900\",\"children\":\"2025년 10월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents\",\"children\":\"[논문리뷰] DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents\",\"children\":\"arXiv에 게시된 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-23 13:08:59+0900\",\"children\":\"2025년 10월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\",\"children\":\"[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\",\"children\":\"Yujie Zhou이 arXiv에 게시한 'UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies\",\"children\":\"[논문리뷰] PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies\",\"children\":\"James Glass이 arXiv에 게시한 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues\",\"children\":\"[논문리뷰] MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues\",\"children\":\"arXiv에 게시된 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions\",\"children\":\"[논문리뷰] IF-VidCap: Can Video Caption Models Follow Instructions?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions\",\"children\":\"arXiv에 게시된 'IF-VidCap: Can Video Caption Models Follow Instructions?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence\",\"children\":\"[논문리뷰] DSI-Bench: A Benchmark for Dynamic Spatial Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence\",\"children\":\"arXiv에 게시된 'DSI-Bench: A Benchmark for Dynamic Spatial Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing\",\"children\":\"[논문리뷰] PICABench: How Far Are We from Physically Realistic Image Editing?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing\",\"children\":\"Kaiwen Zhu이 arXiv에 게시한 'PICABench: How Far Are We from Physically Realistic Image Editing?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-21 13:08:30+0900\",\"children\":\"2025년 10월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing\"}]]}]]}],[\"$\",\"article\",\"2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models\",\"children\":\"[논문리뷰] MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models\",\"children\":\"arXiv에 게시된 'MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-21 13:08:30+0900\",\"children\":\"2025년 10월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain\",\"children\":\"[논문리뷰] FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain\",\"children\":\"Arman Cohan이 arXiv에 게시한 'FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-20 13:04:24+0900\",\"children\":\"2025년 10월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain\"}]]}]]}],[\"$\",\"article\",\"2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning\",\"children\":\"[논문리뷰] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning\",\"children\":\"Ke Wang이 arXiv에 게시한 'MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-17 13:09:57+0900\",\"children\":\"2025년 10월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\",\"children\":\"[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\",\"children\":\"arXiv에 게시된 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs\",\"children\":\"[논문리뷰] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs\",\"children\":\"arXiv에 게시된 'ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\",\"children\":\"[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\",\"children\":\"Wieland Brendel이 arXiv에 게시한 'MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\",\"children\":\"[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\",\"children\":\"arXiv에 게시된 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math\",\"children\":\"[논문리뷰] Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math\",\"children\":\"arXiv에 게시된 'Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math\"}]]}]]}],[\"$\",\"article\",\"2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning\",\"children\":\"[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding \u0026 Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning\",\"children\":\"arXiv에 게시된 'ExpVid: A Benchmark for Experiment Video Understanding \u0026 Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-15 13:01:40+0900\",\"children\":\"2025년 10월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-Understanding-DeepResearch-via-Reports\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports\",\"children\":\"[논문리뷰] Understanding DeepResearch via Reports\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports\",\"children\":\"Chengen Huang이 arXiv에 게시한 'Understanding DeepResearch via Reports' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval\",\"children\":\"[논문리뷰] MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval\",\"children\":\"Tingyu Song이 arXiv에 게시한 'MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\",\"children\":\"[논문리뷰] AutoPR: Let's Automate Your Academic Promotion!\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\",\"children\":\"Yixin Yuan이 arXiv에 게시한 'AutoPR: Let's Automate Your Academic Promotion!' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\",\"children\":\"[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\",\"children\":\"arXiv에 게시된 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\",\"children\":\"[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\",\"children\":\"Mohit Bansal이 arXiv에 게시한 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization\",\"children\":\"[논문리뷰] MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization\",\"children\":\"vanilla1116이 arXiv에 게시한 'MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization\"}]]}]]}],[\"$\",\"article\",\"2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline\",\"children\":\"[논문리뷰] MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline\",\"children\":\"arXiv에 게시된 'MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-09 13:45:06+0900\",\"children\":\"2025년 10월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline\"}]]}]]}],[\"$\",\"article\",\"2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark\",\"children\":\"[논문리뷰] EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark\",\"children\":\"Tianwen Qian이 arXiv에 게시한 'EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-08 13:48:12+0900\",\"children\":\"2025년 10월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL\",\"children\":\"[논문리뷰] LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL\",\"children\":\"arXiv에 게시된 'LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL\"}]]}]]}],[\"$\",\"article\",\"2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys\",\"children\":\"[논문리뷰] SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys\",\"children\":\"Shuo Wang이 arXiv에 게시한 'SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-06 13:29:11+0900\",\"children\":\"2025년 10월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys\"}]]}]]}],[\"$\",\"article\",\"2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus\",\"children\":\"[논문리뷰] SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus\",\"children\":\"Zhonghao Zhang이 arXiv에 게시한 'SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-06 13:29:11+0900\",\"children\":\"2025년 10월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus\"}]]}]]}],[\"$\",\"article\",\"2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses\",\"children\":\"[논문리뷰] BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses\",\"children\":\"Julian McAuley이 arXiv에 게시한 'BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-02 13:30:22+0900\",\"children\":\"2025년 10월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap\",\"children\":\"[논문리뷰] Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap\",\"children\":\"Hengfan Zhang이 arXiv에 게시한 'Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes\",\"children\":\"[논문리뷰] VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes\",\"children\":\"Muhammad Huzaifa이 arXiv에 게시한 'VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use\",\"children\":\"[논문리뷰] MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use\",\"children\":\"arXiv에 게시된 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use\"}]]}]]}],[\"$\",\"article\",\"2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\",\"children\":\"[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\",\"children\":\"Yuran Wang이 arXiv에 게시한 'RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-30 13:52:24+0900\",\"children\":\"2025년 9월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling\",\"children\":\"[논문리뷰] EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling\",\"children\":\"arXiv에 게시된 'EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-30 13:52:24+0900\",\"children\":\"2025년 9월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models\",\"children\":\"[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models\",\"children\":\"Shawn Guo이 arXiv에 게시한 'V-GameGym: Visual Game Generation for Code Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\",\"children\":\"[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\",\"children\":\"Javad Lavaei이 arXiv에 게시한 'StyleBench: Evaluating thinking styles in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition\",\"children\":\"[논문리뷰] Does FLUX Already Know How to Perform Physically Plausible Image Composition?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition\",\"children\":\"Chen Zhao이 arXiv에 게시한 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback\",\"children\":\"[논문리뷰] BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback\",\"children\":\"Dongha Lee이 arXiv에 게시한 'BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback\"}]]}]]}],[\"$\",\"article\",\"2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction\",\"children\":\"[논문리뷰] VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction\",\"children\":\"So Fukuda이 arXiv에 게시한 'VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-24 13:14:19+0900\",\"children\":\"2025년 9월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction\"}]]}]]}],[\"$\",\"article\",\"2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation\",\"children\":\"[논문리뷰] OpenGVL - Benchmarking Visual Temporal Progress for Data Curation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation\",\"children\":\"Viktor Petrenko이 arXiv에 게시한 'OpenGVL - Benchmarking Visual Temporal Progress for Data Curation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-24 13:14:19+0900\",\"children\":\"2025년 9월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation\"}]]}]]}],[\"$\",\"article\",\"2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR\",\"children\":\"[논문리뷰] Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR\",\"children\":\"Zeina Aldallal이 arXiv에 게시한 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-24 13:14:19+0900\",\"children\":\"2025년 9월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery\",\"children\":\"[논문리뷰] VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery\",\"children\":\"Shiya Huang이 arXiv에 게시한 'VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks\",\"children\":\"[논문리뷰] SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks\",\"children\":\"Yannis Yiming He이 arXiv에 게시한 'SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects\",\"children\":\"[논문리뷰] CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects\",\"children\":\"Hang Yu이 arXiv에 게시한 'CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\",\"children\":\"[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\",\"children\":\"Jaeho Lee이 arXiv에 게시한 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations\",\"children\":\"[논문리뷰] ARE: Scaling Up Agent Environments and Evaluations\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations\",\"children\":\"Matteo Bettini이 arXiv에 게시한 'ARE: Scaling Up Agent Environments and Evaluations' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations\"}]]}]]}],[\"$\",\"article\",\"2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs\",\"children\":\"[논문리뷰] SteeringControl: Holistic Evaluation of Alignment Steering in LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs\",\"children\":\"Zhun Wang이 arXiv에 게시한 'SteeringControl: Holistic Evaluation of Alignment Steering in LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-18 13:07:00+0900\",\"children\":\"2025년 9월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam\",\"children\":\"[논문리뷰] GenExam: A Multidisciplinary Text-to-Image Exam\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam\",\"children\":\"Yu Qiao이 arXiv에 게시한 'GenExam: A Multidisciplinary Text-to-Image Exam' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-18 13:07:00+0900\",\"children\":\"2025년 9월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam\"}]]}]]}],[\"$\",\"article\",\"2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models\",\"children\":\"[논문리뷰] Measuring Epistemic Humility in Multimodal Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models\",\"children\":\"Kaiyang Zhou이 arXiv에 게시한 'Measuring Epistemic Humility in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-16 13:16:41+0900\",\"children\":\"2025년 9월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions\",\"children\":\"[논문리뷰] VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions\",\"children\":\"Dong Zhang이 arXiv에 게시한 'VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-15 13:12:08+0900\",\"children\":\"2025년 9월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions\"}]]}]]}],[\"$\",\"article\",\"2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China\",\"children\":\"[논문리뷰] CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China\",\"children\":\"XU Han이 arXiv에 게시한 'CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-15 13:12:08+0900\",\"children\":\"2025년 9월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China\"}]]}]]}],[\"$\",\"article\",\"2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering\",\"children\":\"[논문리뷰] LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering\",\"children\":\"Jianguo Zhang이 arXiv에 게시한 'LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-12 13:12:46+0900\",\"children\":\"2025년 9월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering\"}]]}]]}],[\"$\",\"article\",\"2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark\",\"children\":\"[논문리뷰] FLUX-Reason-6M \u0026 PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark\",\"children\":\"Shuai Bai이 arXiv에 게시한 'FLUX-Reason-6M \u0026 PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-12 13:12:46+0900\",\"children\":\"2025년 9월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark\"}]]}]]}],[\"$\",\"article\",\"2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants\",\"children\":\"[논문리뷰] HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants\",\"children\":\"Jacy Reese Anthis이 arXiv에 게시한 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-11 13:02:36+0900\",\"children\":\"2025년 9월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants\"}]]}]]}],[\"$\",\"article\",\"2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\",\"children\":\"[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\",\"children\":\"Dipanjan Das이 arXiv에 게시한 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-10 13:11:01+0900\",\"children\":\"2025년 9월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\"}]]}]]}],[\"$\",\"article\",\"2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents\",\"children\":\"[논문리뷰] MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents\",\"children\":\"Zhengxi Lu이 arXiv에 게시한 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-09 13:19:09+0900\",\"children\":\"2025년 9월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions\",\"children\":\"[논문리뷰] Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions\",\"children\":\"Yu Fu이 arXiv에 게시한 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-05 13:07:20+0900\",\"children\":\"2025년 9월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions\"}]]}]]}],[\"$\",\"article\",\"2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks\",\"children\":\"[논문리뷰] DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks\",\"children\":\"Jiaxuan Lu이 arXiv에 게시한 'DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-05 13:07:20+0900\",\"children\":\"2025년 9월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games\",\"children\":\"[논문리뷰] FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games\",\"children\":\"Dongmin Park이 arXiv에 게시한 'FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding\",\"children\":\"[논문리뷰] ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding\",\"children\":\"Xuanyu Zheng이 arXiv에 게시한 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks\",\"children\":\"[논문리뷰] ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks\",\"children\":\"Kai Jia이 arXiv에 게시한 'ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\",\"children\":\"[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\",\"children\":\"Dongchen Huang이 arXiv에 게시한 'CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\"}]]}]]}],[\"$\",\"article\",\"2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods\",\"children\":\"[논문리뷰] SpotEdit: Evaluating Visually-Guided Image Editing Methods\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods\",\"children\":\"Ersin Yumer이 arXiv에 게시한 'SpotEdit: Evaluating Visually-Guided Image Editing Methods' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-26 13:21:57+0900\",\"children\":\"2025년 8월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods\"}]]}]]}],[\"$\",\"article\",\"2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions\",\"children\":\"[논문리뷰] AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions\",\"children\":\"Yidi Du이 arXiv에 게시한 'AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-25 13:13:07+0900\",\"children\":\"2025년 8월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions\"}]]}]]}],[\"$\",\"article\",\"2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior\",\"children\":\"[논문리뷰] INTIMA: A Benchmark for Human-AI Companionship Behavior\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior\",\"children\":\"Yacine Jernite이 arXiv에 게시한 'INTIMA: A Benchmark for Human-AI Companionship Behavior' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-22 13:10:52+0900\",\"children\":\"2025년 8월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior\"}]]}]]}],[\"$\",\"article\",\"2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References\",\"children\":\"[논문리뷰] MultiRef: Controllable Image Generation with Multiple Visual References\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References\",\"children\":\"Shiyun Lang이 arXiv에 게시한 'MultiRef: Controllable Image Generation with Multiple Visual References' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-20 13:26:54+0900\",\"children\":\"2025년 8월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References\"}]]}]]}],[\"$\",\"article\",\"2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\",\"children\":\"[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\",\"children\":\"Fernando López이 arXiv에 게시한 'MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-20 13:26:54+0900\",\"children\":\"2025년 8월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents\",\"children\":\"[논문리뷰] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents\",\"children\":\"Jun Dong이 arXiv에 게시한 'MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-20 13:26:54+0900\",\"children\":\"2025년 8월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds\",\"children\":\"[논문리뷰] HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds\",\"children\":\"Artyom Sorokin이 arXiv에 게시한 'HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-19 13:15:01+0900\",\"children\":\"2025년 8월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds\"}]]}]]}],[\"$\",\"article\",\"2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models\",\"children\":\"[논문리뷰] VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models\",\"children\":\"Dongdong Zhang이 arXiv에 게시한 'VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-14 13:19:02+0900\",\"children\":\"2025년 8월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking\",\"children\":\"[논문리뷰] WideSearch: Benchmarking Agentic Broad Info-Seeking\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking\",\"children\":\"Yan Gao이 arXiv에 게시한 'WideSearch: Benchmarking Agentic Broad Info-Seeking' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-12 13:29:09+0900\",\"children\":\"2025년 8월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking\"}]]}]]}],[\"$\",\"article\",\"2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding\",\"children\":\"[논문리뷰] VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding\",\"children\":\"Tong Yu이 arXiv에 게시한 'VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-12 13:29:09+0900\",\"children\":\"2025년 8월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes\",\"children\":\"[논문리뷰] MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes\",\"children\":\"Xudong Jiang이 arXiv에 게시한 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning\",\"children\":\"[논문리뷰] DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning\",\"children\":\"Ziming Wang이 arXiv에 게시한 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools\",\"children\":\"[논문리뷰] LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools\",\"children\":\"Yaojie Lu이 arXiv에 게시한 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-06 13:46:36+0900\",\"children\":\"2025년 8월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools\"}]]}]]}]]}]]}]]}]}]]}]],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lc\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lc\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"#Benchmark - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Benchmark 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/tags/Benchmark\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"#Benchmark - secrett2633's blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"Benchmark 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/tags/Benchmark\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"#Benchmark - secrett2633's blog\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"Benchmark 태그가 포함된 포스트 목록\"}],[\"$\",\"link\",\"20\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"21\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>