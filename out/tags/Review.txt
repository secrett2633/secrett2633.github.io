2:I[9038,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
3:I[231,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],""]
4:I[227,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
5:I[9275,[],""]
7:I[1343,[],""]
8:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
9:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
6:["tag","Review","d"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":["tags",{"children":[["tag","Review","d"],{"children":["__PAGE__?{\"tag\":\"Review\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["tags",{"children":[["tag","Review","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"CollectionPage\",\"name\":\"#Review - secrett2633's blog\",\"description\":\"Review 태그가 포함된 포스트 목록\",\"url\":\"https://blog.secrett2633.cloud/tags/Review\",\"isPartOf\":{\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\"},\"inLanguage\":\"ko\"}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"#Review\",\"item\":\"https://blog.secrett2633.cloud/tags/Review\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L3",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L3",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L3",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L3",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L3",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L3",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L3",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L3",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L3",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L3",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L3",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L3",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L3",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L3",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/tags/Review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"#Review"}]]}]]]}]}],["$","h1",null,{"className":"page__title mb-6","children":["#","Review"]}],["$","p",null,{"className":"text-gray-500 mb-6","children":[2741,"개의 포스트"]}],["$","div",null,{"className":"entries-list","children":[["$","article","2026-02-19-World-Action-Models-are-Zero-shot-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-World-Action-Models-are-Zero-shot-Policies","children":"[논문리뷰] World Action Models are Zero-shot Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-World-Action-Models-are-Zero-shot-Policies","children":"arXiv에 게시된 'World Action Models are Zero-shot Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-World-Action-Models-are-Zero-shot-Policies"}]]}]]}],["$","article","2026-02-19-Visual-Memory-Injection-Attacks-for-Multi-Turn-Conversations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Visual-Memory-Injection-Attacks-for-Multi-Turn-Conversations","children":"[논문리뷰] Visual Memory Injection Attacks for Multi-Turn Conversations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Visual-Memory-Injection-Attacks-for-Multi-Turn-Conversations","children":"Matthias Hein이 arXiv에 게시한 'Visual Memory Injection Attacks for Multi-Turn Conversations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Visual-Memory-Injection-Attacks-for-Multi-Turn-Conversations"}]]}]]}],["$","article","2026-02-19-Towards-a-Science-of-AI-Agent-Reliability",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Towards-a-Science-of-AI-Agent-Reliability","children":"[논문리뷰] Towards a Science of AI Agent Reliability"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Towards-a-Science-of-AI-Agent-Reliability","children":"arXiv에 게시된 'Towards a Science of AI Agent Reliability' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Towards-a-Science-of-AI-Agent-Reliability"}]]}]]}],["$","article","2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","children":"[논문리뷰] SLA2: Sparse-Linear Attention with Learnable Routing and QAT"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT","children":"arXiv에 게시된 'SLA2: Sparse-Linear Attention with Learnable Routing and QAT' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-SLA2-Sparse-Linear-Attention-with-Learnable-Routing-and-QAT"}]]}]]}],["$","article","2026-02-19-SAM-3D-Body-Robust-Full-Body-Human-Mesh-Recovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SAM-3D-Body-Robust-Full-Body-Human-Mesh-Recovery","children":"[논문리뷰] SAM 3D Body: Robust Full-Body Human Mesh Recovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-SAM-3D-Body-Robust-Full-Body-Human-Mesh-Recovery","children":"Taosha Fan이 arXiv에 게시한 'SAM 3D Body: Robust Full-Body Human Mesh Recovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-SAM-3D-Body-Robust-Full-Body-Human-Mesh-Recovery"}]]}]]}],["$","article","2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation","children":"[논문리뷰] Optimizing Few-Step Generation with Adaptive Matching Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation","children":"arXiv에 게시된 'Optimizing Few-Step Generation with Adaptive Matching Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Optimizing-Few-Step-Generation-with-Adaptive-Matching-Distillation"}]]}]]}],["$","article","2026-02-19-Multi-agent-cooperation-through-in-context-co-player-inference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Multi-agent-cooperation-through-in-context-co-player-inference","children":"[논문리뷰] Multi-agent cooperation through in-context co-player inference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Multi-agent-cooperation-through-in-context-co-player-inference","children":"arXiv에 게시된 'Multi-agent cooperation through in-context co-player inference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Multi-agent-cooperation-through-in-context-co-player-inference"}]]}]]}],["$","article","2026-02-19-MMA-Multimodal-Memory-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-MMA-Multimodal-Memory-Agent","children":"[논문리뷰] MMA: Multimodal Memory Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-MMA-Multimodal-Memory-Agent","children":"arXiv에 게시된 'MMA: Multimodal Memory Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-MMA-Multimodal-Memory-Agent"}]]}]]}],["$","article","2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark","children":"[논문리뷰] MAEB: Massive Audio Embedding Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark","children":"arXiv에 게시된 'MAEB: Massive Audio Embedding Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-MAEB-Massive-Audio-Embedding-Benchmark"}]]}]]}],["$","article","2026-02-19-Learning-Situated-Awareness-in-the-Real-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World","children":"[논문리뷰] Learning Situated Awareness in the Real World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World","children":"Rajiv Dhawan이 arXiv에 게시한 'Learning Situated Awareness in the Real World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Learning-Situated-Awareness-in-the-Real-World"}]]}]]}],["$","article","2026-02-19-Learning-Humanoid-End-Effector-Control-for-Open-Vocabulary-Visual-Loco-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Learning-Humanoid-End-Effector-Control-for-Open-Vocabulary-Visual-Loco-Manipulation","children":"[논문리뷰] Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Learning-Humanoid-End-Effector-Control-for-Open-Vocabulary-Visual-Loco-Manipulation","children":"arXiv에 게시된 'Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Learning-Humanoid-End-Effector-Control-for-Open-Vocabulary-Visual-Loco-Manipulation"}]]}]]}],["$","article","2026-02-19-Empty-Shelves-or-Lost-Keys-Recall-Is-the-Bottleneck-for-Parametric-Factuality",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Empty-Shelves-or-Lost-Keys-Recall-Is-the-Bottleneck-for-Parametric-Factuality","children":"[논문리뷰] Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-Empty-Shelves-or-Lost-Keys-Recall-Is-the-Bottleneck-for-Parametric-Factuality","children":"arXiv에 게시된 'Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-Empty-Shelves-or-Lost-Keys-Recall-Is-the-Bottleneck-for-Parametric-Factuality"}]]}]]}],["$","article","2026-02-19-BiManiBench-A-Hierarchical-Benchmark-for-Evaluating-Bimanual-Coordination-of-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-BiManiBench-A-Hierarchical-Benchmark-for-Evaluating-Bimanual-Coordination-of-Multimodal-Large-Language-Models","children":"[논문리뷰] BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-19-BiManiBench-A-Hierarchical-Benchmark-for-Evaluating-Bimanual-Coordination-of-Multimodal-Large-Language-Models","children":"arXiv에 게시된 'BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-19 00:00:00+0900+0900","children":"2026년 2월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-19-BiManiBench-A-Hierarchical-Benchmark-for-Evaluating-Bimanual-Coordination-of-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2026-02-18-Visual-Persuasion-What-Influences-Decisions-of-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Visual-Persuasion-What-Influences-Decisions-of-Vision-Language-Models","children":"[논문리뷰] Visual Persuasion: What Influences Decisions of Vision-Language Models?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Visual-Persuasion-What-Influences-Decisions-of-Vision-Language-Models","children":"Nikhil Singh이 arXiv에 게시한 'Visual Persuasion: What Influences Decisions of Vision-Language Models?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Visual-Persuasion-What-Influences-Decisions-of-Vision-Language-Models"}]]}]]}],["$","article","2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling","children":"[논문리뷰] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling","children":"Animesh Sinha이 arXiv에 게시한 'UniT: Unified Multimodal Chain-of-Thought Test-time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling"}]]}]]}],["$","article","2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models","children":"[논문리뷰] Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models","children":"Liwei Wang이 arXiv에 게시한 'Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Understanding-vs-Generation-Navigating-Optimization-Dilemma-in-Multimodal-Models"}]]}]]}],["$","article","2026-02-18-Sanity-Checks-for-Sparse-Autoencoders-Do-SAEs-Beat-Random-Baselines",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Sanity-Checks-for-Sparse-Autoencoders-Do-SAEs-Beat-Random-Baselines","children":"[논문리뷰] Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Sanity-Checks-for-Sparse-Autoencoders-Do-SAEs-Beat-Random-Baselines","children":"Ivan Oseledets이 arXiv에 게시한 'Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Sanity-Checks-for-Sparse-Autoencoders-Do-SAEs-Beat-Random-Baselines"}]]}]]}],["$","article","2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens","children":"[논문리뷰] STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens","children":"Zhilong Zheng이 arXiv에 게시한 'STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens"}]]}]]}],["$","article","2026-02-18-Revisiting-the-Platonic-Representation-Hypothesis-An-Aristotelian-View",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Revisiting-the-Platonic-Representation-Hypothesis-An-Aristotelian-View","children":"[논문리뷰] Revisiting the Platonic Representation Hypothesis: An Aristotelian View"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Revisiting-the-Platonic-Representation-Hypothesis-An-Aristotelian-View","children":"Maria Brbić이 arXiv에 게시한 'Revisiting the Platonic Representation Hypothesis: An Aristotelian View' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Revisiting-the-Platonic-Representation-Hypothesis-An-Aristotelian-View"}]]}]]}],["$","article","2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research","children":"[논문리뷰] ResearchGym: Evaluating Language Model Agents on Real-World AI Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research","children":"Arman Cohan이 arXiv에 게시한 'ResearchGym: Evaluating Language Model Agents on Real-World AI Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-ResearchGym-Evaluating-Language-Model-Agents-on-Real-World-AI-Research"}]]}]]}],["$","article","2026-02-18-Prescriptive-Scaling-Reveals-the-Evolution-of-Language-Model-Capabilities",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Prescriptive-Scaling-Reveals-the-Evolution-of-Language-Model-Capabilities","children":"[논문리뷰] Prescriptive Scaling Reveals the Evolution of Language Model Capabilities"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Prescriptive-Scaling-Reveals-the-Evolution-of-Language-Model-Capabilities","children":"Sham Kakade이 arXiv에 게시한 'Prescriptive Scaling Reveals the Evolution of Language Model Capabilities' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Prescriptive-Scaling-Reveals-the-Evolution-of-Language-Model-Capabilities"}]]}]]}],["$","article","2026-02-18-On-Surprising-Effectiveness-of-Masking-Updates-in-Adaptive-Optimizers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-On-Surprising-Effectiveness-of-Masking-Updates-in-Adaptive-Optimizers","children":"[논문리뷰] On Surprising Effectiveness of Masking Updates in Adaptive Optimizers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-On-Surprising-Effectiveness-of-Masking-Updates-in-Adaptive-Optimizers","children":"arXiv에 게시된 'On Surprising Effectiveness of Masking Updates in Adaptive Optimizers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-On-Surprising-Effectiveness-of-Masking-Updates-in-Adaptive-Optimizers"}]]}]]}],["$","article","2026-02-18-Learning-Native-Continuation-for-Action-Chunking-Flow-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Learning-Native-Continuation-for-Action-Chunking-Flow-Policies","children":"[논문리뷰] Learning Native Continuation for Action Chunking Flow Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Learning-Native-Continuation-for-Action-Chunking-Flow-Policies","children":"Di Zhang이 arXiv에 게시한 'Learning Native Continuation for Action Chunking Flow Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Learning-Native-Continuation-for-Action-Chunking-Flow-Policies"}]]}]]}],["$","article","2026-02-18-Geometry-Aware-Rotary-Position-Embedding-for-Consistent-Video-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Geometry-Aware-Rotary-Position-Embedding-for-Consistent-Video-World-Model","children":"[논문리뷰] Geometry-Aware Rotary Position Embedding for Consistent Video World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Geometry-Aware-Rotary-Position-Embedding-for-Consistent-Video-World-Model","children":"arXiv에 게시된 'Geometry-Aware Rotary Position Embedding for Consistent Video World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Geometry-Aware-Rotary-Position-Embedding-for-Consistent-Video-World-Model"}]]}]]}],["$","article","2026-02-18-GLM-5-from-Vibe-Coding-to-Agentic-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-GLM-5-from-Vibe-Coding-to-Agentic-Engineering","children":"[논문리뷰] GLM-5: from Vibe Coding to Agentic Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-GLM-5-from-Vibe-Coding-to-Agentic-Engineering","children":"GLM-5 Team이 arXiv에 게시한 'GLM-5: from Vibe Coding to Agentic Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-GLM-5-from-Vibe-Coding-to-Agentic-Engineering"}]]}]]}],["$","article","2026-02-18-Does-Socialization-Emerge-in-AI-Agent-Society-A-Case-Study-of-Moltbook",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Does-Socialization-Emerge-in-AI-Agent-Society-A-Case-Study-of-Moltbook","children":"[논문리뷰] Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Does-Socialization-Emerge-in-AI-Agent-Society-A-Case-Study-of-Moltbook","children":"Ming Li이 arXiv에 게시한 'Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Does-Socialization-Emerge-in-AI-Agent-Society-A-Case-Study-of-Moltbook"}]]}]]}],["$","article","2026-02-18-ClinAlign-Scaling-Healthcare-Alignment-from-Clinician-Preference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-ClinAlign-Scaling-Healthcare-Alignment-from-Clinician-Preference","children":"[논문리뷰] ClinAlign: Scaling Healthcare Alignment from Clinician Preference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-ClinAlign-Scaling-Healthcare-Alignment-from-Clinician-Preference","children":"Chaohe Zhang이 arXiv에 게시한 'ClinAlign: Scaling Healthcare Alignment from Clinician Preference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-ClinAlign-Scaling-Healthcare-Alignment-from-Clinician-Preference"}]]}]]}],["$","article","2026-02-18-Causal-JEPA-Learning-World-Models-through-Object-Level-Latent-Interventions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Causal-JEPA-Learning-World-Models-through-Object-Level-Latent-Interventions","children":"[논문리뷰] Causal-JEPA: Learning World Models through Object-Level Latent Interventions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-Causal-JEPA-Learning-World-Models-through-Object-Level-Latent-Interventions","children":"arXiv에 게시된 'Causal-JEPA: Learning World Models through Object-Level Latent Interventions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-Causal-JEPA-Learning-World-Models-through-Object-Level-Latent-Interventions"}]]}]]}],["$","article","2026-02-18-COMPOT-Calibration-Optimized-Matrix-Procrustes-Orthogonalization-for-Transformers-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-COMPOT-Calibration-Optimized-Matrix-Procrustes-Orthogonalization-for-Transformers-Compression","children":"[논문리뷰] COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-COMPOT-Calibration-Optimized-Matrix-Procrustes-Orthogonalization-for-Transformers-Compression","children":"arXiv에 게시된 'COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-COMPOT-Calibration-Optimized-Matrix-Procrustes-Orthogonalization-for-Transformers-Compression"}]]}]]}],["$","article","2026-02-17-UniWeTok-An-Unified-Binary-Tokenizer-with-Codebook-Size-2128-for-Unified-Multimodal-Large-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-UniWeTok-An-Unified-Binary-Tokenizer-with-Codebook-Size-2128-for-Unified-Multimodal-Large-Language-Model","children":"[논문리뷰] UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-UniWeTok-An-Unified-Binary-Tokenizer-with-Codebook-Size-2128-for-Unified-Multimodal-Large-Language-Model","children":"arXiv에 게시된 'UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-UniWeTok-An-Unified-Binary-Tokenizer-with-Codebook-Size-2128-for-Unified-Multimodal-Large-Language-Model"}]]}]]}],["$","article","2026-02-17-REDSearcher-A-Scalable-and-Cost-Efficient-Framework-for-Long-Horizon-Search-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-REDSearcher-A-Scalable-and-Cost-Efficient-Framework-for-Long-Horizon-Search-Agents","children":"[논문리뷰] REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-REDSearcher-A-Scalable-and-Cost-Efficient-Framework-for-Long-Horizon-Search-Agents","children":"arXiv에 게시된 'REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-REDSearcher-A-Scalable-and-Cost-Efficient-Framework-for-Long-Horizon-Search-Agents"}]]}]]}],["$","article","2026-02-17-Qute-Towards-Quantum-Native-Database",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Qute-Towards-Quantum-Native-Database","children":"[논문리뷰] Qute: Towards Quantum-Native Database"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Qute-Towards-Quantum-Native-Database","children":"Surui Tang이 arXiv에 게시한 'Qute: Towards Quantum-Native Database' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Qute-Towards-Quantum-Native-Database"}]]}]]}],["$","article","2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model","children":"[논문리뷰] Query as Anchor: Scenario-Adaptive User Representation via Large Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model","children":"arXiv에 게시된 'Query as Anchor: Scenario-Adaptive User Representation via Large Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model"}]]}]]}],["$","article","2026-02-17-Preliminary-sonification-of-ENSO-using-traditional-Javanese-gamelan-scales",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Preliminary-sonification-of-ENSO-using-traditional-Javanese-gamelan-scales","children":"[논문리뷰] Preliminary sonification of ENSO using traditional Javanese gamelan scales"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Preliminary-sonification-of-ENSO-using-traditional-Javanese-gamelan-scales","children":"arXiv에 게시된 'Preliminary sonification of ENSO using traditional Javanese gamelan scales' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Preliminary-sonification-of-ENSO-using-traditional-Javanese-gamelan-scales"}]]}]]}],["$","article","2026-02-17-Nanbeige4-1-3B-A-Small-General-Model-that-Reasons-Aligns-and-Acts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Nanbeige4-1-3B-A-Small-General-Model-that-Reasons-Aligns-and-Acts","children":"[논문리뷰] Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Nanbeige4-1-3B-A-Small-General-Model-that-Reasons-Aligns-and-Acts","children":"arXiv에 게시된 'Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Nanbeige4-1-3B-A-Small-General-Model-that-Reasons-Aligns-and-Acts"}]]}]]}],["$","article","2026-02-17-MoRL-Reinforced-Reasoning-for-Unified-Motion-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-MoRL-Reinforced-Reasoning-for-Unified-Motion-Understanding-and-Generation","children":"[논문리뷰] MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-MoRL-Reinforced-Reasoning-for-Unified-Motion-Understanding-and-Generation","children":"arXiv에 게시된 'MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-MoRL-Reinforced-Reasoning-for-Unified-Motion-Understanding-and-Generation"}]]}]]}],["$","article","2026-02-17-LaViDa-R1-Advancing-Reasoning-for-Unified-Multimodal-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-LaViDa-R1-Advancing-Reasoning-for-Unified-Multimodal-Diffusion-Language-Models","children":"[논문리뷰] LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-LaViDa-R1-Advancing-Reasoning-for-Unified-Multimodal-Diffusion-Language-Models","children":"arXiv에 게시된 'LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-LaViDa-R1-Advancing-Reasoning-for-Unified-Multimodal-Diffusion-Language-Models"}]]}]]}],["$","article","2026-02-17-InnoEval-On-Research-Idea-Evaluation-as-a-Knowledge-Grounded-Multi-Perspective-Reasoning-Problem",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-InnoEval-On-Research-Idea-Evaluation-as-a-Knowledge-Grounded-Multi-Perspective-Reasoning-Problem","children":"[논문리뷰] InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-InnoEval-On-Research-Idea-Evaluation-as-a-Knowledge-Grounded-Multi-Perspective-Reasoning-Problem","children":"arXiv에 게시된 'InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-InnoEval-On-Research-Idea-Evaluation-as-a-Knowledge-Grounded-Multi-Perspective-Reasoning-Problem"}]]}]]}],["$","article","2026-02-17-FireRed-Image-Edit-1-0-Techinical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-FireRed-Image-Edit-1-0-Techinical-Report","children":"[논문리뷰] FireRed-Image-Edit-1.0 Techinical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-FireRed-Image-Edit-1-0-Techinical-Report","children":"Cunzheng Wang이 arXiv에 게시한 'FireRed-Image-Edit-1.0 Techinical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-FireRed-Image-Edit-1-0-Techinical-Report"}]]}]]}],["$","article","2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks","children":"[논문리뷰] Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks","children":"arXiv에 게시된 'Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks"}]]}]]}],["$","article","2026-02-17-Experiential-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Experiential-Reinforcement-Learning","children":"[논문리뷰] Experiential Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Experiential-Reinforcement-Learning","children":"arXiv에 게시된 'Experiential Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Experiential-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-17-DeepImageSearch-Benchmarking-Multimodal-Agents-for-Context-Aware-Image-Retrieval-in-Visual-Histories",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-DeepImageSearch-Benchmarking-Multimodal-Agents-for-Context-Aware-Image-Retrieval-in-Visual-Histories","children":"[논문리뷰] DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-DeepImageSearch-Benchmarking-Multimodal-Agents-for-Context-Aware-Image-Retrieval-in-Visual-Histories","children":"arXiv에 게시된 'DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-DeepImageSearch-Benchmarking-Multimodal-Agents-for-Context-Aware-Image-Retrieval-in-Visual-Histories"}]]}]]}],["$","article","2026-02-17-Data-Darwinism-Part-I-Unlocking-the-Value-of-Scientific-Data-for-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Data-Darwinism-Part-I-Unlocking-the-Value-of-Scientific-Data-for-Pre-training","children":"[논문리뷰] Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Data-Darwinism-Part-I-Unlocking-the-Value-of-Scientific-Data-for-Pre-training","children":"arXiv에 게시된 'Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Data-Darwinism-Part-I-Unlocking-the-Value-of-Scientific-Data-for-Pre-training"}]]}]]}],["$","article","2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents","children":"[논문리뷰] BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents","children":"Yanzhe Dan이 arXiv에 게시한 'BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-BrowseComp-V3-A-Visual-Vertical-and-Verifiable-Benchmark-for-Multimodal-Browsing-Agents"}]]}]]}],["$","article","2026-02-17-Blind-to-the-Human-Touch-Overlap-Bias-in-LLM-Based-Summary-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Blind-to-the-Human-Touch-Overlap-Bias-in-LLM-Based-Summary-Evaluation","children":"[논문리뷰] Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Blind-to-the-Human-Touch-Overlap-Bias-in-LLM-Based-Summary-Evaluation","children":"Puneet Mathur이 arXiv에 게시한 'Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Blind-to-the-Human-Touch-Overlap-Bias-in-LLM-Based-Summary-Evaluation"}]]}]]}],["$","article","2026-02-17-BitDance-Scaling-Autoregressive-Generative-Models-with-Binary-Tokens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-BitDance-Scaling-Autoregressive-Generative-Models-with-Binary-Tokens","children":"[논문리뷰] BitDance: Scaling Autoregressive Generative Models with Binary Tokens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-BitDance-Scaling-Autoregressive-Generative-Models-with-Binary-Tokens","children":"Xuefeng Hu이 arXiv에 게시한 'BitDance: Scaling Autoregressive Generative Models with Binary Tokens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-BitDance-Scaling-Autoregressive-Generative-Models-with-Binary-Tokens"}]]}]]}],["$","article","2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation","children":"[논문리뷰] Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation","children":"Ryan Rossi이 arXiv에 게시한 'Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation"}]]}]]}],["$","article","2026-02-17-Acoustivision-Pro-An-Open-Source-Interactive-Platform-for-Room-Impulse-Response-Analysis-and-Acoustic-Characterization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Acoustivision-Pro-An-Open-Source-Interactive-Platform-for-Room-Impulse-Response-Analysis-and-Acoustic-Characterization","children":"[논문리뷰] Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-Acoustivision-Pro-An-Open-Source-Interactive-Platform-for-Room-Impulse-Response-Analysis-and-Acoustic-Characterization","children":"Mandip Goswami이 arXiv에 게시한 'Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-Acoustivision-Pro-An-Open-Source-Interactive-Platform-for-Room-Impulse-Response-Analysis-and-Acoustic-Characterization"}]]}]]}],["$","article","2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub","children":"[논문리뷰] AIDev: Studying AI Coding Agents on GitHub"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub","children":"Ahmed E. Hassan이 arXiv에 게시한 'AIDev: Studying AI Coding Agents on GitHub' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub"}]]}]]}],["$","article","2026-02-17-A-Critical-Look-at-Targeted-Instruction-Selection-Disentangling-What-Matters-and-What-Doesnt",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-A-Critical-Look-at-Targeted-Instruction-Selection-Disentangling-What-Matters-and-What-Doesnt","children":"[논문리뷰] A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-17-A-Critical-Look-at-Targeted-Instruction-Selection-Disentangling-What-Matters-and-What-Doesnt","children":"arXiv에 게시된 'A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-17 00:00:00+0900+0900","children":"2026년 2월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-17-A-Critical-Look-at-Targeted-Instruction-Selection-Disentangling-What-Matters-and-What-Doesnt"}]]}]]}],["$","article","2026-02-16-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception","children":"[논문리뷰] Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception","children":"arXiv에 게시된 'Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception"}]]}]]}],["$","article","2026-02-16-Xiaomi-Robotics-0-An-Open-Sourced-Vision-Language-Action-Model-with-Real-Time-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Xiaomi-Robotics-0-An-Open-Sourced-Vision-Language-Action-Model-with-Real-Time-Execution","children":"[논문리뷰] Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Xiaomi-Robotics-0-An-Open-Sourced-Vision-Language-Action-Model-with-Real-Time-Execution","children":"arXiv에 게시된 'Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-Xiaomi-Robotics-0-An-Open-Sourced-Vision-Language-Action-Model-with-Real-Time-Execution"}]]}]]}],["$","article","2026-02-16-What-does-RL-improve-for-Visual-Reasoning-A-Frankenstein-Style-Analysis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-What-does-RL-improve-for-Visual-Reasoning-A-Frankenstein-Style-Analysis","children":"[논문리뷰] What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-What-does-RL-improve-for-Visual-Reasoning-A-Frankenstein-Style-Analysis","children":"arXiv에 게시된 'What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-What-does-RL-improve-for-Visual-Reasoning-A-Frankenstein-Style-Analysis"}]]}]]}],["$","article","2026-02-16-Towards-Universal-Video-MLLMs-with-Attribute-Structured-and-Quality-Verified-Instructions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Towards-Universal-Video-MLLMs-with-Attribute-Structured-and-Quality-Verified-Instructions","children":"[논문리뷰] Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Towards-Universal-Video-MLLMs-with-Attribute-Structured-and-Quality-Verified-Instructions","children":"arXiv에 게시된 'Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-Towards-Universal-Video-MLLMs-with-Attribute-Structured-and-Quality-Verified-Instructions"}]]}]]}],["$","article","2026-02-16-Self-EvolveRec-Self-Evolving-Recommender-Systems-with-LLM-based-Directional-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Self-EvolveRec-Self-Evolving-Recommender-Systems-with-LLM-based-Directional-Feedback","children":"[논문리뷰] Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Self-EvolveRec-Self-Evolving-Recommender-Systems-with-LLM-based-Directional-Feedback","children":"Jimin Seo이 arXiv에 게시한 'Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-Self-EvolveRec-Self-Evolving-Recommender-Systems-with-LLM-based-Directional-Feedback"}]]}]]}],["$","article","2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents","children":"[논문리뷰] SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents","children":"Huayu Sha이 arXiv에 게시한 'SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-SciAgentGym-Benchmarking-Multi-Step-Scientific-Tool-use-in-LLM-Agents"}]]}]]}],["$","article","2026-02-16-RLinf-Co-Reinforcement-Learning-Based-Sim-Real-Co-Training-for-VLA-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-RLinf-Co-Reinforcement-Learning-Based-Sim-Real-Co-Training-for-VLA-Models","children":"[논문리뷰] RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-RLinf-Co-Reinforcement-Learning-Based-Sim-Real-Co-Training-for-VLA-Models","children":"arXiv에 게시된 'RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-RLinf-Co-Reinforcement-Learning-Based-Sim-Real-Co-Training-for-VLA-Models"}]]}]]}],["$","article","2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","children":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","children":"arXiv에 게시된 'OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence"}]]}]]}],["$","article","2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs","children":"[논문리뷰] On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs","children":"arXiv에 게시된 'On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs"}]]}]]}],["$","article","2026-02-16-MedXIAOHE-A-Comprehensive-Recipe-for-Building-Medical-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-MedXIAOHE-A-Comprehensive-Recipe-for-Building-Medical-MLLMs","children":"[논문리뷰] MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-MedXIAOHE-A-Comprehensive-Recipe-for-Building-Medical-MLLMs","children":"arXiv에 게시된 'MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-MedXIAOHE-A-Comprehensive-Recipe-for-Building-Medical-MLLMs"}]]}]]}],["$","article","2026-02-16-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs","children":"[논문리뷰] Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs","children":"Ninghao Liu이 arXiv에 게시한 'Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs"}]]}]]}],["$","article","2026-02-16-Learning-Image-based-Tree-Crown-Segmentation-from-Enhanced-Lidar-based-Pseudo-labels",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Learning-Image-based-Tree-Crown-Segmentation-from-Enhanced-Lidar-based-Pseudo-labels","children":"[논문리뷰] Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Learning-Image-based-Tree-Crown-Segmentation-from-Enhanced-Lidar-based-Pseudo-labels","children":"Xiaowei Yu이 arXiv에 게시한 'Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-Learning-Image-based-Tree-Crown-Segmentation-from-Enhanced-Lidar-based-Pseudo-labels"}]]}]]}],["$","article","2026-02-16-Intelligent-AI-Delegation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Intelligent-AI-Delegation","children":"[논문리뷰] Intelligent AI Delegation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-Intelligent-AI-Delegation","children":"arXiv에 게시된 'Intelligent AI Delegation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-Intelligent-AI-Delegation"}]]}]]}],["$","article","2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics","children":"[논문리뷰] GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics","children":"MingMing Cheng이 arXiv에 게시한 'GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics"}]]}]]}],["$","article","2026-02-16-FLAC-Maximum-Entropy-RL-via-Kinetic-Energy-Regularized-Bridge-Matching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-FLAC-Maximum-Entropy-RL-via-Kinetic-Energy-Regularized-Bridge-Matching","children":"[논문리뷰] FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-FLAC-Maximum-Entropy-RL-via-Kinetic-Energy-Regularized-Bridge-Matching","children":"Xiao Ma이 arXiv에 게시한 'FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-FLAC-Maximum-Entropy-RL-via-Kinetic-Energy-Regularized-Bridge-Matching"}]]}]]}],["$","article","2026-02-16-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels","children":"[논문리뷰] DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels","children":"Zhiqiang Tao이 arXiv에 게시한 'DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels"}]]}]]}],["$","article","2026-02-16-CoPE-VideoLM-Codec-Primitives-For-Efficient-Video-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-CoPE-VideoLM-Codec-Primitives-For-Efficient-Video-Language-Models","children":"[논문리뷰] CoPE-VideoLM: Codec Primitives For Efficient Video Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-CoPE-VideoLM-Codec-Primitives-For-Efficient-Video-Language-Models","children":"arXiv에 게시된 'CoPE-VideoLM: Codec Primitives For Efficient Video Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-CoPE-VideoLM-Codec-Primitives-For-Efficient-Video-Language-Models"}]]}]]}],["$","article","2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models","children":"[논문리뷰] BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models","children":"arXiv에 게시된 'BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models"}]]}]]}],["$","article","2026-02-16-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning","children":"[논문리뷰] ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning","children":"arXiv에 게시된 'ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning"}]]}]]}],["$","article","2026-02-13-χ_0-Resource-Aware-Robust-Manipulation-via-Taming-Distributional-Inconsistencies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-χ_0-Resource-Aware-Robust-Manipulation-via-Taming-Distributional-Inconsistencies","children":"[논문리뷰] χ_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-χ_0-Resource-Aware-Robust-Manipulation-via-Taming-Distributional-Inconsistencies","children":"arXiv에 게시된 'χ_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-χ_0-Resource-Aware-Robust-Manipulation-via-Taming-Distributional-Inconsistencies"}]]}]]}],["$","article","2026-02-13-dVoting-Fast-Voting-for-dLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-dVoting-Fast-Voting-for-dLLMs","children":"[논문리뷰] dVoting: Fast Voting for dLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-dVoting-Fast-Voting-for-dLLMs","children":"arXiv에 게시된 'dVoting: Fast Voting for dLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-dVoting-Fast-Voting-for-dLLMs"}]]}]]}],["$","article","2026-02-13-Unveiling-Implicit-Advantage-Symmetry-Why-GRPO-Struggles-with-Exploration-and-Difficulty-Adaptation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Unveiling-Implicit-Advantage-Symmetry-Why-GRPO-Struggles-with-Exploration-and-Difficulty-Adaptation","children":"[논문리뷰] Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Unveiling-Implicit-Advantage-Symmetry-Why-GRPO-Struggles-with-Exploration-and-Difficulty-Adaptation","children":"arXiv에 게시된 'Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Unveiling-Implicit-Advantage-Symmetry-Why-GRPO-Struggles-with-Exploration-and-Difficulty-Adaptation"}]]}]]}],["$","article","2026-02-13-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction","children":"[논문리뷰] Thinking with Drafting: Optical Decompression via Logical Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction","children":"arXiv에 게시된 'Thinking with Drafting: Optical Decompression via Logical Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction"}]]}]]}],["$","article","2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces","children":"[논문리뷰] ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces","children":"Julian McAuley이 arXiv에 게시한 'ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces"}]]}]]}],["$","article","2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning","children":"[논문리뷰] Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning","children":"arXiv에 게시된 'Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-13-The-Devil-Behind-Moltbook-Anthropic-Safety-is-Always-Vanishing-in-Self-Evolving-AI-Societies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-The-Devil-Behind-Moltbook-Anthropic-Safety-is-Always-Vanishing-in-Self-Evolving-AI-Societies","children":"[논문리뷰] The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-The-Devil-Behind-Moltbook-Anthropic-Safety-is-Always-Vanishing-in-Self-Evolving-AI-Societies","children":"Jinyu Hou이 arXiv에 게시한 'The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-The-Devil-Behind-Moltbook-Anthropic-Safety-is-Always-Vanishing-in-Self-Evolving-AI-Societies"}]]}]]}],["$","article","2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching","children":"[논문리뷰] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching","children":"arXiv에 게시된 'Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Stroke-of-Surprise-Progressive-Semantic-Illusions-in-Vector-Sketching"}]]}]]}],["$","article","2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation","children":"[논문리뷰] Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation","children":"Yukuan Xu이 arXiv에 게시한 'Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Sparse-Video-Generation-Propels-Real-World-Beyond-the-View-Vision-Language-Navigation"}]]}]]}],["$","article","2026-02-13-Sci-CoE-Co-evolving-Scientific-Reasoning-LLMs-via-Geometric-Consensus-with-Sparse-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Sci-CoE-Co-evolving-Scientific-Reasoning-LLMs-via-Geometric-Consensus-with-Sparse-Supervision","children":"[논문리뷰] Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Sci-CoE-Co-evolving-Scientific-Reasoning-LLMs-via-Geometric-Consensus-with-Sparse-Supervision","children":"arXiv에 게시된 'Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Sci-CoE-Co-evolving-Scientific-Reasoning-LLMs-via-Geometric-Consensus-with-Sparse-Supervision"}]]}]]}],["$","article","2026-02-13-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning","children":"[논문리뷰] ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning","children":"arXiv에 게시된 'ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning"}]]}]]}],["$","article","2026-02-13-RISE-Self-Improving-Robot-Policy-with-Compositional-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-RISE-Self-Improving-Robot-Policy-with-Compositional-World-Model","children":"[논문리뷰] RISE: Self-Improving Robot Policy with Compositional World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-RISE-Self-Improving-Robot-Policy-with-Compositional-World-Model","children":"arXiv에 게시된 'RISE: Self-Improving Robot Policy with Compositional World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-RISE-Self-Improving-Robot-Policy-with-Compositional-World-Model"}]]}]]}],["$","article","2026-02-13-Pretraining-A-Large-Language-Model-using-Distributed-GPUs-A-Memory-Efficient-Decentralized-Paradigm",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Pretraining-A-Large-Language-Model-using-Distributed-GPUs-A-Memory-Efficient-Decentralized-Paradigm","children":"[논문리뷰] Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Pretraining-A-Large-Language-Model-using-Distributed-GPUs-A-Memory-Efficient-Decentralized-Paradigm","children":"arXiv에 게시된 'Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Pretraining-A-Large-Language-Model-using-Distributed-GPUs-A-Memory-Efficient-Decentralized-Paradigm"}]]}]]}],["$","article","2026-02-13-NarraScore-Bridging-Visual-Narrative-and-Musical-Dynamics-via-Hierarchical-Affective-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-NarraScore-Bridging-Visual-Narrative-and-Musical-Dynamics-via-Hierarchical-Affective-Control","children":"[논문리뷰] NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-NarraScore-Bridging-Visual-Narrative-and-Musical-Dynamics-via-Hierarchical-Affective-Control","children":"arXiv에 게시된 'NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-NarraScore-Bridging-Visual-Narrative-and-Musical-Dynamics-via-Hierarchical-Affective-Control"}]]}]]}],["$","article","2026-02-13-MetaphorStar-Image-Metaphor-Understanding-and-Reasoning-with-End-to-End-Visual-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-MetaphorStar-Image-Metaphor-Understanding-and-Reasoning-with-End-to-End-Visual-Reinforcement-Learning","children":"[논문리뷰] MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-MetaphorStar-Image-Metaphor-Understanding-and-Reasoning-with-End-to-End-Visual-Reinforcement-Learning","children":"Hongsheng Li이 arXiv에 게시한 'MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-MetaphorStar-Image-Metaphor-Understanding-and-Reasoning-with-End-to-End-Visual-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-13-MOSS-Audio-Tokenizer-Scaling-Audio-Tokenizers-for-Future-Audio-Foundation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-MOSS-Audio-Tokenizer-Scaling-Audio-Tokenizers-for-Future-Audio-Foundation-Models","children":"[논문리뷰] MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-MOSS-Audio-Tokenizer-Scaling-Audio-Tokenizers-for-Future-Audio-Foundation-Models","children":"arXiv에 게시된 'MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-MOSS-Audio-Tokenizer-Scaling-Audio-Tokenizers-for-Future-Audio-Foundation-Models"}]]}]]}],["$","article","2026-02-13-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation","children":"[논문리뷰] Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation","children":"arXiv에 게시된 'Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation"}]]}]]}],["$","article","2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments","children":"[논문리뷰] LawThinker: A Deep Research Legal Agent in Dynamic Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments","children":"arXiv에 게시된 'LawThinker: A Deep Research Legal Agent in Dynamic Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments"}]]}]]}],["$","article","2026-02-13-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning","children":"[논문리뷰] GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning","children":"arXiv에 게시된 'GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-13-EgoHumanoid-Unlocking-In-the-Wild-Loco-Manipulation-with-Robot-Free-Egocentric-Demonstration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-EgoHumanoid-Unlocking-In-the-Wild-Loco-Manipulation-with-Robot-Free-Egocentric-Demonstration","children":"[논문리뷰] EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-EgoHumanoid-Unlocking-In-the-Wild-Loco-Manipulation-with-Robot-Free-Egocentric-Demonstration","children":"Yinghui Li이 arXiv에 게시한 'EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-EgoHumanoid-Unlocking-In-the-Wild-Loco-Manipulation-with-Robot-Free-Egocentric-Demonstration"}]]}]]}],["$","article","2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit","children":"[논문리뷰] DeepSight: An All-in-One LM Safety Toolkit"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit","children":"arXiv에 게시된 'DeepSight: An All-in-One LM Safety Toolkit' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-DeepSight-An-All-in-One-LM-Safety-Toolkit"}]]}]]}],["$","article","2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing","children":"[논문리뷰] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing","children":"arXiv에 게시된 'DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing"}]]}]]}],["$","article","2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models","children":"[논문리뷰] Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models","children":"arXiv에 게시된 'Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models"}]]}]]}],["$","article","2026-02-13-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale","children":"[논문리뷰] Adapting Vision-Language Models for E-commerce Understanding at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale","children":"arXiv에 게시된 'Adapting Vision-Language Models for E-commerce Understanding at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale"}]]}]]}],["$","article","2026-02-12-When-to-Memorize-and-When-to-Stop-Gated-Recurrent-Memory-for-Long-Context-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-When-to-Memorize-and-When-to-Stop-Gated-Recurrent-Memory-for-Long-Context-Reasoning","children":"[논문리뷰] When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-When-to-Memorize-and-When-to-Stop-Gated-Recurrent-Memory-for-Long-Context-Reasoning","children":"arXiv에 게시된 'When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-When-to-Memorize-and-When-to-Stop-Gated-Recurrent-Memory-for-Long-Context-Reasoning"}]]}]]}],["$","article","2026-02-12-When-the-Prompt-Becomes-Visual-Vision-Centric-Jailbreak-Attacks-for-Large-Image-Editing-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-When-the-Prompt-Becomes-Visual-Vision-Centric-Jailbreak-Attacks-for-Large-Image-Editing-Models","children":"[논문리뷰] When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-When-the-Prompt-Becomes-Visual-Vision-Centric-Jailbreak-Attacks-for-Large-Image-Editing-Models","children":"arXiv에 게시된 'When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-When-the-Prompt-Becomes-Visual-Vision-Centric-Jailbreak-Attacks-for-Large-Image-Editing-Models"}]]}]]}],["$","article","2026-02-12-Towards-Autonomous-Mathematics-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research","children":"[논문리뷰] Towards Autonomous Mathematics Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research","children":"arXiv에 게시된 'Towards Autonomous Mathematics Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research"}]]}]]}],["$","article","2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions","children":"[논문리뷰] TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions","children":"arXiv에 게시된 'TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions"}]]}]]}],["$","article","2026-02-12-Stroke3D-Lifting-2D-strokes-into-rigged-3D-model-via-latent-diffusion-models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Stroke3D-Lifting-2D-strokes-into-rigged-3D-model-via-latent-diffusion-models","children":"[논문리뷰] Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Stroke3D-Lifting-2D-strokes-into-rigged-3D-model-via-latent-diffusion-models","children":"arXiv에 게시된 'Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Stroke3D-Lifting-2D-strokes-into-rigged-3D-model-via-latent-diffusion-models"}]]}]]}],["$","article","2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters","children":"[논문리뷰] Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters","children":"arXiv에 게시된 'Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Step-3-5-Flash-Open-Frontier-Level-Intelligence-with-11B-Active-Parameters"}]]}]]}],["$","article","2026-02-12-ROCKET-Rapid-Optimization-via-Calibration-guided-Knapsack-Enhanced-Truncation-for-Efficient-Model-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-ROCKET-Rapid-Optimization-via-Calibration-guided-Knapsack-Enhanced-Truncation-for-Efficient-Model-Compression","children":"[논문리뷰] ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-ROCKET-Rapid-Optimization-via-Calibration-guided-Knapsack-Enhanced-Truncation-for-Efficient-Model-Compression","children":"arXiv에 게시된 'ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-ROCKET-Rapid-Optimization-via-Calibration-guided-Knapsack-Enhanced-Truncation-for-Efficient-Model-Compression"}]]}]]}],["$","article","2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search","children":"[논문리뷰] QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search","children":"Hui Zhang이 arXiv에 게시한 'QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-QP-OneModel-A-Unified-Generative-LLM-for-Multi-Task-Query-Understanding-in-Xiaohongshu-Search"}]]}]]}],["$","article","2026-02-12-PhyCritic-Multimodal-Critic-Models-for-Physical-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-PhyCritic-Multimodal-Critic-Models-for-Physical-AI","children":"[논문리뷰] PhyCritic: Multimodal Critic Models for Physical AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-PhyCritic-Multimodal-Critic-Models-for-Physical-AI","children":"arXiv에 게시된 'PhyCritic: Multimodal Critic Models for Physical AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-PhyCritic-Multimodal-Critic-Models-for-Physical-AI"}]]}]]}],["$","article","2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","children":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","children":"arXiv에 게시된 'Online Causal Kalman Filtering for Stable and Effective Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization"}]]}]]}],["$","article","2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models","children":"[논문리뷰] Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models","children":"Zhen Fang이 arXiv에 게시한 'Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models"}]]}]]}],["$","article","2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite","children":"[논문리뷰] GENIUS: Generative Fluid Intelligence Evaluation Suite"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite","children":"Zijun Shen이 arXiv에 게시한 'GENIUS: Generative Fluid Intelligence Evaluation Suite' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-GENIUS-Generative-Fluid-Intelligence-Evaluation-Suite"}]]}]]}],["$","article","2026-02-12-G-LNS-Generative-Large-Neighborhood-Search-for-LLM-Based-Automatic-Heuristic-Design",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-G-LNS-Generative-Large-Neighborhood-Search-for-LLM-Based-Automatic-Heuristic-Design","children":"[논문리뷰] G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-G-LNS-Generative-Large-Neighborhood-Search-for-LLM-Based-Automatic-Heuristic-Design","children":"Liang Zeng이 arXiv에 게시한 'G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-G-LNS-Generative-Large-Neighborhood-Search-for-LLM-Based-Automatic-Heuristic-Design"}]]}]]}],["$","article","2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models","children":"[논문리뷰] Free(): Learning to Forget in Malloc-Only Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models","children":"arXiv에 게시된 'Free(): Learning to Forget in Malloc-Only Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models"}]]}]]}],["$","article","2026-02-12-FeatureBench-Benchmarking-Agentic-Coding-for-Complex-Feature-Development",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-FeatureBench-Benchmarking-Agentic-Coding-for-Complex-Feature-Development","children":"[논문리뷰] FeatureBench: Benchmarking Agentic Coding for Complex Feature Development"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-FeatureBench-Benchmarking-Agentic-Coding-for-Complex-Feature-Development","children":"Jiahe Wang이 arXiv에 게시한 'FeatureBench: Benchmarking Agentic Coding for Complex Feature Development' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-FeatureBench-Benchmarking-Agentic-Coding-for-Complex-Feature-Development"}]]}]]}],["$","article","2026-02-12-Ex-Omni-Enabling-3D-Facial-Animation-Generation-for-Omni-modal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Ex-Omni-Enabling-3D-Facial-Animation-Generation-for-Omni-modal-Large-Language-Models","children":"[논문리뷰] Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Ex-Omni-Enabling-3D-Facial-Animation-Generation-for-Omni-modal-Large-Language-Models","children":"Tianshu Yu이 arXiv에 게시한 'Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Ex-Omni-Enabling-3D-Facial-Animation-Generation-for-Omni-modal-Large-Language-Models"}]]}]]}],["$","article","2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies","children":"[논문리뷰] EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies","children":"Yishuo Yuan이 arXiv에 게시한 'EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-EcoGym-Evaluating-LLMs-for-Long-Horizon-Plan-and-Execute-in-Interactive-Economies"}]]}]]}],["$","article","2026-02-12-DataChef-Cooking-Up-Optimal-Data-Recipes-for-LLM-Adaptation-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-DataChef-Cooking-Up-Optimal-Data-Recipes-for-LLM-Adaptation-via-Reinforcement-Learning","children":"[논문리뷰] DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-DataChef-Cooking-Up-Optimal-Data-Recipes-for-LLM-Adaptation-via-Reinforcement-Learning","children":"Kai Chen이 arXiv에 게시한 'DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-DataChef-Cooking-Up-Optimal-Data-Recipes-for-LLM-Adaptation-via-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-12-Data-Repetition-Beats-Data-Scaling-in-Long-CoT-Supervised-Fine-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Data-Repetition-Beats-Data-Scaling-in-Long-CoT-Supervised-Fine-Tuning","children":"[논문리뷰] Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Data-Repetition-Beats-Data-Scaling-in-Long-CoT-Supervised-Fine-Tuning","children":"Yuki M. Asano이 arXiv에 게시한 'Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Data-Repetition-Beats-Data-Scaling-in-Long-CoT-Supervised-Fine-Tuning"}]]}]]}],["$","article","2026-02-12-CLI-Gym-Scalable-CLI-Task-Generation-via-Agentic-Environment-Inversion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-CLI-Gym-Scalable-CLI-Task-Generation-via-Agentic-Environment-Inversion","children":"[논문리뷰] CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-CLI-Gym-Scalable-CLI-Task-Generation-via-Agentic-Environment-Inversion","children":"Feiyang Pan이 arXiv에 게시한 'CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-CLI-Gym-Scalable-CLI-Task-Generation-via-Agentic-Environment-Inversion"}]]}]]}],["$","article","2026-02-12-Blockwise-Advantage-Estimation-for-Multi-Objective-RL-with-Verifiable-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Blockwise-Advantage-Estimation-for-Multi-Objective-RL-with-Verifiable-Rewards","children":"[논문리뷰] Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-Blockwise-Advantage-Estimation-for-Multi-Objective-RL-with-Verifiable-Rewards","children":"arXiv에 게시된 'Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-Blockwise-Advantage-Estimation-for-Multi-Objective-RL-with-Verifiable-Rewards"}]]}]]}],["$","article","2026-02-12-ASA-Training-Free-Representation-Engineering-for-Tool-Calling-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-ASA-Training-Free-Representation-Engineering-for-Tool-Calling-Agents","children":"[논문리뷰] ASA: Training-Free Representation Engineering for Tool-Calling Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-12-ASA-Training-Free-Representation-Engineering-for-Tool-Calling-Agents","children":"Hongwei Zeng이 arXiv에 게시한 'ASA: Training-Free Representation Engineering for Tool-Calling Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-12-ASA-Training-Free-Representation-Engineering-for-Tool-Calling-Agents"}]]}]]}],["$","article","2026-02-11-VideoWorld-2-Learning-Transferable-Knowledge-from-Real-world-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-VideoWorld-2-Learning-Transferable-Knowledge-from-Real-world-Videos","children":"[논문리뷰] VideoWorld 2: Learning Transferable Knowledge from Real-world Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-VideoWorld-2-Learning-Transferable-Knowledge-from-Real-world-Videos","children":"arXiv에 게시된 'VideoWorld 2: Learning Transferable Knowledge from Real-world Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-VideoWorld-2-Learning-Transferable-Knowledge-from-Real-world-Videos"}]]}]]}],["$","article","2026-02-11-VLA-JEPA-Enhancing-Vision-Language-Action-Model-with-Latent-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-VLA-JEPA-Enhancing-Vision-Language-Action-Model-with-Latent-World-Model","children":"[논문리뷰] VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-VLA-JEPA-Enhancing-Vision-Language-Action-Model-with-Latent-World-Model","children":"Zezhi Liu이 arXiv에 게시한 'VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-VLA-JEPA-Enhancing-Vision-Language-Action-Model-with-Latent-World-Model"}]]}]]}],["$","article","2026-02-11-UI-Venus-1-5-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-UI-Venus-1-5-Technical-Report","children":"[논문리뷰] UI-Venus-1.5 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-UI-Venus-1-5-Technical-Report","children":"arXiv에 게시된 'UI-Venus-1.5 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-UI-Venus-1-5-Technical-Report"}]]}]]}],["$","article","2026-02-11-TreeCUA-Efficiently-Scaling-GUI-Automation-with-Tree-Structured-Verifiable-Evolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-TreeCUA-Efficiently-Scaling-GUI-Automation-with-Tree-Structured-Verifiable-Evolution","children":"[논문리뷰] TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-TreeCUA-Efficiently-Scaling-GUI-Automation-with-Tree-Structured-Verifiable-Evolution","children":"Liming Zheng이 arXiv에 게시한 'TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-TreeCUA-Efficiently-Scaling-GUI-Automation-with-Tree-Structured-Verifiable-Evolution"}]]}]]}],["$","article","2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning","children":"[논문리뷰] SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning","children":"arXiv에 게시된 'SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-SkillRL-Evolving-Agents-via-Recursive-Skill-Augmented-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training","children":"[논문리뷰] ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training","children":"arXiv에 게시된 'ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-ScaleEnv-Scaling-Environment-Synthesis-from-Scratch-for-Generalist-Interactive-Tool-Use-Agent-Training"}]]}]]}],["$","article","2026-02-11-SCALE-Self-uncertainty-Conditioned-Adaptive-Looking-and-Execution-for-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SCALE-Self-uncertainty-Conditioned-Adaptive-Looking-and-Execution-for-Vision-Language-Action-Models","children":"[논문리뷰] SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SCALE-Self-uncertainty-Conditioned-Adaptive-Looking-and-Execution-for-Vision-Language-Action-Models","children":"arXiv에 게시된 'SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-SCALE-Self-uncertainty-Conditioned-Adaptive-Looking-and-Execution-for-Vision-Language-Action-Models"}]]}]]}],["$","article","2026-02-11-SAGE-Scalable-Agentic-3D-Scene-Generation-for-Embodied-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SAGE-Scalable-Agentic-3D-Scene-Generation-for-Embodied-AI","children":"[논문리뷰] SAGE: Scalable Agentic 3D Scene Generation for Embodied AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-SAGE-Scalable-Agentic-3D-Scene-Generation-for-Embodied-AI","children":"arXiv에 게시된 'SAGE: Scalable Agentic 3D Scene Generation for Embodied AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-SAGE-Scalable-Agentic-3D-Scene-Generation-for-Embodied-AI"}]]}]]}],["$","article","2026-02-11-Rethinking-Global-Text-Conditioning-in-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Rethinking-Global-Text-Conditioning-in-Diffusion-Transformers","children":"[논문리뷰] Rethinking Global Text Conditioning in Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Rethinking-Global-Text-Conditioning-in-Diffusion-Transformers","children":"Yuchen Liu이 arXiv에 게시한 'Rethinking Global Text Conditioning in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Rethinking-Global-Text-Conditioning-in-Diffusion-Transformers"}]]}]]}],["$","article","2026-02-11-Prism-Spectral-Aware-Block-Sparse-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Prism-Spectral-Aware-Block-Sparse-Attention","children":"[논문리뷰] Prism: Spectral-Aware Block-Sparse Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Prism-Spectral-Aware-Block-Sparse-Attention","children":"arXiv에 게시된 'Prism: Spectral-Aware Block-Sparse Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Prism-Spectral-Aware-Block-Sparse-Attention"}]]}]]}],["$","article","2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads","children":"[논문리뷰] P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads","children":"arXiv에 게시된 'P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-P1-VL-Bridging-Visual-Perception-and-Scientific-Reasoning-in-Physics-Olympiads"}]]}]]}],["$","article","2026-02-11-Olaf-World-Orienting-Latent-Actions-for-Video-World-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Olaf-World-Orienting-Latent-Actions-for-Video-World-Modeling","children":"[논문리뷰] Olaf-World: Orienting Latent Actions for Video World Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Olaf-World-Orienting-Latent-Actions-for-Video-World-Modeling","children":"Mike Zheng Shou이 arXiv에 게시한 'Olaf-World: Orienting Latent Actions for Video World Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Olaf-World-Orienting-Latent-Actions-for-Video-World-Modeling"}]]}]]}],["$","article","2026-02-11-OPUS-Towards-Efficient-and-Principled-Data-Selection-in-Large-Language-Model-Pre-training-in-Every-Iteration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-OPUS-Towards-Efficient-and-Principled-Data-Selection-in-Large-Language-Model-Pre-training-in-Every-Iteration","children":"[논문리뷰] OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-OPUS-Towards-Efficient-and-Principled-Data-Selection-in-Large-Language-Model-Pre-training-in-Every-Iteration","children":"arXiv에 게시된 'OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-OPUS-Towards-Efficient-and-Principled-Data-Selection-in-Large-Language-Model-Pre-training-in-Every-Iteration"}]]}]]}],["$","article","2026-02-11-Dynamic-Long-Context-Reasoning-over-Compressed-Memory-via-End-to-End-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Dynamic-Long-Context-Reasoning-over-Compressed-Memory-via-End-to-End-Reinforcement-Learning","children":"[논문리뷰] Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Dynamic-Long-Context-Reasoning-over-Compressed-Memory-via-End-to-End-Reinforcement-Learning","children":"arXiv에 게시된 'Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Dynamic-Long-Context-Reasoning-over-Compressed-Memory-via-End-to-End-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-11-Dr-MAS-Stable-Reinforcement-Learning-for-Multi-Agent-LLM-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Dr-MAS-Stable-Reinforcement-Learning-for-Multi-Agent-LLM-Systems","children":"[논문리뷰] Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Dr-MAS-Stable-Reinforcement-Learning-for-Multi-Agent-LLM-Systems","children":"arXiv에 게시된 'Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Dr-MAS-Stable-Reinforcement-Learning-for-Multi-Agent-LLM-Systems"}]]}]]}],["$","article","2026-02-11-DLLM-Searcher-Adapting-Diffusion-Large-Language-Model-for-Search-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-DLLM-Searcher-Adapting-Diffusion-Large-Language-Model-for-Search-Agents","children":"[논문리뷰] DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-DLLM-Searcher-Adapting-Diffusion-Large-Language-Model-for-Search-Agents","children":"arXiv에 게시된 'DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-DLLM-Searcher-Adapting-Diffusion-Large-Language-Model-for-Search-Agents"}]]}]]}],["$","article","2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss","children":"[논문리뷰] Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss","children":"arXiv에 게시된 'Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Condition-Errors-Refinement-in-Autoregressive-Image-Generation-with-Diffusion-Loss"}]]}]]}],["$","article","2026-02-11-Code2World-A-GUI-World-Model-via-Renderable-Code-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Code2World-A-GUI-World-Model-via-Renderable-Code-Generation","children":"[논문리뷰] Code2World: A GUI World Model via Renderable Code Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Code2World-A-GUI-World-Model-via-Renderable-Code-Generation","children":"arXiv에 게시된 'Code2World: A GUI World Model via Renderable Code Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Code2World-A-GUI-World-Model-via-Renderable-Code-Generation"}]]}]]}],["$","article","2026-02-11-Chain-of-Mindset-Reasoning-with-Adaptive-Cognitive-Modes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Chain-of-Mindset-Reasoning-with-Adaptive-Cognitive-Modes","children":"[논문리뷰] Chain of Mindset: Reasoning with Adaptive Cognitive Modes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Chain-of-Mindset-Reasoning-with-Adaptive-Cognitive-Modes","children":"arXiv에 게시된 'Chain of Mindset: Reasoning with Adaptive Cognitive Modes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Chain-of-Mindset-Reasoning-with-Adaptive-Cognitive-Modes"}]]}]]}],["$","article","2026-02-11-BagelVLA-Enhancing-Long-Horizon-Manipulation-via-Interleaved-Vision-Language-Action-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-BagelVLA-Enhancing-Long-Horizon-Manipulation-via-Interleaved-Vision-Language-Action-Generation","children":"[논문리뷰] BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-BagelVLA-Enhancing-Long-Horizon-Manipulation-via-Interleaved-Vision-Language-Action-Generation","children":"Xiaoyu Chen이 arXiv에 게시한 'BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-BagelVLA-Enhancing-Long-Horizon-Manipulation-via-Interleaved-Vision-Language-Action-Generation"}]]}]]}],["$","article","2026-02-11-Agent-World-Model-Infinity-Synthetic-Environments-for-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Agent-World-Model-Infinity-Synthetic-Environments-for-Agentic-Reinforcement-Learning","children":"[논문리뷰] Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Agent-World-Model-Infinity-Synthetic-Environments-for-Agentic-Reinforcement-Learning","children":"arXiv에 게시된 'Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Agent-World-Model-Infinity-Synthetic-Environments-for-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-11-Agent-Banana-High-Fidelity-Image-Editing-with-Agentic-Thinking-and-Tooling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Agent-Banana-High-Fidelity-Image-Editing-with-Agentic-Thinking-and-Tooling","children":"[논문리뷰] Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-11-Agent-Banana-High-Fidelity-Image-Editing-with-Agentic-Thinking-and-Tooling","children":"arXiv에 게시된 'Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-11 00:00:00+0900+0900","children":"2026년 2월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-11-Agent-Banana-High-Fidelity-Image-Editing-with-Agentic-Thinking-and-Tooling"}]]}]]}],["$","article","2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","children":"[논문리뷰] WorldCompass: Reinforcement Learning for Long-Horizon World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models","children":"arXiv에 게시된 'WorldCompass: Reinforcement Learning for Long-Horizon World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-WorldCompass-Reinforcement-Learning-for-Long-Horizon-World-Models"}]]}]]}],["$","article","2026-02-10-Weak-Driven-Learning-How-Weak-Agents-make-Strong-Agents-Stronger",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Weak-Driven-Learning-How-Weak-Agents-make-Strong-Agents-Stronger","children":"[논문리뷰] Weak-Driven Learning: How Weak Agents make Strong Agents Stronger"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Weak-Driven-Learning-How-Weak-Agents-make-Strong-Agents-Stronger","children":"arXiv에 게시된 'Weak-Driven Learning: How Weak Agents make Strong Agents Stronger' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Weak-Driven-Learning-How-Weak-Agents-make-Strong-Agents-Stronger"}]]}]]}],["$","article","2026-02-10-Towards-Bridging-the-Gap-between-Large-Scale-Pretraining-and-Efficient-Finetuning-for-Humanoid-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Towards-Bridging-the-Gap-between-Large-Scale-Pretraining-and-Efficient-Finetuning-for-Humanoid-Control","children":"[논문리뷰] Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Towards-Bridging-the-Gap-between-Large-Scale-Pretraining-and-Efficient-Finetuning-for-Humanoid-Control","children":"Yao Su이 arXiv에 게시한 'Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Towards-Bridging-the-Gap-between-Large-Scale-Pretraining-and-Efficient-Finetuning-for-Humanoid-Control"}]]}]]}],["$","article","2026-02-10-RelayGen-Intra-Generation-Model-Switching-for-Efficient-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-RelayGen-Intra-Generation-Model-Switching-for-Efficient-Reasoning","children":"[논문리뷰] RelayGen: Intra-Generation Model Switching for Efficient Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-RelayGen-Intra-Generation-Model-Switching-for-Efficient-Reasoning","children":"arXiv에 게시된 'RelayGen: Intra-Generation Model Switching for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-RelayGen-Intra-Generation-Model-Switching-for-Efficient-Reasoning"}]]}]]}],["$","article","2026-02-10-Recurrent-Depth-VLA-Implicit-Test-Time-Compute-Scaling-of-Vision-Language-Action-Models-via-Latent-Iterative-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Recurrent-Depth-VLA-Implicit-Test-Time-Compute-Scaling-of-Vision-Language-Action-Models-via-Latent-Iterative-Reasoning","children":"[논문리뷰] Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Recurrent-Depth-VLA-Implicit-Test-Time-Compute-Scaling-of-Vision-Language-Action-Models-via-Latent-Iterative-Reasoning","children":"arXiv에 게시된 'Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Recurrent-Depth-VLA-Implicit-Test-Time-Compute-Scaling-of-Vision-Language-Action-Models-via-Latent-Iterative-Reasoning"}]]}]]}],["$","article","2026-02-10-QuantaAlpha-An-Evolutionary-Framework-for-LLM-Driven-Alpha-Mining",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-QuantaAlpha-An-Evolutionary-Framework-for-LLM-Driven-Alpha-Mining","children":"[논문리뷰] QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-QuantaAlpha-An-Evolutionary-Framework-for-LLM-Driven-Alpha-Mining","children":"arXiv에 게시된 'QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-QuantaAlpha-An-Evolutionary-Framework-for-LLM-Driven-Alpha-Mining"}]]}]]}],["$","article","2026-02-10-Modality-Gap-Driven-Subspace-Alignment-Training-Paradigm-For-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Modality-Gap-Driven-Subspace-Alignment-Training-Paradigm-For-Multimodal-Large-Language-Models","children":"[논문리뷰] Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Modality-Gap-Driven-Subspace-Alignment-Training-Paradigm-For-Multimodal-Large-Language-Models","children":"Hanzhen Zhao이 arXiv에 게시한 'Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Modality-Gap-Driven-Subspace-Alignment-Training-Paradigm-For-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation","children":"[논문리뷰] MOVA: Towards Scalable and Synchronized Video-Audio Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation","children":"arXiv에 게시된 'MOVA: Towards Scalable and Synchronized Video-Audio Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-MOVA-Towards-Scalable-and-Synchronized-Video-Audio-Generation"}]]}]]}],["$","article","2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory","children":"[논문리뷰] Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory","children":"arXiv에 게시된 'Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Learning-Query-Aware-Budget-Tier-Routing-for-Runtime-Agent-Memory"}]]}]]}],["$","article","2026-02-10-LatentChem-From-Textual-CoT-to-Latent-Thinking-in-Chemical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-LatentChem-From-Textual-CoT-to-Latent-Thinking-in-Chemical-Reasoning","children":"[논문리뷰] LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-LatentChem-From-Textual-CoT-to-Latent-Thinking-in-Chemical-Reasoning","children":"Jia Zhang이 arXiv에 게시한 'LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-LatentChem-From-Textual-CoT-to-Latent-Thinking-in-Chemical-Reasoning"}]]}]]}],["$","article","2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth","children":"[논문리뷰] LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth","children":"arXiv에 게시된 'LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth"}]]}]]}],["$","article","2026-02-10-LLaDA2-1-Speeding-Up-Text-Diffusion-via-Token-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-LLaDA2-1-Speeding-Up-Text-Diffusion-via-Token-Editing","children":"[논문리뷰] LLaDA2.1: Speeding Up Text Diffusion via Token Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-LLaDA2-1-Speeding-Up-Text-Diffusion-via-Token-Editing","children":"arXiv에 게시된 'LLaDA2.1: Speeding Up Text Diffusion via Token Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-LLaDA2-1-Speeding-Up-Text-Diffusion-via-Token-Editing"}]]}]]}],["$","article","2026-02-10-InternAgent-1-5-A-Unified-Agentic-Framework-for-Long-Horizon-Autonomous-Scientific-Discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-InternAgent-1-5-A-Unified-Agentic-Framework-for-Long-Horizon-Autonomous-Scientific-Discovery","children":"[논문리뷰] InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-InternAgent-1-5-A-Unified-Agentic-Framework-for-Long-Horizon-Autonomous-Scientific-Discovery","children":"Xiangchao Yan이 arXiv에 게시한 'InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-InternAgent-1-5-A-Unified-Agentic-Framework-for-Long-Horizon-Autonomous-Scientific-Discovery"}]]}]]}],["$","article","2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant","children":"[논문리뷰] GISA: A Benchmark for General Information-Seeking Assistant"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant","children":"arXiv에 게시된 'GISA: A Benchmark for General Information-Seeking Assistant' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-GISA-A-Benchmark-for-General-Information-Seeking-Assistant"}]]}]]}],["$","article","2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments","children":"[논문리뷰] GEBench: Benchmarking Image Generation Models as GUI Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments","children":"arXiv에 게시된 'GEBench: Benchmarking Image Generation Models as GUI Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-GEBench-Benchmarking-Image-Generation-Models-as-GUI-Environments"}]]}]]}],["$","article","2026-02-10-Fundamental-Reasoning-Paradigms-Induce-Out-of-Domain-Generalization-in-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Fundamental-Reasoning-Paradigms-Induce-Out-of-Domain-Generalization-in-Language-Models","children":"[논문리뷰] Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Fundamental-Reasoning-Paradigms-Induce-Out-of-Domain-Generalization-in-Language-Models","children":"Maria Liakata이 arXiv에 게시한 'Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Fundamental-Reasoning-Paradigms-Induce-Out-of-Domain-Generalization-in-Language-Models"}]]}]]}],["$","article","2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition","children":"[논문리뷰] Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition","children":"arXiv에 게시된 'Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Demo-ICL-In-Context-Learning-for-Procedural-Video-Knowledge-Acquisition"}]]}]]}],["$","article","2026-02-10-Alleviating-Sparse-Rewards-by-Modeling-Step-Wise-and-Long-Term-Sampling-Effects-in-Flow-Based-GRPO",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Alleviating-Sparse-Rewards-by-Modeling-Step-Wise-and-Long-Term-Sampling-Effects-in-Flow-Based-GRPO","children":"[논문리뷰] Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-Alleviating-Sparse-Rewards-by-Modeling-Step-Wise-and-Long-Term-Sampling-Effects-in-Flow-Based-GRPO","children":"arXiv에 게시된 'Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-Alleviating-Sparse-Rewards-by-Modeling-Step-Wise-and-Long-Term-Sampling-Effects-in-Flow-Based-GRPO"}]]}]]}],["$","article","2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research","children":"[논문리뷰] AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research","children":"arXiv에 게시된 'AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-AgentCPM-Report-Interleaving-Drafting-and-Deepening-for-Open-Ended-Deep-Research"}]]}]]}],["$","article","2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents","children":"[논문리뷰] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents","children":"arXiv에 게시된 'AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-10 00:00:00+0900+0900","children":"2026년 2월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-10-AIRS-Bench-a-Suite-of-Tasks-for-Frontier-AI-Research-Science-Agents"}]]}]]}],["$","article","2026-02-09-Self-Improving-World-Modelling-with-Latent-Actions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Self-Improving-World-Modelling-with-Latent-Actions","children":"[논문리뷰] Self-Improving World Modelling with Latent Actions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Self-Improving-World-Modelling-with-Latent-Actions","children":"Anna Korhonen이 arXiv에 게시한 'Self-Improving World Modelling with Latent Actions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-Self-Improving-World-Modelling-with-Latent-Actions"}]]}]]}],["$","article","2026-02-09-Self-Improving-Multilingual-Long-Reasoning-via-Translation-Reasoning-Integrated-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Self-Improving-Multilingual-Long-Reasoning-via-Translation-Reasoning-Integrated-Training","children":"[논문리뷰] Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Self-Improving-Multilingual-Long-Reasoning-via-Translation-Reasoning-Integrated-Training","children":"Liqian Huang이 arXiv에 게시한 'Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-Self-Improving-Multilingual-Long-Reasoning-via-Translation-Reasoning-Integrated-Training"}]]}]]}],["$","article","2026-02-09-SEMA-Simple-yet-Effective-Learning-for-Multi-Turn-Jailbreak-Attacks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-SEMA-Simple-yet-Effective-Learning-for-Multi-Turn-Jailbreak-Attacks","children":"[논문리뷰] SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-SEMA-Simple-yet-Effective-Learning-for-Multi-Turn-Jailbreak-Attacks","children":"arXiv에 게시된 'SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-SEMA-Simple-yet-Effective-Learning-for-Multi-Turn-Jailbreak-Attacks"}]]}]]}],["$","article","2026-02-09-RaBiT-Residual-Aware-Binarization-Training-for-Accurate-and-Efficient-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-RaBiT-Residual-Aware-Binarization-Training-for-Accurate-and-Efficient-LLMs","children":"[논문리뷰] RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-RaBiT-Residual-Aware-Binarization-Training-for-Accurate-and-Efficient-LLMs","children":"arXiv에 게시된 'RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-RaBiT-Residual-Aware-Binarization-Training-for-Accurate-and-Efficient-LLMs"}]]}]]}],["$","article","2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks","children":"[논문리뷰] PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks","children":"Zhixin Wang이 arXiv에 게시한 'PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-PlanViz-Evaluating-Planning-Oriented-Image-Generation-and-Editing-for-Computer-Use-Tasks"}]]}]]}],["$","article","2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey","children":"[논문리뷰] POINTS-GUI-G: GUI-Grounding Journey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey","children":"Le Tian이 arXiv에 게시한 'POINTS-GUI-G: GUI-Grounding Journey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-POINTS-GUI-G-GUI-Grounding-Journey"}]]}]]}],["$","article","2026-02-09-On-the-Entropy-Dynamics-in-Reinforcement-Fine-Tuning-of-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-On-the-Entropy-Dynamics-in-Reinforcement-Fine-Tuning-of-Large-Language-Models","children":"[논문리뷰] On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-On-the-Entropy-Dynamics-in-Reinforcement-Fine-Tuning-of-Large-Language-Models","children":"Yanxi Chen이 arXiv에 게시한 'On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-On-the-Entropy-Dynamics-in-Reinforcement-Fine-Tuning-of-Large-Language-Models"}]]}]]}],["$","article","2026-02-09-OmniMoE-An-Efficient-MoE-by-Orchestrating-Atomic-Experts-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-OmniMoE-An-Efficient-MoE-by-Orchestrating-Atomic-Experts-at-Scale","children":"[논문리뷰] OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-OmniMoE-An-Efficient-MoE-by-Orchestrating-Atomic-Experts-at-Scale","children":"arXiv에 게시된 'OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-OmniMoE-An-Efficient-MoE-by-Orchestrating-Atomic-Experts-at-Scale"}]]}]]}],["$","article","2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions","children":"[논문리뷰] OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions","children":"heroding77이 arXiv에 게시한 'OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-OdysseyArena-Benchmarking-Large-Language-Models-For-Long-Horizon-Active-and-Inductive-Interactions"}]]}]]}],["$","article","2026-02-09-MemGUI-Bench-Benchmarking-Memory-of-Mobile-GUI-Agents-in-Dynamic-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-MemGUI-Bench-Benchmarking-Memory-of-Mobile-GUI-Agents-in-Dynamic-Environments","children":"[논문리뷰] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-MemGUI-Bench-Benchmarking-Memory-of-Mobile-GUI-Agents-in-Dynamic-Environments","children":"arXiv에 게시된 'MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-MemGUI-Bench-Benchmarking-Memory-of-Mobile-GUI-Agents-in-Dynamic-Environments"}]]}]]}],["$","article","2026-02-09-MSign-An-Optimizer-Preventing-Training-Instability-in-Large-Language-Models-via-Stable-Rank-Restoration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-MSign-An-Optimizer-Preventing-Training-Instability-in-Large-Language-Models-via-Stable-Rank-Restoration","children":"[논문리뷰] MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-MSign-An-Optimizer-Preventing-Training-Instability-in-Large-Language-Models-via-Stable-Rank-Restoration","children":"arXiv에 게시된 'MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-MSign-An-Optimizer-Preventing-Training-Instability-in-Large-Language-Models-via-Stable-Rank-Restoration"}]]}]]}],["$","article","2026-02-09-Judging-What-We-Cannot-Solve-A-Consequence-Based-Approach-for-Oracle-Free-Evaluation-of-Research-Level-Math",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Judging-What-We-Cannot-Solve-A-Consequence-Based-Approach-for-Oracle-Free-Evaluation-of-Research-Level-Math","children":"[논문리뷰] Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Judging-What-We-Cannot-Solve-A-Consequence-Based-Approach-for-Oracle-Free-Evaluation-of-Research-Level-Math","children":"Amit Agarwal이 arXiv에 게시한 'Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-Judging-What-We-Cannot-Solve-A-Consequence-Based-Approach-for-Oracle-Free-Evaluation-of-Research-Level-Math"}]]}]]}],["$","article","2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning","children":"[논문리뷰] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning","children":"arXiv에 게시된 'InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-09-Group-Evolving-Agents-Open-Ended-Self-Improvement-via-Experience-Sharing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Group-Evolving-Agents-Open-Ended-Self-Improvement-via-Experience-Sharing","children":"[논문리뷰] Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Group-Evolving-Agents-Open-Ended-Self-Improvement-via-Experience-Sharing","children":"Zhen Zhang이 arXiv에 게시한 'Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-Group-Evolving-Agents-Open-Ended-Self-Improvement-via-Experience-Sharing"}]]}]]}],["$","article","2026-02-09-F-GRPO-Dont-Let-Your-Policy-Learn-the-Obvious-and-Forget-the-Rare",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-F-GRPO-Dont-Let-Your-Policy-Learn-the-Obvious-and-Forget-the-Rare","children":"[논문리뷰] F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-F-GRPO-Dont-Let-Your-Policy-Learn-the-Obvious-and-Forget-the-Rare","children":"arXiv에 게시된 'F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-F-GRPO-Dont-Let-Your-Policy-Learn-the-Obvious-and-Forget-the-Rare"}]]}]]}],["$","article","2026-02-09-Canzona-A-Unified-Asynchronous-and-Load-Balanced-Framework-for-Distributed-Matrix-based-Optimizers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Canzona-A-Unified-Asynchronous-and-Load-Balanced-Framework-for-Distributed-Matrix-based-Optimizers","children":"[논문리뷰] Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Canzona-A-Unified-Asynchronous-and-Load-Balanced-Framework-for-Distributed-Matrix-based-Optimizers","children":"arXiv에 게시된 'Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-Canzona-A-Unified-Asynchronous-and-Load-Balanced-Framework-for-Distributed-Matrix-based-Optimizers"}]]}]]}],["$","article","2026-02-09-Baichuan-M3-Modeling-Clinical-Inquiry-for-Reliable-Medical-Decision-Making",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Baichuan-M3-Modeling-Clinical-Inquiry-for-Reliable-Medical-Decision-Making","children":"[논문리뷰] Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Baichuan-M3-Modeling-Clinical-Inquiry-for-Reliable-Medical-Decision-Making","children":"arXiv에 게시된 'Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-Baichuan-M3-Modeling-Clinical-Inquiry-for-Reliable-Medical-Decision-Making"}]]}]]}],["$","article","2026-02-09-Back-to-Basics-Revisiting-Exploration-in-Reinforcement-Learning-for-LLM-Reasoning-via-Generative-Probabilities",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Back-to-Basics-Revisiting-Exploration-in-Reinforcement-Learning-for-LLM-Reasoning-via-Generative-Probabilities","children":"[논문리뷰] Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-Back-to-Basics-Revisiting-Exploration-in-Reinforcement-Learning-for-LLM-Reasoning-via-Generative-Probabilities","children":"Ivan Oseledets이 arXiv에 게시한 'Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-Back-to-Basics-Revisiting-Exploration-in-Reinforcement-Learning-for-LLM-Reasoning-via-Generative-Probabilities"}]]}]]}],["$","article","2026-02-09-AudioSAE-Towards-Understanding-of-Audio-Processing-Models-with-Sparse-AutoEncoders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-AudioSAE-Towards-Understanding-of-Audio-Processing-Models-with-Sparse-AutoEncoders","children":"[논문리뷰] AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-AudioSAE-Towards-Understanding-of-Audio-Processing-Models-with-Sparse-AutoEncoders","children":"arXiv에 게시된 'AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-AudioSAE-Towards-Understanding-of-Audio-Processing-Models-with-Sparse-AutoEncoders"}]]}]]}],["$","article","2026-02-06-V-Retrver-Evidence-Driven-Agentic-Reasoning-for-Universal-Multimodal-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-V-Retrver-Evidence-Driven-Agentic-Reasoning-for-Universal-Multimodal-Retrieval","children":"[논문리뷰] V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-V-Retrver-Evidence-Driven-Agentic-Reasoning-for-Universal-Multimodal-Retrieval","children":"Zeyu Zhang이 arXiv에 게시한 'V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-V-Retrver-Evidence-Driven-Agentic-Reasoning-for-Universal-Multimodal-Retrieval"}]]}]]}],["$","article","2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning","children":"[논문리뷰] Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning","children":"arXiv에 게시된 'Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Thinking-in-Frames-How-Visual-Context-and-Test-Time-Scaling-Empower-Video-Reasoning"}]]}]]}],["$","article","2026-02-06-SwimBird-Eliciting-Switchable-Reasoning-Mode-in-Hybrid-Autoregressive-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-SwimBird-Eliciting-Switchable-Reasoning-Mode-in-Hybrid-Autoregressive-MLLMs","children":"[논문리뷰] SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-SwimBird-Eliciting-Switchable-Reasoning-Mode-in-Hybrid-Autoregressive-MLLMs","children":"arXiv에 게시된 'SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-SwimBird-Eliciting-Switchable-Reasoning-Mode-in-Hybrid-Autoregressive-MLLMs"}]]}]]}],["$","article","2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight","children":"[논문리뷰] Steering LLMs via Scalable Interactive Oversight"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight","children":"arXiv에 게시된 'Steering LLMs via Scalable Interactive Oversight' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight"}]]}]]}],["$","article","2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening","children":"[논문리뷰] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening","children":"arXiv에 게시된 'Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Spider-Sense-Intrinsic-Risk-Sensing-for-Efficient-Agent-Defense-with-Hierarchical-Adaptive-Screening"}]]}]]}],["$","article","2026-02-06-Semantic-Search-over-9-Million-Mathematical-Theorems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Semantic-Search-over-9-Million-Mathematical-Theorems","children":"[논문리뷰] Semantic Search over 9 Million Mathematical Theorems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Semantic-Search-over-9-Million-Mathematical-Theorems","children":"arXiv에 게시된 'Semantic Search over 9 Million Mathematical Theorems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Semantic-Search-over-9-Million-Mathematical-Theorems"}]]}]]}],["$","article","2026-02-06-SAGE-Benchmarking-and-Improving-Retrieval-for-Deep-Research-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-SAGE-Benchmarking-and-Improving-Retrieval-for-Deep-Research-Agents","children":"[논문리뷰] SAGE: Benchmarking and Improving Retrieval for Deep Research Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-SAGE-Benchmarking-and-Improving-Retrieval-for-Deep-Research-Agents","children":"Chen Zhao이 arXiv에 게시한 'SAGE: Benchmarking and Improving Retrieval for Deep Research Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-SAGE-Benchmarking-and-Improving-Retrieval-for-Deep-Research-Agents"}]]}]]}],["$","article","2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities","children":"[논문리뷰] Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities","children":"arXiv에 게시된 'Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities"}]]}]]}],["$","article","2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents","children":"[논문리뷰] Reinforcement World Model Learning for LLM-based Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents","children":"arXiv에 게시된 'Reinforcement World Model Learning for LLM-based Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents"}]]}]]}],["$","article","2026-02-06-Reinforced-Attention-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Reinforced-Attention-Learning","children":"[논문리뷰] Reinforced Attention Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Reinforced-Attention-Learning","children":"arXiv에 게시된 'Reinforced Attention Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Reinforced-Attention-Learning"}]]}]]}],["$","article","2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules","children":"[논문리뷰] RISE-Video: Can Video Generators Decode Implicit World Rules?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules","children":"Zicheng Zhang이 arXiv에 게시한 'RISE-Video: Can Video Generators Decode Implicit World Rules?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules"}]]}]]}],["$","article","2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments","children":"[논문리뷰] ProAct: Agentic Lookahead in Interactive Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments","children":"arXiv에 게시된 'ProAct: Agentic Lookahead in Interactive Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments"}]]}]]}],["$","article","2026-02-06-Multi-Task-GRPO-Reliable-LLM-Reasoning-Across-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Multi-Task-GRPO-Reliable-LLM-Reasoning-Across-Tasks","children":"[논문리뷰] Multi-Task GRPO: Reliable LLM Reasoning Across Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Multi-Task-GRPO-Reliable-LLM-Reasoning-Across-Tasks","children":"Zhiyong Wang이 arXiv에 게시한 'Multi-Task GRPO: Reliable LLM Reasoning Across Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Multi-Task-GRPO-Reliable-LLM-Reasoning-Across-Tasks"}]]}]]}],["$","article","2026-02-06-Length-Unbiased-Sequence-Policy-Optimization-Revealing-and-Controlling-Response-Length-Variation-in-RLVR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Length-Unbiased-Sequence-Policy-Optimization-Revealing-and-Controlling-Response-Length-Variation-in-RLVR","children":"[논문리뷰] Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Length-Unbiased-Sequence-Policy-Optimization-Revealing-and-Controlling-Response-Length-Variation-in-RLVR","children":"Zhixiong Zeng이 arXiv에 게시한 'Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Length-Unbiased-Sequence-Policy-Optimization-Revealing-and-Controlling-Response-Length-Variation-in-RLVR"}]]}]]}],["$","article","2026-02-06-LatentMem-Customizing-Latent-Memory-for-Multi-Agent-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-LatentMem-Customizing-Latent-Memory-for-Multi-Agent-Systems","children":"[논문리뷰] LatentMem: Customizing Latent Memory for Multi-Agent Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-LatentMem-Customizing-Latent-Memory-for-Multi-Agent-Systems","children":"Zefeng He이 arXiv에 게시한 'LatentMem: Customizing Latent Memory for Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-LatentMem-Customizing-Latent-Memory-for-Multi-Agent-Systems"}]]}]]}],["$","article","2026-02-06-InterPrior-Scaling-Generative-Control-for-Physics-Based-Human-Object-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-InterPrior-Scaling-Generative-Control-for-Physics-Based-Human-Object-Interactions","children":"[논문리뷰] InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-InterPrior-Scaling-Generative-Control-for-Physics-Based-Human-Object-Interactions","children":"Xiaohan Fei이 arXiv에 게시한 'InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-InterPrior-Scaling-Generative-Control-for-Physics-Based-Human-Object-Interactions"}]]}]]}],["$","article","2026-02-06-Dr-Kernel-Reinforcement-Learning-Done-Right-for-Triton-Kernel-Generations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Dr-Kernel-Reinforcement-Learning-Done-Right-for-Triton-Kernel-Generations","children":"[논문리뷰] Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Dr-Kernel-Reinforcement-Learning-Done-Right-for-Triton-Kernel-Generations","children":"arXiv에 게시된 'Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Dr-Kernel-Reinforcement-Learning-Done-Right-for-Triton-Kernel-Generations"}]]}]]}],["$","article","2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","children":"[논문리뷰] Context Forcing: Consistent Autoregressive Video Generation with Long Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context","children":"arXiv에 게시된 'Context Forcing: Consistent Autoregressive Video Generation with Long Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Context-Forcing-Consistent-Autoregressive-Video-Generation-with-Long-Context"}]]}]]}],["$","article","2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty","children":"[논문리뷰] CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty","children":"arXiv에 게시된 'CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-CAR-bench-Evaluating-the-Consistency-and-Limit-Awareness-of-LLM-Agents-under-Real-World-Uncertainty"}]]}]]}],["$","article","2026-02-06-Breaking-the-Static-Graph-Context-Aware-Traversal-for-Robust-Retrieval-Augmented-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Breaking-the-Static-Graph-Context-Aware-Traversal-for-Robust-Retrieval-Augmented-Generation","children":"[논문리뷰] Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-Breaking-the-Static-Graph-Context-Aware-Traversal-for-Robust-Retrieval-Augmented-Generation","children":"Qintian Guo이 arXiv에 게시한 'Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-Breaking-the-Static-Graph-Context-Aware-Traversal-for-Robust-Retrieval-Augmented-Generation"}]]}]]}],["$","article","2026-02-06-BABE-Biology-Arena-BEnchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark","children":"[논문리뷰] BABE: Biology Arena BEnchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark","children":"arXiv에 게시된 'BABE: Biology Arena BEnchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-06 00:00:00+0900+0900","children":"2026년 2월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark"}]]}]]}],["$","article","2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning","children":"[논문리뷰] WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning","children":"arXiv에 게시된 'WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-05-Vibe-AIGC-A-New-Paradigm-for-Content-Generation-via-Agentic-Orchestration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Vibe-AIGC-A-New-Paradigm-for-Content-Generation-via-Agentic-Orchestration","children":"[논문리뷰] Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Vibe-AIGC-A-New-Paradigm-for-Content-Generation-via-Agentic-Orchestration","children":"arXiv에 게시된 'Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Vibe-AIGC-A-New-Paradigm-for-Content-Generation-via-Agentic-Orchestration"}]]}]]}],["$","article","2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models","children":"[논문리뷰] VLS: Steering Pretrained Robot Policies via Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models","children":"arXiv에 게시된 'VLS: Steering Pretrained Robot Policies via Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-VLS-Steering-Pretrained-Robot-Policies-via-Vision-Language-Models"}]]}]]}],["$","article","2026-02-05-Training-Data-Efficiency-in-Multimodal-Process-Reward-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Training-Data-Efficiency-in-Multimodal-Process-Reward-Models","children":"[논문리뷰] Training Data Efficiency in Multimodal Process Reward Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Training-Data-Efficiency-in-Multimodal-Process-Reward-Models","children":"Haolin Liu이 arXiv에 게시한 'Training Data Efficiency in Multimodal Process Reward Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Training-Data-Efficiency-in-Multimodal-Process-Reward-Models"}]]}]]}],["$","article","2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents","children":"[논문리뷰] TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents","children":"Qiushi Sun이 arXiv에 게시한 'TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-TIDE-Trajectory-based-Diagnostic-Evaluation-of-Test-Time-Improvement-in-LLM-Agents"}]]}]]}],["$","article","2026-02-05-SoMA-A-Real-to-Sim-Neural-Simulator-for-Robotic-Soft-body-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-SoMA-A-Real-to-Sim-Neural-Simulator-for-Robotic-Soft-body-Manipulation","children":"[논문리뷰] SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-SoMA-A-Real-to-Sim-Neural-Simulator-for-Robotic-Soft-body-Manipulation","children":"arXiv에 게시된 'SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-SoMA-A-Real-to-Sim-Neural-Simulator-for-Robotic-Soft-body-Manipulation"}]]}]]}],["$","article","2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers","children":"[논문리뷰] Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers","children":"arXiv에 게시된 'Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Semantic-Routing-Exploring-Multi-Layer-LLM-Feature-Weighting-for-Diffusion-Transformers"}]]}]]}],["$","article","2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning","children":"[논문리뷰] Self-Hinting Language Models Enhance Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning","children":"arXiv에 게시된 'Self-Hinting Language Models Enhance Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-05-Rethinking-the-Trust-Region-in-LLM-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Rethinking-the-Trust-Region-in-LLM-Reinforcement-Learning","children":"[논문리뷰] Rethinking the Trust Region in LLM Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Rethinking-the-Trust-Region-in-LLM-Reinforcement-Learning","children":"arXiv에 게시된 'Rethinking the Trust Region in LLM Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Rethinking-the-Trust-Region-in-LLM-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-05-Residual-Context-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Residual-Context-Diffusion-Language-Models","children":"[논문리뷰] Residual Context Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Residual-Context-Diffusion-Language-Models","children":"arXiv에 게시된 'Residual Context Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Residual-Context-Diffusion-Language-Models"}]]}]]}],["$","article","2026-02-05-Quant-VideoGen-Auto-Regressive-Long-Video-Generation-via-2-Bit-KV-Cache-Quantization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Quant-VideoGen-Auto-Regressive-Long-Video-Generation-via-2-Bit-KV-Cache-Quantization","children":"[논문리뷰] Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Quant-VideoGen-Auto-Regressive-Long-Video-Generation-via-2-Bit-KV-Cache-Quantization","children":"arXiv에 게시된 'Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Quant-VideoGen-Auto-Regressive-Long-Video-Generation-via-2-Bit-KV-Cache-Quantization"}]]}]]}],["$","article","2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR","children":"[논문리뷰] PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR","children":"Alejandro Lozano이 arXiv에 게시한 'PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR"}]]}]]}],["$","article","2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models","children":"[논문리뷰] OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models","children":"Yiyan Ji이 arXiv에 게시한 'OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models"}]]}]]}],["$","article","2026-02-05-HySparse-A-Hybrid-Sparse-Attention-Architecture-with-Oracle-Token-Selection-and-KV-Cache-Sharing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-HySparse-A-Hybrid-Sparse-Attention-Architecture-with-Oracle-Token-Selection-and-KV-Cache-Sharing","children":"[논문리뷰] HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-HySparse-A-Hybrid-Sparse-Attention-Architecture-with-Oracle-Token-Selection-and-KV-Cache-Sharing","children":"arXiv에 게시된 'HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-HySparse-A-Hybrid-Sparse-Attention-Architecture-with-Oracle-Token-Selection-and-KV-Cache-Sharing"}]]}]]}],["$","article","2026-02-05-HY3D-Bench-Generation-of-3D-Assets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets","children":"[논문리뷰] HY3D-Bench: Generation of 3D Assets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets","children":"arXiv에 게시된 'HY3D-Bench: Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-HY3D-Bench-Generation-of-3D-Assets"}]]}]]}],["$","article","2026-02-05-FASA-Frequency-aware-Sparse-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-FASA-Frequency-aware-Sparse-Attention","children":"[논문리뷰] FASA: Frequency-aware Sparse Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-FASA-Frequency-aware-Sparse-Attention","children":"arXiv에 게시된 'FASA: Frequency-aware Sparse Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-FASA-Frequency-aware-Sparse-Attention"}]]}]]}],["$","article","2026-02-05-EgoActor-Grounding-Task-Planning-into-Spatial-aware-Egocentric-Actions-for-Humanoid-Robots-via-Visual-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-EgoActor-Grounding-Task-Planning-into-Spatial-aware-Egocentric-Actions-for-Humanoid-Robots-via-Visual-Language-Models","children":"[논문리뷰] EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-EgoActor-Grounding-Task-Planning-into-Spatial-aware-Egocentric-Actions-for-Humanoid-Robots-via-Visual-Language-Models","children":"Ziyi Bai이 arXiv에 게시한 'EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-EgoActor-Grounding-Task-Planning-into-Spatial-aware-Egocentric-Actions-for-Humanoid-Robots-via-Visual-Language-Models"}]]}]]}],["$","article","2026-02-05-ERNIE-5-0-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-ERNIE-5-0-Technical-Report","children":"[논문리뷰] ERNIE 5.0 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-ERNIE-5-0-Technical-Report","children":"HasuerYu이 arXiv에 게시한 'ERNIE 5.0 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-ERNIE-5-0-Technical-Report"}]]}]]}],["$","article","2026-02-05-BatCoder-Self-Supervised-Bidirectional-Code-Documentation-Learning-via-Back-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-BatCoder-Self-Supervised-Bidirectional-Code-Documentation-Learning-via-Back-Translation","children":"[논문리뷰] BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-BatCoder-Self-Supervised-Bidirectional-Code-Documentation-Learning-via-Back-Translation","children":"Xiaohua Wang이 arXiv에 게시한 'BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-BatCoder-Self-Supervised-Bidirectional-Code-Documentation-Learning-via-Back-Translation"}]]}]]}],["$","article","2026-02-05-AutoFigure-Generating-and-Refining-Publication-Ready-Scientific-Illustrations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-AutoFigure-Generating-and-Refining-Publication-Ready-Scientific-Illustrations","children":"[논문리뷰] AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-AutoFigure-Generating-and-Refining-Publication-Ready-Scientific-Illustrations","children":"arXiv에 게시된 'AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-AutoFigure-Generating-and-Refining-Publication-Ready-Scientific-Illustrations"}]]}]]}],["$","article","2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning","children":"[논문리뷰] Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning","children":"arXiv에 게시된 'Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-Agent-Omit-Training-Efficient-LLM-Agents-for-Adaptive-Thought-and-Observation-Omission-via-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-05-A-RAG-Scaling-Agentic-Retrieval-Augmented-Generation-via-Hierarchical-Retrieval-Interfaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-A-RAG-Scaling-Agentic-Retrieval-Augmented-Generation-via-Hierarchical-Retrieval-Interfaces","children":"[논문리뷰] A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-05-A-RAG-Scaling-Agentic-Retrieval-Augmented-Generation-via-Hierarchical-Retrieval-Interfaces","children":"arXiv에 게시된 'A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-05 00:00:00+0900+0900","children":"2026년 2월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-05-A-RAG-Scaling-Agentic-Retrieval-Augmented-Generation-via-Hierarchical-Retrieval-Interfaces"}]]}]]}],["$","article","2026-02-04-daVinci-Agency-Unlocking-Long-Horizon-Agency-Data-Efficiently",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-daVinci-Agency-Unlocking-Long-Horizon-Agency-Data-Efficiently","children":"[논문리뷰] daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-daVinci-Agency-Unlocking-Long-Horizon-Agency-Data-Efficiently","children":"arXiv에 게시된 'daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-daVinci-Agency-Unlocking-Long-Horizon-Agency-Data-Efficiently"}]]}]]}],["$","article","2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling","children":"[논문리뷰] WideSeek: Advancing Wide Research via Multi-Agent Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling","children":"Zhongtao Jiang이 arXiv에 게시한 'WideSeek: Advancing Wide Research via Multi-Agent Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-WideSeek-Advancing-Wide-Research-via-Multi-Agent-Scaling"}]]}]]}],["$","article","2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation","children":"[논문리뷰] Unified Personalized Reward Model for Vision Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation","children":"arXiv에 게시된 'Unified Personalized Reward Model for Vision Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Unified-Personalized-Reward-Model-for-Vision-Generation"}]]}]]}],["$","article","2026-02-04-Token-Sparse-Attention-Efficient-Long-Context-Inference-with-Interleaved-Token-Selection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Token-Sparse-Attention-Efficient-Long-Context-Inference-with-Interleaved-Token-Selection","children":"[논문리뷰] Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Token-Sparse-Attention-Efficient-Long-Context-Inference-with-Interleaved-Token-Selection","children":"Jae-Joon Kim이 arXiv에 게시한 'Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Token-Sparse-Attention-Efficient-Long-Context-Inference-with-Interleaved-Token-Selection"}]]}]]}],["$","article","2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy","children":"[논문리뷰] SimpleGPT: Improving GPT via A Simple Normalization Strategy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy","children":"Rong Xiao이 arXiv에 게시한 'SimpleGPT: Improving GPT via A Simple Normalization Strategy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy"}]]}]]}],["$","article","2026-02-04-SWE-World-Building-Software-Engineering-Agents-in-Docker-Free-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-SWE-World-Building-Software-Engineering-Agents-in-Docker-Free-Environments","children":"[논문리뷰] SWE-World: Building Software Engineering Agents in Docker-Free Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-SWE-World-Building-Software-Engineering-Agents-in-Docker-Free-Environments","children":"arXiv에 게시된 'SWE-World: Building Software Engineering Agents in Docker-Free Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-SWE-World-Building-Software-Engineering-Agents-in-Docker-Free-Environments"}]]}]]}],["$","article","2026-02-04-SWE-Master-Unleashing-the-Potential-of-Software-Engineering-Agents-via-Post-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-SWE-Master-Unleashing-the-Potential-of-Software-Engineering-Agents-via-Post-Training","children":"[논문리뷰] SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-SWE-Master-Unleashing-the-Potential-of-Software-Engineering-Agents-via-Post-Training","children":"arXiv에 게시된 'SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-SWE-Master-Unleashing-the-Potential-of-Software-Engineering-Agents-via-Post-Training"}]]}]]}],["$","article","2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks","children":"[논문리뷰] Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks","children":"arXiv에 게시된 'Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Research-on-World-Models-Is-Not-Merely-Injecting-World-Knowledge-into-Specific-Tasks"}]]}]]}],["$","article","2026-02-04-Parallel-Probe-Towards-Efficient-Parallel-Thinking-via-2D-Probing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Parallel-Probe-Towards-Efficient-Parallel-Thinking-via-2D-Probing","children":"[논문리뷰] Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Parallel-Probe-Towards-Efficient-Parallel-Thinking-via-2D-Probing","children":"arXiv에 게시된 'Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Parallel-Probe-Towards-Efficient-Parallel-Thinking-via-2D-Probing"}]]}]]}],["$","article","2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs","children":"[논문리뷰] No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs","children":"arXiv에 게시된 'No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs"}]]}]]}],["$","article","2026-02-04-MARS-Modular-Agent-with-Reflective-Search-for-Automated-AI-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-MARS-Modular-Agent-with-Reflective-Search-for-Automated-AI-Research","children":"[논문리뷰] MARS: Modular Agent with Reflective Search for Automated AI Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-MARS-Modular-Agent-with-Reflective-Search-for-Automated-AI-Research","children":"arXiv에 게시된 'MARS: Modular Agent with Reflective Search for Automated AI Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-MARS-Modular-Agent-with-Reflective-Search-for-Automated-AI-Research"}]]}]]}],["$","article","2026-02-04-Less-Noise-More-Voice-Reinforcement-Learning-for-Reasoning-via-Instruction-Purification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Less-Noise-More-Voice-Reinforcement-Learning-for-Reasoning-via-Instruction-Purification","children":"[논문리뷰] Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Less-Noise-More-Voice-Reinforcement-Learning-for-Reasoning-via-Instruction-Purification","children":"arXiv에 게시된 'Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Less-Noise-More-Voice-Reinforcement-Learning-for-Reasoning-via-Instruction-Purification"}]]}]]}],["$","article","2026-02-04-Learning-Query-Specific-Rubrics-from-Human-Preferences-for-DeepResearch-Report-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Learning-Query-Specific-Rubrics-from-Human-Preferences-for-DeepResearch-Report-Generation","children":"[논문리뷰] Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Learning-Query-Specific-Rubrics-from-Human-Preferences-for-DeepResearch-Report-Generation","children":"arXiv에 게시된 'Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Learning-Query-Specific-Rubrics-from-Human-Preferences-for-DeepResearch-Report-Generation"}]]}]]}],["$","article","2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis","children":"[논문리뷰] Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis","children":"arXiv에 게시된 'Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Diversity-Preserved-Distribution-Matching-Distillation-for-Fast-Visual-Synthesis"}]]}]]}],["$","article","2026-02-04-Decouple-Searching-from-Training-Scaling-Data-Mixing-via-Model-Merging-for-Large-Language-Model-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Decouple-Searching-from-Training-Scaling-Data-Mixing-via-Model-Merging-for-Large-Language-Model-Pre-training","children":"[논문리뷰] Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Decouple-Searching-from-Training-Scaling-Data-Mixing-via-Model-Merging-for-Large-Language-Model-Pre-training","children":"Haifeng Liu이 arXiv에 게시한 'Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Decouple-Searching-from-Training-Scaling-Data-Mixing-via-Model-Merging-for-Large-Language-Model-Pre-training"}]]}]]}],["$","article","2026-02-04-CodeOCR-On-the-Effectiveness-of-Vision-Language-Models-in-Code-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-CodeOCR-On-the-Effectiveness-of-Vision-Language-Models-in-Code-Understanding","children":"[논문리뷰] CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-CodeOCR-On-the-Effectiveness-of-Vision-Language-Models-in-Code-Understanding","children":"arXiv에 게시된 'CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-CodeOCR-On-the-Effectiveness-of-Vision-Language-Models-in-Code-Understanding"}]]}]]}],["$","article","2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs","children":"[논문리뷰] CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs","children":"arXiv에 게시된 'CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs"}]]}]]}],["$","article","2026-02-04-Balancing-Understanding-and-Generation-in-Discrete-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Balancing-Understanding-and-Generation-in-Discrete-Diffusion-Models","children":"[논문리뷰] Balancing Understanding and Generation in Discrete Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-Balancing-Understanding-and-Generation-in-Discrete-Diffusion-Models","children":"Jianbin Jiao이 arXiv에 게시한 'Balancing Understanding and Generation in Discrete Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-Balancing-Understanding-and-Generation-in-Discrete-Diffusion-Models"}]]}]]}],["$","article","2026-02-04-AdaptMMBench-Benchmarking-Adaptive-Multimodal-Reasoning-for-Mode-Selection-and-Reasoning-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-AdaptMMBench-Benchmarking-Adaptive-Multimodal-Reasoning-for-Mode-Selection-and-Reasoning-Process","children":"[논문리뷰] AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-AdaptMMBench-Benchmarking-Adaptive-Multimodal-Reasoning-for-Mode-Selection-and-Reasoning-Process","children":"Shilin Yan이 arXiv에 게시한 'AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-AdaptMMBench-Benchmarking-Adaptive-Multimodal-Reasoning-for-Mode-Selection-and-Reasoning-Process"}]]}]]}],["$","article","2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration","children":"[논문리뷰] AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration","children":"Zhaoyang Yu이 arXiv에 게시한 'AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration"}]]}]]}],["$","article","2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation","children":"[논문리뷰] 3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation","children":"arXiv에 게시된 '3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-3D-Aware-Implicit-Motion-Control-for-View-Adaptive-Human-Video-Generation"}]]}]]}],["$","article","2026-02-03-WildGraphBench-Benchmarking-GraphRAG-with-Wild-Source-Corpora",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-WildGraphBench-Benchmarking-GraphRAG-with-Wild-Source-Corpora","children":"[논문리뷰] WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-WildGraphBench-Benchmarking-GraphRAG-with-Wild-Source-Corpora","children":"arXiv에 게시된 'WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-WildGraphBench-Benchmarking-GraphRAG-with-Wild-Source-Corpora"}]]}]]}],["$","article","2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles","children":"[논문리뷰] Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles","children":"arXiv에 게시된 'Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Wiki-Live-Challenge-Challenging-Deep-Research-Agents-with-Expert-Level-Wikipedia-Articles"}]]}]]}],["$","article","2026-02-03-Vision-DeepResearch-Incentivizing-DeepResearch-Capability-in-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Vision-DeepResearch-Incentivizing-DeepResearch-Capability-in-Multimodal-Large-Language-Models","children":"[논문리뷰] Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Vision-DeepResearch-Incentivizing-DeepResearch-Capability-in-Multimodal-Large-Language-Models","children":"Zhen Fang이 arXiv에 게시한 'Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Vision-DeepResearch-Incentivizing-DeepResearch-Capability-in-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models","children":"[논문리뷰] Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models","children":"Shuang Chen이 arXiv에 게시한 'Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Vision-DeepResearch-Benchmark-Rethinking-Visual-and-Textual-Search-for-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2026-02-03-UniReason-1-0-A-Unified-Reasoning-Framework-for-World-Knowledge-Aligned-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-UniReason-1-0-A-Unified-Reasoning-Framework-for-World-Knowledge-Aligned-Image-Generation-and-Editing","children":"[논문리뷰] UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-UniReason-1-0-A-Unified-Reasoning-Framework-for-World-Knowledge-Aligned-Image-Generation-and-Editing","children":"Size Wu이 arXiv에 게시한 'UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-UniReason-1-0-A-Unified-Reasoning-Framework-for-World-Knowledge-Aligned-Image-Generation-and-Editing"}]]}]]}],["$","article","2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model","children":"[논문리뷰] Toward Cognitive Supersensing in Multimodal Large Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model","children":"Yifan Xu이 arXiv에 게시한 'Toward Cognitive Supersensing in Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Toward-Cognitive-Supersensing-in-Multimodal-Large-Language-Model"}]]}]]}],["$","article","2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions","children":"[논문리뷰] SWE-Universe: Scale Real-World Verifiable Environments to Millions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions","children":"arXiv에 게시된 'SWE-Universe: Scale Real-World Verifiable Environments to Millions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-SWE-Universe-Scale-Real-World-Verifiable-Environments-to-Millions"}]]}]]}],["$","article","2026-02-03-SPARKLING-Balancing-Signal-Preservation-and-Symmetry-Breaking-for-Width-Progressive-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SPARKLING-Balancing-Signal-Preservation-and-Symmetry-Breaking-for-Width-Progressive-Learning","children":"[논문리뷰] SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SPARKLING-Balancing-Signal-Preservation-and-Symmetry-Breaking-for-Width-Progressive-Learning","children":"arXiv에 게시된 'SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-SPARKLING-Balancing-Signal-Preservation-and-Symmetry-Breaking-for-Width-Progressive-Learning"}]]}]]}],["$","article","2026-02-03-SLIME-Stabilized-Likelihood-Implicit-Margin-Enforcement-for-Preference-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SLIME-Stabilized-Likelihood-Implicit-Margin-Enforcement-for-Preference-Optimization","children":"[논문리뷰] SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-SLIME-Stabilized-Likelihood-Implicit-Margin-Enforcement-for-Preference-Optimization","children":"arXiv에 게시된 'SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-SLIME-Stabilized-Likelihood-Implicit-Margin-Enforcement-for-Preference-Optimization"}]]}]]}],["$","article","2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System","children":"[논문리뷰] RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System","children":"arXiv에 게시된 'RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System"}]]}]]}],["$","article","2026-02-03-PixelGen-Pixel-Diffusion-Beats-Latent-Diffusion-with-Perceptual-Loss",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-PixelGen-Pixel-Diffusion-Beats-Latent-Diffusion-with-Perceptual-Loss","children":"[논문리뷰] PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-PixelGen-Pixel-Diffusion-Beats-Latent-Diffusion-with-Perceptual-Loss","children":"arXiv에 게시된 'PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-PixelGen-Pixel-Diffusion-Beats-Latent-Diffusion-with-Perceptual-Loss"}]]}]]}],["$","article","2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards","children":"[논문리뷰] PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards","children":"arXiv에 게시된 'PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-PISCES-Annotation-free-Text-to-Video-Post-Training-via-Optimal-Transport-Aligned-Rewards"}]]}]]}],["$","article","2026-02-03-Mind-Brush-Integrating-Agentic-Cognitive-Search-and-Reasoning-into-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Mind-Brush-Integrating-Agentic-Cognitive-Search-and-Reasoning-into-Image-Generation","children":"[논문리뷰] Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Mind-Brush-Integrating-Agentic-Cognitive-Search-and-Reasoning-into-Image-Generation","children":"Chenjue Zhang이 arXiv에 게시한 'Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Mind-Brush-Integrating-Agentic-Cognitive-Search-and-Reasoning-into-Image-Generation"}]]}]]}],["$","article","2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars","children":"[논문리뷰] Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars","children":"Teng Hu이 arXiv에 게시한 'Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Making-Avatars-Interact-Towards-Text-Driven-Human-Object-Interaction-for-Controllable-Talking-Avatars"}]]}]]}],["$","article","2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence","children":"[논문리뷰] Kimi K2.5: Visual Agentic Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence","children":"arXiv에 게시된 'Kimi K2.5: Visual Agentic Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Kimi-K2-5-Visual-Agentic-Intelligence"}]]}]]}],["$","article","2026-02-03-How-Well-Do-Models-Follow-Visual-Instructions-VIBE-A-Systematic-Benchmark-for-Visual-Instruction-Driven-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-How-Well-Do-Models-Follow-Visual-Instructions-VIBE-A-Systematic-Benchmark-for-Visual-Instruction-Driven-Image-Editing","children":"[논문리뷰] How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-How-Well-Do-Models-Follow-Visual-Instructions-VIBE-A-Systematic-Benchmark-for-Visual-Instruction-Driven-Image-Editing","children":"Haochen Tian이 arXiv에 게시한 'How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-How-Well-Do-Models-Follow-Visual-Instructions-VIBE-A-Systematic-Benchmark-for-Visual-Instruction-Driven-Image-Editing"}]]}]]}],["$","article","2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots","children":"[논문리뷰] Green-VLA: Staged Vision-Language-Action Model for Generalist Robots"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots","children":"arXiv에 게시된 'Green-VLA: Staged Vision-Language-Action Model for Generalist Robots' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots"}]]}]]}],["$","article","2026-02-03-FSVideo-Fast-Speed-Video-Diffusion-Model-in-a-Highly-Compressed-Latent-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-FSVideo-Fast-Speed-Video-Diffusion-Model-in-a-Highly-Compressed-Latent-Space","children":"[논문리뷰] FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-FSVideo-Fast-Speed-Video-Diffusion-Model-in-a-Highly-Compressed-Latent-Space","children":"arXiv에 게시된 'FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-FSVideo-Fast-Speed-Video-Diffusion-Model-in-a-Highly-Compressed-Latent-Space"}]]}]]}],["$","article","2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents","children":"[논문리뷰] FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents","children":"arXiv에 게시된 'FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-FS-Researcher-Test-Time-Scaling-for-Long-Horizon-Research-Tasks-with-File-System-Based-Agents"}]]}]]}],["$","article","2026-02-03-Closing-the-Loop-Universal-Repository-Representation-with-RPG-Encoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Closing-the-Loop-Universal-Repository-Representation-with-RPG-Encoder","children":"[논문리뷰] Closing the Loop: Universal Repository Representation with RPG-Encoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Closing-the-Loop-Universal-Repository-Representation-with-RPG-Encoder","children":"Steven Liu이 arXiv에 게시한 'Closing the Loop: Universal Repository Representation with RPG-Encoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Closing-the-Loop-Universal-Repository-Representation-with-RPG-Encoder"}]]}]]}],["$","article","2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation","children":"[논문리뷰] Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation","children":"arXiv에 게시된 'Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Causal-Forcing-Autoregressive-Diffusion-Distillation-Done-Right-for-High-Quality-Real-Time-Interactive-Video-Generation"}]]}]]}],["$","article","2026-02-03-Beyond-Pixels-Visual-Metaphor-Transfer-via-Schema-Driven-Agentic-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Beyond-Pixels-Visual-Metaphor-Transfer-via-Schema-Driven-Agentic-Reasoning","children":"[논문리뷰] Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-03-Beyond-Pixels-Visual-Metaphor-Transfer-via-Schema-Driven-Agentic-Reasoning","children":"arXiv에 게시된 'Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-03-Beyond-Pixels-Visual-Metaphor-Transfer-via-Schema-Driven-Agentic-Reasoning"}]]}]]}],["$","article","2026-02-02-TTCS-Test-Time-Curriculum-Synthesis-for-Self-Evolving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-TTCS-Test-Time-Curriculum-Synthesis-for-Self-Evolving","children":"[논문리뷰] TTCS: Test-Time Curriculum Synthesis for Self-Evolving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-TTCS-Test-Time-Curriculum-Synthesis-for-Self-Evolving","children":"Chengsong Huang이 arXiv에 게시한 'TTCS: Test-Time Curriculum Synthesis for Self-Evolving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-TTCS-Test-Time-Curriculum-Synthesis-for-Self-Evolving"}]]}]]}],["$","article","2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models","children":"[논문리뷰] THINKSAFE: Self-Generated Safety Alignment for Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models","children":"Minki Kang이 arXiv에 게시한 'THINKSAFE: Self-Generated Safety Alignment for Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models"}]]}]]}],["$","article","2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance","children":"[논문리뷰] TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance","children":"Daniil Grebenkin이 arXiv에 게시한 'TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-TAM-Eval-Evaluating-LLMs-for-Automated-Unit-Test-Maintenance"}]]}]]}],["$","article","2026-02-02-Statistical-Estimation-of-Adversarial-Risk-in-Large-Language-Models-under-Best-of-N-Sampling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Statistical-Estimation-of-Adversarial-Risk-in-Large-Language-Models-under-Best-of-N-Sampling","children":"[논문리뷰] Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Statistical-Estimation-of-Adversarial-Risk-in-Large-Language-Models-under-Best-of-N-Sampling","children":"arXiv에 게시된 'Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Statistical-Estimation-of-Adversarial-Risk-in-Large-Language-Models-under-Best-of-N-Sampling"}]]}]]}],["$","article","2026-02-02-SSL-Sweet-Spot-Learning-for-Differentiated-Guidance-in-Agentic-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-SSL-Sweet-Spot-Learning-for-Differentiated-Guidance-in-Agentic-Optimization","children":"[논문리뷰] SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-SSL-Sweet-Spot-Learning-for-Differentiated-Guidance-in-Agentic-Optimization","children":"Bolin Ni이 arXiv에 게시한 'SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-SSL-Sweet-Spot-Learning-for-Differentiated-Guidance-in-Agentic-Optimization"}]]}]]}],["$","article","2026-02-02-Routing-the-Lottery-Adaptive-Subnetworks-for-Heterogeneous-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Routing-the-Lottery-Adaptive-Subnetworks-for-Heterogeneous-Data","children":"[논문리뷰] Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Routing-the-Lottery-Adaptive-Subnetworks-for-Heterogeneous-Data","children":"Michal Byra이 arXiv에 게시한 'Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Routing-the-Lottery-Adaptive-Subnetworks-for-Heterogeneous-Data"}]]}]]}],["$","article","2026-02-02-Robust-Tool-Use-via-Fission-GRPO-Learning-to-Recover-from-Execution-Errors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Robust-Tool-Use-via-Fission-GRPO-Learning-to-Recover-from-Execution-Errors","children":"[논문리뷰] Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Robust-Tool-Use-via-Fission-GRPO-Learning-to-Recover-from-Execution-Errors","children":"Bin Liang이 arXiv에 게시한 'Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Robust-Tool-Use-via-Fission-GRPO-Learning-to-Recover-from-Execution-Errors"}]]}]]}],["$","article","2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality","children":"[논문리뷰] Revisiting Diffusion Model Predictions Through Dimensionality"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality","children":"Chaoyang Wang이 arXiv에 게시한 'Revisiting Diffusion Model Predictions Through Dimensionality' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Revisiting-Diffusion-Model-Predictions-Through-Dimensionality"}]]}]]}],["$","article","2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought","children":"[논문리뷰] ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought","children":"Zhifeng Gao이 arXiv에 게시한 'ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought"}]]}]]}],["$","article","2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation","children":"[논문리뷰] RM -RF: Reward Model for Run-Free Unit Test Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation","children":"Vadim Alperovich이 arXiv에 게시한 'RM -RF: Reward Model for Run-Free Unit Test Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation"}]]}]]}],["$","article","2026-02-02-Pushing-the-Boundaries-of-Natural-Reasoning-Interleaved-Bonus-from-Formal-Logic-Verification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Pushing-the-Boundaries-of-Natural-Reasoning-Interleaved-Bonus-from-Formal-Logic-Verification","children":"[논문리뷰] Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Pushing-the-Boundaries-of-Natural-Reasoning-Interleaved-Bonus-from-Formal-Logic-Verification","children":"arXiv에 게시된 'Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Pushing-the-Boundaries-of-Natural-Reasoning-Interleaved-Bonus-from-Formal-Logic-Verification"}]]}]]}],["$","article","2026-02-02-PaperBanana-Automating-Academic-Illustration-for-AI-Scientists",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-PaperBanana-Automating-Academic-Illustration-for-AI-Scientists","children":"[논문리뷰] PaperBanana: Automating Academic Illustration for AI Scientists"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-PaperBanana-Automating-Academic-Illustration-for-AI-Scientists","children":"arXiv에 게시된 'PaperBanana: Automating Academic Illustration for AI Scientists' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-PaperBanana-Automating-Academic-Illustration-for-AI-Scientists"}]]}]]}],["$","article","2026-02-02-PaddleOCR-VL-1-5-Towards-a-Multi-Task-0-9B-VLM-for-Robust-In-the-Wild-Document-Parsing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-PaddleOCR-VL-1-5-Towards-a-Multi-Task-0-9B-VLM-for-Robust-In-the-Wild-Document-Parsing","children":"[논문리뷰] PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-PaddleOCR-VL-1-5-Towards-a-Multi-Task-0-9B-VLM-for-Robust-In-the-Wild-Document-Parsing","children":"Zelun Zhang이 arXiv에 게시한 'PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-PaddleOCR-VL-1-5-Towards-a-Multi-Task-0-9B-VLM-for-Robust-In-the-Wild-Document-Parsing"}]]}]]}],["$","article","2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning","children":"[논문리뷰] MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning","children":"Yuxin Chen이 arXiv에 게시한 'MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning"}]]}]]}],["$","article","2026-02-02-Latent-Chain-of-Thought-as-Planning-Decoupling-Reasoning-from-Verbalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Latent-Chain-of-Thought-as-Planning-Decoupling-Reasoning-from-Verbalization","children":"[논문리뷰] Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Latent-Chain-of-Thought-as-Planning-Decoupling-Reasoning-from-Verbalization","children":"arXiv에 게시된 'Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Latent-Chain-of-Thought-as-Planning-Decoupling-Reasoning-from-Verbalization"}]]}]]}],["$","article","2026-02-02-FourierSampler-Unlocking-Non-Autoregressive-Potential-in-Diffusion-Language-Models-via-Frequency-Guided-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-FourierSampler-Unlocking-Non-Autoregressive-Potential-in-Diffusion-Language-Models-via-Frequency-Guided-Generation","children":"[논문리뷰] FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-FourierSampler-Unlocking-Non-Autoregressive-Potential-in-Diffusion-Language-Models-via-Frequency-Guided-Generation","children":"arXiv에 게시된 'FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-FourierSampler-Unlocking-Non-Autoregressive-Potential-in-Diffusion-Language-Models-via-Frequency-Guided-Generation"}]]}]]}],["$","article","2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","children":"[논문리뷰] DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning","children":"arXiv에 게시된 'DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-DreamActor-M2-Universal-Character-Image-Animation-via-Spatiotemporal-In-Context-Learning"}]]}]]}],["$","article","2026-02-02-DenseGRPO-From-Sparse-to-Dense-Reward-for-Flow-Matching-Model-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DenseGRPO-From-Sparse-to-Dense-Reward-for-Flow-Matching-Model-Alignment","children":"[논문리뷰] DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DenseGRPO-From-Sparse-to-Dense-Reward-for-Flow-Matching-Model-Alignment","children":"arXiv에 게시된 'DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-DenseGRPO-From-Sparse-to-Dense-Reward-for-Flow-Matching-Model-Alignment"}]]}]]}],["$","article","2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience","children":"[논문리뷰] Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience","children":"arXiv에 게시된 'Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience"}]]}]]}],["$","article","2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation","children":"[논문리뷰] DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation","children":"Jong Chul Ye이 arXiv에 게시한 'DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-DINO-SAE-DINO-Spherical-Autoencoder-for-High-Fidelity-Image-Reconstruction-and-Generation"}]]}]]}],["$","article","2026-02-02-Continual-GUI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Continual-GUI-Agents","children":"[논문리뷰] Continual GUI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-Continual-GUI-Agents","children":"arXiv에 게시된 'Continual GUI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-Continual-GUI-Agents"}]]}]]}],["$","article","2026-02-02-ASTRA-Automated-Synthesis-of-agentic-Trajectories-and-Reinforcement-Arenas",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-ASTRA-Automated-Synthesis-of-agentic-Trajectories-and-Reinforcement-Arenas","children":"[논문리뷰] ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-ASTRA-Automated-Synthesis-of-agentic-Trajectories-and-Reinforcement-Arenas","children":"Kaichi Yu이 arXiv에 게시한 'ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-ASTRA-Automated-Synthesis-of-agentic-Trajectories-and-Reinforcement-Arenas"}]]}]]}],["$","article","2026-01-30-VTC-R1-Vision-Text-Compression-for-Efficient-Long-Context-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-VTC-R1-Vision-Text-Compression-for-Efficient-Long-Context-Reasoning","children":"[논문리뷰] VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-VTC-R1-Vision-Text-Compression-for-Efficient-Long-Context-Reasoning","children":"arXiv에 게시된 'VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-VTC-R1-Vision-Text-Compression-for-Efficient-Long-Context-Reasoning"}]]}]]}],["$","article","2026-01-30-Typhoon-S-Minimal-Open-Post-Training-for-Sovereign-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Typhoon-S-Minimal-Open-Post-Training-for-Sovereign-Large-Language-Models","children":"[논문리뷰] Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Typhoon-S-Minimal-Open-Post-Training-for-Sovereign-Large-Language-Models","children":"arXiv에 게시된 'Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Typhoon-S-Minimal-Open-Post-Training-for-Sovereign-Large-Language-Models"}]]}]]}],["$","article","2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models","children":"[논문리뷰] Self-Improving Pretraining: using post-trained models to pretrain better models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models","children":"arXiv에 게시된 'Self-Improving Pretraining: using post-trained models to pretrain better models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Self-Improving-Pretraining-using-post-trained-models-to-pretrain-better-models"}]]}]]}],["$","article","2026-01-30-Scaling-Embeddings-Outperforms-Scaling-Experts-in-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Scaling-Embeddings-Outperforms-Scaling-Experts-in-Language-Models","children":"[논문리뷰] Scaling Embeddings Outperforms Scaling Experts in Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Scaling-Embeddings-Outperforms-Scaling-Experts-in-Language-Models","children":"arXiv에 게시된 'Scaling Embeddings Outperforms Scaling Experts in Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Scaling-Embeddings-Outperforms-Scaling-Experts-in-Language-Models"}]]}]]}],["$","article","2026-01-30-Scalable-Power-Sampling-Unlocking-Efficient-Training-Free-Reasoning-for-LLMs-via-Distribution-Sharpening",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Scalable-Power-Sampling-Unlocking-Efficient-Training-Free-Reasoning-for-LLMs-via-Distribution-Sharpening","children":"[논문리뷰] Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Scalable-Power-Sampling-Unlocking-Efficient-Training-Free-Reasoning-for-LLMs-via-Distribution-Sharpening","children":"Haitham Bou Ammar이 arXiv에 게시한 'Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Scalable-Power-Sampling-Unlocking-Efficient-Training-Free-Reasoning-for-LLMs-via-Distribution-Sharpening"}]]}]]}],["$","article","2026-01-30-Qwen3-ASR-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Qwen3-ASR-Technical-Report","children":"[논문리뷰] Qwen3-ASR Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Qwen3-ASR-Technical-Report","children":"arXiv에 게시된 'Qwen3-ASR Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Qwen3-ASR-Technical-Report"}]]}]]}],["$","article","2026-01-30-PLANING-A-Loosely-Coupled-Triangle-Gaussian-Framework-for-Streaming-3D-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-PLANING-A-Loosely-Coupled-Triangle-Gaussian-Framework-for-Streaming-3D-Reconstruction","children":"[논문리뷰] PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-PLANING-A-Loosely-Coupled-Triangle-Gaussian-Framework-for-Streaming-3D-Reconstruction","children":"arXiv에 게시된 'PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-PLANING-A-Loosely-Coupled-Triangle-Gaussian-Framework-for-Streaming-3D-Reconstruction"}]]}]]}],["$","article","2026-01-30-OCRVerse-Towards-Holistic-OCR-in-End-to-End-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-OCRVerse-Towards-Holistic-OCR-in-End-to-End-Vision-Language-Models","children":"[논문리뷰] OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-OCRVerse-Towards-Holistic-OCR-in-End-to-End-Vision-Language-Models","children":"Liming Zheng이 arXiv에 게시한 'OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-OCRVerse-Towards-Holistic-OCR-in-End-to-End-Vision-Language-Models"}]]}]]}],["$","article","2026-01-30-MetricAnything-Scaling-Metric-Depth-Pretraining-with-Noisy-Heterogeneous-Sources",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MetricAnything-Scaling-Metric-Depth-Pretraining-with-Noisy-Heterogeneous-Sources","children":"[논문리뷰] MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MetricAnything-Scaling-Metric-Depth-Pretraining-with-Noisy-Heterogeneous-Sources","children":"Jianxun Cui이 arXiv에 게시한 'MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-MetricAnything-Scaling-Metric-Depth-Pretraining-with-Noisy-Heterogeneous-Sources"}]]}]]}],["$","article","2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods","children":"[논문리뷰] MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods","children":"arXiv에 게시된 'MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods"}]]}]]}],["$","article","2026-01-30-MAD-Modality-Adaptive-Decoding-for-Mitigating-Cross-Modal-Hallucinations-in-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MAD-Modality-Adaptive-Decoding-for-Mitigating-Cross-Modal-Hallucinations-in-Multimodal-Large-Language-Models","children":"[논문리뷰] MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MAD-Modality-Adaptive-Decoding-for-Mitigating-Cross-Modal-Hallucinations-in-Multimodal-Large-Language-Models","children":"Yong Man Ro이 arXiv에 게시한 'MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-MAD-Modality-Adaptive-Decoding-for-Mitigating-Cross-Modal-Hallucinations-in-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2026-01-30-Llama-3-1-FoundationAI-SecurityLLM-Reasoning-8B-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Llama-3-1-FoundationAI-SecurityLLM-Reasoning-8B-Technical-Report","children":"[논문리뷰] Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Llama-3-1-FoundationAI-SecurityLLM-Reasoning-8B-Technical-Report","children":"arXiv에 게시된 'Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Llama-3-1-FoundationAI-SecurityLLM-Reasoning-8B-Technical-Report"}]]}]]}],["$","article","2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience","children":"[논문리뷰] Language-based Trial and Error Falls Behind in the Era of Experience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience","children":"arXiv에 게시된 'Language-based Trial and Error Falls Behind in the Era of Experience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience"}]]}]]}],["$","article","2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives","children":"[논문리뷰] Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives","children":"arXiv에 게시된 'Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Idea2Story-An-Automated-Pipeline-for-Transforming-Research-Concepts-into-Complete-Scientific-Narratives"}]]}]]}],["$","article","2026-01-30-Exploring-Reasoning-Reward-Model-for-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Exploring-Reasoning-Reward-Model-for-Agents","children":"[논문리뷰] Exploring Reasoning Reward Model for Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Exploring-Reasoning-Reward-Model-for-Agents","children":"Zhixun Li이 arXiv에 게시한 'Exploring Reasoning Reward Model for Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Exploring-Reasoning-Reward-Model-for-Agents"}]]}]]}],["$","article","2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models","children":"[논문리뷰] Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models","children":"arXiv에 게시된 'Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Everything-in-Its-Place-Benchmarking-Spatial-Intelligence-of-Text-to-Image-Models"}]]}]]}],["$","article","2026-01-30-DynamicVLA-A-Vision-Language-Action-Model-for-Dynamic-Object-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-DynamicVLA-A-Vision-Language-Action-Model-for-Dynamic-Object-Manipulation","children":"[논문리뷰] DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-DynamicVLA-A-Vision-Language-Action-Model-for-Dynamic-Object-Manipulation","children":"arXiv에 게시된 'DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-DynamicVLA-A-Vision-Language-Action-Model-for-Dynamic-Object-Manipulation"}]]}]]}],["$","article","2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories","children":"[논문리뷰] Discovering Hidden Gems in Model Repositories"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories","children":"Yedid Hoshen이 arXiv에 게시한 'Discovering Hidden Gems in Model Repositories' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories"}]]}]]}],["$","article","2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents","children":"[논문리뷰] DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents","children":"arXiv에 게시된 'DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-DeepSearchQA-Bridging-the-Comprehensiveness-Gap-for-Deep-Research-Agents"}]]}]]}],["$","article","2026-01-30-ConceptMoE-Adaptive-Token-to-Concept-Compression-for-Implicit-Compute-Allocation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-ConceptMoE-Adaptive-Token-to-Concept-Compression-for-Implicit-Compute-Allocation","children":"[논문리뷰] ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-ConceptMoE-Adaptive-Token-to-Concept-Compression-for-Implicit-Compute-Allocation","children":"arXiv에 게시된 'ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-ConceptMoE-Adaptive-Token-to-Concept-Compression-for-Implicit-Compute-Allocation"}]]}]]}],["$","article","2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning","children":"[논문리뷰] Beyond Imitation: Reinforcement Learning for Active Latent Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning","children":"Wee Sun Lee이 arXiv에 게시한 'Beyond Imitation: Reinforcement Learning for Active Latent Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-Beyond-Imitation-Reinforcement-Learning-for-Active-Latent-Planning"}]]}]]}],["$","article","2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts","children":"[논문리뷰] AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts","children":"arXiv에 게시된 'AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-AgentLongBench-A-Controllable-Long-Benchmark-For-Long-Contexts-Agents-via-Environment-Rollouts"}]]}]]}],["$","article","2026-01-29-UPLiFT-Efficient-Pixel-Dense-Feature-Upsampling-with-Local-Attenders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-UPLiFT-Efficient-Pixel-Dense-Feature-Upsampling-with-Local-Attenders","children":"[논문리뷰] UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-UPLiFT-Efficient-Pixel-Dense-Feature-Upsampling-with-Local-Attenders","children":"arXiv에 게시된 'UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-UPLiFT-Efficient-Pixel-Dense-Feature-Upsampling-with-Local-Attenders"}]]}]]}],["$","article","2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning","children":"[논문리뷰] Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning","children":"Shuai Zhang이 arXiv에 게시한 'Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Spark-Strategic-Policy-Aware-Exploration-via-Dynamic-Branching-for-Long-Horizon-Agentic-Learning"}]]}]]}],["$","article","2026-01-29-SketchDynamics-Exploring-Free-Form-Sketches-for-Dynamic-Intent-Expression-in-Animation-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-SketchDynamics-Exploring-Free-Form-Sketches-for-Dynamic-Intent-Expression-in-Animation-Generation","children":"[논문리뷰] SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-SketchDynamics-Exploring-Free-Form-Sketches-for-Dynamic-Intent-Expression-in-Animation-Generation","children":"Hongbo Fu이 arXiv에 게시한 'SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-SketchDynamics-Exploring-Free-Form-Sketches-for-Dynamic-Intent-Expression-in-Animation-Generation"}]]}]]}],["$","article","2026-01-29-Shallow-π-Knowledge-Distillation-for-Flow-based-VLAs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Shallow-π-Knowledge-Distillation-for-Flow-based-VLAs","children":"[논문리뷰] Shallow-π: Knowledge Distillation for Flow-based VLAs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Shallow-π-Knowledge-Distillation-for-Flow-based-VLAs","children":"arXiv에 게시된 'Shallow-π: Knowledge Distillation for Flow-based VLAs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Shallow-π-Knowledge-Distillation-for-Flow-based-VLAs"}]]}]]}],["$","article","2026-01-29-SERA-Soft-Verified-Efficient-Repository-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-SERA-Soft-Verified-Efficient-Repository-Agents","children":"[논문리뷰] SERA: Soft-Verified Efficient Repository Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-SERA-Soft-Verified-Efficient-Repository-Agents","children":"arXiv에 게시된 'SERA: Soft-Verified Efficient Repository Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-SERA-Soft-Verified-Efficient-Repository-Agents"}]]}]]}],["$","article","2026-01-29-SE-DiCoW-Self-Enrolled-Diarization-Conditioned-Whisper",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-SE-DiCoW-Self-Enrolled-Diarization-Conditioned-Whisper","children":"[논문리뷰] SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-SE-DiCoW-Self-Enrolled-Diarization-Conditioned-Whisper","children":"arXiv에 게시된 'SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-SE-DiCoW-Self-Enrolled-Diarization-Conditioned-Whisper"}]]}]]}],["$","article","2026-01-29-Reinforcement-Learning-via-Self-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Reinforcement-Learning-via-Self-Distillation","children":"[논문리뷰] Reinforcement Learning via Self-Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Reinforcement-Learning-via-Self-Distillation","children":"arXiv에 게시된 'Reinforcement Learning via Self-Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Reinforcement-Learning-via-Self-Distillation"}]]}]]}],["$","article","2026-01-29-RIR-Mega-Speech-A-Reverberant-Speech-Corpus-with-Comprehensive-Acoustic-Metadata-and-Reproducible-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-RIR-Mega-Speech-A-Reverberant-Speech-Corpus-with-Comprehensive-Acoustic-Metadata-and-Reproducible-Evaluation","children":"[논문리뷰] RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-RIR-Mega-Speech-A-Reverberant-Speech-Corpus-with-Comprehensive-Acoustic-Metadata-and-Reproducible-Evaluation","children":"mandipgoswami이 arXiv에 게시한 'RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-RIR-Mega-Speech-A-Reverberant-Speech-Corpus-with-Comprehensive-Acoustic-Metadata-and-Reproducible-Evaluation"}]]}]]}],["$","article","2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution","children":"[논문리뷰] OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution","children":"Yusai Zhao이 arXiv에 게시한 'OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-OmegaUse-Building-a-General-Purpose-GUI-Agent-for-Autonomous-Task-Execution"}]]}]]}],["$","article","2026-01-29-Linear-representations-in-language-models-can-change-dramatically-over-a-conversation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Linear-representations-in-language-models-can-change-dramatically-over-a-conversation","children":"[논문리뷰] Linear representations in language models can change dramatically over a conversation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Linear-representations-in-language-models-can-change-dramatically-over-a-conversation","children":"arXiv에 게시된 'Linear representations in language models can change dramatically over a conversation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Linear-representations-in-language-models-can-change-dramatically-over-a-conversation"}]]}]]}],["$","article","2026-01-29-Innovator-VL-A-Multimodal-Large-Language-Model-for-Scientific-Discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Innovator-VL-A-Multimodal-Large-Language-Model-for-Scientific-Discovery","children":"[논문리뷰] Innovator-VL: A Multimodal Large Language Model for Scientific Discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Innovator-VL-A-Multimodal-Large-Language-Model-for-Scientific-Discovery","children":"arXiv에 게시된 'Innovator-VL: A Multimodal Large Language Model for Scientific Discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Innovator-VL-A-Multimodal-Large-Language-Model-for-Scientific-Discovery"}]]}]]}],["$","article","2026-01-29-Harder-Is-Better-Boosting-Mathematical-Reasoning-via-Difficulty-Aware-GRPO-and-Multi-Aspect-Question-Reformulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Harder-Is-Better-Boosting-Mathematical-Reasoning-via-Difficulty-Aware-GRPO-and-Multi-Aspect-Question-Reformulation","children":"[논문리뷰] Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Harder-Is-Better-Boosting-Mathematical-Reasoning-via-Difficulty-Aware-GRPO-and-Multi-Aspect-Question-Reformulation","children":"arXiv에 게시된 'Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Harder-Is-Better-Boosting-Mathematical-Reasoning-via-Difficulty-Aware-GRPO-and-Multi-Aspect-Question-Reformulation"}]]}]]}],["$","article","2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection","children":"[논문리뷰] GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection","children":"arXiv에 게시된 'GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection"}]]}]]}],["$","article","2026-01-29-DeepSeek-OCR-2-Visual-Causal-Flow",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-DeepSeek-OCR-2-Visual-Causal-Flow","children":"[논문리뷰] DeepSeek-OCR 2: Visual Causal Flow"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-DeepSeek-OCR-2-Visual-Causal-Flow","children":"arXiv에 게시된 'DeepSeek-OCR 2: Visual Causal Flow' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-DeepSeek-OCR-2-Visual-Causal-Flow"}]]}]]}],["$","article","2026-01-29-Advancing-Open-source-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Advancing-Open-source-World-Models","children":"[논문리뷰] Advancing Open-source World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-29-Advancing-Open-source-World-Models","children":"arXiv에 게시된 'Advancing Open-source World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-29 00:00:00+0900+0900","children":"2026년 1월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-29-Advancing-Open-source-World-Models"}]]}]]}],["$","article","2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text","children":"[논문리뷰] World Craft: Agentic Framework to Create Visualizable Worlds via Text"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text","children":"arXiv에 게시된 'World Craft: Agentic Framework to Create Visualizable Worlds via Text' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text"}]]}]]}],["$","article","2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models","children":"[논문리뷰] Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models","children":"arXiv에 게시된 'Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models"}]]}]]}],["$","article","2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment","children":"[논문리뷰] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment","children":"arXiv에 게시된 'TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment"}]]}]]}],["$","article","2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection","children":"[논문리뷰] Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection","children":"arXiv에 게시된 'Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection"}]]}]]}],["$","article","2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training","children":"[논문리뷰] Revisiting Parameter Server in LLM Post-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training","children":"arXiv에 게시된 'Revisiting Parameter Server in LLM Post-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training"}]]}]]}],["$","article","2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep","children":"[논문리뷰] Post-LayerNorm Is Back: Stable, ExpressivE, and Deep"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep","children":"arXiv에 게시된 'Post-LayerNorm Is Back: Stable, ExpressivE, and Deep' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep"}]]}]]}],["$","article","2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences","children":"[논문리뷰] HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences","children":"Taro Watanabe이 arXiv에 게시한 'HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences"}]]}]]}],["$","article","2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery","children":"[논문리뷰] GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery","children":"arXiv에 게시된 'GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery"}]]}]]}],["$","article","2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning","children":"[논문리뷰] FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning","children":"arXiv에 게시된 'FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning"}]]}]]}],["$","article","2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security","children":"[논문리뷰] AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security","children":"arXiv에 게시된 'AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security"}]]}]]}],["$","article","2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning","children":"[논문리뷰] AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning","children":"arXiv에 게시된 'AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning"}]]}]]}],["$","article","2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking","children":"[논문리뷰] AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking","children":"arXiv에 게시된 'AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking"}]]}]]}],["$","article","2026-01-28-A-Pragmatic-VLA-Foundation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-A-Pragmatic-VLA-Foundation-Model","children":"[논문리뷰] A Pragmatic VLA Foundation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-28-A-Pragmatic-VLA-Foundation-Model","children":"arXiv에 게시된 'A Pragmatic VLA Foundation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-28 00:00:00+0900+0900","children":"2026년 1월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-28-A-Pragmatic-VLA-Foundation-Model"}]]}]]}],["$","article","2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code","children":"[논문리뷰] iFSQ: Improving FSQ for Image Generation with 1 Line of Code"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code","children":"arXiv에 게시된 'iFSQ: Improving FSQ for Image Generation with 1 Line of Code' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code"}]]}]]}],["$","article","2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering","children":"[논문리뷰] daVinci-Dev: Agent-native Mid-training for Software Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering","children":"arXiv에 게시된 'daVinci-Dev: Agent-native Mid-training for Software Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering"}]]}]]}],["$","article","2026-01-27-VIBEVOICE-ASR-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report","children":"[논문리뷰] VIBEVOICE-ASR Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report","children":"arXiv에 게시된 'VIBEVOICE-ASR Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report"}]]}]]}],["$","article","2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation","children":"[논문리뷰] The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation","children":"arXiv에 게시된 'The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation"}]]}]]}],["$","article","2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability","children":"[논문리뷰] Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability","children":"arXiv에 게시된 'Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability"}]]}]]}],["$","article","2026-01-27-SkyReels-V3-Technique-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report","children":"[논문리뷰] SkyReels-V3 Technique Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report","children":"arXiv에 게시된 'SkyReels-V3 Technique Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-SkyReels-V3-Technique-Report"}]]}]]}],["$","article","2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility","children":"[논문리뷰] Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility","children":"arXiv에 게시된 'Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility"}]]}]]}],["$","article","2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion","children":"[논문리뷰] STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion","children":"arXiv에 게시된 'STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion"}]]}]]}],["$","article","2026-01-27-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback","children":"[논문리뷰] SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback","children":"arXiv에 게시된 'SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback"}]]}]]}],["$","article","2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents","children":"[논문리뷰] Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents","children":"arXiv에 게시된 'Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents"}]]}]]}],["$","article","2026-01-27-Less-Is-More-Until-It-Breaks-Security-Pitfalls-of-Vision-Token-Compression-in-Large-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Less-Is-More-Until-It-Breaks-Security-Pitfalls-of-Vision-Token-Compression-in-Large-Vision-Language-Models","children":"[논문리뷰] Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Less-Is-More-Until-It-Breaks-Security-Pitfalls-of-Vision-Token-Compression-in-Large-Vision-Language-Models","children":"Guanhong Tao이 arXiv에 게시한 'Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Less-Is-More-Until-It-Breaks-Security-Pitfalls-of-Vision-Token-Compression-in-Large-Vision-Language-Models"}]]}]]}],["$","article","2026-01-27-End-to-End-Joint-ASR-and-Speaker-Role-Diarization-with-Child-Adult-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-End-to-End-Joint-ASR-and-Speaker-Role-Diarization-with-Child-Adult-Interactions","children":"[논문리뷰] End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-End-to-End-Joint-ASR-and-Speaker-Role-Diarization-with-Child-Adult-Interactions","children":"Shrikanth Narayanan이 arXiv에 게시한 'End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-End-to-End-Joint-ASR-and-Speaker-Role-Diarization-with-Child-Adult-Interactions"}]]}]]}],["$","article","2026-01-27-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers","children":"[논문리뷰] Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers","children":"arXiv에 게시된 'Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers"}]]}]]}],["$","article","2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints","children":"[논문리뷰] DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints","children":"arXiv에 게시된 'DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints"}]]}]]}],["$","article","2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal","children":"[논문리뷰] DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal","children":"Jiaxuan You이 arXiv에 게시한 'DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal"}]]}]]}],["$","article","2026-01-27-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs","children":"[논문리뷰] Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs","children":"arXiv에 게시된 'Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs"}]]}]]}],["$","article","2026-01-27-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval","children":"[논문리뷰] CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval","children":"arXiv에 게시된 'CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval"}]]}]]}],["$","article","2026-01-27-Agentic-Very-Long-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Agentic-Very-Long-Video-Understanding","children":"[논문리뷰] Agentic Very Long Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-Agentic-Very-Long-Video-Understanding","children":"arXiv에 게시된 'Agentic Very Long Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-Agentic-Very-Long-Video-Understanding"}]]}]]}],["$","article","2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation","children":"[논문리뷰] AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation","children":"arXiv에 게시된 'AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-27 00:00:00+0900+0900","children":"2026년 1월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-27-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation"}]]}]]}],["$","article","2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents","children":"[논문리뷰] VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents","children":"arXiv에 게시된 'VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-VisGym-Diverse-Customizable-Scalable-Environments-for-Multimodal-Agents"}]]}]]}],["$","article","2026-01-26-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers","children":"[논문리뷰] TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers","children":"arXiv에 게시된 'TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers"}]]}]]}],["$","article","2026-01-26-SWE-Pruner-Self-Adaptive-Context-Pruning-for-Coding-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-SWE-Pruner-Self-Adaptive-Context-Pruning-for-Coding-Agents","children":"[논문리뷰] SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-SWE-Pruner-Self-Adaptive-Context-Pruning-for-Coding-Agents","children":"arXiv에 게시된 'SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-SWE-Pruner-Self-Adaptive-Context-Pruning-for-Coding-Agents"}]]}]]}],["$","article","2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer","children":"[논문리뷰] SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer","children":"arXiv에 게시된 'SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-SALAD-Achieve-High-Sparsity-Attention-via-Efficient-Linear-Attention-Tuning-for-Video-Diffusion-Transformer"}]]}]]}],["$","article","2026-01-26-Memory-V2V-Augmenting-Video-to-Video-Diffusion-Models-with-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Memory-V2V-Augmenting-Video-to-Video-Diffusion-Models-with-Memory","children":"[논문리뷰] Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Memory-V2V-Augmenting-Video-to-Video-Diffusion-Models-with-Memory","children":"arXiv에 게시된 'Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Memory-V2V-Augmenting-Video-to-Video-Diffusion-Models-with-Memory"}]]}]]}],["$","article","2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences","children":"[논문리뷰] MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences","children":"Jianwen Sun이 arXiv에 게시한 'MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences"}]]}]]}],["$","article","2026-01-26-Mecellem-Models-Turkish-Models-Trained-from-Scratch-and-Continually-Pre-trained-for-the-Legal-Domain",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Mecellem-Models-Turkish-Models-Trained-from-Scratch-and-Continually-Pre-trained-for-the-Legal-Domain","children":"[논문리뷰] Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Mecellem-Models-Turkish-Models-Trained-from-Scratch-and-Continually-Pre-trained-for-the-Legal-Domain","children":"arXiv에 게시된 'Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Mecellem-Models-Turkish-Models-Trained-from-Scratch-and-Continually-Pre-trained-for-the-Legal-Domain"}]]}]]}],["$","article","2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report","children":"[논문리뷰] LongCat-Flash-Thinking-2601 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report","children":"arXiv에 게시된 'LongCat-Flash-Thinking-2601 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-LongCat-Flash-Thinking-2601-Technical-Report"}]]}]]}],["$","article","2026-01-26-Knowledge-is-Not-Enough-Injecting-RL-Skills-for-Continual-Adaptation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Knowledge-is-Not-Enough-Injecting-RL-Skills-for-Continual-Adaptation","children":"[논문리뷰] Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Knowledge-is-Not-Enough-Injecting-RL-Skills-for-Continual-Adaptation","children":"arXiv에 게시된 'Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Knowledge-is-Not-Enough-Injecting-RL-Skills-for-Continual-Adaptation"}]]}]]}],["$","article","2026-01-26-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow","children":"[논문리뷰] Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow","children":"arXiv에 게시된 'Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow"}]]}]]}],["$","article","2026-01-26-Inference-Time-Scaling-of-Verification-Self-Evolving-Deep-Research-Agents-via-Test-Time-Rubric-Guided-Verification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Inference-Time-Scaling-of-Verification-Self-Evolving-Deep-Research-Agents-via-Test-Time-Rubric-Guided-Verification","children":"[논문리뷰] Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Inference-Time-Scaling-of-Verification-Self-Evolving-Deep-Research-Agents-via-Test-Time-Rubric-Guided-Verification","children":"arXiv에 게시된 'Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Inference-Time-Scaling-of-Verification-Self-Evolving-Deep-Research-Agents-via-Test-Time-Rubric-Guided-Verification"}]]}]]}],["$","article","2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization","children":"[논문리뷰] Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization","children":"Gabriele Bavota이 arXiv에 게시한 'Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization"}]]}]]}],["$","article","2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","children":"[논문리뷰] Endless Terminals: Scaling RL Environments for Terminal Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","children":"arXiv에 게시된 'Endless Terminals: Scaling RL Environments for Terminal Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents"}]]}]]}],["$","article","2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind","children":"[논문리뷰] Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind","children":"Yi R Fung이 arXiv에 게시한 'Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind"}]]}]]}],["$","article","2026-01-26-DSGym-A-Holistic-Framework-for-Evaluating-and-Training-Data-Science-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-DSGym-A-Holistic-Framework-for-Evaluating-and-Training-Data-Science-Agents","children":"[논문리뷰] DSGym: A Holistic Framework for Evaluating and Training Data Science Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-26-DSGym-A-Holistic-Framework-for-Evaluating-and-Training-Data-Science-Agents","children":"Yongchan Kwon이 arXiv에 게시한 'DSGym: A Holistic Framework for Evaluating and Training Data Science Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-26-DSGym-A-Holistic-Framework-for-Evaluating-and-Training-Data-Science-Agents"}]]}]]}],["$","article","2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior","children":"[논문리뷰] VideoMaMa: Mask-Guided Video Matting via Generative Prior"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior","children":"arXiv에 게시된 'VideoMaMa: Mask-Guided Video Matting via Generative Prior' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior"}]]}]]}],["$","article","2026-01-23-VIOLA-Towards-Video-In-Context-Learning-with-Minimal-Annotations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-VIOLA-Towards-Video-In-Context-Learning-with-Minimal-Annotations","children":"[논문리뷰] VIOLA: Towards Video In-Context Learning with Minimal Annotations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-VIOLA-Towards-Video-In-Context-Learning-with-Minimal-Annotations","children":"Ryo Hachiuma이 arXiv에 게시한 'VIOLA: Towards Video In-Context Learning with Minimal Annotations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-VIOLA-Towards-Video-In-Context-Learning-with-Minimal-Annotations"}]]}]]}],["$","article","2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs","children":"[논문리뷰] Towards Automated Kernel Generation in the Era of LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs","children":"Yixin Shen이 arXiv에 게시한 'Towards Automated Kernel Generation in the Era of LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs"}]]}]]}],["$","article","2026-01-23-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models","children":"[논문리뷰] The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models","children":"arXiv에 게시된 'The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models"}]]}]]}],["$","article","2026-01-23-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces","children":"[논문리뷰] Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces","children":"Harsh Raj이 arXiv에 게시한 'Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces"}]]}]]}],["$","article","2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model","children":"[논문리뷰] Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model","children":"arXiv에 게시된 'Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model"}]]}]]}],["$","article","2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders","children":"[논문리뷰] Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders","children":"arXiv에 게시된 'Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders"}]]}]]}],["$","article","2026-01-23-SAMTok-Representing-Any-Mask-with-Two-Words",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-SAMTok-Representing-Any-Mask-with-Two-Words","children":"[논문리뷰] SAMTok: Representing Any Mask with Two Words"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-SAMTok-Representing-Any-Mask-with-Two-Words","children":"arXiv에 게시된 'SAMTok: Representing Any Mask with Two Words' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-SAMTok-Representing-Any-Mask-with-Two-Words"}]]}]]}],["$","article","2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing","children":"[논문리뷰] Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing","children":"Dingkun Long이 arXiv에 게시한 'Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing"}]]}]]}],["$","article","2026-01-23-Qwen3-TTS-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Qwen3-TTS-Technical-Report","children":"[논문리뷰] Qwen3-TTS Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Qwen3-TTS-Technical-Report","children":"arXiv에 게시된 'Qwen3-TTS Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Qwen3-TTS-Technical-Report"}]]}]]}],["$","article","2026-01-23-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation","children":"[논문리뷰] OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation","children":"arXiv에 게시된 'OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation"}]]}]]}],["$","article","2026-01-23-Numba-Accelerated-2D-Diffusion-Limited-Aggregation-Implementation-and-Fractal-Characterization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Numba-Accelerated-2D-Diffusion-Limited-Aggregation-Implementation-and-Fractal-Characterization","children":"[논문리뷰] Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Numba-Accelerated-2D-Diffusion-Limited-Aggregation-Implementation-and-Fractal-Characterization","children":"arXiv에 게시된 'Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Numba-Accelerated-2D-Diffusion-Limited-Aggregation-Implementation-and-Fractal-Characterization"}]]}]]}],["$","article","2026-01-23-Learning-to-Discover-at-Test-Time",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Learning-to-Discover-at-Test-Time","children":"[논문리뷰] Learning to Discover at Test Time"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Learning-to-Discover-at-Test-Time","children":"arXiv에 게시된 'Learning to Discover at Test Time' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Learning-to-Discover-at-Test-Time"}]]}]]}],["$","article","2026-01-23-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence","children":"[논문리뷰] LLM-in-Sandbox Elicits General Agentic Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence","children":"arXiv에 게시된 'LLM-in-Sandbox Elicits General Agentic Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence"}]]}]]}],["$","article","2026-01-23-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding","children":"[논문리뷰] HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding","children":"arXiv에 게시된 'HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding"}]]}]]}],["$","article","2026-01-23-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience","children":"[논문리뷰] EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience","children":"Linsen Guo이 arXiv에 게시한 'EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience"}]]}]]}],["$","article","2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning","children":"[논문리뷰] Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning","children":"arXiv에 게시된 'Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning"}]]}]]}],["$","article","2026-01-23-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries","children":"[논문리뷰] BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries","children":"arXiv에 게시된 'BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries"}]]}]]}],["$","article","2026-01-23-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion","children":"[논문리뷰] ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion","children":"arXiv에 게시된 'ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion"}]]}]]}],["$","article","2026-01-23-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360","children":"[논문리뷰] 360Anything: Geometry-Free Lifting of Images and Videos to 360°"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-23-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360","children":"arXiv에 게시된 '360Anything: Geometry-Free Lifting of Images and Videos to 360°' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-23 00:00:00+0900+0900","children":"2026년 1월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-23-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360"}]]}]]}],["$","article","2026-01-22-sangkuriang-A-pseudo-spectral-Python-library-for-Korteweg-de-Vries-soliton-simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-sangkuriang-A-pseudo-spectral-Python-library-for-Korteweg-de-Vries-soliton-simulation","children":"[논문리뷰] sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-sangkuriang-A-pseudo-spectral-Python-library-for-Korteweg-de-Vries-soliton-simulation","children":"arXiv에 게시된 'sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-sangkuriang-A-pseudo-spectral-Python-library-for-Korteweg-de-Vries-soliton-simulation"}]]}]]}],["$","article","2026-01-22-XR-Cross-Modal-Agents-for-Composed-Image-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-XR-Cross-Modal-Agents-for-Composed-Image-Retrieval","children":"[논문리뷰] XR: Cross-Modal Agents for Composed Image Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-XR-Cross-Modal-Agents-for-Composed-Image-Retrieval","children":"arXiv에 게시된 'XR: Cross-Modal Agents for Composed Image Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-XR-Cross-Modal-Agents-for-Composed-Image-Retrieval"}]]}]]}],["$","article","2026-01-22-Typhoon-OCR-Open-Vision-Language-Model-For-Thai-Document-Extraction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Typhoon-OCR-Open-Vision-Language-Model-For-Thai-Document-Extraction","children":"[논문리뷰] Typhoon OCR: Open Vision-Language Model For Thai Document Extraction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Typhoon-OCR-Open-Vision-Language-Model-For-Thai-Document-Extraction","children":"arXiv에 게시된 'Typhoon OCR: Open Vision-Language Model For Thai Document Extraction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Typhoon-OCR-Open-Vision-Language-Model-For-Thai-Document-Extraction"}]]}]]}],["$","article","2026-01-22-Typhoon-ASR-Real-time-FastConformer-Transducer-for-Thai-Automatic-Speech-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Typhoon-ASR-Real-time-FastConformer-Transducer-for-Thai-Automatic-Speech-Recognition","children":"[논문리뷰] Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Typhoon-ASR-Real-time-FastConformer-Transducer-for-Thai-Automatic-Speech-Recognition","children":"arXiv에 게시된 'Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Typhoon-ASR-Real-time-FastConformer-Transducer-for-Thai-Automatic-Speech-Recognition"}]]}]]}],["$","article","2026-01-22-The-Responsibility-Vacuum-Organizational-Failure-in-Scaled-Agent-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-The-Responsibility-Vacuum-Organizational-Failure-in-Scaled-Agent-Systems","children":"[논문리뷰] The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-The-Responsibility-Vacuum-Organizational-Failure-in-Scaled-Agent-Systems","children":"Roman Bondar이 arXiv에 게시한 'The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-The-Responsibility-Vacuum-Organizational-Failure-in-Scaled-Agent-Systems"}]]}]]}],["$","article","2026-01-22-RoboBrain-2-5-Depth-in-Sight-Time-in-Mind",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-RoboBrain-2-5-Depth-in-Sight-Time-in-Mind","children":"[논문리뷰] RoboBrain 2.5: Depth in Sight, Time in Mind"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-RoboBrain-2-5-Depth-in-Sight-Time-in-Mind","children":"Yuheng Ji이 arXiv에 게시한 'RoboBrain 2.5: Depth in Sight, Time in Mind' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-RoboBrain-2-5-Depth-in-Sight-Time-in-Mind"}]]}]]}],["$","article","2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World","children":"[논문리뷰] Rethinking Video Generation Model for the Embodied World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World","children":"arXiv에 게시된 'Rethinking Video Generation Model for the Embodied World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Rethinking-Video-Generation-Model-for-the-Embodied-World"}]]}]]}],["$","article","2026-01-22-Render-of-Thought-Rendering-Textual-Chain-of-Thought-as-Images-for-Visual-Latent-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Render-of-Thought-Rendering-Textual-Chain-of-Thought-as-Images-for-Visual-Latent-Reasoning","children":"[논문리뷰] Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Render-of-Thought-Rendering-Textual-Chain-of-Thought-as-Images-for-Visual-Latent-Reasoning","children":"arXiv에 게시된 'Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Render-of-Thought-Rendering-Textual-Chain-of-Thought-as-Images-for-Visual-Latent-Reasoning"}]]}]]}],["$","article","2026-01-22-Quantifying-Speaker-Embedding-Phonological-Rule-Interactions-in-Accented-Speech-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Quantifying-Speaker-Embedding-Phonological-Rule-Interactions-in-Accented-Speech-Synthesis","children":"[논문리뷰] Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Quantifying-Speaker-Embedding-Phonological-Rule-Interactions-in-Accented-Speech-Synthesis","children":"Jihwan Lee이 arXiv에 게시한 'Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Quantifying-Speaker-Embedding-Phonological-Rule-Interactions-in-Accented-Speech-Synthesis"}]]}]]}],["$","article","2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance","children":"[논문리뷰] Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance","children":"arXiv에 게시된 'Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Paper2Rebuttal-A-Multi-Agent-Framework-for-Transparent-Author-Response-Assistance"}]]}]]}],["$","article","2026-01-22-Numina-Lean-Agent-An-Open-and-General-Agentic-Reasoning-System-for-Formal-Mathematics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Numina-Lean-Agent-An-Open-and-General-Agentic-Reasoning-System-for-Formal-Mathematics","children":"[논문리뷰] Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Numina-Lean-Agent-An-Open-and-General-Agentic-Reasoning-System-for-Formal-Mathematics","children":"arXiv에 게시된 'Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Numina-Lean-Agent-An-Open-and-General-Agentic-Reasoning-System-for-Formal-Mathematics"}]]}]]}],["$","article","2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents","children":"[논문리뷰] MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents","children":"Samiul Alam이 arXiv에 게시한 'MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents"}]]}]]}],["$","article","2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models","children":"[논문리뷰] Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models","children":"arXiv에 게시된 'Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models"}]]}]]}],["$","article","2026-01-22-FinVault-Benchmarking-Financial-Agent-Safety-in-Execution-Grounded-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-FinVault-Benchmarking-Financial-Agent-Safety-in-Execution-Grounded-Environments","children":"[논문리뷰] FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-FinVault-Benchmarking-Financial-Agent-Safety-in-Execution-Grounded-Environments","children":"arXiv에 게시된 'FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-FinVault-Benchmarking-Financial-Agent-Safety-in-Execution-Grounded-Environments"}]]}]]}],["$","article","2026-01-22-Facilitating-Proactive-and-Reactive-Guidance-for-Decision-Making-on-the-Web-A-Design-Probe-with-WebSeek",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Facilitating-Proactive-and-Reactive-Guidance-for-Decision-Making-on-the-Web-A-Design-Probe-with-WebSeek","children":"[논문리뷰] Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Facilitating-Proactive-and-Reactive-Guidance-for-Decision-Making-on-the-Web-A-Design-Probe-with-WebSeek","children":"Arpit Narechania이 arXiv에 게시한 'Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Facilitating-Proactive-and-Reactive-Guidance-for-Decision-Making-on-the-Web-A-Design-Probe-with-WebSeek"}]]}]]}],["$","article","2026-01-22-FARE-Fast-Slow-Agentic-Robotic-Exploration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-FARE-Fast-Slow-Agentic-Robotic-Exploration","children":"[논문리뷰] FARE: Fast-Slow Agentic Robotic Exploration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-FARE-Fast-Slow-Agentic-Robotic-Exploration","children":"Jingsong Liang이 arXiv에 게시한 'FARE: Fast-Slow Agentic Robotic Exploration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-FARE-Fast-Slow-Agentic-Robotic-Exploration"}]]}]]}],["$","article","2026-01-22-Agentic-Reasoning-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Agentic-Reasoning-for-Large-Language-Models","children":"[논문리뷰] Agentic Reasoning for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-Agentic-Reasoning-for-Large-Language-Models","children":"arXiv에 게시된 'Agentic Reasoning for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-Agentic-Reasoning-for-Large-Language-Models"}]]}]]}],["$","article","2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization","children":"[논문리뷰] AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization","children":"arXiv에 게시된 'AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-22 00:00:00+0900+0900","children":"2026년 1월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-22-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization"}]]}]]}],["$","article","2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation","children":"[논문리뷰] UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation","children":"arXiv에 게시된 'UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-UniX-Unifying-Autoregression-and-Diffusion-for-Chest-X-Ray-Understanding-and-Generation"}]]}]]}],["$","article","2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning","children":"[논문리뷰] Toward Efficient Agents: Memory, Tool learning, and Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning","children":"arXiv에 게시된 'Toward Efficient Agents: Memory, Tool learning, and Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning"}]]}]]}],["$","article","2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents","children":"[논문리뷰] ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents","children":"arXiv에 게시된 'ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents"}]]}]]}],["$","article","2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning","children":"[논문리뷰] Think3D: Thinking with Space for Spatial Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning","children":"Yuhan Wu이 arXiv에 게시한 'Think3D: Thinking with Space for Spatial Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Think3D-Thinking-with-Space-for-Spatial-Reasoning"}]]}]]}],["$","article","2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment","children":"[논문리뷰] SciCoQA: Quality Assurance for Scientific Paper--Code Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment","children":"arXiv에 게시된 'SciCoQA: Quality Assurance for Scientific Paper--Code Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-SciCoQA-Quality-Assurance-for-Scientific-Paper-Code-Alignment"}]]}]]}],["$","article","2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models","children":"[논문리뷰] PRiSM: Benchmarking Phone Realization in Speech Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models","children":"arXiv에 게시된 'PRiSM: Benchmarking Phone Realization in Speech Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-PRiSM-Benchmarking-Phone-Realization-in-Speech-Models"}]]}]]}],["$","article","2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing","children":"[논문리뷰] On the Evidentiary Limits of Membership Inference for Copyright Auditing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing","children":"Marten van Dijk이 arXiv에 게시한 'On the Evidentiary Limits of Membership Inference for Copyright Auditing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing"}]]}]]}],["$","article","2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","children":"[논문리뷰] OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer","children":"arXiv에 게시된 'OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer"}]]}]]}],["$","article","2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models","children":"[논문리뷰] MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models","children":"arXiv에 게시된 'MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-MemoryRewardBench-Benchmarking-Reward-Models-for-Long-Term-Memory-Management-in-Large-Language-Models"}]]}]]}],["$","article","2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR","children":"[논문리뷰] LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR","children":"arXiv에 게시된 'LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR"}]]}]]}],["$","article","2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals","children":"[논문리뷰] LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals","children":"arXiv에 게시된 'LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-LIBERTy-A-Causal-Framework-for-Benchmarking-Concept-Based-Explanations-of-LLMs-with-Structural-Counterfactuals"}]]}]]}],["$","article","2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning","children":"[논문리뷰] KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning","children":"Aleksandr I. Panov이 arXiv에 게시한 'KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning"}]]}]]}],["$","article","2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs","children":"[논문리뷰] FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs","children":"arXiv에 게시된 'FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs"}]]}]]}],["$","article","2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD","children":"[논문리뷰] Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD","children":"arXiv에 게시된 'Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD"}]]}]]}],["$","article","2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","children":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","children":"arXiv에 게시된 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation"}]]}]]}],["$","article","2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization","children":"[논문리뷰] Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization","children":"arXiv에 게시된 'Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization"}]]}]]}],["$","article","2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning","children":"[논문리뷰] Aligning Agentic World Models via Knowledgeable Experience Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning","children":"arXiv에 게시된 'Aligning Agentic World Models via Knowledgeable Experience Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Aligning-Agentic-World-Models-via-Knowledgeable-Experience-Learning"}]]}]]}],["$","article","2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search","children":"[논문리뷰] Agentic-R: Learning to Retrieve for Agentic Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search","children":"Daiting Shi이 arXiv에 게시한 'Agentic-R: Learning to Retrieve for Agentic Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search"}]]}]]}],["$","article","2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey","children":"[논문리뷰] Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey","children":"arXiv에 게시된 'Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey"}]]}]]}],["$","article","2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus","children":"[논문리뷰] A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus","children":"Özay Ezerceli이 arXiv에 게시한 'A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-A-Hybrid-Protocol-for-Large-Scale-Semantic-Dataset-Generation-in-Low-Resource-Languages-The-Turkish-Semantic-Relations-Corpus"}]]}]]}],["$","article","2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification","children":"[논문리뷰] A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification","children":"arXiv에 게시된 'A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-21-A-BERTology-View-of-LLM-Orchestrations-Token-and-Layer-Selective-Probes-for-Efficient-Single-Pass-Classification"}]]}]]}],["$","article","2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation","children":"[논문리뷰] YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation","children":"arXiv에 게시된 'YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-YaPO-Learnable-Sparse-Activation-Steering-Vectors-for-Domain-Adaptation"}]]}]]}],["$","article","2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models","children":"[논문리뷰] The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models","children":"Jack Lindsey이 arXiv에 게시한 'The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-The-Assistant-Axis-Situating-and-Stabilizing-the-Default-Persona-of-Language-Models"}]]}]]}],["$","article","2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs","children":"[논문리뷰] Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs","children":"Lecheng Yan이 arXiv에 게시한 'Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-Spurious-Rewards-Paradox-Mechanistically-Understanding-How-RLVR-Activates-Memorization-Shortcuts-in-LLMs"}]]}]]}],["$","article","2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature","children":"[논문리뷰] SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature","children":"arXiv에 게시된 'SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-SIN-Bench-Tracing-Native-Evidence-Chains-in-Long-Context-Multimodal-Scientific-Interleaved-Literature"}]]}]]}],["$","article","2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge","children":"[논문리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge","children":"arXiv에 게시된 'Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge"}]]}]]}],["$","article","2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation","children":"[논문리뷰] Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation","children":"Ziyang Yan이 arXiv에 게시한 'Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-Medical-SAM3-A-Foundation-Model-for-Universal-Prompt-Driven-Medical-Image-Segmentation"}]]}]]}],["$","article","2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","children":"[논문리뷰] CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation","children":"Hengshuang이 arXiv에 게시한 'CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-CoDance-An-Unbind-Rebind-Paradigm-for-Robust-Multi-Subject-Animation"}]]}]]}],["$","article","2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion","children":"[논문리뷰] CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion","children":"arXiv에 게시된 'CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-CLARE-Continual-Learning-for-Vision-Language-Action-Models-via-Autonomous-Adapter-Routing-and-Expansion"}]]}]]}],["$","article","2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development","children":"[논문리뷰] ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development","children":"arXiv에 게시된 'ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-ABC-Bench-Benchmarking-Agentic-Backend-Coding-in-Real-World-Development"}]]}]]}],["$","article","2026-01-19-When-Personalization-Misleads-Understanding-and-Mitigating-Hallucinations-in-Personalized-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-When-Personalization-Misleads-Understanding-and-Mitigating-Hallucinations-in-Personalized-LLMs","children":"[논문리뷰] When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-When-Personalization-Misleads-Understanding-and-Mitigating-Hallucinations-in-Personalized-LLMs","children":"arXiv에 게시된 'When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-19 00:00:00+0900+0900","children":"2026년 1월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-19-When-Personalization-Misleads-Understanding-and-Mitigating-Hallucinations-in-Personalized-LLMs"}]]}]]}],["$","article","2026-01-19-Reasoning-Models-Generate-Societies-of-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-Reasoning-Models-Generate-Societies-of-Thought","children":"[논문리뷰] Reasoning Models Generate Societies of Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-Reasoning-Models-Generate-Societies-of-Thought","children":"James Evans이 arXiv에 게시한 'Reasoning Models Generate Societies of Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-19 00:00:00+0900+0900","children":"2026년 1월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-19-Reasoning-Models-Generate-Societies-of-Thought"}]]}]]}],["$","article","2026-01-19-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes","children":"[논문리뷰] More Images, More Problems? A Controlled Analysis of VLM Failure Modes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes","children":"arXiv에 게시된 'More Images, More Problems? A Controlled Analysis of VLM Failure Modes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-19 00:00:00+0900+0900","children":"2026년 1월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-19-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes"}]]}]]}],["$","article","2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models","children":"[논문리뷰] Language of Thought Shapes Output Diversity in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models","children":"arXiv에 게시된 'Language of Thought Shapes Output Diversity in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-19 00:00:00+0900+0900","children":"2026년 1월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models"}]]}]]}],["$","article","2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems","children":"[논문리뷰] AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems","children":"Xipeng Qiu이 arXiv에 게시한 'AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-19 00:00:00+0900+0900","children":"2026년 1월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-19-AstroReason-Bench-Evaluating-Unified-Agentic-Planning-across-Heterogeneous-Space-Planning-Problems"}]]}]]}],["$","article","2026-01-16-VQ-Seg-Vector-Quantized-Token-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-VQ-Seg-Vector-Quantized-Token-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation","children":"[논문리뷰] VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-VQ-Seg-Vector-Quantized-Token-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation","children":"Lei Zhu이 arXiv에 게시한 'VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-VQ-Seg-Vector-Quantized-Token-Perturbation-for-Semi-Supervised-Medical-Image-Segmentation"}]]}]]}],["$","article","2026-01-16-VIBE-Visual-Instruction-Based-Editor",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-VIBE-Visual-Instruction-Based-Editor","children":"[논문리뷰] VIBE: Visual Instruction Based Editor"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-VIBE-Visual-Instruction-Based-Editor","children":"Bulat Suleimanov이 arXiv에 게시한 'VIBE: Visual Instruction Based Editor' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-VIBE-Visual-Instruction-Based-Editor"}]]}]]}],["$","article","2026-01-16-Urban-Socio-Semantic-Segmentation-with-Vision-Language-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Urban-Socio-Semantic-Segmentation-with-Vision-Language-Reasoning","children":"[논문리뷰] Urban Socio-Semantic Segmentation with Vision-Language Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Urban-Socio-Semantic-Segmentation-with-Vision-Language-Reasoning","children":"arXiv에 게시된 'Urban Socio-Semantic Segmentation with Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Urban-Socio-Semantic-Segmentation-with-Vision-Language-Reasoning"}]]}]]}],["$","article","2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","children":"[논문리뷰] Transition Matching Distillation for Fast Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation","children":"arXiv에 게시된 'Transition Matching Distillation for Fast Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Transition-Matching-Distillation-for-Fast-Video-Generation"}]]}]]}],["$","article","2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering","children":"[논문리뷰] Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering","children":"arXiv에 게시된 'Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Toward-Ultra-Long-Horizon-Agentic-Science-Cognitive-Accumulation-for-Machine-Learning-Engineering"}]]}]]}],["$","article","2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback","children":"[논문리뷰] ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback","children":"Shikun Zhang이 arXiv에 게시한 'ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-ToolSafe-Enhancing-Tool-Invocation-Safety-of-LLM-based-agents-via-Proactive-Step-level-Guardrail-and-Feedback"}]]}]]}],["$","article","2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders","children":"[논문리뷰] Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders","children":"arXiv에 게시된 'Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Think-Then-Generate-Reasoning-Aware-Text-to-Image-Diffusion-with-LLM-Encoders"}]]}]]}],["$","article","2026-01-16-STEP3-VL-10B-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-STEP3-VL-10B-Technical-Report","children":"[논문리뷰] STEP3-VL-10B Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-STEP3-VL-10B-Technical-Report","children":"arXiv에 게시된 'STEP3-VL-10B Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-STEP3-VL-10B-Technical-Report"}]]}]]}],["$","article","2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs","children":"[논문리뷰] Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs","children":"arXiv에 게시된 'Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Rewarding-the-Rare-Uniqueness-Aware-RL-for-Creative-Problem-Solving-in-LLMs"}]]}]]}],["$","article","2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding","children":"[논문리뷰] Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding","children":"Mohammadreza Salehi이 arXiv에 게시한 'Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Molmo2-Open-Weights-and-Data-for-Vision-Language-Models-with-Video-Understanding-and-Grounding"}]]}]]}],["$","article","2026-01-16-MatchTIR-Fine-Grained-Supervision-for-Tool-Integrated-Reasoning-via-Bipartite-Matching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-MatchTIR-Fine-Grained-Supervision-for-Tool-Integrated-Reasoning-via-Bipartite-Matching","children":"[논문리뷰] MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-MatchTIR-Fine-Grained-Supervision-for-Tool-Integrated-Reasoning-via-Bipartite-Matching","children":"arXiv에 게시된 'MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-MatchTIR-Fine-Grained-Supervision-for-Tool-Integrated-Reasoning-via-Bipartite-Matching"}]]}]]}],["$","article","2026-01-16-LSRIF-Logic-Structured-Reinforcement-Learning-for-Instruction-Following",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-LSRIF-Logic-Structured-Reinforcement-Learning-for-Instruction-Following","children":"[논문리뷰] LSRIF: Logic-Structured Reinforcement Learning for Instruction Following"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-LSRIF-Logic-Structured-Reinforcement-Learning-for-Instruction-Following","children":"arXiv에 게시된 'LSRIF: Logic-Structured Reinforcement Learning for Instruction Following' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-LSRIF-Logic-Structured-Reinforcement-Learning-for-Instruction-Following"}]]}]]}],["$","article","2026-01-16-FlowAct-R1-Towards-Interactive-Humanoid-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-FlowAct-R1-Towards-Interactive-Humanoid-Video-Generation","children":"[논문리뷰] FlowAct-R1: Towards Interactive Humanoid Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-FlowAct-R1-Towards-Interactive-Humanoid-Video-Generation","children":"arXiv에 게시된 'FlowAct-R1: Towards Interactive Humanoid Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-FlowAct-R1-Towards-Interactive-Humanoid-Video-Generation"}]]}]]}],["$","article","2026-01-16-EvasionBench-Detecting-Evasive-Answers-in-Financial-QA-via-Multi-Model-Consensus-and-LLM-as-Judge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-EvasionBench-Detecting-Evasive-Answers-in-Financial-QA-via-Multi-Model-Consensus-and-LLM-as-Judge","children":"[논문리뷰] EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-EvasionBench-Detecting-Evasive-Answers-in-Financial-QA-via-Multi-Model-Consensus-and-LLM-as-Judge","children":"Yi Yang이 arXiv에 게시한 'EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-EvasionBench-Detecting-Evasive-Answers-in-Financial-QA-via-Multi-Model-Consensus-and-LLM-as-Judge"}]]}]]}],["$","article","2026-01-16-DanQing-An-Up-to-Date-Large-Scale-Chinese-Vision-Language-Pre-training-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-DanQing-An-Up-to-Date-Large-Scale-Chinese-Vision-Language-Pre-training-Dataset","children":"[논문리뷰] DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-DanQing-An-Up-to-Date-Large-Scale-Chinese-Vision-Language-Pre-training-Dataset","children":"Lan Wu이 arXiv에 게시한 'DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-DanQing-An-Up-to-Date-Large-Scale-Chinese-Vision-Language-Pre-training-Dataset"}]]}]]}],["$","article","2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning","children":"[논문리뷰] Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning","children":"arXiv에 게시된 'Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning"}]]}]]}],["$","article","2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation","children":"[논문리뷰] CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation","children":"arXiv에 게시된 'CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-CoF-T2I-Video-Models-as-Pure-Visual-Reasoners-for-Text-to-Image-Generation"}]]}]]}],["$","article","2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning","children":"[논문리뷰] Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning","children":"arXiv에 게시된 'Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning"}]]}]]}],["$","article","2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images","children":"[논문리뷰] Alterbute: Editing Intrinsic Attributes of Objects in Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images","children":"arXiv에 게시된 'Alterbute: Editing Intrinsic Attributes of Objects in Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Alterbute-Editing-Intrinsic-Attributes-of-Objects-in-Images"}]]}]]}],["$","article","2026-01-16-Action100M-A-Large-scale-Video-Action-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Action100M-A-Large-scale-Video-Action-Dataset","children":"[논문리뷰] Action100M: A Large-scale Video Action Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-Action100M-A-Large-scale-Video-Action-Dataset","children":"arXiv에 게시된 'Action100M: A Large-scale Video Action Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-Action100M-A-Large-scale-Video-Action-Dataset"}]]}]]}],["$","article","2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5","children":"[논문리뷰] A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5","children":"Yutao Wu이 arXiv에 게시한 'A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-16 00:00:00+0900+0900","children":"2026년 1월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5"}]]}]]}],["$","article","2026-01-15-TranslateGemma-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-TranslateGemma-Technical-Report","children":"[논문리뷰] TranslateGemma Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-TranslateGemma-Technical-Report","children":"arXiv에 게시된 'TranslateGemma Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-TranslateGemma-Technical-Report"}]]}]]}],["$","article","2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory","children":"[논문리뷰] The AI Hippocampus: How Far are We From Human Memory?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory","children":"Tong Wu이 arXiv에 게시한 'The AI Hippocampus: How Far are We From Human Memory?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-The-AI-Hippocampus-How-Far-are-We-From-Human-Memory"}]]}]]}],["$","article","2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL","children":"[논문리뷰] SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL","children":"arXiv에 게시된 'SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-SkinFlow-Efficient-Information-Transmission-for-Open-Dermatological-Diagnosis-via-Dynamic-Visual-Encoding-and-Staged-RL"}]]}]]}],["$","article","2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding","children":"[논문리뷰] OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding","children":"arXiv에 게시된 'OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-OpenVoxel-Training-Free-Grouping-and-Captioning-Voxels-for-Open-Vocabulary-3D-Scene-Understanding"}]]}]]}],["$","article","2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models","children":"[논문리뷰] Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models","children":"Wenjie Li이 arXiv에 게시한 'Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Imagine-then-Plan-Agent-Learning-from-Adaptive-Lookahead-with-World-Models"}]]}]]}],["$","article","2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations","children":"[논문리뷰] Geometric Stability: The Missing Axis of Representations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations","children":"pcr2120이 arXiv에 게시한 'Geometric Stability: The Missing Axis of Representations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Geometric-Stability-The-Missing-Axis-of-Representations"}]]}]]}],["$","article","2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection","children":"[논문리뷰] FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection","children":"arXiv에 게시된 'FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-FocusUI-Efficient-UI-Grounding-via-Position-Preserving-Visual-Token-Selection"}]]}]]}],["$","article","2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models","children":"[논문리뷰] Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models","children":"Xiao Yang이 arXiv에 게시한 'Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models"}]]}]]}],["$","article","2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning","children":"[논문리뷰] Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning","children":"arXiv에 게시된 'Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning"}]]}]]}],["$","article","2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents","children":"[논문리뷰] ExpSeek: Self-Triggered Experience Seeking for Web Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents","children":"arXiv에 게시된 'ExpSeek: Self-Triggered Experience Seeking for Web Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-ExpSeek-Self-Triggered-Experience-Seeking-for-Web-Agents"}]]}]]}],["$","article","2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines","children":"[논문리뷰] EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines","children":"arXiv에 게시된 'EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-EvoFSM-Controllable-Self-Evolution-for-Deep-Research-with-Finite-State-Machines"}]]}]]}],["$","article","2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","children":"[논문리뷰] Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering","children":"Ayush Tewari이 arXiv에 게시한 'Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Efficient-Camera-Controlled-Video-Generation-of-Static-Scenes-via-Sparse-Diffusion-and-3D-Rendering"}]]}]]}],["$","article","2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning","children":"[논문리뷰] Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning","children":"arXiv에 게시된 'Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Distribution-Aligned-Sequence-Distillation-for-Superior-Long-CoT-Reasoning"}]]}]]}],["$","article","2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation","children":"[논문리뷰] DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation","children":"arXiv에 게시된 'DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-DeepResearchEval-An-Automated-Framework-for-Deep-Research-Task-Construction-and-Agentic-Evaluation"}]]}]]}],["$","article","2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization","children":"[논문리뷰] Controlled Self-Evolution for Algorithmic Code Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization","children":"arXiv에 게시된 'Controlled Self-Evolution for Algorithmic Code Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization"}]]}]]}],["$","article","2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity","children":"[논문리뷰] Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity","children":"Chi Zhang이 arXiv에 게시한 'Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity"}]]}]]}],["$","article","2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation","children":"[논문리뷰] A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation","children":"Kai He이 arXiv에 게시한 'A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-A3-Bench-Benchmarking-Memory-Driven-Scientific-Reasoning-via-Anchor-and-Attractor-Activation"}]]}]]}],["$","article","2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory","children":"[논문리뷰] VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory","children":"arXiv에 게시된 'VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-VLingNav-Embodied-Navigation-with-Adaptive-Reasoning-and-Visual-Assisted-Linguistic-Memory"}]]}]]}],["$","article","2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale","children":"[논문리뷰] User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale","children":"arXiv에 게시된 'User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-User-Oriented-Multi-Turn-Dialogue-Generation-with-Tool-Use-at-scale"}]]}]]}],["$","article","2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking","children":"[논문리뷰] Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking","children":"Zhen Ye이 arXiv에 게시한 'Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-Towards-Comprehensive-Stage-wise-Benchmarking-of-Large-Language-Models-in-Fact-Checking"}]]}]]}],["$","article","2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents","children":"[논문리뷰] The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents","children":"Junjue Wang이 arXiv에 게시한 'The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents"}]]}]]}],["$","article","2026-01-14-Solar-Open-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Solar-Open-Technical-Report","children":"[논문리뷰] Solar Open Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Solar-Open-Technical-Report","children":"arXiv에 게시된 'Solar Open Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-Solar-Open-Technical-Report"}]]}]]}],["$","article","2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices","children":"[논문리뷰] SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices","children":"arXiv에 게시된 'SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-SnapGen-Unleashing-Diffusion-Transformers-for-Efficient-High-Fidelity-Image-Generation-on-Edge-Devices"}]]}]]}],["$","article","2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands","children":"[논문리뷰] ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands","children":"arXiv에 게시된 'ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-ShowUI-π-Flow-based-Generative-Models-as-GUI-Dexterous-Hands"}]]}]]}],["$","article","2026-01-14-Motion-Attribution-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation","children":"[논문리뷰] Motion Attribution for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation","children":"arXiv에 게시된 'Motion Attribution for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-Motion-Attribution-for-Video-Generation"}]]}]]}],["$","article","2026-01-14-Ministral-3",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Ministral-3","children":"[논문리뷰] Ministral 3"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Ministral-3","children":"arXiv에 게시된 'Ministral 3' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-Ministral-3"}]]}]]}],["$","article","2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning","children":"[논문리뷰] MemoBrain: Executive Memory as an Agentic Brain for Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning","children":"Zheng Liu이 arXiv에 게시한 'MemoBrain: Executive Memory as an Agentic Brain for Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-MemoBrain-Executive-Memory-as-an-Agentic-Brain-for-Reasoning"}]]}]]}],["$","article","2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences","children":"[논문리뷰] MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences","children":"Rui Xu이 arXiv에 게시한 'MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-MemGovern-Enhancing-Code-Agents-through-Learning-from-Governed-Human-Experiences"}]]}]]}],["$","article","2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions","children":"[논문리뷰] KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions","children":"Chenglong Li이 arXiv에 게시한 'KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-KnowMe-Bench-Benchmarking-Person-Understanding-for-Lifelong-Digital-Companions"}]]}]]}],["$","article","2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning","children":"[논문리뷰] JudgeRLVR: Judge First, Generate Second for Efficient Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning","children":"Sujian Li이 arXiv에 게시한 'JudgeRLVR: Judge First, Generate Second for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-JudgeRLVR-Judge-First-Generate-Second-for-Efficient-Reasoning"}]]}]]}],["$","article","2026-01-14-EpiCaR-Knowing-What-You-Dont-Know-Matters-for-Better-Reasoning-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-EpiCaR-Knowing-What-You-Dont-Know-Matters-for-Better-Reasoning-in-LLMs","children":"[논문리뷰] EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-EpiCaR-Knowing-What-You-Dont-Know-Matters-for-Better-Reasoning-in-LLMs","children":"arXiv에 게시된 'EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-EpiCaR-Knowing-What-You-Dont-Know-Matters-for-Better-Reasoning-in-LLMs"}]]}]]}],["$","article","2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance","children":"[논문리뷰] End-to-End Video Character Replacement without Structural Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance","children":"arXiv에 게시된 'End-to-End Video Character Replacement without Structural Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance"}]]}]]}],["$","article","2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","children":"[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","children":"arXiv에 게시된 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking"}]]}]]}],["$","article","2026-01-14-Aligning-Text-Code-and-Vision-A-Multi-Objective-Reinforcement-Learning-Framework-for-Text-to-Visualization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Aligning-Text-Code-and-Vision-A-Multi-Objective-Reinforcement-Learning-Framework-for-Text-to-Visualization","children":"[논문리뷰] Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-14-Aligning-Text-Code-and-Vision-A-Multi-Objective-Reinforcement-Learning-Framework-for-Text-to-Visualization","children":"arXiv에 게시된 'Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-14-Aligning-Text-Code-and-Vision-A-Multi-Objective-Reinforcement-Learning-Framework-for-Text-to-Visualization"}]]}]]}],["$","article","2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests","children":"[논문리뷰] X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests","children":"Jane Luo이 arXiv에 게시한 'X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests"}]]}]]}],["$","article","2026-01-13-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models","children":"[논문리뷰] What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models","children":"arXiv에 게시된 'What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models"}]]}]]}],["$","article","2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning","children":"[논문리뷰] Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning","children":"Shuo Zhang이 arXiv에 게시한 'Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning"}]]}]]}],["$","article","2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","children":"[논문리뷰] TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","children":"Hao Wang이 arXiv에 게시한 'TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning"}]]}]]}],["$","article","2026-01-13-Structured-Episodic-Event-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Structured-Episodic-Event-Memory","children":"[논문리뷰] Structured Episodic Event Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Structured-Episodic-Event-Memory","children":"arXiv에 게시된 'Structured Episodic Event Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Structured-Episodic-Event-Memory"}]]}]]}],["$","article","2026-01-13-PaCoRe-Learning-to-Scale-Test-Time-Compute-with-Parallel-Coordinated-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-PaCoRe-Learning-to-Scale-Test-Time-Compute-with-Parallel-Coordinated-Reasoning","children":"[논문리뷰] PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-PaCoRe-Learning-to-Scale-Test-Time-Compute-with-Parallel-Coordinated-Reasoning","children":"arXiv에 게시된 'PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-PaCoRe-Learning-to-Scale-Test-Time-Compute-with-Parallel-Coordinated-Reasoning"}]]}]]}],["$","article","2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning","children":"[논문리뷰] OpenTinker: Separating Concerns in Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning","children":"Jiaxuan You이 arXiv에 게시한 'OpenTinker: Separating Concerns in Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-OpenTinker-Separating-Concerns-in-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2026-01-13-On-the-Fallacy-of-Global-Token-Perplexity-in-Spoken-Language-Model-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-On-the-Fallacy-of-Global-Token-Perplexity-in-Spoken-Language-Model-Evaluation","children":"[논문리뷰] On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-On-the-Fallacy-of-Global-Token-Perplexity-in-Spoken-Language-Model-Evaluation","children":"Ju-Chieh Chou이 arXiv에 게시한 'On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-On-the-Fallacy-of-Global-Token-Perplexity-in-Spoken-Language-Model-Evaluation"}]]}]]}],["$","article","2026-01-13-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent","children":"[논문리뷰] OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent","children":"arXiv에 게시된 'OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent"}]]}]]}],["$","article","2026-01-13-MegaFlow-Large-Scale-Distributed-Orchestration-System-for-the-Agentic-Era",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-MegaFlow-Large-Scale-Distributed-Orchestration-System-for-the-Agentic-Era","children":"[논문리뷰] MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-MegaFlow-Large-Scale-Distributed-Orchestration-System-for-the-Agentic-Era","children":"Fan Zhou이 arXiv에 게시한 'MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-MegaFlow-Large-Scale-Distributed-Orchestration-System-for-the-Agentic-Era"}]]}]]}],["$","article","2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head","children":"[논문리뷰] MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head","children":"arXiv에 게시된 'MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head"}]]}]]}],["$","article","2026-01-13-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors","children":"[논문리뷰] Lost in the Noise: How Reasoning Models Fail with Contextual Distractors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors","children":"arXiv에 게시된 'Lost in the Noise: How Reasoning Models Fail with Contextual Distractors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors"}]]}]]}],["$","article","2026-01-13-GlimpRouter-Efficient-Collaborative-Inference-by-Glimpsing-One-Token-of-Thoughts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-GlimpRouter-Efficient-Collaborative-Inference-by-Glimpsing-One-Token-of-Thoughts","children":"[논문리뷰] GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-GlimpRouter-Efficient-Collaborative-Inference-by-Glimpsing-One-Token-of-Thoughts","children":"arXiv에 게시된 'GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-GlimpRouter-Efficient-Collaborative-Inference-by-Glimpsing-One-Token-of-Thoughts"}]]}]]}],["$","article","2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration","children":"[논문리뷰] ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration","children":"arXiv에 게시된 'ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-ET-Agent-Incentivizing-Effective-Tool-Integrated-Reasoning-Agent-via-Behavior-Calibration"}]]}]]}],["$","article","2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving","children":"[논문리뷰] DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving","children":"arXiv에 게시된 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-DrivingGen-A-Comprehensive-Benchmark-for-Generative-Video-World-Models-in-Autonomous-Driving"}]]}]]}],["$","article","2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data","children":"[논문리뷰] Dr. Zero: Self-Evolving Search Agents without Training Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data","children":"Shaoliang Nie이 arXiv에 게시한 'Dr. Zero: Self-Evolving Search Agents without Training Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Dr-Zero-Self-Evolving-Search-Agents-without-Training-Data"}]]}]]}],["$","article","2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction","children":"[논문리뷰] Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction","children":"Zhengkang Guo이 arXiv에 게시한 'Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Controllable-Memory-Usage-Balancing-Anchoring-and-Innovation-in-Long-Term-Human-Agent-Interaction"}]]}]]}],["$","article","2026-01-13-Boosting-Latent-Diffusion-Models-via-Disentangled-Representation-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Boosting-Latent-Diffusion-Models-via-Disentangled-Representation-Alignment","children":"[논문리뷰] Boosting Latent Diffusion Models via Disentangled Representation Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Boosting-Latent-Diffusion-Models-via-Disentangled-Representation-Alignment","children":"arXiv에 게시된 'Boosting Latent Diffusion Models via Disentangled Representation Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Boosting-Latent-Diffusion-Models-via-Disentangled-Representation-Alignment"}]]}]]}],["$","article","2026-01-13-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models","children":"[논문리뷰] Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models","children":"Chenchen Jing이 arXiv에 게시한 'Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models"}]]}]]}],["$","article","2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language","children":"[논문리뷰] BabyVision: Visual Reasoning Beyond Language"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language","children":"Yiyan Liang이 arXiv에 게시한 'BabyVision: Visual Reasoning Beyond Language' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-BabyVision-Visual-Reasoning-Beyond-Language"}]]}]]}],["$","article","2026-01-13-Are-LLM-Decisions-Faithful-to-Verbal-Confidence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Are-LLM-Decisions-Faithful-to-Verbal-Confidence","children":"[논문리뷰] Are LLM Decisions Faithful to Verbal Confidence?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-Are-LLM-Decisions-Faithful-to-Verbal-Confidence","children":"arXiv에 게시된 'Are LLM Decisions Faithful to Verbal Confidence?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-Are-LLM-Decisions-Faithful-to-Verbal-Confidence"}]]}]]}],["$","article","2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction","children":"[논문리뷰] VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction","children":"Yu Sun이 arXiv에 게시한 'VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-VideoAR-Autoregressive-Video-Generation-via-Next-Frame-Scale-Prediction"}]]}]]}],["$","article","2026-01-12-Thinking-with-Map-Reinforced-Parallel-Map-Augmented-Agent-for-Geolocalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Thinking-with-Map-Reinforced-Parallel-Map-Augmented-Agent-for-Geolocalization","children":"[논문리뷰] Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Thinking-with-Map-Reinforced-Parallel-Map-Augmented-Agent-for-Geolocalization","children":"arXiv에 게시된 'Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Thinking-with-Map-Reinforced-Parallel-Map-Augmented-Agent-for-Geolocalization"}]]}]]}],["$","article","2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents","children":"[논문리뷰] SmartSearch: Process Reward-Guided Query Refinement for Search Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents","children":"Guanting Dong이 arXiv에 게시한 'SmartSearch: Process Reward-Guided Query Refinement for Search Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents"}]]}]]}],["$","article","2026-01-12-Qwen3-VL-Embedding-and-Qwen3-VL-Reranker-A-Unified-Framework-for-State-of-the-Art-Multimodal-Retrieval-and-Ranking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Qwen3-VL-Embedding-and-Qwen3-VL-Reranker-A-Unified-Framework-for-State-of-the-Art-Multimodal-Retrieval-and-Ranking","children":"[논문리뷰] Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Qwen3-VL-Embedding-and-Qwen3-VL-Reranker-A-Unified-Framework-for-State-of-the-Art-Multimodal-Retrieval-and-Ranking","children":"arXiv에 게시된 'Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Qwen3-VL-Embedding-and-Qwen3-VL-Reranker-A-Unified-Framework-for-State-of-the-Art-Multimodal-Retrieval-and-Ranking"}]]}]]}],["$","article","2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning","children":"[논문리뷰] Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning","children":"Zhicheng Dou이 arXiv에 게시한 'Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Memory-Matters-More-Event-Centric-Memory-as-a-Logic-Map-for-Agent-Searching-and-Reasoning"}]]}]]}],["$","article","2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","children":"[논문리뷰] Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals","children":"Arjan Chakravarthy이 arXiv에 게시한 'Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Goal-Force-Teaching-Video-Models-To-Accomplish-Physics-Conditioned-Goals"}]]}]]}],["$","article","2026-01-12-GenCtrl-A-Formal-Controllability-Toolkit-for-Generative-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-GenCtrl-A-Formal-Controllability-Toolkit-for-Generative-Models","children":"[논문리뷰] GenCtrl -- A Formal Controllability Toolkit for Generative Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-GenCtrl-A-Formal-Controllability-Toolkit-for-Generative-Models","children":"arXiv에 게시된 'GenCtrl -- A Formal Controllability Toolkit for Generative Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-GenCtrl-A-Formal-Controllability-Toolkit-for-Generative-Models"}]]}]]}],["$","article","2026-01-12-Distilling-Feedback-into-Memory-as-a-Tool",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Distilling-Feedback-into-Memory-as-a-Tool","children":"[논문리뷰] Distilling Feedback into Memory-as-a-Tool"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-Distilling-Feedback-into-Memory-as-a-Tool","children":"vicgalle이 arXiv에 게시한 'Distilling Feedback into Memory-as-a-Tool' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-Distilling-Feedback-into-Memory-as-a-Tool"}]]}]]}],["$","article","2026-01-12-CaricatureGS-Exaggerating-3D-Gaussian-Splatting-Faces-With-Gaussian-Curvature",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-CaricatureGS-Exaggerating-3D-Gaussian-Splatting-Faces-With-Gaussian-Curvature","children":"[논문리뷰] CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-12-CaricatureGS-Exaggerating-3D-Gaussian-Splatting-Faces-With-Gaussian-Curvature","children":"arXiv에 게시된 'CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-12 00:00:00+0900+0900","children":"2026년 1월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-12-CaricatureGS-Exaggerating-3D-Gaussian-Splatting-Faces-With-Gaussian-Curvature"}]]}]]}],["$","article","2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice","children":"[논문리뷰] VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice","children":"arXiv에 게시된 'VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-VideoAuto-R1-Video-Auto-Reasoning-via-Thinking-Once-Answering-Twice"}]]}]]}],["$","article","2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","children":"[논문리뷰] VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control","children":"Ying Shan이 arXiv에 게시한 'VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-VerseCrafter-Dynamic-Realistic-Video-World-Model-with-4D-Geometric-Control"}]]}]]}],["$","article","2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset","children":"[논문리뷰] Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset","children":"YuanFu Yang이 arXiv에 게시한 'Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Towards-Open-Vocabulary-Industrial-Defect-Understanding-with-a-Large-Scale-Multimodal-Dataset"}]]}]]}],["$","article","2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute","children":"[논문리뷰] Token-Level LLM Collaboration via FusionRoute"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute","children":"Furong Huang이 arXiv에 게시한 'Token-Level LLM Collaboration via FusionRoute' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Token-Level-LLM-Collaboration-via-FusionRoute"}]]}]]}],["$","article","2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models","children":"[논문리뷰] The Illusion of Specialization: Unveiling the Domain-Invariant 'Standing Committee' in Mixture-of-Experts Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models","children":"arXiv에 게시된 'The Illusion of Specialization: Unveiling the Domain-Invariant 'Standing Committee' in Mixture-of-Experts Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-The-Illusion-of-Specialization-Unveiling-the-Domain-Invariant-Standing-Committee-in-Mixture-of-Experts-Models"}]]}]]}],["$","article","2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","children":"[논문리뷰] RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation","children":"Mingda Jia이 arXiv에 게시한 'RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-RoboVIP-Multi-View-Video-Generation-with-Visual-Identity-Prompting-Augments-Robot-Manipulation"}]]}]]}],["$","article","2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding","children":"[논문리뷰] RelayLLM: Efficient Reasoning via Collaborative Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding","children":"Haolin Liu이 arXiv에 게시한 'RelayLLM: Efficient Reasoning via Collaborative Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-RelayLLM-Efficient-Reasoning-via-Collaborative-Decoding"}]]}]]}],["$","article","2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","children":"[논문리뷰] Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","children":"Yu Xu이 arXiv에 게시한 'Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing"}]]}]]}],["$","article","2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes","children":"[논문리뷰] RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes","children":"Yu-Lun Liu이 arXiv에 게시한 'RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-RL-AWB-Deep-Reinforcement-Learning-for-Auto-White-Balance-Correction-in-Low-Light-Night-time-Scenes"}]]}]]}],["$","article","2026-01-09-Plenoptic-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Plenoptic-Video-Generation","children":"[논문리뷰] Plenoptic Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Plenoptic-Video-Generation","children":"arXiv에 게시된 'Plenoptic Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Plenoptic-Video-Generation"}]]}]]}],["$","article","2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study","children":"[논문리뷰] Memorization in 3D Shape Generation: An Empirical Study"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study","children":"arXiv에 게시된 'Memorization in 3D Shape Generation: An Empirical Study' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Memorization-in-3D-Shape-Generation-An-Empirical-Study"}]]}]]}],["$","article","2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers","children":"[논문리뷰] Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers","children":"arXiv에 게시된 'Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers"}]]}]]}],["$","article","2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization","children":"[논문리뷰] GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization","children":"arXiv에 게시된 'GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-GDPO-Group-reward-Decoupled-Normalization-Policy-Optimization-for-Multi-reward-RL-Optimization"}]]}]]}],["$","article","2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models","children":"[논문리뷰] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models","children":"arXiv에 게시된 'Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Few-Tokens-Matter-Entropy-Guided-Attacks-on-Vision-Language-Models"}]]}]]}],["$","article","2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach","children":"[논문리뷰] Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach","children":"Carl James Debono이 arXiv에 게시한 'Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Enhancing-Object-Detection-with-Privileged-Information-A-Model-Agnostic-Teacher-Student-Approach"}]]}]]}],["$","article","2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking","children":"[논문리뷰] DocDancer: Towards Agentic Document-Grounded Information Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking","children":"arXiv에 게시된 'DocDancer: Towards Agentic Document-Grounded Information Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-DocDancer-Towards-Agentic-Document-Grounded-Information-Seeking"}]]}]]}],["$","article","2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","children":"[논문리뷰] DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","children":"Jing Ma이 arXiv에 게시한 'DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs"}]]}]]}],["$","article","2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering","children":"[논문리뷰] AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering","children":"Di Zhang이 arXiv에 게시한 'AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-AgentDevel-Reframing-Self-Evolving-LLM-Agents-as-Release-Engineering"}]]}]]}],["$","article","2026-01-09-Agent-as-a-Judge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Agent-as-a-Judge","children":"[논문리뷰] Agent-as-a-Judge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Agent-as-a-Judge","children":"Meng Liu이 arXiv에 게시한 'Agent-as-a-Judge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Agent-as-a-Judge"}]]}]]}],["$","article","2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search","children":"[논문리뷰] AT^2PO: Agentic Turn-based Policy Optimization via Tree Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search","children":"arXiv에 게시된 'AT^2PO: Agentic Turn-based Policy Optimization via Tree Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-AT2PO-Agentic-Turn-based-Policy-Optimization-via-Tree-Search"}]]}]]}],["$","article","2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts","children":"[논문리뷰] Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts","children":"arXiv에 게시된 'Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-Why-LLMs-Arent-Scientists-Yet-Lessons-from-Four-Autonomous-Research-Attempts"}]]}]]}],["$","article","2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing","children":"[논문리뷰] ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing","children":"arXiv에 게시된 'ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing"}]]}]]}],["$","article","2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization","children":"[논문리뷰] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization","children":"arXiv에 게시된 'RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-RGS-SLAM-Robust-Gaussian-Splatting-SLAM-with-One-Shot-Dense-Initialization"}]]}]]}],["$","article","2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics","children":"[논문리뷰] MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics","children":"arXiv에 게시된 'MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics"}]]}]]}],["$","article","2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents","children":"[논문리뷰] MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents","children":"Bingzhe Li이 arXiv에 게시한 'MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents"}]]}]]}],["$","article","2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning","children":"[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning","children":"Guanchen Wu이 arXiv에 게시한 'EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning"}]]}]]}],["$","article","2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting","children":"[논문리뷰] Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting","children":"arXiv에 게시된 'Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-Entropy-Adaptive-Fine-Tuning-Resolving-Confident-Conflicts-to-Mitigate-Forgetting"}]]}]]}],["$","article","2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models","children":"[논문리뷰] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models","children":"arXiv에 게시된 'E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-E-GRPO-High-Entropy-Steps-Drive-Effective-Reinforcement-Learning-for-Flow-Models"}]]}]]}],["$","article","2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework","children":"[논문리뷰] X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework","children":"Shwetank Shekhar Singh이 arXiv에 게시한 'X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-X-MuTeST-A-Multilingual-Benchmark-for-Explainable-Hate-Speech-Detection-and-A-Novel-LLM-consulted-Explanation-Framework"}]]}]]}],["$","article","2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision","children":"[논문리뷰] UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision","children":"XinYu Sun이 arXiv에 게시한 'UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-UniCorn-Towards-Self-Improving-Unified-Multimodal-Models-through-Self-Generated-Supervision"}]]}]]}],["$","article","2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs","children":"[논문리뷰] Steerability of Instrumental-Convergence Tendencies in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs","children":"j-hoscilowic이 arXiv에 게시한 'Steerability of Instrumental-Convergence Tendencies in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-Steerability-of-Instrumental-Convergence-Tendencies-in-LLMs"}]]}]]}],["$","article","2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models","children":"[논문리뷰] SOP: A Scalable Online Post-Training System for Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models","children":"arXiv에 게시된 'SOP: A Scalable Online Post-Training System for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-SOP-A-Scalable-Online-Post-Training-System-for-Vision-Language-Action-Models"}]]}]]}],["$","article","2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation","children":"[논문리뷰] Parallel Latent Reasoning for Sequential Recommendation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation","children":"Yuning Jiang이 arXiv에 게시한 'Parallel Latent Reasoning for Sequential Recommendation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-Parallel-Latent-Reasoning-for-Sequential-Recommendation"}]]}]]}],["$","article","2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents","children":"[논문리뷰] NitroGen: An Open Foundation Model for Generalist Gaming Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents","children":"arXiv에 게시된 'NitroGen: An Open Foundation Model for Generalist Gaming Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-NitroGen-An-Open-Foundation-Model-for-Generalist-Gaming-Agents"}]]}]]}],["$","article","2026-01-07-MiMo-V2-Flash-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-MiMo-V2-Flash-Technical-Report","children":"[논문리뷰] MiMo-V2-Flash Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-MiMo-V2-Flash-Technical-Report","children":"arXiv에 게시된 'MiMo-V2-Flash Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-MiMo-V2-Flash-Technical-Report"}]]}]]}],["$","article","2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model","children":"[논문리뷰] LTX-2: Efficient Joint Audio-Visual Foundation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model","children":"Andrew Kvochko이 arXiv에 게시한 'LTX-2: Efficient Joint Audio-Visual Foundation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-LTX-2-Efficient-Joint-Audio-Visual-Foundation-Model"}]]}]]}],["$","article","2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields","children":"[논문리뷰] InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields","children":"arXiv에 게시된 'InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-InfiniDepth-Arbitrary-Resolution-and-Fine-Grained-Depth-Estimation-with-Neural-Implicit-Fields"}]]}]]}],["$","article","2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing","children":"[논문리뷰] FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing","children":"Peng Tang이 arXiv에 게시한 'FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-FFP-300K-Scaling-First-Frame-Propagation-for-Generalizable-Video-Editing"}]]}]]}],["$","article","2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization","children":"[논문리뷰] DreamStyle: A Unified Framework for Video Stylization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization","children":"arXiv에 게시된 'DreamStyle: A Unified Framework for Video Stylization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-DreamStyle-A-Unified-Framework-for-Video-Stylization"}]]}]]}],["$","article","2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving","children":"[논문리뷰] CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving","children":"Tao Feng이 arXiv에 게시한 'CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-07 00:00:00+0900+0900","children":"2026년 1월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-07-CogFlow-Bridging-Perception-and-Reasoning-through-Knowledge-Internalization-for-Visual-Mathematical-Problem-Solving"}]]}]]}],["$","article","2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context","children":"[논문리뷰] VINO: A Unified Visual Generator with Interleaved OmniModal Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context","children":"Kun Gai이 arXiv에 게시한 'VINO: A Unified Visual Generator with Interleaved OmniModal Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-VINO-A-Unified-Visual-Generator-with-Interleaved-OmniModal-Context"}]]}]]}],["$","article","2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation","children":"[논문리뷰] VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation","children":"arXiv에 게시된 'VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-VAR-RL-Done-Right-Tackling-Asynchronous-Policy-Conflicts-in-Visual-Autoregressive-Generation"}]]}]]}],["$","article","2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion","children":"[논문리뷰] Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion","children":"Shiying Wang이 arXiv에 게시한 'Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Toward-Stable-Semi-Supervised-Remote-Sensing-Segmentation-via-Co-Guidance-and-Co-Fusion"}]]}]]}],["$","article","2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes","children":"[논문리뷰] Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes","children":"Shuo Yang이 arXiv에 게시한 'Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Talk2Move-Reinforcement-Learning-for-Text-Instructed-Object-Level-Geometric-Transformation-in-Scenes"}]]}]]}],["$","article","2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving","children":"[논문리뷰] SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving","children":"arXiv에 게시된 'SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-SWE-Lego-Pushing-the-Limits-of-Supervised-Fine-tuning-for-Software-Issue-Resolving"}]]}]]}],["$","article","2026-01-06-Recursive-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Recursive-Language-Models","children":"[논문리뷰] Recursive Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Recursive-Language-Models","children":"arXiv에 게시된 'Recursive Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Recursive-Language-Models"}]]}]]}],["$","article","2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents","children":"[논문리뷰] Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents","children":"arXiv에 게시된 'Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Project-Ariadne-A-Structural-Causal-Framework-for-Auditing-Faithfulness-in-LLM-Agents"}]]}]]}],["$","article","2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment","children":"[논문리뷰] OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment","children":"Chunchun Ma이 arXiv에 게시한 'OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-OpenNovelty-An-LLM-powered-Agentic-System-for-Verifiable-Scholarly-Novelty-Assessment"}]]}]]}],["$","article","2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation","children":"[논문리뷰] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation","children":"arXiv에 게시된 'NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-NextFlow-Unified-Sequential-Modeling-Activates-Multimodal-Understanding-and-Generation"}]]}]]}],["$","article","2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models","children":"[논문리뷰] M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models","children":"Jun-Cheng Chen이 arXiv에 게시한 'M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-M-ErasureBench-A-Comprehensive-Multimodal-Evaluation-Benchmark-for-Concept-Erasure-in-Diffusion-Models"}]]}]]}],["$","article","2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs","children":"[논문리뷰] KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs","children":"Yi Yang이 arXiv에 게시한 'KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-KV-Embedding-Training-free-Text-Embedding-via-Internal-KV-Re-routing-in-Decoder-only-LLMs"}]]}]]}],["$","article","2026-01-06-K-EXAONE-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-K-EXAONE-Technical-Report","children":"[논문리뷰] K-EXAONE Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-K-EXAONE-Technical-Report","children":"arXiv에 게시된 'K-EXAONE Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-K-EXAONE-Technical-Report"}]]}]]}],["$","article","2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams","children":"[논문리뷰] InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams","children":"arXiv에 게시된 'InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-InfiniteVGGT-Visual-Geometry-Grounded-Transformer-for-Endless-Streams"}]]}]]}],["$","article","2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset","children":"[논문리뷰] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset","children":"arXiv에 게시된 'IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-IMA-ISIC-Archive-Multi-Annotator-Dermoscopic-Skin-Lesion-Segmentation-Dataset"}]]}]]}],["$","article","2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking","children":"[논문리뷰] GARDO: Reinforcing Diffusion Models without Reward Hacking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking","children":"Zhiyong Wang이 arXiv에 게시한 'GARDO: Reinforcing Diffusion Models without Reward Hacking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-GARDO-Reinforcing-Diffusion-Models-without-Reward-Hacking"}]]}]]}],["$","article","2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling","children":"[논문리뷰] Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling","children":"arXiv에 게시된 'Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Falcon-H1R-Pushing-the-Reasoning-Frontiers-with-a-Hybrid-Model-for-Efficient-Test-Time-Scaling"}]]}]]}],["$","article","2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer","children":"[논문리뷰] DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer","children":"arXiv에 게시된 'DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-DreamID-VBridging-the-Image-to-Video-Gap-for-High-Fidelity-Face-Swapping-via-Diffusion-Transformer"}]]}]]}],["$","article","2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits","children":"[논문리뷰] Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits","children":"arXiv에 게시된 'Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-Can-LLMs-Predict-Their-Own-Failures-Self-Awareness-via-Internal-Circuits"}]]}]]}],["$","article","2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs","children":"[논문리뷰] COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs","children":"arXiv에 게시된 'COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-06 00:00:00+0900+0900","children":"2026년 1월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-06-COMPASS-A-Framework-for-Evaluating-Organization-Specific-Policy-Alignment-in-LLMs"}]]}]]}],["$","article","2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization","children":"[논문리뷰] Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization","children":"arXiv에 게시된 'Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Youtu-Agent-Scaling-Agent-Productivity-with-Automated-Generation-and-Hybrid-Policy-Optimization"}]]}]]}],["$","article","2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation","children":"[논문리뷰] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation","children":"arXiv에 게시된 'Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Taming-Hallucinations-Boosting-MLLMs-Video-Understanding-via-Counterfactual-Video-Generation"}]]}]]}],["$","article","2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning","children":"[논문리뷰] SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning","children":"arXiv에 게시된 'SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-SenseNova-MARS-Empowering-Multimodal-Agentic-Reasoning-and-Search-via-Reinforcement-Learning"}]]}]]}],["$","article","2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures","children":"[논문리뷰] Nested Learning: The Illusion of Deep Learning Architectures"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures","children":"Vahab Mirrokni이 arXiv에 게시한 'Nested Learning: The Illusion of Deep Learning Architectures' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Nested-Learning-The-Illusion-of-Deep-Learning-Architectures"}]]}]]}],["$","article","2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos","children":"[논문리뷰] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos","children":"Feng Wang이 arXiv에 게시한 'NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-NeoVerse-Enhancing-4D-World-Model-with-in-the-wild-Monocular-Videos"}]]}]]}],["$","article","2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing","children":"[논문리뷰] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing","children":"Jian Yang이 arXiv에 게시한 'MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-MorphAny3D-Unleashing-the-Power-of-Structured-Latent-in-3D-Morphing"}]]}]]}],["$","article","2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs","children":"[논문리뷰] InfoSynth: Information-Guided Benchmark Synthesis for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs","children":"arXiv에 게시된 'InfoSynth: Information-Guided Benchmark Synthesis for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-InfoSynth-Information-Guided-Benchmark-Synthesis-for-LLMs"}]]}]]}],["$","article","2026-01-05-Fast-weight-Product-Key-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Fast-weight-Product-Key-Memory","children":"[논문리뷰] Fast-weight Product Key Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Fast-weight-Product-Key-Memory","children":"arXiv에 게시된 'Fast-weight Product Key Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Fast-weight-Product-Key-Memory"}]]}]]}],["$","article","2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction","children":"[논문리뷰] Diversity or Precision? A Deep Dive into Next Token Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction","children":"arXiv에 게시된 'Diversity or Precision? A Deep Dive into Next Token Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction"}]]}]]}],["$","article","2026-01-05-Deep-Delta-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Deep-Delta-Learning","children":"[논문리뷰] Deep Delta Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Deep-Delta-Learning","children":"Quanquan Gu이 arXiv에 게시한 'Deep Delta Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Deep-Delta-Learning"}]]}]]}],["$","article","2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation","children":"[논문리뷰] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation","children":"Sung Ju Hwang이 arXiv에 게시한 'Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-Avatar-Forcing-Real-Time-Interactive-Head-Avatar-Generation-for-Natural-Conversation"}]]}]]}],["$","article","2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction","children":"[논문리뷰] AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction","children":"Yu-Lun Liu이 arXiv에 게시한 'AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-05 00:00:00+0900+0900","children":"2026년 1월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-05-AdaGaR-Adaptive-Gabor-Representation-for-Dynamic-Scene-Reconstruction"}]]}]]}],["$","article","2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs","children":"[논문리뷰] On the Role of Discreteness in Diffusion LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs","children":"arXiv에 게시된 'On the Role of Discreteness in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-02 00:00:00+0900+0900","children":"2026년 1월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-02-On-the-Role-of-Discreteness-in-Diffusion-LLMs"}]]}]]}],["$","article","2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space","children":"[논문리뷰] Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space","children":"arXiv에 게시된 'Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-02 00:00:00+0900+0900","children":"2026년 1월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-02-Dynamic-Large-Concept-Models-Latent-Reasoning-in-an-Adaptive-Semantic-Space"}]]}]]}],["$","article","2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models","children":"[논문리뷰] DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models","children":"Siyuan Huang이 arXiv에 게시한 'DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-02 00:00:00+0900+0900","children":"2026년 1월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-02-DiffThinker-Towards-Generative-Multimodal-Reasoning-with-Diffusion-Models"}]]}]]}],["$","article","2026-01-01-mHC-Manifold-Constrained-Hyper-Connections",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections","children":"[논문리뷰] mHC: Manifold-Constrained Hyper-Connections"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections","children":"arXiv에 게시된 'mHC: Manifold-Constrained Hyper-Connections' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections"}]]}]]}],["$","article","2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models","children":"[논문리뷰] Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models","children":"Xinyi Dai이 arXiv에 게시한 'Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Youtu-LLM-Unlocking-the-Native-Agentic-Potential-for-Lightweight-Large-Language-Models"}]]}]]}],["$","article","2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems","children":"[논문리뷰] Valori: A Deterministic Memory Substrate for AI Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems","children":"varam17이 arXiv에 게시한 'Valori: A Deterministic Memory Substrate for AI Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Valori-A-Deterministic-Memory-Substrate-for-AI-Systems"}]]}]]}],["$","article","2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time","children":"[논문리뷰] SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time","children":"Tuanfeng Y. Wang이 arXiv에 게시한 'SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-SpaceTimePilot-Generative-Rendering-of-Dynamic-Scenes-Across-Space-and-Time"}]]}]]}],["$","article","2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future","children":"[논문리뷰] Scaling Open-Ended Reasoning to Predict the Future"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future","children":"arXiv에 게시된 'Scaling Open-Ended Reasoning to Predict the Future' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Scaling-Open-Ended-Reasoning-to-Predict-the-Future"}]]}]]}],["$","article","2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","children":"[논문리뷰] Pretraining Frame Preservation in Autoregressive Video Memory Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression","children":"Beijia Lu이 arXiv에 게시한 'Pretraining Frame Preservation in Autoregressive Video Memory Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Pretraining-Frame-Preservation-in-Autoregressive-Video-Memory-Compression"}]]}]]}],["$","article","2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation","children":"[논문리뷰] PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation","children":"arXiv에 게시된 'PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-PhyGDPO-Physics-Aware-Groupwise-Direct-Preference-Optimization-for-Physically-Consistent-Text-to-Video-Generation"}]]}]]}],["$","article","2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem","children":"[논문리뷰] Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem","children":"Wei Gao이 arXiv에 게시한 'Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem"}]]}]]}],["$","article","2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation","children":"[논문리뷰] JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation","children":"arXiv에 게시된 'JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-JavisGPT-A-Unified-Multi-modal-LLM-for-Sounding-Video-Comprehension-and-Generation"}]]}]]}],["$","article","2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself","children":"[논문리뷰] Guiding a Diffusion Transformer with the Internal Dynamics of Itself"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself","children":"arXiv에 게시된 'Guiding a Diffusion Transformer with the Internal Dynamics of Itself' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Guiding-a-Diffusion-Transformer-with-the-Internal-Dynamics-of-Itself"}]]}]]}],["$","article","2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers","children":"[논문리뷰] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers","children":"Mahşuk Taylan이 arXiv에 게시한 'Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Geometry-Aware-Optimization-for-Respiratory-Sound-Classification-Enhancing-Sensitivity-with-SAM-Optimized-Audio-Spectrogram-Transformers"}]]}]]}],["$","article","2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction","children":"[논문리뷰] GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction","children":"Yu-Lun Liu이 arXiv에 게시한 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-GaMO-Geometry-aware-Multi-view-Diffusion-Outpainting-for-Sparse-View-3D-Reconstruction"}]]}]]}],["$","article","2026-01-01-GR-Dexter-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-GR-Dexter-Technical-Report","children":"[논문리뷰] GR-Dexter Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-GR-Dexter-Technical-Report","children":"arXiv에 게시된 'GR-Dexter Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-GR-Dexter-Technical-Report"}]]}]]}],["$","article","2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems","children":"[논문리뷰] Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems","children":"arXiv에 게시된 'Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Forging-Spatial-Intelligence-A-Roadmap-of-Multi-Modal-Data-Pre-Training-for-Autonomous-Systems"}]]}]]}],["$","article","2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking","children":"[논문리뷰] Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking","children":"Jie Zhou이 arXiv에 게시한 'Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Figure-It-Out-Improving-the-Frontier-of-Reasoning-with-Active-Visual-Thinking"}]]}]]}],["$","article","2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process","children":"[논문리뷰] Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process","children":"arXiv에 게시된 'Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process"}]]}]]}],["$","article","2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models","children":"[논문리뷰] Factorized Learning for Temporally Grounded Video-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models","children":"arXiv에 게시된 'Factorized Learning for Temporally Grounded Video-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Factorized-Learning-for-Temporally-Grounded-Video-Language-Models"}]]}]]}],["$","article","2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts","children":"[논문리뷰] BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts","children":"Mengmeng Wang이 arXiv에 게시한 'BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-BEDA-Belief-Estimation-as-Probabilistic-Constraints-for-Performing-Strategic-Dialogue-Acts"}]]}]]}],["$","article","2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents","children":"[논문리뷰] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents","children":"Shixin Jiang이 arXiv에 게시한 'AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-AI-Meets-Brain-Memory-Systems-from-Cognitive-Neuroscience-to-Autonomous-Agents"}]]}]]}],["$","article","2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement","children":"[논문리뷰] UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement","children":"Kaiyi Zhang이 arXiv에 게시한 'UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement"}]]}]]}],["$","article","2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization","children":"[논문리뷰] GraphLocator: Graph-guided Causal Reasoning for Issue Localization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization","children":"Wei Zhang이 arXiv에 게시한 'GraphLocator: Graph-guided Causal Reasoning for Issue Localization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization"}]]}]]}],["$","article","2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs","children":"[논문리뷰] GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs","children":"arXiv에 게시된 'GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs"}]]}]]}],["$","article","2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR","children":"[논문리뷰] Evaluating Parameter Efficient Methods for RLVR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR","children":"arXiv에 게시된 'Evaluating Parameter Efficient Methods for RLVR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-Evaluating-Parameter-Efficient-Methods-for-RLVR"}]]}]]}],["$","article","2025-12-31-End-to-End-Test-Time-Training-for-Long-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-End-to-End-Test-Time-Training-for-Long-Context","children":"[논문리뷰] End-to-End Test-Time Training for Long Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-End-to-End-Test-Time-Training-for-Long-Context","children":"Marcel Rød이 arXiv에 게시한 'End-to-End Test-Time Training for Long Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-End-to-End-Test-Time-Training-for-Long-Context"}]]}]]}],["$","article","2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation","children":"[논문리뷰] DreamOmni3: Scribble-based Editing and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation","children":"arXiv에 게시된 'DreamOmni3: Scribble-based Editing and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-31 00:00:00+0900+0900","children":"2025년 12월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-31-DreamOmni3-Scribble-based-Editing-and-Generation"}]]}]]}],["$","article","2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model","children":"[논문리뷰] Yume-1.5: A Text-Controlled Interactive World Generation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model","children":"Kaining Ying이 arXiv에 게시한 'Yume-1.5: A Text-Controlled Interactive World Generation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Yume-1-5-A-Text-Controlled-Interactive-World-Generation-Model"}]]}]]}],["$","article","2025-12-30-Web-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Web-World-Models","children":"[논문리뷰] Web World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Web-World-Models","children":"arXiv에 게시된 'Web World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Web-World-Models"}]]}]]}],["$","article","2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web","children":"[논문리뷰] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web","children":"Kaixin Liang이 arXiv에 게시한 'Video-BrowseComp: Benchmarking Agentic Video Research on Open Web' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Video-BrowseComp-Benchmarking-Agentic-Video-Research-on-Open-Web"}]]}]]}],["$","article","2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs","children":"[논문리뷰] VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs","children":"Xihui Liu이 arXiv에 게시한 'VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-VL-LN-Bench-Towards-Long-horizon-Goal-oriented-Navigation-with-Active-Dialogs"}]]}]]}],["$","article","2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards","children":"[논문리뷰] Training AI Co-Scientists Using Rubric Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards","children":"arXiv에 게시된 'Training AI Co-Scientists Using Rubric Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Training-AI-Co-Scientists-Using-Rubric-Rewards"}]]}]]}],["$","article","2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling","children":"[논문리뷰] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling","children":"arXiv에 게시된 'SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-SurgWorld-Learning-Surgical-Robot-Policies-from-Videos-via-World-Modeling"}]]}]]}],["$","article","2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion","children":"[논문리뷰] Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion","children":"Po-Fan Yu이 arXiv에 게시한 'Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Stream-DiffVSR-Low-Latency-Streamable-Video-Super-Resolution-via-Auto-Regressive-Diffusion"}]]}]]}],["$","article","2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers","children":"[논문리뷰] SpotEdit: Selective Region Editing in Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers","children":"arXiv에 게시된 'SpotEdit: Selective Region Editing in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-SpotEdit-Selective-Region-Editing-in-Diffusion-Transformers"}]]}]]}],["$","article","2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents","children":"[논문리뷰] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents","children":"arXiv에 게시된 'SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-SmartSnap-Proactive-Evidence-Seeking-for-Self-Verifying-Agents"}]]}]]}],["$","article","2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting","children":"[논문리뷰] Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting","children":"arXiv에 게시된 'Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Quantile-Rendering-Efficiently-Embedding-High-dimensional-Feature-on-3D-Gaussian-Splatting"}]]}]]}],["$","article","2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding","children":"[논문리뷰] OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding","children":"Jian Liu이 arXiv에 게시한 'OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-OmniAgent-Audio-Guided-Active-Perception-Agent-for-Omnimodal-Audio-Video-Understanding"}]]}]]}],["$","article","2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking","children":"[논문리뷰] Nested Browser-Use Learning for Agentic Information Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking","children":"arXiv에 게시된 'Nested Browser-Use Learning for Agentic Information Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Nested-Browser-Use-Learning-for-Agentic-Information-Seeking"}]]}]]}],["$","article","2025-12-30-Monadic-Context-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Monadic-Context-Engineering","children":"[논문리뷰] Monadic Context Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Monadic-Context-Engineering","children":"arXiv에 게시된 'Monadic Context Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Monadic-Context-Engineering"}]]}]]}],["$","article","2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation","children":"[논문리뷰] LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation","children":"Steffi Chern이 arXiv에 게시한 'LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-LiveTalk-Real-Time-Multimodal-Interactive-Video-Diffusion-via-Improved-On-Policy-Distillation"}]]}]]}],["$","article","2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models","children":"[논문리뷰] GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models","children":"arXiv에 게시된 'GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-GRAN-TED-Generating-Robust-Aligned-and-Nuanced-Text-Embedding-for-Diffusion-Models"}]]}]]}],["$","article","2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone","children":"[논문리뷰] Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone","children":"arXiv에 게시된 'Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Dream-VL-Dream-VLA-Open-Vision-Language-and-Vision-Language-Action-Models-with-Diffusion-Language-Model-Backbone"}]]}]]}],["$","article","2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation","children":"[논문리뷰] Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation","children":"arXiv에 게시된 'Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Diffusion-Knows-Transparency-Repurposing-Video-Diffusion-for-Transparent-Object-Depth-and-Normal-Estimation"}]]}]]}],["$","article","2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models","children":"[논문리뷰] DiRL: An Efficient Post-Training Framework for Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models","children":"arXiv에 게시된 'DiRL: An Efficient Post-Training Framework for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-DiRL-An-Efficient-Post-Training-Framework-for-Diffusion-Language-Models"}]]}]]}],["$","article","2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss","children":"[논문리뷰] Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss","children":"arXiv에 게시된 'Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Coupling-Experts-and-Routers-in-Mixture-of-Experts-via-an-Auxiliary-Loss"}]]}]]}],["$","article","2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design","children":"[논문리뷰] An Information Theoretic Perspective on Agentic System Design"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design","children":"arXiv에 게시된 'An Information Theoretic Perspective on Agentic System Design' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-An-Information-Theoretic-Perspective-on-Agentic-System-Design"}]]}]]}],["$","article","2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy","children":"[논문리뷰] Act2Goal: From World Model To General Goal-conditioned Policy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy","children":"arXiv에 게시된 'Act2Goal: From World Model To General Goal-conditioned Policy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-30 00:00:00+0900+0900","children":"2025년 12월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-30-Act2Goal-From-World-Model-To-General-Goal-conditioned-Policy"}]]}]]}],["$","article","2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture","children":"[논문리뷰] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture","children":"Kaiwen Zhu이 arXiv에 게시한 'UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture"}]]}]]}],["$","article","2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models","children":"[논문리뷰] TimeBill: Time-Budgeted Inference for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models","children":"Yehan Ma이 arXiv에 게시한 'TimeBill: Time-Budgeted Inference for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models"}]]}]]}],["$","article","2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers","children":"[논문리뷰] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers","children":"arXiv에 게시된 'SlideTailor: Personalized Presentation Slide Generation for Scientific Papers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers"}]]}]]}],["$","article","2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning","children":"[논문리뷰] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning","children":"arXiv에 게시된 'See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning"}]]}]]}],["$","article","2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents","children":"[논문리뷰] SWE-RM: Execution-free Feedback For Software Engineering Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents","children":"X. W.이 arXiv에 게시한 'SWE-RM: Execution-free Feedback For Software Engineering Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents"}]]}]]}],["$","article","2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning","children":"[논문리뷰] SVBench: Evaluation of Video Generation Models on Social Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning","children":"Xiaojie Xu이 arXiv에 게시한 'SVBench: Evaluation of Video Generation Models on Social Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning"}]]}]]}],["$","article","2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right","children":"[논문리뷰] ProEdit: Inversion-based Editing From Prompts Done Right"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right","children":"Kun-Yu Lin이 arXiv에 게시한 'ProEdit: Inversion-based Editing From Prompts Done Right' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right"}]]}]]}],["$","article","2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding","children":"[논문리뷰] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding","children":"Yixin Chen이 arXiv에 게시한 'Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding"}]]}]]}],["$","article","2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding","children":"[논문리뷰] Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding","children":"arXiv에 게시된 'Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding"}]]}]]}],["$","article","2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents","children":"[논문리뷰] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents","children":"arXiv에 게시된 'MAI-UI Technical Report: Real-World Centric Foundation GUI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents"}]]}]]}],["$","article","2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion","children":"[논문리뷰] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion","children":"arXiv에 게시된 'InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion"}]]}]]}],["$","article","2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search","children":"[논문리뷰] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search","children":"Jierun Chen이 arXiv에 게시한 'InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search"}]]}]]}],["$","article","2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication","children":"[논문리뷰] A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication","children":"A. I. Perminov이 arXiv에 게시한 'A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication"}]]}]]}],["$","article","2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation","children":"[논문리뷰] VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation","children":"Yicong Li이 arXiv에 게시한 'VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation"}]]}]]}],["$","article","2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","children":"[논문리뷰] Spatia: Video Generation with Updatable Spatial Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory","children":"arXiv에 게시된 'Spatia: Video Generation with Updatable Spatial Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-Spatia-Video-Generation-with-Updatable-Spatial-Memory"}]]}]]}],["$","article","2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models","children":"[논문리뷰] Schoenfeld's Anatomy of Mathematical Reasoning by Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models","children":"Tianyi Zhou이 arXiv에 게시한 'Schoenfeld's Anatomy of Mathematical Reasoning by Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-Schoenfelds-Anatomy-of-Mathematical-Reasoning-by-Language-Models"}]]}]]}],["$","article","2025-12-26-Latent-Implicit-Visual-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Latent-Implicit-Visual-Reasoning","children":"[논문리뷰] Latent Implicit Visual Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-Latent-Implicit-Visual-Reasoning","children":"arXiv에 게시된 'Latent Implicit Visual Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-Latent-Implicit-Visual-Reasoning"}]]}]]}],["$","article","2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode","children":"[논문리뷰] How Much 3D Do Video Foundation Models Encode?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode","children":"arXiv에 게시된 'How Much 3D Do Video Foundation Models Encode?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-How-Much-3D-Do-Video-Foundation-Models-Encode"}]]}]]}],["$","article","2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training","children":"[논문리뷰] GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training","children":"Yuanchun Shi이 arXiv에 게시한 'GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-26 00:00:00+0900+0900","children":"2025년 12월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-26-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training"}]]}]]}],["$","article","2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","children":"[논문리뷰] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times","children":"arXiv에 게시된 'TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-TurboDiffusion-Accelerating-Video-Diffusion-Models-by-100-200-Times"}]]}]]}],["$","article","2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior","children":"[논문리뷰] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior","children":"arXiv에 게시된 'TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-TokSuite-Measuring-the-Impact-of-Tokenizer-Choice-on-Language-Model-Behavior"}]]}]]}],["$","article","2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation","children":"[논문리뷰] T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation","children":"arXiv에 게시된 'T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-T2AV-Compass-Towards-Unified-Evaluation-for-Text-to-Audio-Video-Generation"}]]}]]}],["$","article","2025-12-25-Streaming-Video-Instruction-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Streaming-Video-Instruction-Tuning","children":"[논문리뷰] Streaming Video Instruction Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Streaming-Video-Instruction-Tuning","children":"Kaiyang Zhou이 arXiv에 게시한 'Streaming Video Instruction Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-Streaming-Video-Instruction-Tuning"}]]}]]}],["$","article","2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios","children":"[논문리뷰] SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios","children":"Nghi D. Q. Bui이 arXiv에 게시한 'SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-SWE-EVO-Benchmarking-Coding-Agents-in-Long-Horizon-Software-Evolution-Scenarios"}]]}]]}],["$","article","2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning","children":"[논문리뷰] Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning","children":"arXiv에 게시된 'Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-Nemotron-3-Nano-Open-Efficient-Mixture-of-Experts-Hybrid-Mamba-Transformer-Model-for-Agentic-Reasoning"}]]}]]}],["$","article","2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence","children":"[논문리뷰] NVIDIA Nemotron 3: Efficient and Open Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence","children":"arXiv에 게시된 'NVIDIA Nemotron 3: Efficient and Open Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-NVIDIA-Nemotron-3-Efficient-and-Open-Intelligence"}]]}]]}],["$","article","2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment","children":"[논문리뷰] Multi-hop Reasoning via Early Knowledge Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment","children":"Xuanjing Huang이 arXiv에 게시한 'Multi-hop Reasoning via Early Knowledge Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-Multi-hop-Reasoning-via-Early-Knowledge-Alignment"}]]}]]}],["$","article","2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models","children":"[논문리뷰] Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models","children":"arXiv에 게시된 'Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-Learning-to-Reason-in-4D-Dynamic-Spatial-Understanding-for-Vision-Language-Models"}]]}]]}],["$","article","2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations","children":"[논문리뷰] Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations","children":"arXiv에 게시된 'Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-Learning-from-Next-Frame-Prediction-Autoregressive-Video-Modeling-Encodes-Effective-Representations"}]]}]]}],["$","article","2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics","children":"[논문리뷰] LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics","children":"arXiv에 게시된 'LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-LLM-Swiss-Round-Aggregating-Multi-Benchmark-Performance-via-Competitive-Swiss-System-Dynamics"}]]}]]}],["$","article","2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming","children":"[논문리뷰] HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming","children":"arXiv에 게시된 'HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-HiStream-Efficient-High-Resolution-Video-Generation-via-Redundancy-Eliminated-Streaming"}]]}]]}],["$","article","2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation","children":"[논문리뷰] DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation","children":"arXiv에 게시된 'DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-DreaMontage-Arbitrary-Frame-Guided-One-Shot-Video-Generation"}]]}]]}],["$","article","2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models","children":"[논문리뷰] Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models","children":"Yu-Lun Liu이 arXiv에 게시한 'Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-25 00:00:00+0900+0900","children":"2025년 12월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-25-Beyond-Memorization-A-Multi-Modal-Ordinal-Regression-Benchmark-to-Expose-Popularity-Bias-in-Vision-Language-Models"}]]}]]}],["$","article","2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub","children":"[논문리뷰] Toxicity Ahead: Forecasting Conversational Derailment on GitHub"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub","children":"Kostadin Damevski이 arXiv에 게시한 'Toxicity Ahead: Forecasting Conversational Derailment on GitHub' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Toxicity-Ahead-Forecasting-Conversational-Derailment-on-GitHub"}]]}]]}],["$","article","2025-12-24-Step-DeepResearch-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Step-DeepResearch-Technical-Report","children":"[논문리뷰] Step-DeepResearch Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Step-DeepResearch-Technical-Report","children":"arXiv에 게시된 'Step-DeepResearch Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Step-DeepResearch-Technical-Report"}]]}]]}],["$","article","2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs","children":"[논문리뷰] SpatialTree: How Spatial Abilities Branch Out in MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs","children":"arXiv에 게시된 'SpatialTree: How Spatial Abilities Branch Out in MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-SpatialTree-How-Spatial-Abilities-Branch-Out-in-MLLMs"}]]}]]}],["$","article","2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems","children":"[논문리뷰] Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems","children":"Luisa Bentivogli이 arXiv에 게시한 'Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Simulstream-Open-Source-Toolkit-for-Evaluation-and-Demonstration-of-Streaming-Speech-to-Text-Translation-Systems"}]]}]]}],["$","article","2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","children":"[논문리뷰] SemanticGen: Video Generation in Semantic Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space","children":"arXiv에 게시된 'SemanticGen: Video Generation in Semantic Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-SemanticGen-Video-Generation-in-Semantic-Space"}]]}]]}],["$","article","2025-12-24-SAM-Audio-Segment-Anything-in-Audio",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SAM-Audio-Segment-Anything-in-Audio","children":"[논문리뷰] SAM Audio: Segment Anything in Audio"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-SAM-Audio-Segment-Anything-in-Audio","children":"arXiv에 게시된 'SAM Audio: Segment Anything in Audio' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-SAM-Audio-Segment-Anything-in-Audio"}]]}]]}],["$","article","2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library","children":"[논문리뷰] Reinforcement Learning for Self-Improving Agent with Skill Library"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library","children":"Soumya Smruti Mishra이 arXiv에 게시한 'Reinforcement Learning for Self-Improving Agent with Skill Library' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Reinforcement-Learning-for-Self-Improving-Agent-with-Skill-Library"}]]}]]}],["$","article","2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models","children":"[논문리뷰] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models","children":"arXiv에 게시된 'QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-QuantiPhy-A-Quantitative-Benchmark-Evaluating-Physical-Reasoning-Abilities-of-Vision-Language-Models"}]]}]]}],["$","article","2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation","children":"[논문리뷰] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation","children":"arXiv에 게시된 'Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation"}]]}]]}],["$","article","2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems","children":"[논문리뷰] MemEvolve: Meta-Evolution of Agent Memory Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems","children":"Junhao Wang이 arXiv에 게시한 'MemEvolve: Meta-Evolution of Agent Memory Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-MemEvolve-Meta-Evolution-of-Agent-Memory-Systems"}]]}]]}],["$","article","2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos","children":"[논문리뷰] LongVideoAgent: Multi-Agent Reasoning with Long Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos","children":"Renjie Pi이 arXiv에 게시한 'LongVideoAgent: Multi-Agent Reasoning with Long Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos"}]]}]]}],["$","article","2025-12-24-INTELLECT-3-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-INTELLECT-3-Technical-Report","children":"[논문리뷰] INTELLECT-3: Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-INTELLECT-3-Technical-Report","children":"arXiv에 게시된 'INTELLECT-3: Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-INTELLECT-3-Technical-Report"}]]}]]}],["$","article","2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination","children":"[논문리뷰] FaithLens: Detecting and Explaining Faithfulness Hallucination"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination","children":"arXiv에 게시된 'FaithLens: Detecting and Explaining Faithfulness Hallucination' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-FaithLens-Detecting-and-Explaining-Faithfulness-Hallucination"}]]}]]}],["$","article","2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies","children":"[논문리뷰] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies","children":"arXiv에 게시된 'Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies"}]]}]]}],["$","article","2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling","children":"[논문리뷰] Active Intelligence in Video Avatars via Closed-loop World Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling","children":"Cheng Meng이 arXiv에 게시한 'Active Intelligence in Video Avatars via Closed-loop World Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-24 00:00:00+0900+0900","children":"2025년 12월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-24-Active-Intelligence-in-Video-Avatars-via-Closed-loop-World-Modeling"}]]}]]}],["$","article","2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion","children":"[논문리뷰] WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion","children":"arXiv에 게시된 'WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-WorldWarp-Propagating-3D-Geometry-with-Asynchronous-Video-Diffusion"}]]}]]}],["$","article","2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives","children":"[논문리뷰] Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives","children":"Sujata Ghosh이 arXiv에 게시한 'Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Understanding-Syllogistic-Reasoning-in-LLMs-from-Formal-and-Natural-Language-Perspectives"}]]}]]}],["$","article","2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models","children":"[논문리뷰] UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models","children":"Yuqing Ma이 arXiv에 게시한 'UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-UCoder-Unsupervised-Code-Generation-by-Internal-Probing-of-Large-Language-Models"}]]}]]}],["$","article","2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding","children":"[논문리뷰] The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding","children":"Ziwei Liu이 arXiv에 게시한 'The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-The-Prism-Hypothesis-Harmonizing-Semantic-and-Pixel-Representations-via-Unified-Autoencoding"}]]}]]}],["$","article","2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory","children":"[논문리뷰] StoryMem: Multi-shot Long Video Storytelling with Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory","children":"arXiv에 게시된 'StoryMem: Multi-shot Long Video Storytelling with Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-StoryMem-Multi-shot-Long-Video-Storytelling-with-Memory"}]]}]]}],["$","article","2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing","children":"[논문리뷰] Region-Constraint In-Context Generation for Instructional Video Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing","children":"arXiv에 게시된 'Region-Constraint In-Context Generation for Instructional Video Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Region-Constraint-In-Context-Generation-for-Instructional-Video-Editing"}]]}]]}],["$","article","2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs","children":"[논문리뷰] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs","children":"arXiv에 게시된 'Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Reasoning-Palette-Modulating-Reasoning-via-Latent-Contextualization-for-Controllable-Exploration-for-VLMs"}]]}]]}],["$","article","2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface","children":"[논문리뷰] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface","children":"Liliang Chen이 arXiv에 게시한 'Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Real2Edit2Real-Generating-Robotic-Demonstrations-via-a-3D-Control-Interface"}]]}]]}],["$","article","2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation","children":"[논문리뷰] QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation","children":"Lu Cheng이 arXiv에 게시한 'QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-QuCo-RAG-Quantifying-Uncertainty-from-the-Pre-training-Corpus-for-Dynamic-Retrieval-Augmented-Generation"}]]}]]}],["$","article","2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming","children":"[논문리뷰] Name That Part: 3D Part Segmentation and Naming"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming","children":"Alan Yuille이 arXiv에 게시한 'Name That Part: 3D Part Segmentation and Naming' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Name-That-Part-3D-Part-Segmentation-and-Naming"}]]}]]}],["$","article","2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments","children":"[논문리뷰] MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments","children":"arXiv에 게시된 'MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-MobileWorld-Benchmarking-Autonomous-Mobile-Agents-in-Agent-User-Interactive-and-MCP-Augmented-Environments"}]]}]]}],["$","article","2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry","children":"[논문리뷰] MatSpray: Fusing 2D Material World Knowledge on 3D Geometry"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry","children":"arXiv에 게시된 'MatSpray: Fusing 2D Material World Knowledge on 3D Geometry' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-MatSpray-Fusing-2D-Material-World-Knowledge-on-3D-Geometry"}]]}]]}],["$","article","2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding","children":"[논문리뷰] LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding","children":"arXiv에 게시된 'LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-LoPA-Scaling-dLLM-Inference-via-Lookahead-Parallel-Decoding"}]]}]]}],["$","article","2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry","children":"[논문리뷰] LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry","children":"Yuan Shen이 arXiv에 게시한 'LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-LoGoPlanner-Localization-Grounded-Navigation-Policy-with-Metric-aware-Visual-Geometry"}]]}]]}],["$","article","2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","children":"[논문리뷰] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation","children":"arXiv에 게시된 'Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Infinite-Homography-as-Robust-Conditioning-for-Camera-Controlled-Video-Generation"}]]}]]}],["$","article","2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators","children":"[논문리뷰] GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators","children":"arXiv에 게시된 'GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-GenEnv-Difficulty-Aligned-Co-Evolution-Between-LLM-Agents-and-Environment-Simulators"}]]}]]}],["$","article","2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital","children":"[논문리뷰] Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital","children":"arXiv에 게시된 'Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Does-It-Tie-Out-Towards-Autonomous-Legal-Agents-in-Venture-Capital"}]]}]]}],["$","article","2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI","children":"[논문리뷰] DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI","children":"arXiv에 게시된 'DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-DataFlow-An-LLM-Driven-Framework-for-Unified-Data-Preparation-and-Workflow-Automation-in-the-Era-of-Data-Centric-AI"}]]}]]}],["$","article","2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction","children":"[논문리뷰] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction","children":"Hong Jiao이 arXiv에 게시한 'Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction"}]]}]]}],["$","article","2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States","children":"[논문리뷰] Brain-Grounded Axes for Reading and Steering LLM States"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States","children":"Sandro Andric이 arXiv에 게시한 'Brain-Grounded Axes for Reading and Steering LLM States' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-23 00:00:00+0900+0900","children":"2025년 12월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-23-Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States"}]]}]]}],["$","article","2025-12-22-When-Reasoning-Meets-Its-Laws",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-When-Reasoning-Meets-Its-Laws","children":"[논문리뷰] When Reasoning Meets Its Laws"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-When-Reasoning-Meets-Its-Laws","children":"Liu Ziyin이 arXiv에 게시한 'When Reasoning Meets Its Laws' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-When-Reasoning-Meets-Its-Laws"}]]}]]}],["$","article","2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs","children":"[논문리뷰] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs","children":"Lihong Li이 arXiv에 게시한 'Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Turn-PPO-Turn-Level-Advantage-Estimation-with-PPO-for-Improved-Multi-Turn-RL-in-Agentic-LLMs"}]]}]]}],["$","article","2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models","children":"[논문리뷰] StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models","children":"arXiv에 게시된 'StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-StageVAR-Stage-Aware-Acceleration-for-Visual-Autoregressive-Models"}]]}]]}],["$","article","2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience","children":"[논문리뷰] Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience","children":"arXiv에 게시된 'Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience"}]]}]]}],["$","article","2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories","children":"[논문리뷰] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories","children":"arXiv에 게시된 'SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-SWE-Bench-A-Framework-for-the-Scalable-Generation-of-Software-Engineering-Benchmarks-from-Open-Source-Repositories"}]]}]]}],["$","article","2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding","children":"[논문리뷰] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding","children":"Runtao Liu이 arXiv에 게시한 'Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Robust-R1-Degradation-Aware-Reasoning-for-Robust-Visual-Understanding"}]]}]]}],["$","article","2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras","children":"[논문리뷰] RadarGen: Automotive Radar Point Cloud Generation from Cameras"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras","children":"Or Litany이 arXiv에 게시한 'RadarGen: Automotive Radar Point Cloud Generation from Cameras' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-RadarGen-Automotive-Radar-Point-Cloud-Generation-from-Cameras"}]]}]]}],["$","article","2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows","children":"[논문리뷰] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows","children":"Yuhao Zhou이 arXiv에 게시한 'Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Probing-Scientific-General-Intelligence-of-LLMs-with-Scientist-Aligned-Workflows"}]]}]]}],["$","article","2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers","children":"[논문리뷰] Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers","children":"arXiv에 게시된 'Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Physics-of-Language-Models-Part-4-1-Architecture-Design-and-the-Magic-of-Canon-Layers"}]]}]]}],["$","article","2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence","children":"[논문리뷰] PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence","children":"arXiv에 게시된 'PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-PhysBrain-Human-Egocentric-Data-as-a-Bridge-from-Vision-Language-Models-to-Physical-Intelligence"}]]}]]}],["$","article","2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents","children":"[논문리뷰] Meta-RL Induces Exploration in Language Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents","children":"Maria Brbic이 arXiv에 게시한 'Meta-RL Induces Exploration in Language Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Meta-RL-Induces-Exploration-in-Language-Agents"}]]}]]}],["$","article","2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering","children":"[논문리뷰] HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering","children":"arXiv에 게시된 'HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-HERBench-A-Benchmark-for-Multi-Evidence-Integration-in-Video-Question-Answering"}]]}]]}],["$","article","2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation","children":"[논문리뷰] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation","children":"arXiv에 게시된 'GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-GroundingME-Exposing-the-Visual-Grounding-Gap-in-MLLMs-through-Multi-Dimensional-Evaluation"}]]}]]}],["$","article","2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing","children":"[논문리뷰] Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing","children":"arXiv에 게시된 'Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Both-Semantics-and-Reconstruction-Matter-Making-Representation-Encoders-Ready-for-Text-to-Image-Generation-and-Editing"}]]}]]}],["$","article","2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge","children":"[논문리뷰] Are We on the Right Way to Assessing LLM-as-a-Judge?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge","children":"arXiv에 게시된 'Are We on the Right Way to Assessing LLM-as-a-Judge?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge"}]]}]]}],["$","article","2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","children":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","children":"arXiv에 게시된 'An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges"}]]}]]}],["$","article","2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation","children":"[논문리뷰] 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation","children":"arXiv에 게시된 '4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation"}]]}]]}],["$","article","2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework","children":"[논문리뷰] 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework","children":"Hendrik P. A. Lensch이 arXiv에 게시한 '3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-22-3D-RE-GEN-3D-Reconstruction-of-Indoor-Scenes-with-a-Generative-Framework"}]]}]]}],["$","article","2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks","children":"[논문리뷰] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks","children":"arXiv에 게시된 'VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks"}]]}]]}],["$","article","2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","children":"[논문리뷰] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text","children":"arXiv에 게시된 'The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text"}]]}]]}],["$","article","2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","children":"[논문리뷰] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors","children":"arXiv에 게시된 'StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors"}]]}]]}],["$","article","2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model","children":"[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model","children":"arXiv에 게시된 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model"}]]}]]}],["$","article","2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing","children":"[논문리뷰] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing","children":"Yuqi Liu이 arXiv에 게시한 'RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing"}]]}]]}],["$","article","2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion","children":"[논문리뷰] REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion","children":"Giorgos Sfikas이 arXiv에 게시한 'REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion"}]]}]]}],["$","article","2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners","children":"[논문리뷰] Next-Embedding Prediction Makes Strong Vision Learners"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners","children":"arXiv에 게시된 'Next-Embedding Prediction Makes Strong Vision Learners' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners"}]]}]]}],["$","article","2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models","children":"[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models","children":"arXiv에 게시된 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models"}]]}]]}],["$","article","2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image","children":"[논문리뷰] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image","children":"arXiv에 게시된 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image"}]]}]]}],["$","article","2025-12-19-Kling-Omni-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Kling-Omni-Technical-Report","children":"[논문리뷰] Kling-Omni Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Kling-Omni-Technical-Report","children":"arXiv에 게시된 'Kling-Omni Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Kling-Omni-Technical-Report"}]]}]]}],["$","article","2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language","children":"[논문리뷰] Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language","children":"arXiv에 게시된 'Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language"}]]}]]}],["$","article","2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs","children":"[논문리뷰] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs","children":"Carlos Escolano이 arXiv에 게시한 'Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs"}]]}]]}],["$","article","2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image","children":"[논문리뷰] Generative Refocusing: Flexible Defocus Control from a Single Image"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image","children":"Yu-Lun Liu이 arXiv에 게시한 'Generative Refocusing: Flexible Defocus Control from a Single Image' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image"}]]}]]}],["$","article","2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering","children":"[논문리뷰] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering","children":"Hendrik P. A. Lensch이 arXiv에 게시한 'FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering"}]]}]]}],["$","article","2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","children":"[논문리뷰] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction","children":"arXiv에 게시된 'FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction"}]]}]]}],["$","article","2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward","children":"[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward","children":"arXiv에 게시된 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward"}]]}]]}],["$","article","2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification","children":"[논문리뷰] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification","children":"arXiv에 게시된 'Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification"}]]}]]}],["$","article","2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation","children":"[논문리뷰] Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation","children":"Wenxuan Lu이 arXiv에 게시한 'Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation"}]]}]]}],["$","article","2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers","children":"[논문리뷰] DeContext as Defense: Safe Image Editing in Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers","children":"arXiv에 게시된 'DeContext as Defense: Safe Image Editing in Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers"}]]}]]}],["$","article","2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection","children":"[논문리뷰] Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection","children":"Jiarong Ou이 arXiv에 게시한 'Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection"}]]}]]}],["$","article","2025-12-19-Adaptation-of-Agentic-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Adaptation-of-Agentic-AI","children":"[논문리뷰] Adaptation of Agentic AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-Adaptation-of-Agentic-AI","children":"Zhiyi Shi이 arXiv에 게시한 'Adaptation of Agentic AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-Adaptation-of-Agentic-AI"}]]}]]}],["$","article","2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos","children":"[논문리뷰] AdaTooler-V: Adaptive Tool-Use for Images and Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos","children":"Zhixun Li이 arXiv에 게시한 'AdaTooler-V: Adaptive Tool-Use for Images and Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos"}]]}]]}],["$","article","2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory","children":"[논문리뷰] WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory","children":"Sung Won Han이 arXiv에 게시한 'WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-WAY-Estimation-of-Vessel-Destination-in-Worldwide-AIS-Trajectory"}]]}]]}],["$","article","2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression","children":"[논문리뷰] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression","children":"arXiv에 게시된 'VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-VTCBench-Can-Vision-Language-Models-Understand-Long-Context-with-Vision-Text-Compression"}]]}]]}],["$","article","2025-12-18-Universal-Reasoning-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Universal-Reasoning-Model","children":"[논문리뷰] Universal Reasoning Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Universal-Reasoning-Model","children":"arXiv에 게시된 'Universal Reasoning Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Universal-Reasoning-Model"}]]}]]}],["$","article","2025-12-18-Step-GUI-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Step-GUI-Technical-Report","children":"[논문리뷰] Step-GUI Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Step-GUI-Technical-Report","children":"arXiv에 게시된 'Step-GUI Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Step-GUI-Technical-Report"}]]}]]}],["$","article","2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning","children":"[논문리뷰] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning","children":"arXiv에 게시된 'Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Skyra-AI-Generated-Video-Detection-via-Grounded-Artifact-Reasoning"}]]}]]}],["$","article","2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness","children":"[논문리뷰] SCOPE: Prompt Evolution for Enhancing Agent Effectiveness"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness","children":"Yunhe Wang이 arXiv에 게시한 'SCOPE: Prompt Evolution for Enhancing Agent Effectiveness' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-SCOPE-Prompt-Evolution-for-Enhancing-Agent-Effectiveness"}]]}]]}],["$","article","2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning","children":"[논문리뷰] SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning","children":"arXiv에 게시된 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content","children":"[논문리뷰] Robust and Calibrated Detection of Authentic Multimedia Content"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content","children":"arXiv에 게시된 'Robust and Calibrated Detection of Authentic Multimedia Content' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Robust-and-Calibrated-Detection-of-Authentic-Multimedia-Content"}]]}]]}],["$","article","2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition","children":"[논문리뷰] Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition","children":"Xiao Xu이 arXiv에 게시한 'Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition"}]]}]]}],["$","article","2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","children":"[논문리뷰] MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","children":"Peizhou Cao이 arXiv에 게시한 'MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence"}]]}]]}],["$","article","2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training","children":"[논문리뷰] In Pursuit of Pixel Supervision for Visual Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training","children":"Dong Wang이 arXiv에 게시한 'In Pursuit of Pixel Supervision for Visual Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training"}]]}]]}],["$","article","2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning","children":"[논문리뷰] IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning","children":"arXiv에 게시된 'IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-IC-Effect-Precise-and-Efficient-Video-Effects-Editing-via-In-Context-Learning"}]]}]]}],["$","article","2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices","children":"[논문리뷰] HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices","children":"Yuhang Dong이 arXiv에 게시한 'HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-HyperVL-An-Efficient-and-Dynamic-Multimodal-Large-Language-Model-for-Edge-Devices"}]]}]]}],["$","article","2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing","children":"[논문리뷰] Fast and Accurate Causal Parallel Decoding using Jacobi Forcing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing","children":"Tajana Rosing이 arXiv에 게시한 'Fast and Accurate Causal Parallel Decoding using Jacobi Forcing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Fast-and-Accurate-Causal-Parallel-Decoding-using-Jacobi-Forcing"}]]}]]}],["$","article","2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models","children":"[논문리뷰] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models","children":"arXiv에 게시된 'DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-DiffusionVL-Translating-Any-Autoregressive-Models-into-Diffusion-Vision-Language-Models"}]]}]]}],["$","article","2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models","children":"[논문리뷰] DEER: Draft with Diffusion, Verify with Autoregressive Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models","children":"Zhijie Deng이 arXiv에 게시한 'DEER: Draft with Diffusion, Verify with Autoregressive Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-DEER-Draft-with-Diffusion-Verify-with-Autoregressive-Models"}]]}]]}],["$","article","2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning","children":"[논문리뷰] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning","children":"arXiv에 게시된 'Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning"}]]}]]}],["$","article","2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans","children":"[논문리뷰] Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans","children":"Ming Hu이 arXiv에 게시한 'Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-Video-Reality-Test-Can-AI-Generated-ASMR-Videos-fool-VLMs-and-Humans"}]]}]]}],["$","article","2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models","children":"[논문리뷰] Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models","children":"arXiv에 게시된 'Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-Sparse-LaViDa-Sparse-Multimodal-Discrete-Diffusion-Language-Models"}]]}]]}],["$","article","2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement","children":"[논문리뷰] ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement","children":"Zhaohe Liao이 arXiv에 게시한 'ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-ShowTable-Unlocking-Creative-Table-Visualization-with-Collaborative-Reflection-and-Refinement"}]]}]]}],["$","article","2025-12-17-RecGPT-V2-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-RecGPT-V2-Technical-Report","children":"[논문리뷰] RecGPT-V2 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-RecGPT-V2-Technical-Report","children":"Dian Chen이 arXiv에 게시한 'RecGPT-V2 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-RecGPT-V2-Technical-Report"}]]}]]}],["$","article","2025-12-17-Olmo-3",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Olmo-3","children":"[논문리뷰] Olmo 3"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Olmo-3","children":"arXiv에 게시된 'Olmo 3' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-Olmo-3"}]]}]]}],["$","article","2025-12-17-MMGR-Multi-Modal-Generative-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-MMGR-Multi-Modal-Generative-Reasoning","children":"[논문리뷰] MMGR: Multi-Modal Generative Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-MMGR-Multi-Modal-Generative-Reasoning","children":"Haozhe Zhao이 arXiv에 게시한 'MMGR: Multi-Modal Generative Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-MMGR-Multi-Modal-Generative-Reasoning"}]]}]]}],["$","article","2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference","children":"[논문리뷰] Janus: Disaggregating Attention and Experts for Scalable MoE Inference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference","children":"arXiv에 게시된 'Janus: Disaggregating Attention and Experts for Scalable MoE Inference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-Janus-Disaggregating-Attention-and-Experts-for-Scalable-MoE-Inference"}]]}]]}],["$","article","2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning","children":"[논문리뷰] A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning","children":"Hongfei Zhang이 arXiv에 게시한 'A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-17 00:00:00+0900+0900","children":"2025년 12월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-17-A4-Agent-An-Agentic-Framework-for-Zero-Shot-Affordance-Reasoning"}]]}]]}],["$","article","2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions","children":"[논문리뷰] V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions","children":"Kwesi Cobbina이 arXiv에 게시한 'V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-V-REX-Benchmarking-Exploratory-Visual-Reasoning-via-Chain-of-Questions"}]]}]]}],["$","article","2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation","children":"[논문리뷰] Towards Scalable Pre-training of Visual Tokenizers for Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation","children":"arXiv에 게시된 'Towards Scalable Pre-training of Visual Tokenizers for Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Towards-Scalable-Pre-training-of-Visual-Tokenizers-for-Generation"}]]}]]}],["$","article","2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans","children":"[논문리뷰] Towards Interactive Intelligence for Digital Humans"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans","children":"Yifei Huang이 arXiv에 게시한 'Towards Interactive Intelligence for Digital Humans' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Towards-Interactive-Intelligence-for-Digital-Humans"}]]}]]}],["$","article","2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection","children":"[논문리뷰] Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection","children":"arXiv에 게시된 'Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Toward-Ambulatory-Vision-Learning-Visually-Grounded-Active-View-Selection"}]]}]]}],["$","article","2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge","children":"[논문리뷰] Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge","children":"Jinwei Gu이 arXiv에 게시한 'Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Openpi-Comet-Competition-Solution-For-2025-BEHAVIOR-Challenge"}]]}]]}],["$","article","2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents","children":"[논문리뷰] NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents","children":"chongyang09이 arXiv에 게시한 'NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-NL2Repo-Bench-Towards-Long-Horizon-Repository-Generation-Evaluation-of-Coding-Agents"}]]}]]}],["$","article","2025-12-16-Memory-in-the-Age-of-AI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Memory-in-the-Age-of-AI-Agents","children":"[논문리뷰] Memory in the Age of AI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Memory-in-the-Age-of-AI-Agents","children":"Yanwei Yue이 arXiv에 게시한 'Memory in the Age of AI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Memory-in-the-Age-of-AI-Agents"}]]}]]}],["$","article","2025-12-16-KlingAvatar-2-0-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-KlingAvatar-2-0-Technical-Report","children":"[논문리뷰] KlingAvatar 2.0 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-KlingAvatar-2-0-Technical-Report","children":"arXiv에 게시된 'KlingAvatar 2.0 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-KlingAvatar-2-0-Technical-Report"}]]}]]}],["$","article","2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver","children":"[논문리뷰] Image Diffusion Preview with Consistency Solver"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver","children":"arXiv에 게시된 'Image Diffusion Preview with Consistency Solver' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-16 00:00:00+0900+0900","children":"2025년 12월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-16-Image-Diffusion-Preview-with-Consistency-Solver"}]]}]]}],["$","article","2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","children":"[논문리뷰] V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties","children":"arXiv에 게시된 'V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-V-RGBX-Video-Editing-with-Accurate-Controls-over-Intrinsic-Properties"}]]}]]}],["$","article","2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge","children":"[논문리뷰] Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge","children":"Akash Karnatak이 arXiv에 게시한 'Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge"}]]}]]}],["$","article","2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","children":"[논문리뷰] Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation","children":"Qifeng Chen이 arXiv에 게시한 'Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Structure-From-Tracking-Distilling-Structure-Preserving-Motion-for-Video-Generation"}]]}]]}],["$","article","2025-12-15-Sliding-Window-Attention-Adaptation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation","children":"[논문리뷰] Sliding Window Attention Adaptation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation","children":"arXiv에 게시된 'Sliding Window Attention Adaptation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation"}]]}]]}],["$","article","2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second","children":"[논문리뷰] Sharp Monocular View Synthesis in Less Than a Second"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second","children":"arXiv에 게시된 'Sharp Monocular View Synthesis in Less Than a Second' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Sharp-Monocular-View-Synthesis-in-Less-Than-a-Second"}]]}]]}],["$","article","2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models","children":"[논문리뷰] Scaling Behavior of Discrete Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models","children":"arXiv에 게시된 'Scaling Behavior of Discrete Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Scaling-Behavior-of-Discrete-Diffusion-Language-Models"}]]}]]}],["$","article","2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder","children":"[논문리뷰] SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder","children":"arXiv에 게시된 'SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-SVG-T2I-Scaling-Up-Text-to-Image-Latent-Diffusion-Model-Without-Variational-Autoencoder"}]]}]]}],["$","article","2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming","children":"[논문리뷰] PersonaLive! Expressive Portrait Image Animation for Live Streaming"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming","children":"Jue Wang이 arXiv에 게시한 'PersonaLive! Expressive Portrait Image Animation for Live Streaming' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-PersonaLive-Expressive-Portrait-Image-Animation-for-Live-Streaming"}]]}]]}],["$","article","2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes","children":"[논문리뷰] MeshSplatting: Differentiable Rendering with Opaque Meshes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes","children":"Matheus Gadelha이 arXiv에 게시한 'MeshSplatting: Differentiable Rendering with Opaque Meshes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes"}]]}]]}],["$","article","2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator","children":"[논문리뷰] LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator","children":"arXiv에 게시된 'LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-LEO-RobotAgent-A-General-purpose-Robotic-Agent-for-Language-driven-Embodied-Operator"}]]}]]}],["$","article","2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","children":"[논문리뷰] Exploring MLLM-Diffusion Information Transfer with MetaCanvas"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas","children":"arXiv에 게시된 'Exploring MLLM-Diffusion Information Transfer with MetaCanvas' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Exploring-MLLM-Diffusion-Information-Transfer-with-MetaCanvas"}]]}]]}],["$","article","2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video","children":"[논문리뷰] EgoX: Egocentric Video Generation from a Single Exocentric Video"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video","children":"arXiv에 게시된 'EgoX: Egocentric Video Generation from a Single Exocentric Video' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-EgoX-Egocentric-Video-Generation-from-a-Single-Exocentric-Video"}]]}]]}],["$","article","2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry","children":"[논문리뷰] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry","children":"Yanchao Li이 arXiv에 게시한 'DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-DentalGPT-Incentivizing-Multimodal-Complex-Reasoning-in-Dentistry"}]]}]]}],["$","article","2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images","children":"[논문리뷰] CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images","children":"Enzo Ferrante이 arXiv에 게시한 'CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-CheXmask-U-Quantifying-uncertainty-in-landmark-based-anatomical-segmentation-for-X-ray-images"}]]}]]}],["$","article","2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction","children":"[논문리뷰] VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction","children":"arXiv에 게시된 'VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-VQRAE-Representation-Quantization-Autoencoders-for-Multimodal-Understanding-Generation-and-Reconstruction"}]]}]]}],["$","article","2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task","children":"[논문리뷰] Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task","children":"arXiv에 게시된 'Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Tool-Augmented-Spatiotemporal-Reasoning-for-Streamlining-Video-Question-Answering-Task"}]]}]]}],["$","article","2025-12-12-Thinking-with-Images-via-Self-Calling-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Thinking-with-Images-via-Self-Calling-Agent","children":"[논문리뷰] Thinking with Images via Self-Calling Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Thinking-with-Images-via-Self-Calling-Agent","children":"Qixiang Ye이 arXiv에 게시한 'Thinking with Images via Self-Calling Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Thinking-with-Images-via-Self-Calling-Agent"}]]}]]}],["$","article","2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality","children":"[논문리뷰] The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality","children":"arXiv에 게시된 'The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-The-FACTS-Leaderboard-A-Comprehensive-Benchmark-for-Large-Language-Model-Factuality"}]]}]]}],["$","article","2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground","children":"[논문리뷰] T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground","children":"arXiv에 게시된 'T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-T-pro-2-0-An-Efficient-Russian-Hybrid-Reasoning-Model-and-Playground"}]]}]]}],["$","article","2025-12-12-Stronger-Normalization-Free-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Stronger-Normalization-Free-Transformers","children":"[논문리뷰] Stronger Normalization-Free Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Stronger-Normalization-Free-Transformers","children":"Zhuang Liu이 arXiv에 게시한 'Stronger Normalization-Free Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Stronger-Normalization-Free-Transformers"}]]}]]}],["$","article","2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning","children":"[논문리뷰] ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning","children":"Yujin Han이 arXiv에 게시한 'ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-ReViSE-Towards-Reason-Informed-Video-Editing-in-Unified-Models-with-Self-Reflective-Learning"}]]}]]}],["$","article","2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification","children":"[논문리뷰] OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification","children":"arXiv에 게시된 'OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification"}]]}]]}],["$","article","2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos","children":"[논문리뷰] MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos","children":"Qi Wang이 arXiv에 게시한 'MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-MoCapAnything-Unified-3D-Motion-Capture-for-Arbitrary-Skeletons-from-Monocular-Videos"}]]}]]}],["$","article","2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents","children":"[논문리뷰] MOA: Multi-Objective Alignment for Role-Playing Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents","children":"Yongbin Li이 arXiv에 게시한 'MOA: Multi-Objective Alignment for Role-Playing Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-MOA-Multi-Objective-Alignment-for-Role-Playing-Agents"}]]}]]}],["$","article","2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving","children":"[논문리뷰] Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving","children":"arXiv에 게시된 'Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Long-horizon-Reasoning-Agent-for-Olympiad-Level-Mathematical-Problem-Solving"}]]}]]}],["$","article","2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos","children":"[논문리뷰] H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos","children":"Mike Zheng Shou이 arXiv에 게시한 'H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-H2R-Grounder-A-Paired-Data-Free-Paradigm-for-Translating-Human-Interaction-Videos-into-Physically-Grounded-Robot-Videos"}]]}]]}],["$","article","2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models","children":"[논문리뷰] From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models","children":"arXiv에 게시된 'From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-From-Macro-to-Micro-Benchmarking-Microscopic-Spatial-Intelligence-on-Molecules-via-Vision-Language-Models"}]]}]]}],["$","article","2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents","children":"[논문리뷰] Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents","children":"Xiaodong Gu이 arXiv에 게시한 'Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Fed-SE-Federated-Self-Evolution-for-Privacy-Constrained-Multi-Environment-LLM-Agents"}]]}]]}],["$","article","2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator","children":"[논문리뷰] Evaluating Gemini Robotics Policies in a Veo World Simulator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator","children":"arXiv에 게시된 'Evaluating Gemini Robotics Policies in a Veo World Simulator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Evaluating-Gemini-Robotics-Policies-in-a-Veo-World-Simulator"}]]}]]}],["$","article","2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale","children":"[논문리뷰] Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale","children":"arXiv에 게시된 'Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Confucius-Code-Agent-An-Open-sourced-AI-Software-Engineer-at-Industrial-Scale"}]]}]]}],["$","article","2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation","children":"[논문리뷰] Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation","children":"arXiv에 게시된 'Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Are-We-Ready-for-RL-in-Text-to-3D-Generation-A-Progressive-Investigation"}]]}]]}],["$","article","2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning","children":"[논문리뷰] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning","children":"arXiv에 게시된 'Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-Achieving-Olympia-Level-Geometry-Large-Language-Model-Agent-via-Complexity-Boosting-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation","children":"[논문리뷰] WonderZoom: Multi-Scale 3D World Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation","children":"Jiajun Wu이 arXiv에 게시한 'WonderZoom: Multi-Scale 3D World Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-WonderZoom-Multi-Scale-3D-World-Generation"}]]}]]}],["$","article","2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory","children":"[논문리뷰] VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory","children":"arXiv에 게시된 'VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-VideoSSM-Autoregressive-Long-Video-Generation-with-Hybrid-State-Space-Memory"}]]}]]}],["$","article","2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","children":"[논문리뷰] UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","children":"arXiv에 게시된 'UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving"}]]}]]}],["$","article","2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression","children":"[논문리뷰] TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression","children":"arXiv에 게시된 'TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-TED-4DGS-Temporally-Activated-and-Embedding-based-Deformation-for-4DGS-Compression"}]]}]]}],["$","article","2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","children":"[논문리뷰] StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation","children":"Guixun Luo이 arXiv에 게시한 'StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-StereoWorld-Geometry-Aware-Monocular-to-Stereo-Video-Generation"}]]}]]}],["$","article","2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication","children":"[논문리뷰] Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication","children":"Hengshu Zhu이 arXiv에 게시한 'Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Reinventing-Clinical-Dialogue-Agentic-Paradigms-for-LLM-Enabled-Healthcare-Communication"}]]}]]}],["$","article","2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models","children":"[논문리뷰] Pay Less Attention to Function Words for Free Robustness of Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models","children":"arXiv에 게시된 'Pay Less Attention to Function Words for Free Robustness of Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models"}]]}]]}],["$","article","2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer","children":"[논문리뷰] OmniPSD: Layered PSD Generation with Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer","children":"Cheng Liu이 arXiv에 게시한 'OmniPSD: Layered PSD Generation with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-OmniPSD-Layered-PSD-Generation-with-Diffusion-Transformer"}]]}]]}],["$","article","2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models","children":"[논문리뷰] Learning Unmasking Policies for Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models","children":"arXiv에 게시된 'Learning Unmasking Policies for Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Learning-Unmasking-Policies-for-Diffusion-Language-Models"}]]}]]}],["$","article","2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models","children":"[논문리뷰] InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models","children":"arXiv에 게시된 'InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-InfiniteVL-Synergizing-Linear-and-Sparse-Attention-for-Highly-Efficient-Unlimited-Input-Vision-Language-Models"}]]}]]}],["$","article","2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting","children":"[논문리뷰] IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting","children":"arXiv에 게시된 'IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-IF-Bench-Benchmarking-and-Enhancing-MLLMs-for-Infrared-Images-with-Generative-Visual-Prompting"}]]}]]}],["$","article","2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models","children":"[논문리뷰] HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models","children":"arXiv에 게시된 'HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-HiF-VLA-Hindsight-Insight-and-Foresight-through-Motion-Representation-for-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules","children":"[논문리뷰] Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules","children":"Yang Zhang이 arXiv에 게시한 'Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Fast-Decoding-Diffusion-Language-Models-via-Progress-Aware-Confidence-Schedules"}]]}]]}],["$","article","2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing","children":"[논문리뷰] EtCon: Edit-then-Consolidate for Reliable Knowledge Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing","children":"Chenglin Li이 arXiv에 게시한 'EtCon: Edit-then-Consolidate for Reliable Knowledge Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing"}]]}]]}],["$","article","2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding","children":"[논문리뷰] Composing Concepts from Images and Videos via Concept-prompt Binding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding","children":"arXiv에 게시된 'Composing Concepts from Images and Videos via Concept-prompt Binding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Composing-Concepts-from-Images-and-Videos-via-Concept-prompt-Binding"}]]}]]}],["$","article","2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain","children":"[논문리뷰] BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain","children":"tamarott이 arXiv에 게시한 'BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-BrainExplore-Large-Scale-Discovery-of-Interpretable-Visual-Representations-in-the-Human-Brain"}]]}]]}],["$","article","2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS","children":"[논문리뷰] Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS","children":"Morteza Abolghasemi이 arXiv에 게시한 'Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-Beyond-Unified-Models-A-Service-Oriented-Approach-to-Low-Latency-Context-Aware-Phonemization-for-Real-Time-TTS"}]]}]]}],["$","article","2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","children":"[논문리뷰] Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance","children":"arXiv에 게시된 'Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Wan-Move-Motion-controllable-Video-Generation-via-Latent-Trajectory-Guidance"}]]}]]}],["$","article","2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform","children":"[논문리뷰] Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform","children":"Muyao Niu이 arXiv에 게시한 'Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Visionary-The-World-Model-Carrier-Built-on-WebGPU-Powered-Gaussian-Splatting-Platform"}]]}]]}],["$","article","2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models","children":"[논문리뷰] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models","children":"Weirui Ye이 arXiv에 게시한 'TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-TreeGRPO-Tree-Advantage-GRPO-for-Online-RL-Post-Training-of-Diffusion-Models"}]]}]]}],["$","article","2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels","children":"[논문리뷰] TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels","children":"Tianyu Huang이 arXiv에 게시한 'TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-TrackingWorld-World-centric-Monocular-3D-Tracking-of-Almost-All-Pixels"}]]}]]}],["$","article","2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models","children":"[논문리뷰] ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models","children":"Xiuyu Li이 arXiv에 게시한 'ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models"}]]}]]}],["$","article","2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs","children":"[논문리뷰] Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs","children":"arXiv에 게시된 'Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Same-Content-Different-Answers-Cross-Modal-Inconsistency-in-MLLMs"}]]}]]}],["$","article","2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting","children":"[논문리뷰] SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting","children":"Sung-Ho Bae이 arXiv에 게시한 'SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-SUCCESS-GS-Survey-of-Compactness-and-Compression-for-Efficient-Static-and-Dynamic-Gaussian-Splatting"}]]}]]}],["$","article","2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality","children":"[논문리뷰] Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality","children":"arXiv에 게시된 'Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Preserving-Source-Video-Realism-High-Fidelity-Face-Swapping-for-Cinematic-Quality"}]]}]]}],["$","article","2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks","children":"[논문리뷰] Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks","children":"arXiv에 게시된 'Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Predicting-Time-Dependent-Flow-Over-Complex-Geometries-Using-Operator-Networks"}]]}]]}],["$","article","2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory","children":"[논문리뷰] OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory","children":"arXiv에 게시된 'OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory"}]]}]]}],["$","article","2025-12-10-Modular-Neural-Image-Signal-Processing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Modular-Neural-Image-Signal-Processing","children":"[논문리뷰] Modular Neural Image Signal Processing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Modular-Neural-Image-Signal-Processing","children":"Michael S. Brown이 arXiv에 게시한 'Modular Neural Image Signal Processing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Modular-Neural-Image-Signal-Processing"}]]}]]}],["$","article","2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","children":"[논문리뷰] MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment","children":"arXiv에 게시된 'MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment"}]]}]]}],["$","article","2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning","children":"[논문리뷰] LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning","children":"arXiv에 게시된 'LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning"}]]}]]}],["$","article","2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation","children":"[논문리뷰] Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation","children":"arXiv에 게시된 'Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Ground-Slow-Move-Fast-A-Dual-System-Foundation-Model-for-Generalizable-Vision-and-Language-Navigation"}]]}]]}],["$","article","2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs","children":"[논문리뷰] From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs","children":"arXiv에 게시된 'From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs"}]]}]]}],["$","article","2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time","children":"[논문리뷰] Efficiently Reconstructing Dynamic Scenes One D4RT at a Time"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time","children":"arXiv에 게시된 'Efficiently Reconstructing Dynamic Scenes One D4RT at a Time' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Efficiently-Reconstructing-Dynamic-Scenes-One-D4RT-at-a-Time"}]]}]]}],["$","article","2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce","children":"[논문리뷰] EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce","children":"arXiv에 게시된 'EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-EcomBench-Towards-Holistic-Evaluation-of-Foundation-Agents-in-E-commerce"}]]}]]}],["$","article","2025-12-10-DeepCode-Open-Agentic-Coding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-DeepCode-Open-Agentic-Coding","children":"[논문리뷰] DeepCode: Open Agentic Coding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-DeepCode-Open-Agentic-Coding","children":"Chao Huang이 arXiv에 게시한 'DeepCode: Open Agentic Coding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-DeepCode-Open-Agentic-Coding"}]]}]]}],["$","article","2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training","children":"[논문리뷰] Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training","children":"Dim P. Papadopoulos이 arXiv에 게시한 'Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training"}]]}]]}],["$","article","2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering","children":"[논문리뷰] Voxify3D: Pixel Art Meets Volumetric Rendering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering","children":"Yu-Lun Liu이 arXiv에 게시한 'Voxify3D: Pixel Art Meets Volumetric Rendering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering"}]]}]]}],["$","article","2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators","children":"[논문리뷰] VideoVLA: Video Generators Can Be Generalizable Robot Manipulators"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators","children":"Yaobo Liang이 arXiv에 게시한 'VideoVLA: Video Generators Can Be Generalizable Robot Manipulators' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators"}]]}]]}],["$","article","2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning","children":"[논문리뷰] VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning","children":"Yansong Tang이 arXiv에 게시한 'VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","children":"[논문리뷰] UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation","children":"arXiv에 게시된 'UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation"}]]}]]}],["$","article","2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","children":"[논문리뷰] Unified Video Editing with Temporal Reasoner"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","children":"arXiv에 게시된 'Unified Video Editing with Temporal Reasoner' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner"}]]}]]}],["$","article","2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation","children":"[논문리뷰] Scaling Zero-Shot Reference-to-Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation","children":"arXiv에 게시된 'Scaling Zero-Shot Reference-to-Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Scaling-Zero-Shot-Reference-to-Video-Generation"}]]}]]}],["$","article","2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation","children":"[논문리뷰] Rethinking Training Dynamics in Scale-wise Autoregressive Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation","children":"arXiv에 게시된 'Rethinking Training Dynamics in Scale-wise Autoregressive Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation"}]]}]]}],["$","article","2025-12-09-Relational-Visual-Similarity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Relational-Visual-Similarity","children":"[논문리뷰] Relational Visual Similarity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Relational-Visual-Similarity","children":"Jing Shi이 arXiv에 게시한 'Relational Visual Similarity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Relational-Visual-Similarity"}]]}]]}],["$","article","2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","children":"[논문리뷰] ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","children":"Taojun Ding이 arXiv에 게시한 'ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation"}]]}]]}],["$","article","2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","children":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","children":"arXiv에 게시된 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models"}]]}]]}],["$","article","2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation","children":"[논문리뷰] OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation","children":"Simeng Qin이 arXiv에 게시한 'OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation"}]]}]]}],["$","article","2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning","children":"[논문리뷰] Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning","children":"arXiv에 게시된 'Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader","children":"[논문리뷰] Multi-view Pyramid Transformer: Look Coarser to See Broader"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader","children":"Jungwoo Kim이 arXiv에 게시한 'Multi-view Pyramid Transformer: Look Coarser to See Broader' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Multi-view-Pyramid-Transformer-Look-Coarser-to-See-Broader"}]]}]]}],["$","article","2025-12-09-LongCat-Image-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-LongCat-Image-Technical-Report","children":"[논문리뷰] LongCat-Image Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-LongCat-Image-Technical-Report","children":"arXiv에 게시된 'LongCat-Image Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-LongCat-Image-Technical-Report"}]]}]]}],["$","article","2025-12-09-Group-Representational-Position-Encoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Group-Representational-Position-Encoding","children":"[논문리뷰] Group Representational Position Encoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Group-Representational-Position-Encoding","children":"arXiv에 게시된 'Group Representational Position Encoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Group-Representational-Position-Encoding"}]]}]]}],["$","article","2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","children":"[논문리뷰] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing","children":"arXiv에 게시된 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing"}]]}]]}],["$","article","2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems","children":"[논문리뷰] DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems","children":"arXiv에 게시된 'DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-DoVer-Intervention-Driven-Auto-Debugging-for-LLM-Multi-Agent-Systems"}]]}]]}],["$","article","2025-12-09-Distribution-Matching-Variational-AutoEncoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder","children":"[논문리뷰] Distribution Matching Variational AutoEncoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder","children":"arXiv에 게시된 'Distribution Matching Variational AutoEncoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Distribution-Matching-Variational-AutoEncoder"}]]}]]}],["$","article","2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning","children":"[논문리뷰] Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning","children":"arXiv에 게시된 'Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Decouple-to-Generalize-Context-First-Self-Evolving-Learning-for-Data-Scarce-Vision-Language-Reasoning"}]]}]]}],["$","article","2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue","children":"[논문리뷰] DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue","children":"YijunLiao이 arXiv에 게시한 'DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-DZ-TDPO-Non-Destructive-Temporal-Alignment-for-Mutable-State-Tracking-in-Long-Context-Dialogue"}]]}]]}],["$","article","2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning","children":"[논문리뷰] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning","children":"Jiacheng Chen이 arXiv에 게시한 'Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs","children":"[논문리뷰] Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs","children":"arXiv에 게시된 'Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs"}]]}]]}],["$","article","2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty","children":"[논문리뷰] World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty","children":"Anirudha Majumdar이 arXiv에 게시한 'World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty"}]]}]]}],["$","article","2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows","children":"[논문리뷰] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows","children":"arXiv에 게시된 'TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows"}]]}]]}],["$","article","2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation","children":"[논문리뷰] TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation","children":"Salih Tileylioglu이 arXiv에 게시한 'TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation"}]]}]]}],["$","article","2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling","children":"[논문리뷰] SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling","children":"Marc Pollefeys이 arXiv에 게시한 'SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling"}]]}]]}],["$","article","2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations","children":"[논문리뷰] Self-Improving VLM Judges Without Human Annotations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations","children":"arXiv에 게시된 'Self-Improving VLM Judges Without Human Annotations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations"}]]}]]}],["$","article","2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs","children":"[논문리뷰] SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs","children":"Minghui Yu이 arXiv에 게시한 'SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs"}]]}]]}],["$","article","2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations","children":"[논문리뷰] SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations","children":"arXiv에 게시된 'SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations"}]]}]]}],["$","article","2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards","children":"[논문리뷰] RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards","children":"Zilong Huang이 arXiv에 게시한 'RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards"}]]}]]}],["$","article","2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning","children":"[논문리뷰] ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning","children":"Shengju Qian이 arXiv에 게시한 'ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation","children":"[논문리뷰] ProPhy: Progressive Physical Alignment for Dynamic World Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation","children":"Yuhao Cheng이 arXiv에 게시한 'ProPhy: Progressive Physical Alignment for Dynamic World Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation"}]]}]]}],["$","article","2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image","children":"[논문리뷰] Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image","children":"arXiv에 게시된 'Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image"}]]}]]}],["$","article","2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks","children":"[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks","children":"Yang Li이 arXiv에 게시한 'From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks"}]]}]]}],["$","article","2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning","children":"[논문리뷰] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning","children":"Zijia Lin이 arXiv에 게시한 'Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor","children":"[논문리뷰] EditThinker: Unlocking Iterative Reasoning for Any Image Editor"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor","children":"Ziyu Guo이 arXiv에 게시한 'EditThinker: Unlocking Iterative Reasoning for Any Image Editor' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor"}]]}]]}],["$","article","2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence","children":"[논문리뷰] COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence","children":"Jiawei Sheng이 arXiv에 게시한 'COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence"}]]}]]}],["$","article","2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence","children":"[논문리뷰] AI & Human Co-Improvement for Safer Co-Superintelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence","children":"arXiv에 게시된 'AI & Human Co-Improvement for Safer Co-Superintelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence"}]]}]]}],["$","article","2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers","children":"[논문리뷰] UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers","children":"arXiv에 게시된 'UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers"}]]}]]}],["$","article","2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation","children":"[논문리뷰] TV2TV: A Unified Framework for Interleaved Language and Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation","children":"arXiv에 게시된 'TV2TV: A Unified Framework for Interleaved Language and Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation"}]]}]]}],["$","article","2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting","children":"[논문리뷰] Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting","children":"Yu-Lun Liu이 arXiv에 게시한 'Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting"}]]}]]}],["$","article","2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs","children":"[논문리뷰] SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs","children":"arXiv에 게시된 'SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs"}]]}]]}],["$","article","2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion","children":"[논문리뷰] Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion","children":"arXiv에 게시된 'Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Semantics-Lead-the-Way-Harmonizing-Semantic-and-Texture-Modeling-with-Asynchronous-Latent-Diffusion"}]]}]]}],["$","article","2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization","children":"[논문리뷰] SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization","children":"arXiv에 게시된 'SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-SeeNav-Agent-Enhancing-Vision-Language-Navigation-with-Visual-Prompt-and-Step-Level-Policy-Optimization"}]]}]]}],["$","article","2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds","children":"[논문리뷰] SIMA 2: A Generalist Embodied Agent for Virtual Worlds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds","children":"arXiv에 게시된 'SIMA 2: A Generalist Embodied Agent for Virtual Worlds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-SIMA-2-A-Generalist-Embodied-Agent-for-Virtual-Worlds"}]]}]]}],["$","article","2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation","children":"[논문리뷰] Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation","children":"Hao Ouyang이 arXiv에 게시한 'Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Reward-Forcing-Efficient-Streaming-Video-Generation-with-Rewarded-Distribution-Matching-Distillation"}]]}]]}],["$","article","2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance","children":"[논문리뷰] REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance","children":"Yaxin Fan이 arXiv에 게시한 'REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-REFLEX-Self-Refining-Explainable-Fact-Checking-via-Disentangling-Truth-into-Style-and-Substance"}]]}]]}],["$","article","2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory","children":"[논문리뷰] QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory","children":"Nan-Yow Chen이 arXiv에 게시한 'QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-QKAN-LSTM-Quantum-inspired-Kolmogorov-Arnold-Long-Short-term-Memory"}]]}]]}],["$","article","2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing","children":"[논문리뷰] PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing","children":"arXiv에 게시된 'PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-PaperDebugger-A-Plugin-Based-Multi-Agent-System-for-In-Editor-Academic-Writing-Review-and-Editing"}]]}]]}],["$","article","2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral","children":"[논문리뷰] On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral","children":"Christos Thrampoulidis이 arXiv에 게시한 'On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-On-GRPO-Collapse-in-Search-R1-The-Lazy-Likelihood-Displacement-Death-Spiral"}]]}]]}],["$","article","2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction","children":"[논문리뷰] Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction","children":"arXiv에 게시된 'Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Nex-N1-Agentic-Models-Trained-via-a-Unified-Ecosystem-for-Large-Scale-Environment-Construction"}]]}]]}],["$","article","2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation","children":"[논문리뷰] NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation","children":"Vitor Guizilini이 arXiv에 게시한 'NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-NeuralRemaster-Phase-Preserving-Diffusion-for-Structure-Aligned-Generation"}]]}]]}],["$","article","2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing","children":"[논문리뷰] Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing","children":"Jun Wang이 arXiv에 게시한 'Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Model-Based-and-Sample-Efficient-AI-Assisted-Math-Discovery-in-Sphere-Packing"}]]}]]}],["$","article","2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment","children":"[논문리뷰] Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment","children":"arXiv에 게시된 'Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Mitigating-Object-and-Action-Hallucinations-in-Multimodal-LLMs-via-Self-Augmented-Contrastive-Alignment"}]]}]]}],["$","article","2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates","children":"[논문리뷰] Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates","children":"Nikolaos Aletras이 arXiv에 게시한 'Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Mitigating-Catastrophic-Forgetting-in-Target-Language-Adaptation-of-LLMs-via-Source-Shielded-Updates"}]]}]]}],["$","article","2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length","children":"[논문리뷰] Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length","children":"Shifeng Zhang이 arXiv에 게시한 'Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Live-Avatar-Streaming-Real-time-Audio-Driven-Avatar-Generation-with-Infinite-Length"}]]}]]}],["$","article","2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale","children":"[논문리뷰] LATTICE: Democratize High-Fidelity 3D Generation at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale","children":"Qingxiang Lin이 arXiv에 게시한 'LATTICE: Democratize High-Fidelity 3D Generation at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-LATTICE-Democratize-High-Fidelity-3D-Generation-at-Scale"}]]}]]}],["$","article","2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior","children":"[논문리뷰] Generative Neural Video Compression via Video Diffusion Prior"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior","children":"arXiv에 게시된 'Generative Neural Video Compression via Video Diffusion Prior' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Generative-Neural-Video-Compression-via-Video-Diffusion-Prior"}]]}]]}],["$","article","2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces","children":"[논문리뷰] GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces","children":"Sezer Karaoglu이 arXiv에 게시한 'GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-GaussianBlender-Instant-Stylization-of-3D-Gaussians-with-Disentangled-Latent-Spaces"}]]}]]}],["$","article","2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring","children":"[논문리뷰] FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring","children":"Munchurl Kim이 arXiv에 게시한 'FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-FMA-Net-Motion-and-Exposure-Aware-Real-World-Joint-Video-Super-Resolution-and-Deblurring"}]]}]]}],["$","article","2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion","children":"[논문리뷰] EgoLCD: Egocentric Video Generation with Long Context Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion","children":"arXiv에 게시된 'EgoLCD: Egocentric Video Generation with Long Context Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-EgoLCD-Egocentric-Video-Generation-with-Long-Context-Diffusion"}]]}]]}],["$","article","2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling","children":"[논문리뷰] DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling","children":"arXiv에 게시된 'DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-DynamicVerse-A-Physically-Aware-Multimodal-Framework-for-4D-World-Modeling"}]]}]]}],["$","article","2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation","children":"[논문리뷰] DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation","children":"Ziyu Guo이 arXiv에 게시한 'DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-DraCo-Draft-as-CoT-for-Text-to-Image-Preview-and-Rare-Concept-Generation"}]]}]]}],["$","article","2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle","children":"[논문리뷰] DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle","children":"arXiv에 게시된 'DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-DAComp-Benchmarking-Data-Agents-across-the-Full-Data-Intelligence-Lifecycle"}]]}]]}],["$","article","2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","children":"[논문리뷰] BulletTime: Decoupled Control of Time and Camera Pose for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation","children":"Jan Ackermann이 arXiv에 게시한 'BulletTime: Decoupled Control of Time and Camera Pose for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-BulletTime-Decoupled-Control-of-Time-and-Camera-Pose-for-Video-Generation"}]]}]]}],["$","article","2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models","children":"[논문리뷰] Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models","children":"arXiv에 게시된 'Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-Aligned-but-Stereotypical-The-Hidden-Influence-of-System-Prompts-on-Social-Bias-in-LVLM-Based-Text-to-Image-Models"}]]}]]}],["$","article","2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning","children":"[논문리뷰] ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning","children":"arXiv에 게시된 'ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-ARM-Thinker-Reinforcing-Multimodal-Generative-Reward-Models-with-Agentic-Tool-Use-and-Visual-Reasoning"}]]}]]}],["$","article","2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer","children":"[논문리뷰] 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer","children":"arXiv에 게시된 '4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-05 00:00:00+0900+0900","children":"2025년 12월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-05-4DLangVGGT-4D-Language-Visual-Geometry-Grounded-Transformer"}]]}]]}],["$","article","2025-12-04-ViDiC-Video-Difference-Captioning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-ViDiC-Video-Difference-Captioning","children":"[논문리뷰] ViDiC: Video Difference Captioning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-ViDiC-Video-Difference-Captioning","children":"jiakaiW이 arXiv에 게시한 'ViDiC: Video Difference Captioning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-ViDiC-Video-Difference-Captioning"}]]}]]}],["$","article","2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs","children":"[논문리뷰] UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs","children":"arXiv에 게시된 'UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-UniQL-Unified-Quantization-and-Low-rank-Compression-for-Adaptive-Edge-LLMs"}]]}]]}],["$","article","2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images","children":"[논문리뷰] Thinking with Programming Vision: Towards a Unified View for Thinking with Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images","children":"Tao Jin이 arXiv에 게시한 'Thinking with Programming Vision: Towards a Unified View for Thinking with Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-Thinking-with-Programming-Vision-Towards-a-Unified-View-for-Thinking-with-Images"}]]}]]}],["$","article","2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach","children":"[논문리뷰] Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach","children":"Xiu Li이 arXiv에 게시한 'Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-Steering-Vision-Language-Action-Models-as-Anti-Exploration-A-Test-Time-Scaling-Approach"}]]}]]}],["$","article","2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL","children":"[논문리뷰] SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL","children":"arXiv에 게시된 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-SpaceTools-Tool-Augmented-Spatial-Reasoning-via-Double-Interactive-RL"}]]}]]}],["$","article","2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors","children":"[논문리뷰] SkillFactory: Self-Distillation For Learning Cognitive Behaviors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors","children":"Manya Wadhwa이 arXiv에 게시한 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-SkillFactory-Self-Distillation-For-Learning-Cognitive-Behaviors"}]]}]]}],["$","article","2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment","children":"[논문리뷰] SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment","children":"Yi Yang이 arXiv에 게시한 'SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-SR-GRPO-Stable-Rank-as-an-Intrinsic-Geometric-Reward-for-Large-Language-Model-Alignment"}]]}]]}],["$","article","2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","children":"[논문리뷰] RELIC: Interactive Video World Model with Long-Horizon Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory","children":"Chongjian Ge이 arXiv에 게시한 'RELIC: Interactive Video World Model with Long-Horizon Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-RELIC-Interactive-Video-World-Model-with-Long-Horizon-Memory"}]]}]]}],["$","article","2025-12-04-Qwen3-VL-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Qwen3-VL-Technical-Report","children":"[논문리뷰] Qwen3-VL Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Qwen3-VL-Technical-Report","children":"arXiv에 게시된 'Qwen3-VL Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-Qwen3-VL-Technical-Report"}]]}]]}],["$","article","2025-12-04-PretrainZero-Reinforcement-Active-Pretraining",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining","children":"[논문리뷰] PretrainZero: Reinforcement Active Pretraining"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining","children":"Guoqi Li이 arXiv에 게시한 'PretrainZero: Reinforcement Active Pretraining' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining"}]]}]]}],["$","article","2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video","children":"[논문리뷰] OneThinker: All-in-one Reasoning Model for Image and Video"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video","children":"Kaixuan Fan이 arXiv에 게시한 'OneThinker: All-in-one Reasoning Model for Image and Video' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-OneThinker-All-in-one-Reasoning-Model-for-Image-and-Video"}]]}]]}],["$","article","2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model","children":"[논문리뷰] Jina-VLM: Small Multilingual Vision Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model","children":"arXiv에 게시된 'Jina-VLM: Small Multilingual Vision Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-Jina-VLM-Small-Multilingual-Vision-Language-Model"}]]}]]}],["$","article","2025-12-04-In-Context-Representation-Hijacking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-In-Context-Representation-Hijacking","children":"[논문리뷰] In-Context Representation Hijacking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-In-Context-Representation-Hijacking","children":"yossig이 arXiv에 게시한 'In-Context Representation Hijacking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-In-Context-Representation-Hijacking"}]]}]]}],["$","article","2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment","children":"[논문리뷰] Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment","children":"arXiv에 게시된 'Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-Flowing-Backwards-Improving-Normalizing-Flows-via-Reverse-Representation-Alignment"}]]}]]}],["$","article","2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation","children":"[논문리뷰] CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation","children":"Yi Yao이 arXiv에 게시한 'CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-CookAnything-A-Framework-for-Flexible-and-Consistent-Multi-Step-Recipe-Image-Generation"}]]}]]}],["$","article","2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs","children":"[논문리뷰] AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs","children":"Tosho Hirasawa이 arXiv에 게시한 'AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-AlignBench-Benchmarking-Fine-Grained-Image-Text-Alignment-with-Synthetic-Image-Caption-Pairs"}]]}]]}],["$","article","2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models","children":"[논문리뷰] Adversarial Confusion Attack: Disrupting Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models","children":"Artur Janicki이 arXiv에 게시한 'Adversarial Confusion Attack: Disrupting Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-04 00:00:00+0900+0900","children":"2025년 12월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-04-Adversarial-Confusion-Attack-Disrupting-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation","children":"[논문리뷰] YingVideo-MV: Music-Driven Multi-Stage Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation","children":"Chaofan Ding이 arXiv에 게시한 'YingVideo-MV: Music-Driven Multi-Stage Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-YingVideo-MV-Music-Driven-Multi-Stage-Video-Generation"}]]}]]}],["$","article","2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning","children":"[논문리뷰] WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning","children":"arXiv에 게시된 'WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-WorldMM-Dynamic-Multimodal-Memory-Agent-for-Long-Video-Reasoning"}]]}]]}],["$","article","2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","children":"[논문리뷰] Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation","children":"Yu Ning이 arXiv에 게시한 'Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Video4Spatial-Towards-Visuospatial-Intelligence-with-Context-Guided-Video-Generation"}]]}]]}],["$","article","2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation","children":"[논문리뷰] ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation","children":"arXiv에 게시된 'ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-ViSAudio-End-to-End-Video-Driven-Binaural-Spatial-Audio-Generation"}]]}]]}],["$","article","2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models","children":"[논문리뷰] The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models","children":"arXiv에 게시된 'The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models"}]]}]]}],["$","article","2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition","children":"[논문리뷰] TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition","children":"Zichen Wen이 arXiv에 게시한 'TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-TRivia-Self-supervised-Fine-tuning-of-Vision-Language-Models-for-Table-Recognition"}]]}]]}],["$","article","2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead","children":"[논문리뷰] SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead","children":"arXiv에 게시된 'SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-SwiftVLA-Unlocking-Spatiotemporal-Dynamics-for-Lightweight-VLA-Models-at-Minimal-Overhead"}]]}]]}],["$","article","2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch","children":"[논문리뷰] Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch","children":"arXiv에 게시된 'Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Skywork-R1V4-Toward-Agentic-Multimodal-Intelligence-through-Interleaved-Thinking-with-Images-and-DeepResearch"}]]}]]}],["$","article","2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds","children":"[논문리뷰] SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds","children":"Xuhong He이 arXiv에 게시한 'SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-SimWorld-An-Open-ended-Realistic-Simulator-for-Autonomous-Agents-in-Physical-and-Social-Worlds"}]]}]]}],["$","article","2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale","children":"[논문리뷰] SimScale: Learning to Drive via Real-World Simulation at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale","children":"arXiv에 게시된 'SimScale: Learning to Drive via Real-World Simulation at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-SimScale-Learning-to-Drive-via-Real-World-Simulation-at-Scale"}]]}]]}],["$","article","2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization","children":"[논문리뷰] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization","children":"arXiv에 게시된 'Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Revisiting-the-Necessity-of-Lengthy-Chain-of-Thought-in-Vision-centric-Reasoning-Generalization"}]]}]]}],["$","article","2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI","children":"[논문리뷰] PAI-Bench: A Comprehensive Benchmark For Physical AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI","children":"Humphrey Shi이 arXiv에 게시한 'PAI-Bench: A Comprehensive Benchmark For Physical AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-PAI-Bench-A-Comprehensive-Benchmark-For-Physical-AI"}]]}]]}],["$","article","2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework","children":"[논문리뷰] MultiShotMaster: A Controllable Multi-Shot Video Generation Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework","children":"arXiv에 게시된 'MultiShotMaster: A Controllable Multi-Shot Video Generation Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-MultiShotMaster-A-Controllable-Multi-Shot-Video-Generation-Framework"}]]}]]}],["$","article","2025-12-03-Mixture-of-Horizons-in-Action-Chunking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Mixture-of-Horizons-in-Action-Chunking","children":"[논문리뷰] Mixture of Horizons in Action Chunking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Mixture-of-Horizons-in-Action-Chunking","children":"Zelong Sun이 arXiv에 게시한 'Mixture of Horizons in Action Chunking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Mixture-of-Horizons-in-Action-Chunking"}]]}]]}],["$","article","2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models","children":"[논문리뷰] Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models","children":"arXiv에 게시된 'Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Masks-Can-Be-Distracting-On-Context-Comprehension-in-Diffusion-Language-Models"}]]}]]}],["$","article","2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory","children":"[논문리뷰] MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory","children":"arXiv에 게시된 'MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-MG-Nav-Dual-Scale-Visual-Navigation-via-Sparse-Spatial-Memory"}]]}]]}],["$","article","2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision","children":"[논문리뷰] Guided Self-Evolving LLMs with Minimal Human Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision","children":"arXiv에 게시된 'Guided Self-Evolving LLMs with Minimal Human Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Guided-Self-Evolving-LLMs-with-Minimal-Human-Supervision"}]]}]]}],["$","article","2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample","children":"[논문리뷰] Glance: Accelerating Diffusion Models with 1 Sample"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample","children":"Linjie Li이 arXiv에 게시한 'Glance: Accelerating Diffusion Models with 1 Sample' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Glance-Accelerating-Diffusion-Models-with-1-Sample"}]]}]]}],["$","article","2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning","children":"[논문리뷰] GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning","children":"Kaijun Tan이 arXiv에 게시한 'GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-GUI-Exploration-Lab-Enhancing-Screen-Navigation-in-Agents-via-Multi-Turn-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","children":"[논문리뷰] DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation","children":"Zixin Zhang이 arXiv에 게시한 'DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-DualCamCtrl-Dual-Branch-Diffusion-Model-for-Geometry-Aware-Camera-Controlled-Video-Generation"}]]}]]}],["$","article","2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","children":"[논문리뷰] Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation","children":"arXiv에 게시된 'Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Does-Hearing-Help-Seeing-Investigating-Audio-Video-Joint-Denoising-for-Video-Generation"}]]}]]}],["$","article","2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models","children":"[논문리뷰] DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models","children":"arXiv에 게시된 'DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-DiG-Flow-Discrepancy-Guided-Flow-Matching-for-Robust-VLA-Models"}]]}]]}],["$","article","2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models","children":"[논문리뷰] DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models","children":"arXiv에 게시된 'DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models"}]]}]]}],["$","article","2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization","children":"[논문리뷰] CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization","children":"arXiv에 게시된 'CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-CodeV-Code-with-Images-for-Faithful-Visual-Reasoning-via-Tool-Aware-Policy-Optimization"}]]}]]}],["$","article","2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click","children":"[논문리뷰] Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click","children":"arXiv에 게시된 'Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Click2Graph-Interactive-Panoptic-Video-Scene-Graphs-from-a-Single-Click"}]]}]]}],["$","article","2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning","children":"[논문리뷰] CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning","children":"arXiv에 게시된 'CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-CUDA-L2-Surpassing-cuBLAS-Performance-for-Matrix-Multiplication-through-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models","children":"[논문리뷰] C^2DLM: Causal Concept-Guided Diffusion Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models","children":"Xinpeng Dong이 arXiv에 게시한 'C^2DLM: Causal Concept-Guided Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models"}]]}]]}],["$","article","2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation","children":"[논문리뷰] BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation","children":"arXiv에 게시된 'BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-BlockVid-Block-Diffusion-for-High-Quality-and-Consistent-Minute-Long-Video-Generation"}]]}]]}],["$","article","2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning","children":"[논문리뷰] Artemis: Structured Visual Reasoning for Perception Policy Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning","children":"Piotr Koniusz이 arXiv에 게시한 'Artemis: Structured Visual Reasoning for Perception Policy Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-03 00:00:00+0900+0900","children":"2025년 12월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-03-Artemis-Structured-Visual-Reasoning-for-Perception-Policy-Learning"}]]}]]}],["$","article","2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing","children":"[논문리뷰] WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing","children":"Wendong Bu이 arXiv에 게시한 'WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-WiseEdit-Benchmarking-Cognition-and-Creativity-Informed-Image-Editing"}]]}]]}],["$","article","2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models","children":"[논문리뷰] Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models","children":"Mikhail Burtsev이 arXiv에 게시한 'Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models"}]]}]]}],["$","article","2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation","children":"[논문리뷰] Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation","children":"Wenhua Wu이 arXiv에 게시한 'Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Where-Culture-Fades-Revealing-the-Cultural-Gap-in-Text-to-Image-Generation"}]]}]]}],["$","article","2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","children":"[논문리뷰] What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards","children":"arXiv에 게시된 'What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-What-about-gravity-in-video-generation-Post-Training-Newtons-Laws-with-Verifiable-Rewards"}]]}]]}],["$","article","2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference","children":"[논문리뷰] VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference","children":"arXiv에 게시된 'VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-VLASH-Real-Time-VLAs-via-Future-State-Aware-Asynchronous-Inference"}]]}]]}],["$","article","2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment","children":"[논문리뷰] The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment","children":"arXiv에 게시된 'The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-The-Consistency-Critic-Correcting-Inconsistencies-in-Generated-Images-via-Reference-Guided-Attentive-Alignment"}]]}]]}],["$","article","2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models","children":"[논문리뷰] The Art of Scaling Test-Time Compute for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models","children":"Tanmoy Chakraborty이 arXiv에 게시한 'The Art of Scaling Test-Time Compute for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-The-Art-of-Scaling-Test-Time-Compute-for-Large-Language-Models"}]]}]]}],["$","article","2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models","children":"[논문리뷰] TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models","children":"arXiv에 게시된 'TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-TUNA-Taming-Unified-Visual-Representations-for-Native-Unified-Multimodal-Models"}]]}]]}],["$","article","2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models","children":"[논문리뷰] Structured Extraction from Business Process Diagrams Using Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models","children":"Barry Devereux이 arXiv에 게시한 'Structured Extraction from Business Process Diagrams Using Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Structured-Extraction-from-Business-Process-Diagrams-Using-Vision-Language-Models"}]]}]]}],["$","article","2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos","children":"[논문리뷰] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos","children":"arXiv에 게시된 'StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-StreamGaze-Gaze-Guided-Temporal-Reasoning-and-Proactive-Understanding-in-Streaming-Videos"}]]}]]}],["$","article","2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices","children":"[논문리뷰] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices","children":"arXiv에 게시된 'Stabilizing Reinforcement Learning with LLMs: Formulation and Practices' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Stabilizing-Reinforcement-Learning-with-LLMs-Formulation-and-Practices"}]]}]]}],["$","article","2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs","children":"[논문리뷰] SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs","children":"arXiv에 게시된 'SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-SpeContext-Enabling-Efficient-Long-context-Reasoning-with-Speculative-Context-Sparsity-in-LLMs"}]]}]]}],["$","article","2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf","children":"[논문리뷰] Seeing the Wind from a Falling Leaf"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf","children":"Emily Yue-Ting Jia이 arXiv에 게시한 'Seeing the Wind from a Falling Leaf' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Seeing-the-Wind-from-a-Falling-Leaf"}]]}]]}],["$","article","2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models","children":"[논문리뷰] Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models","children":"arXiv에 게시된 'Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Script-Graph-Structured-and-Query-Conditioned-Semantic-Token-Pruning-for-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling","children":"[논문리뷰] SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling","children":"arXiv에 게시된 'SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-SCALE-Selective-Resource-Allocation-for-Overcoming-Performance-Bottlenecks-in-Mathematical-Test-time-Scaling"}]]}]]}],["$","article","2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization","children":"[논문리뷰] Rectifying LLM Thought from Lens of Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization","children":"Kai Chen이 arXiv에 게시한 'Rectifying LLM Thought from Lens of Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization"}]]}]]}],["$","article","2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models","children":"[논문리뷰] PromptBridge: Cross-Model Prompt Transfer for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models","children":"Wei Wei이 arXiv에 게시한 'PromptBridge: Cross-Model Prompt Transfer for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models"}]]}]]}],["$","article","2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic","children":"[논문리뷰] OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic","children":"arXiv에 게시된 'OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic"}]]}]]}],["$","article","2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion","children":"[논문리뷰] OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion","children":"arXiv에 게시된 'OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion"}]]}]]}],["$","article","2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model","children":"[논문리뷰] Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model","children":"Ying-Cong Chen이 arXiv에 게시한 'Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Lotus-2-Advancing-Geometric-Dense-Prediction-with-Powerful-Image-Generative-Model"}]]}]]}],["$","article","2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling","children":"[논문리뷰] LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling","children":"arXiv에 게시된 'LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling"}]]}]]}],["$","article","2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds","children":"[논문리뷰] Learning Eigenstructures of Unstructured Data Manifolds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds","children":"arXiv에 게시된 'Learning Eigenstructures of Unstructured Data Manifolds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Learning-Eigenstructures-of-Unstructured-Data-Manifolds"}]]}]]}],["$","article","2025-12-02-LFM2-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-LFM2-Technical-Report","children":"[논문리뷰] LFM2 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-LFM2-Technical-Report","children":"arXiv에 게시된 'LFM2 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-LFM2-Technical-Report"}]]}]]}],["$","article","2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision","children":"[논문리뷰] InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision","children":"arXiv에 게시된 'InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-InternVideo-Next-Towards-General-Video-Foundation-Models-without-Video-Text-Supervision"}]]}]]}],["$","article","2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout","children":"[논문리뷰] Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout","children":"Pinar Yanardag이 arXiv에 게시한 'Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Infinity-RoPE-Action-Controllable-Infinite-Video-Generation-Emerges-From-Autoregressive-Self-Rollout"}]]}]]}],["$","article","2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages","children":"[논문리뷰] IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages","children":"arXiv에 게시된 'IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-IndicParam-Benchmark-to-evaluate-LLMs-on-low-resource-Indic-Languages"}]]}]]}],["$","article","2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents","children":"[논문리뷰] How Far Are We from Genuinely Useful Deep Research Agents?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents","children":"Xinran Zhou이 arXiv에 게시한 'How Far Are We from Genuinely Useful Deep Research Agents?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-How-Far-Are-We-from-Genuinely-Useful-Deep-Research-Agents"}]]}]]}],["$","article","2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents","children":"[논문리뷰] HiconAgent: History Context-aware Policy Optimization for GUI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents","children":"Kaiwen Zhou이 arXiv에 게시한 'HiconAgent: History Context-aware Policy Optimization for GUI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-HiconAgent-History-Context-aware-Policy-Optimization-for-GUI-Agents"}]]}]]}],["$","article","2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks","children":"[논문리뷰] Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks","children":"arXiv에 게시된 'Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks"}]]}]]}],["$","article","2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation","children":"[논문리뷰] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation","children":"arXiv에 게시된 'GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-GR-RL-Going-Dexterous-and-Precise-for-Long-Horizon-Robotic-Manipulation"}]]}]]}],["$","article","2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence","children":"[논문리뷰] From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence","children":"arXiv에 게시된 'From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-From-Code-Foundation-Models-to-Agents-and-Applications-A-Practical-Guide-to-Code-Intelligence"}]]}]]}],["$","article","2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning","children":"[논문리뷰] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning","children":"arXiv에 게시된 'Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Flash-DMD-Towards-High-Fidelity-Few-Step-Image-Generation-with-Efficient-Distillation-and-Joint-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights","children":"[논문리뷰] Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights","children":"arXiv에 게시된 'Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Envision-Benchmarking-Unified-Understanding-Generation-for-Causal-World-Process-Insights"}]]}]]}],["$","article","2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation","children":"[논문리뷰] Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation","children":"MElHuseyni이 arXiv에 게시한 'Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Doppler-Enhanced-Deep-Learning-Improving-Thyroid-Nodule-Segmentation-with-YOLOv5-Instance-Segmentation"}]]}]]}],["$","article","2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images","children":"[논문리뷰] Asking like Socrates: Socrates helps VLMs understand remote sensing images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images","children":"Xinran He이 arXiv에 게시한 'Asking like Socrates: Socrates helps VLMs understand remote sensing images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Asking-like-Socrates-Socrates-helps-VLMs-understand-remote-sensing-images"}]]}]]}],["$","article","2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution","children":"[논문리뷰] Agentic Policy Optimization via Instruction-Policy Co-Evolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution","children":"arXiv에 게시된 'Agentic Policy Optimization via Instruction-Policy Co-Evolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution"}]]}]]}],["$","article","2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression","children":"[논문리뷰] Accelerating Streaming Video Large Language Models via Hierarchical Token Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression","children":"arXiv에 게시된 'Accelerating Streaming Video Large Language Models via Hierarchical Token Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Accelerating-Streaming-Video-Large-Language-Models-via-Hierarchical-Token-Compression"}]]}]]}],["$","article","2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer","children":"[논문리뷰] Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer","children":"arXiv에 게시된 'Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Z-Image-An-Efficient-Image-Generation-Foundation-Model-with-Single-Stream-Diffusion-Transformer"}]]}]]}],["$","article","2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection","children":"[논문리뷰] YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection","children":"Avishai Weizman이 arXiv에 게시한 'YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-YOLO-Meets-Mixture-of-Experts-Adaptive-Expert-Routing-for-Robust-Object-Detection"}]]}]]}],["$","article","2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM","children":"[논문리뷰] Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM","children":"arXiv에 게시된 'Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Xmodel-2-5-1-3B-Data-Efficient-Reasoning-SLM"}]]}]]}],["$","article","2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models","children":"[논문리뷰] World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models","children":"Na Min An이 arXiv에 게시한 'World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-World-in-a-Frame-Understanding-Culture-Mixing-as-a-New-Challenge-for-Vision-Language-Models"}]]}]]}],["$","article","2025-12-01-Vision-Bridge-Transformer-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale","children":"[논문리뷰] Vision Bridge Transformer at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale","children":"Xinchao Wang이 arXiv에 게시한 'Vision Bridge Transformer at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Vision-Bridge-Transformer-at-Scale"}]]}]]}],["$","article","2025-12-01-The-Collapse-of-Patches",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-The-Collapse-of-Patches","children":"[논문리뷰] The Collapse of Patches"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-The-Collapse-of-Patches","children":"Weidong Cai이 arXiv에 게시한 'The Collapse of Patches' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-The-Collapse-of-Patches"}]]}]]}],["$","article","2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps","children":"[논문리뷰] Test-time scaling of diffusions with flow maps"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps","children":"Sanja Fidler이 arXiv에 게시한 'Test-time scaling of diffusions with flow maps' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Test-time-scaling-of-diffusions-with-flow-maps"}]]}]]}],["$","article","2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs","children":"[논문리뷰] SO-Bench: A Structural Output Evaluation of Multimodal LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs","children":"arXiv에 게시된 'SO-Bench: A Structural Output Evaluation of Multimodal LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-SO-Bench-A-Structural-Output-Evaluation-of-Multimodal-LLMs"}]]}]]}],["$","article","2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists","children":"[논문리뷰] RefineBench: Evaluating Refinement Capability of Language Models via Checklists"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists","children":"arXiv에 게시된 'RefineBench: Evaluating Refinement Capability of Language Models via Checklists' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-RefineBench-Evaluating-Refinement-Capability-of-Language-Models-via-Checklists"}]]}]]}],["$","article","2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models","children":"[논문리뷰] Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models","children":"Yehudit Aperstein이 arXiv에 게시한 'Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Recognition-of-Abnormal-Events-in-Surveillance-Videos-using-Weakly-Supervised-Dual-Encoder-Models"}]]}]]}],["$","article","2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models","children":"[논문리뷰] REASONEDIT: Towards Reasoning-Enhanced Image Editing Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models","children":"arXiv에 게시된 'REASONEDIT: Towards Reasoning-Enhanced Image Editing Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-REASONEDIT-Towards-Reasoning-Enhanced-Image-Editing-Models"}]]}]]}],["$","article","2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model","children":"[논문리뷰] OralGPT-Omni: A Versatile Dental Multimodal Large Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model","children":"arXiv에 게시된 'OralGPT-Omni: A Versatile Dental Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-OralGPT-Omni-A-Versatile-Dental-Multimodal-Large-Language-Model"}]]}]]}],["$","article","2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement","children":"[논문리뷰] OmniRefiner: Reinforcement-Guided Local Diffusion Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement","children":"Yiren Song이 arXiv에 게시한 'OmniRefiner: Reinforcement-Guided Local Diffusion Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-OmniRefiner-Reinforcement-Guided-Local-Diffusion-Refinement"}]]}]]}],["$","article","2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models","children":"[논문리뷰] Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models","children":"arXiv에 게시된 'Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Nemotron-Flash-Towards-Latency-Optimal-Hybrid-Small-Language-Models"}]]}]]}],["$","article","2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey","children":"[논문리뷰] MRI Super-Resolution with Deep Learning: A Comprehensive Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey","children":"arXiv에 게시된 'MRI Super-Resolution with Deep Learning: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-MRI-Super-Resolution-with-Deep-Learning-A-Comprehensive-Survey"}]]}]]}],["$","article","2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge","children":"[논문리뷰] Layer-Aware Video Composition via Split-then-Merge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge","children":"Wen-Sheng Chu이 arXiv에 게시한 'Layer-Aware Video Composition via Split-then-Merge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Layer-Aware-Video-Composition-via-Split-then-Merge"}]]}]]}],["$","article","2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning","children":"[논문리뷰] Geometrically-Constrained Agent for Spatial Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning","children":"Lehan He이 arXiv에 게시한 'Geometrically-Constrained Agent for Spatial Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Geometrically-Constrained-Agent-for-Spatial-Reasoning"}]]}]]}],["$","article","2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images","children":"[논문리뷰] From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images","children":"Filippos Kokkinos이 arXiv에 게시한 'From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-From-Pixels-to-Feelings-Aligning-MLLMs-with-Human-Cognitive-Perception-of-Images"}]]}]]}],["$","article","2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information","children":"[논문리뷰] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information","children":"Kristian Kersting이 arXiv에 게시한 'Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information"}]]}]]}],["$","article","2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets","children":"[논문리뷰] Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets","children":"Avishai Weizman이 arXiv에 게시한 'Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Find-the-Leak-Fix-the-Split-Cluster-Based-Method-to-Prevent-Leakage-in-Video-Derived-Datasets"}]]}]]}],["$","article","2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning","children":"[논문리뷰] FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning","children":"Simin Chen이 arXiv에 게시한 'FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-FedRE-A-Representation-Entanglement-Framework-for-Model-Heterogeneous-Federated-Learning"}]]}]]}],["$","article","2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration","children":"[논문리뷰] Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration","children":"arXiv에 게시된 'Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Fast3Dcache-Training-free-3D-Geometry-Synthesis-Acceleration"}]]}]]}],["$","article","2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models","children":"[논문리뷰] Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models","children":"Wei Wu이 arXiv에 게시한 'Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models"}]]}]]}],["$","article","2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action","children":"[논문리뷰] DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action","children":"Zhuoyang Liu이 arXiv에 게시한 'DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-DualVLA-Building-a-Generalizable-Embodied-Agent-via-Partial-Decoupling-of-Reasoning-and-Action"}]]}]]}],["$","article","2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space","children":"[논문리뷰] DiP: Taming Diffusion Models in Pixel Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space","children":"Xu Chen이 arXiv에 게시한 'DiP: Taming Diffusion Models in Pixel Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-DiP-Taming-Diffusion-Models-in-Pixel-Space"}]]}]]}],["$","article","2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning","children":"[논문리뷰] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning","children":"arXiv에 게시된 'DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-DeepSeekMath-V2-Towards-Self-Verifiable-Mathematical-Reasoning"}]]}]]}],["$","article","2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield","children":"[논문리뷰] Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield","children":"arXiv에 게시된 'Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Decoupled-DMD-CFG-Augmentation-as-the-Spear-Distribution-Matching-as-the-Shield"}]]}]]}],["$","article","2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself","children":"[논문리뷰] CaptionQA: Is Your Caption as Useful as the Image Itself?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself","children":"Zicheng Liu이 arXiv에 게시한 'CaptionQA: Is Your Caption as Useful as the Image Itself?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-CaptionQA-Is-Your-Caption-as-Useful-as-the-Image-Itself"}]]}]]}],["$","article","2025-12-01-Captain-Safari-A-World-Engine",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine","children":"[논문리뷰] Captain Safari: A World Engine"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine","children":"Yitong Li이 arXiv에 게시한 'Captain Safari: A World Engine' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Captain-Safari-A-World-Engine"}]]}]]}],["$","article","2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model","children":"[논문리뷰] Architecture Decoupling Is Not All You Need For Unified Multimodal Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model","children":"Hongyu Li이 arXiv에 게시한 'Architecture Decoupling Is Not All You Need For Unified Multimodal Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Architecture-Decoupling-Is-Not-All-You-Need-For-Unified-Multimodal-Model"}]]}]]}],["$","article","2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement","children":"[논문리뷰] AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement","children":"Yicheng Ji이 arXiv에 게시한 'AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-AnyTalker-Scaling-Multi-Person-Talking-Video-Generation-with-Interactivity-Refinement"}]]}]]}],["$","article","2025-12-01-Adversarial-Flow-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Adversarial-Flow-Models","children":"[논문리뷰] Adversarial Flow Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Adversarial-Flow-Models","children":"arXiv에 게시된 'Adversarial Flow Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Adversarial-Flow-Models"}]]}]]}],["$","article","2025-11-28-What-does-it-mean-to-understand-language",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-What-does-it-mean-to-understand-language","children":"[논문리뷰] What does it mean to understand language?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-What-does-it-mean-to-understand-language","children":"arXiv에 게시된 'What does it mean to understand language?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-What-does-it-mean-to-understand-language"}]]}]]}],["$","article","2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","children":"[논문리뷰] Video Generation Models Are Good Latent Reward Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models","children":"arXiv에 게시된 'Video Generation Models Are Good Latent Reward Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-Video-Generation-Models-Are-Good-Latent-Reward-Models"}]]}]]}],["$","article","2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following","children":"[논문리뷰] Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following","children":"arXiv에 게시된 'Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-Multi-Crit-Benchmarking-Multimodal-Judges-on-Pluralistic-Criteria-Following"}]]}]]}],["$","article","2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing","children":"[논문리뷰] MIRA: Multimodal Iterative Reasoning Agent for Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing","children":"Jiebo Luo이 arXiv에 게시한 'MIRA: Multimodal Iterative Reasoning Agent for Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-MIRA-Multimodal-Iterative-Reasoning-Agent-for-Image-Editing"}]]}]]}],["$","article","2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls","children":"[논문리뷰] Canvas-to-Image: Compositional Image Generation with Multimodal Controls"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls","children":"Kfir Aberman이 arXiv에 게시한 'Canvas-to-Image: Compositional Image Generation with Multimodal Controls' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-Canvas-to-Image-Compositional-Image-Generation-with-Multimodal-Controls"}]]}]]}],["$","article","2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory","children":"[논문리뷰] Agentic Learner with Grow-and-Refine Multimodal Semantic Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory","children":"Qunyi Xie이 arXiv에 게시한 'Agentic Learner with Grow-and-Refine Multimodal Semantic Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-28 00:00:00+0900+0900","children":"2025년 11월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-28-Agentic-Learner-with-Grow-and-Refine-Multimodal-Semantic-Memory"}]]}]]}],["$","article","2025-11-27-Terminal-Velocity-Matching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Terminal-Velocity-Matching","children":"[논문리뷰] Terminal Velocity Matching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Terminal-Velocity-Matching","children":"Jiaming Song이 arXiv에 게시한 'Terminal Velocity Matching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Terminal-Velocity-Matching"}]]}]]}],["$","article","2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning","children":"[논문리뷰] SPHINX: A Synthetic Environment for Visual Perception and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning","children":"Nidhi Rastogi이 arXiv에 게시한 'SPHINX: A Synthetic Environment for Visual Perception and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-SPHINX-A-Synthetic-Environment-for-Visual-Perception-and-Reasoning"}]]}]]}],["$","article","2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy","children":"[논문리뷰] Revisiting Generalization Across Difficulty Levels: It's Not So Easy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy","children":"arXiv에 게시된 'Revisiting Generalization Across Difficulty Levels: It's Not So Easy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Revisiting-Generalization-Across-Difficulty-Levels-Its-Not-So-Easy"}]]}]]}],["$","article","2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale","children":"[논문리뷰] RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale","children":"Yangcheng Yu이 arXiv에 게시한 'RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-RAISECity-A-Multimodal-Agent-Framework-for-Reality-Aligned-3D-World-Generation-at-City-Scale"}]]}]]}],["$","article","2025-11-27-NVIDIA-Nemotron-Parse-1-1",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-NVIDIA-Nemotron-Parse-1-1","children":"[논문리뷰] NVIDIA Nemotron Parse 1.1"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-NVIDIA-Nemotron-Parse-1-1","children":"arXiv에 게시된 'NVIDIA Nemotron Parse 1.1' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-NVIDIA-Nemotron-Parse-1-1"}]]}]]}],["$","article","2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language","children":"[논문리뷰] Monet: Reasoning in Latent Visual Space Beyond Images and Language"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language","children":"Pengfei Wan이 arXiv에 게시한 'Monet: Reasoning in Latent Visual Space Beyond Images and Language' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Monet-Reasoning-in-Latent-Visual-Space-Beyond-Images-and-Language"}]]}]]}],["$","article","2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots","children":"[논문리뷰] MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots","children":"Rui Yang이 arXiv에 게시한 'MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-MobileVLA-R1-Reinforcing-Vision-Language-Action-for-Mobile-Robots"}]]}]]}],["$","article","2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems","children":"[논문리뷰] Latent Collaboration in Multi-Agent Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems","children":"arXiv에 게시된 'Latent Collaboration in Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems"}]]}]]}],["$","article","2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation","children":"[논문리뷰] Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation","children":"Jiahao He이 arXiv에 게시한 'Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Inferix-A-Block-Diffusion-based-Next-Generation-Inference-Engine-for-World-Simulation"}]]}]]}],["$","article","2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs","children":"[논문리뷰] Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs","children":"Xin Yang이 arXiv에 게시한 'Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Image-Free-Timestep-Distillation-via-Continuous-Time-Consistency-with-Trajectory-Sampled-Pairs"}]]}]]}],["$","article","2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation","children":"[논문리뷰] I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation","children":"arXiv에 게시된 'I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-I-GLIDE-Input-Groups-for-Latent-Health-Indicators-in-Degradation-Estimation"}]]}]]}],["$","article","2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy","children":"[논문리뷰] Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy","children":"arXiv에 게시된 'Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Harmony-Harmonizing-Audio-and-Video-Generation-through-Cross-Task-Synergy"}]]}]]}],["$","article","2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization","children":"[논문리뷰] Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization","children":"Youngjung Uh이 arXiv에 게시한 'Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Frequency-Adaptive-Sharpness-Regularization-for-Improving-3D-Gaussian-Splatting-Generalization"}]]}]]}],["$","article","2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","children":"[논문리뷰] Block Cascading: Training Free Acceleration of Block-Causal Video Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models","children":"arXiv에 게시된 'Block Cascading: Training Free Acceleration of Block-Causal Video Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-27 00:00:00+0900+0900","children":"2025년 11월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-27-Block-Cascading-Training-Free-Acceleration-of-Block-Causal-Video-Models"}]]}]]}],["$","article","2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation","children":"[논문리뷰] iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation","children":"arXiv에 게시된 'iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation"}]]}]]}],["$","article","2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion","children":"[논문리뷰] Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion","children":"Zhifei Yang이 arXiv에 게시한 'Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion"}]]}]]}],["$","article","2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering","children":"[논문리뷰] VQ-VA World: Towards High-Quality Visual Question-Visual Answering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering","children":"Feng Li이 arXiv에 게시한 'VQ-VA World: Towards High-Quality Visual Question-Visual Answering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering"}]]}]]}],["$","article","2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields","children":"[논문리뷰] Unified all-atom molecule generation with neural fields"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields","children":"arXiv에 게시된 'Unified all-atom molecule generation with neural fields' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields"}]]}]]}],["$","article","2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers","children":"[논문리뷰] UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers","children":"arXiv에 게시된 'UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers"}]]}]]}],["$","article","2025-11-26-Soft-Adaptive-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization","children":"[논문리뷰] Soft Adaptive Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization","children":"arXiv에 게시된 'Soft Adaptive Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization"}]]}]]}],["$","article","2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System","children":"[논문리뷰] SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System","children":"arXiv에 게시된 'SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System"}]]}]]}],["$","article","2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs","children":"[논문리뷰] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs","children":"arXiv에 게시된 'Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs"}]]}]]}],["$","article","2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space","children":"[논문리뷰] SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space","children":"Yulan He이 arXiv에 게시한 'SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space"}]]}]]}],["$","article","2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding","children":"[논문리뷰] ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding","children":"arXiv에 게시된 'ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding"}]]}]]}],["$","article","2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","children":"[논문리뷰] PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding","children":"Hongzhi Zhang이 arXiv에 게시한 'PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding"}]]}]]}],["$","article","2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation","children":"[논문리뷰] OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation","children":"arXiv에 게시된 'OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation"}]]}]]}],["$","article","2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts","children":"[논문리뷰] MedSAM3: Delving into Segment Anything with Medical Concepts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts","children":"Yi Lu이 arXiv에 게시한 'MedSAM3: Delving into Segment Anything with Medical Concepts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts"}]]}]]}],["$","article","2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts","children":"[논문리뷰] MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts","children":"arXiv에 게시된 'MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts"}]]}]]}],["$","article","2025-11-26-HunyuanOCR-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-HunyuanOCR-Technical-Report","children":"[논문리뷰] HunyuanOCR Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-HunyuanOCR-Technical-Report","children":"arXiv에 게시된 'HunyuanOCR Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-HunyuanOCR-Technical-Report"}]]}]]}],["$","article","2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI","children":"[논문리뷰] GigaWorld-0: World Models as Data Engine to Empower Embodied AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI","children":"Chaojun Ni이 arXiv에 게시한 'GigaWorld-0: World Models as Data Engine to Empower Embodied AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI"}]]}]]}],["$","article","2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms","children":"[논문리뷰] GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms","children":"arXiv에 게시된 'GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms"}]]}]]}],["$","article","2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use","children":"[논문리뷰] Fara-7B: An Efficient Agentic Model for Computer Use"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use","children":"arXiv에 게시된 'Fara-7B: An Efficient Agentic Model for Computer Use' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use"}]]}]]}],["$","article","2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward","children":"[논문리뷰] Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward","children":"arXiv에 게시된 'Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward"}]]}]]}],["$","article","2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection","children":"[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection","children":"Mike Zheng Shou이 arXiv에 게시한 'DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection"}]]}]]}],["$","article","2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning","children":"[논문리뷰] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning","children":"arXiv에 게시된 'Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-Agent0-VL-Exploring-Self-Evolving-Agent-for-Tool-Integrated-Vision-Language-Reasoning"}]]}]]}],["$","article","2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios","children":"[논문리뷰] UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios","children":"arXiv에 게시된 'UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios"}]]}]]}],["$","article","2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets","children":"[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets","children":"Zhaowei Lu이 arXiv에 게시한 'Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets"}]]}]]}],["$","article","2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis","children":"[논문리뷰] SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis","children":"Hongwen Zhang이 arXiv에 게시한 'SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis"}]]}]]}],["$","article","2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning","children":"[논문리뷰] Plan-X: Instruct Video Generation via Semantic Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning","children":"Chenxu Zhang이 arXiv에 게시한 'Plan-X: Instruct Video Generation via Semantic Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning"}]]}]]}],["$","article","2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models","children":"[논문리뷰] Pillar-0: A New Frontier for Radiology Foundation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models","children":"arXiv에 게시된 'Pillar-0: A New Frontier for Radiology Foundation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models"}]]}]]}],["$","article","2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking","children":"[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking","children":"Elias Stengel-Eskin이 arXiv에 게시한 'PRInTS: Reward Modeling for Long-Horizon Information Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking"}]]}]]}],["$","article","2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO","children":"[논문리뷰] Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO","children":"arXiv에 게시된 'Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO"}]]}]]}],["$","article","2025-11-25-MIST-Mutual-Information-Via-Supervised-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training","children":"[논문리뷰] MIST: Mutual Information Via Supervised Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training","children":"Kyunghyun Cho이 arXiv에 게시한 'MIST: Mutual Information Via Supervised Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training"}]]}]]}],["$","article","2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models","children":"[논문리뷰] MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models","children":"arXiv에 게시된 'MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models"}]]}]]}],["$","article","2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark","children":"[논문리뷰] M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark","children":"Bangwei Guo이 arXiv에 게시한 'M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark"}]]}]]}],["$","article","2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control","children":"[논문리뷰] In-Video Instructions: Visual Signals as Generative Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control","children":"arXiv에 게시된 'In-Video Instructions: Visual Signals as Generative Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control"}]]}]]}],["$","article","2025-11-25-HunyuanVideo-1-5-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report","children":"[논문리뷰] HunyuanVideo 1.5 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report","children":"Fang Yang이 arXiv에 게시한 'HunyuanVideo 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report"}]]}]]}],["$","article","2025-11-25-General-Agentic-Memory-Via-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research","children":"[논문리뷰] General Agentic Memory Via Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research","children":"arXiv에 게시된 'General Agentic Memory Via Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research"}]]}]]}],["$","article","2025-11-25-Flow-Map-Distillation-Without-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data","children":"[논문리뷰] Flow Map Distillation Without Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data","children":"Tommi Jaakkola이 arXiv에 게시한 'Flow Map Distillation Without Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data"}]]}]]}],["$","article","2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration","children":"[논문리뷰] Fidelity-Aware Recommendation Explanations via Stochastic Path Integration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration","children":"Oren Barkan이 arXiv에 게시한 'Fidelity-Aware Recommendation Explanations via Stochastic Path Integration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration"}]]}]]}],["$","article","2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems","children":"[논문리뷰] Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems","children":"Oren Barkan이 arXiv에 게시한 'Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems"}]]}]]}],["$","article","2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation","children":"[논문리뷰] DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation","children":"arXiv에 게시된 'DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation"}]]}]]}],["$","article","2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research","children":"[논문리뷰] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research","children":"arXiv에 게시된 'DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research"}]]}]]}],["$","article","2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation","children":"[논문리뷰] Controllable Layer Decomposition for Reversible Multi-Layer Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation","children":"arXiv에 게시된 'Controllable Layer Decomposition for Reversible Multi-Layer Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation"}]]}]]}],["$","article","2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface","children":"[논문리뷰] Computer-Use Agents as Judges for Generative User Interface"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface","children":"arXiv에 게시된 'Computer-Use Agents as Judges for Generative User Interface' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface"}]]}]]}],["$","article","2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens","children":"[논문리뷰] Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens","children":"Stephanie Fu이 arXiv에 게시한 'Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Chain-of-Visual-Thought-Teaching-VLMs-to-See-and-Think-Better-with-Continuous-Visual-Tokens"}]]}]]}],["$","article","2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling","children":"[논문리뷰] Budget-Aware Tool-Use Enables Effective Agent Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling","children":"arXiv에 게시된 'Budget-Aware Tool-Use Enables Effective Agent Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-Budget-Aware-Tool-Use-Enables-Effective-Agent-Scaling"}]]}]]}],["$","article","2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning","children":"[논문리뷰] AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning","children":"Alphamasterliu이 arXiv에 게시한 'AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-AutoEnv-Automated-Environments-for-Measuring-Cross-Environment-Agent-Learning"}]]}]]}],["$","article","2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser","children":"[논문리뷰] AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser","children":"arXiv에 게시된 'AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-25 00:00:00+0900+0900","children":"2025년 11월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser"}]]}]]}],["$","article","2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds","children":"[논문리뷰] WorldGen: From Text to Traversable and Interactive 3D Worlds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds","children":"arXiv에 게시된 'WorldGen: From Text to Traversable and Interactive 3D Worlds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-WorldGen-From-Text-to-Traversable-and-Interactive-3D-Worlds"}]]}]]}],["$","article","2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models","children":"[논문리뷰] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models","children":"Yudong Zhang이 arXiv에 게시한 'VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-VisMem-Latent-Vision-Memory-Unlocks-Potential-of-Vision-Language-Models"}]]}]]}],["$","article","2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination","children":"[논문리뷰] Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination","children":"Jing Bi이 arXiv에 게시한 'Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Video-R4-Reinforcing-Text-Rich-Video-Reasoning-with-Visual-Rumination"}]]}]]}],["$","article","2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation","children":"[논문리뷰] VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation","children":"Gim Hee Lee이 arXiv에 게시한 'VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-VLA-4D-Embedding-4D-Awareness-into-Vision-Language-Action-Models-for-SpatioTemporally-Coherent-Robotic-Manipulation"}]]}]]}],["$","article","2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story","children":"[논문리뷰] Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story","children":"Kristian Kuznetsov이 arXiv에 게시한 'Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Unveiling-Intrinsic-Dimension-of-Texts-from-Academic-Abstract-to-Creative-Story"}]]}]]}],["$","article","2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection","children":"[논문리뷰] Taming Generative Synthetic Data for X-ray Prohibited Item Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection","children":"Renshuai Tao이 arXiv에 게시한 'Taming Generative Synthetic Data for X-ray Prohibited Item Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection"}]]}]]}],["$","article","2025-11-24-SAM-3-Segment-Anything-with-Concepts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-SAM-3-Segment-Anything-with-Concepts","children":"[논문리뷰] SAM 3: Segment Anything with Concepts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-SAM-3-Segment-Anything-with-Concepts","children":"arXiv에 게시된 'SAM 3: Segment Anything with Concepts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-SAM-3-Segment-Anything-with-Concepts"}]]}]]}],["$","article","2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model","children":"[논문리뷰] RynnVLA-002: A Unified Vision-Language-Action and World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model","children":"arXiv에 게시된 'RynnVLA-002: A Unified Vision-Language-Action and World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-RynnVLA-002-A-Unified-Vision-Language-Action-and-World-Model"}]]}]]}],["$","article","2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations","children":"[논문리뷰] Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations","children":"Noam Koenigstein이 arXiv에 게시한 'Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Rethinking-Saliency-Maps-A-Cognitive-Human-Aligned-Taxonomy-and-Evaluation-Framework-for-Explanations"}]]}]]}],["$","article","2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","children":"[논문리뷰] Planning with Sketch-Guided Verification for Physics-Aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation","children":"Shayegan Omidshafiei이 arXiv에 게시한 'Planning with Sketch-Guided Verification for Physics-Aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Planning-with-Sketch-Guided-Verification-for-Physics-Aware-Video-Generation"}]]}]]}],["$","article","2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs","children":"[논문리뷰] Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs","children":"arXiv에 게시된 'Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Parrot-Persuasion-and-Agreement-Robustness-Rating-of-Output-Truth-A-Sycophancy-Robustness-Benchmark-for-LLMs"}]]}]]}],["$","article","2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe","children":"[논문리뷰] OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe","children":"arXiv에 게시된 'OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-OpenMMReasoner-Pushing-the-Frontiers-for-Multimodal-Reasoning-with-an-Open-and-General-Recipe"}]]}]]}],["$","article","2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists","children":"[논문리뷰] OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists","children":"Weiquan Lin이 arXiv에 게시한 'OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-OmniScientist-Toward-a-Co-evolving-Ecosystem-of-Human-and-AI-Scientists"}]]}]]}],["$","article","2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents","children":"[논문리뷰] O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents","children":"arXiv에 게시된 'O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-O-Mem-Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents"}]]}]]}],["$","article","2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models","children":"[논문리뷰] Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models","children":"arXiv에 게시된 'Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Multi-Faceted-Attack-Exposing-Cross-Model-Vulnerabilities-in-Defense-Equipped-Vision-Language-Models"}]]}]]}],["$","article","2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging","children":"[논문리뷰] MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging","children":"arXiv에 게시된 'MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-MergeDNA-Context-aware-Genome-Modeling-with-Dynamic-Tokenization-through-Token-Merging"}]]}]]}],["$","article","2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight","children":"[논문리뷰] Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight","children":"arXiv에 게시된 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Mantis-A-Versatile-Vision-Language-Action-Model-with-Disentangled-Visual-Foresight"}]]}]]}],["$","article","2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process","children":"[논문리뷰] Loomis Painter: Reconstructing the Painting Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process","children":"arXiv에 게시된 'Loomis Painter: Reconstructing the Painting Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Loomis-Painter-Reconstructing-the-Painting-Process"}]]}]]}],["$","article","2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process","children":"[논문리뷰] Insights from the ICLR Peer Review and Rebuttal Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process","children":"Nedjma Ousidhoum이 arXiv에 게시한 'Insights from the ICLR Peer Review and Rebuttal Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Insights-from-the-ICLR-Peer-Review-and-Rebuttal-Process"}]]}]]}],["$","article","2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization","children":"[논문리뷰] GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization","children":"arXiv에 게시된 'GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-GeoVista-Web-Augmented-Agentic-Visual-Reasoning-for-Geolocalization"}]]}]]}],["$","article","2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models","children":"[논문리뷰] Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models","children":"Serena Yeung-Levy이 arXiv에 게시한 'Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Downscaling-Intelligence-Exploring-Perception-and-Reasoning-Bottlenecks-in-Small-Multimodal-Models"}]]}]]}],["$","article","2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models","children":"[논문리뷰] Diversity Has Always Been There in Your Visual Autoregressive Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models","children":"Yaxing Wang이 arXiv에 게시한 'Diversity Has Always Been There in Your Visual Autoregressive Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-24 00:00:00+0900+0900","children":"2025년 11월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-24-Diversity-Has-Always-Been-There-in-Your-Visual-Autoregressive-Models"}]]}]]}],["$","article","2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO","children":"[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO","children":"arXiv에 게시된 'Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Video-as-Answer-Predict-and-Generate-Next-Video-Event-with-Joint-GRPO"}]]}]]}],["$","article","2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models","children":"[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models","children":"Baijiong Lin이 arXiv에 게시한 'V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-V-ReasonBench-Toward-Unified-Reasoning-Benchmark-Suite-for-Video-Generation-Models"}]]}]]}],["$","article","2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval","children":"[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval","children":"arXiv에 게시된 'TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-TurkColBERT-A-Benchmark-of-Dense-and-Late-Interaction-Models-for-Turkish-Information-Retrieval"}]]}]]}],["$","article","2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding","children":"[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding","children":"arXiv에 게시된 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-TimeViper-A-Hybrid-Mamba-Transformer-Vision-Language-Model-for-Efficient-Long-Video-Understanding"}]]}]]}],["$","article","2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation","children":"[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation","children":"Xinyan Chen이 arXiv에 게시한 'Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Thinking-while-Generating-Interleaving-Textual-Reasoning-throughout-Visual-Generation"}]]}]]}],["$","article","2025-11-21-Step-Audio-R1-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report","children":"[논문리뷰] Step-Audio-R1 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report","children":"arXiv에 게시된 'Step-Audio-R1 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report"}]]}]]}],["$","article","2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models","children":"[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models","children":"arXiv에 게시된 'Scaling Spatial Intelligence with Multimodal Foundation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Scaling-Spatial-Intelligence-with-Multimodal-Foundation-Models"}]]}]]}],["$","article","2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models","children":"[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models","children":"arXiv에 게시된 'SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking","children":"[논문리뷰] SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking","children":"arXiv에 게시된 'SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-SAM2S-Segment-Anything-in-Surgical-Videos-via-Semantic-Long-term-Tracking"}]]}]]}],["$","article","2025-11-21-SAM-3D-3Dfy-Anything-in-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-SAM-3D-3Dfy-Anything-in-Images","children":"[논문리뷰] SAM 3D: 3Dfy Anything in Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-SAM-3D-3Dfy-Anything-in-Images","children":"arXiv에 게시된 'SAM 3D: 3Dfy Anything in Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-SAM-3D-3Dfy-Anything-in-Images"}]]}]]}],["$","article","2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes","children":"[논문리뷰] PartUV: Part-Based UV Unwrapping of 3D Meshes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes","children":"Hao Su이 arXiv에 게시한 'PartUV: Part-Based UV Unwrapping of 3D Meshes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-PartUV-Part-Based-UV-Unwrapping-of-3D-Meshes"}]]}]]}],["$","article","2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs","children":"[논문리뷰] Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs","children":"arXiv에 게시된 'Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Nemotron-Elastic-Towards-Efficient-Many-in-One-Reasoning-LLMs"}]]}]]}],["$","article","2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion","children":"[논문리뷰] NaTex: Seamless Texture Generation as Latent Color Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion","children":"arXiv에 게시된 'NaTex: Seamless Texture Generation as Latent Color Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-NaTex-Seamless-Texture-Generation-as-Latent-Color-Diffusion"}]]}]]}],["$","article","2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report","children":"[논문리뷰] MiMo-Embodied: X-Embodied Foundation Model Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report","children":"arXiv에 게시된 'MiMo-Embodied: X-Embodied Foundation Model Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-MiMo-Embodied-X-Embodied-Foundation-Model-Technical-Report"}]]}]]}],["$","article","2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization","children":"[논문리뷰] First Frame Is the Place to Go for Video Content Customization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization","children":"arXiv에 게시된 'First Frame Is the Place to Go for Video Content Customization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-First-Frame-Is-the-Place-to-Go-for-Video-Content-Customization"}]]}]]}],["$","article","2025-11-21-Draft-and-Refine-with-Visual-Experts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Draft-and-Refine-with-Visual-Experts","children":"[논문리뷰] Draft and Refine with Visual Experts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Draft-and-Refine-with-Visual-Experts","children":"arXiv에 게시된 'Draft and Refine with Visual Experts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Draft-and-Refine-with-Visual-Experts"}]]}]]}],["$","article","2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity","children":"[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity","children":"arXiv에 게시된 'What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-What-Does-It-Take-to-Be-a-Good-AI-Research-Agent-Studying-the-Role-of-Ideation-Diversity"}]]}]]}],["$","article","2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images","children":"[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images","children":"arXiv에 게시된 'VisPlay: Self-Evolving Vision-Language Models from Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-VisPlay-Self-Evolving-Vision-Language-Models-from-Images"}]]}]]}],["$","article","2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks","children":"[논문리뷰] Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks","children":"Yiran Peng이 arXiv에 게시한 'Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Reasoning-via-Video-The-First-Evaluation-of-Video-Models-Reasoning-Abilities-through-Maze-Solving-Tasks"}]]}]]}],["$","article","2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation","children":"[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation","children":"arXiv에 게시된 'Mixture of States: Routing Token-Level Dynamics for Multimodal Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Mixture-of-States-Routing-Token-Level-Dynamics-for-Multimodal-Generation"}]]}]]}],["$","article","2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation","children":"[논문리뷰] Medal S: Spatio-Textual Prompt Model for Medical Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation","children":"Tao Chen이 arXiv에 게시한 'Medal S: Spatio-Textual Prompt Model for Medical Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation"}]]}]]}],["$","article","2025-11-20-MHR-Momentum-Human-Rig",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-MHR-Momentum-Human-Rig","children":"[논문리뷰] MHR: Momentum Human Rig"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-MHR-Momentum-Human-Rig","children":"Chris Twigg이 arXiv에 게시한 'MHR: Momentum Human Rig' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-MHR-Momentum-Human-Rig"}]]}]]}],["$","article","2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","children":"[논문리뷰] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation","children":"Vladimir Arkhipkin이 arXiv에 게시한 'Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Kandinsky-5-0-A-Family-of-Foundation-Models-for-Image-and-Video-Generation"}]]}]]}],["$","article","2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset","children":"[논문리뷰] Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset","children":"arXiv에 게시된 'Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Instruction-Guided-Lesion-Segmentation-for-Chest-X-rays-with-Automatically-Generated-Large-Scale-Dataset"}]]}]]}],["$","article","2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI","children":"[논문리뷰] FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI","children":"Xinyu Yin이 arXiv에 게시한 'FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-FreeAskWorld-An-Interactive-and-Closed-Loop-Simulator-for-Human-Centric-Embodied-AI"}]]}]]}],["$","article","2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges","children":"[논문리뷰] Aligning Generative Music AI with Human Preferences: Methods and Challenges"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges","children":"Abhinaba Roy이 arXiv에 게시한 'Aligning Generative Music AI with Human Preferences: Methods and Challenges' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-Aligning-Generative-Music-AI-with-Human-Preferences-Methods-and-Challenges"}]]}]]}],["$","article","2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries","children":"[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries","children":"arXiv에 게시된 'ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-20 00:00:00+0900+0900","children":"2025년 11월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries"}]]}]]}],["$","article","2025-11-19-eat-Physically-Grounded-Feature-Representation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-eat-Physically-Grounded-Feature-Representation","children":"[논문리뷰] Φeat: Physically-Grounded Feature Representation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-eat-Physically-Grounded-Feature-Representation","children":"arXiv에 게시된 'Φeat: Physically-Grounded Feature Representation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-eat-Physically-Grounded-Feature-Representation"}]]}]]}],["$","article","2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning","children":"[논문리뷰] VIDEOP2R: Video Understanding from Perception to Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning","children":"arXiv에 게시된 'VIDEOP2R: Video Understanding from Perception to Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-VIDEOP2R-Video-Understanding-from-Perception-to-Reasoning"}]]}]]}],["$","article","2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models","children":"[논문리뷰] TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models","children":"Rong Zhao이 arXiv에 게시한 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-TopoPerception-A-Shortcut-Free-Evaluation-of-Global-Visual-Perception-in-Large-Vision-Language-Models"}]]}]]}],["$","article","2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding","children":"[논문리뷰] REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding","children":"Jingyang Chen이 arXiv에 게시한 'REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-REVISOR-Beyond-Textual-Reflection-Towards-Multimodal-Introspective-Reasoning-in-Long-Form-Video-Understanding"}]]}]]}],["$","article","2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations","children":"[논문리뷰] Proactive Hearing Assistants that Isolate Egocentric Conversations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations","children":"arXiv에 게시된 'Proactive Hearing Assistants that Isolate Egocentric Conversations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Proactive-Hearing-Assistants-that-Isolate-Egocentric-Conversations"}]]}]]}],["$","article","2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution","children":"[논문리뷰] Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution","children":"Sudeep Pillai이 arXiv에 게시한 'Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Orion-A-Unified-Visual-Agent-for-Multimodal-Perception-Advanced-Visual-Reasoning-and-Execution"}]]}]]}],["$","article","2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models","children":"[논문리뷰] OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models","children":"Jian liu이 arXiv에 게시한 'OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models"}]]}]]}],["$","article","2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models","children":"[논문리뷰] Mitigating Label Length Bias in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models","children":"Katharina von der Wense이 arXiv에 게시한 'Mitigating Label Length Bias in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models"}]]}]]}],["$","article","2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs","children":"[논문리뷰] MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs","children":"Kaijie Chen이 arXiv에 게시한 'MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-MVI-Bench-A-Comprehensive-Benchmark-for-Evaluating-Robustness-to-Misleading-Visual-Inputs-in-LVLMs"}]]}]]}],["$","article","2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework","children":"[논문리뷰] Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework","children":"arXiv에 게시된 'Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Large-Language-Models-Meet-Extreme-Multi-label-Classification-Scaling-and-Multi-modal-Framework"}]]}]]}],["$","article","2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost","children":"[논문리뷰] LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost","children":"Kengo Tajiri이 arXiv에 게시한 'LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost"}]]}]]}],["$","article","2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models","children":"[논문리뷰] Error-Driven Scene Editing for 3D Grounding in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models","children":"arXiv에 게시된 'Error-Driven Scene Editing for 3D Grounding in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Error-Driven-Scene-Editing-for-3D-Grounding-in-Large-Language-Models"}]]}]]}],["$","article","2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark","children":"[논문리뷰] Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark","children":"Yuzhang Shang이 arXiv에 게시한 'Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Can-World-Simulators-Reason-Gen-ViRe-A-Generative-Visual-Reasoning-Benchmark"}]]}]]}],["$","article","2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models","children":"[논문리뷰] AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models","children":"arXiv에 게시된 'AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-AraLingBench-A-Human-Annotated-Benchmark-for-Evaluating-Arabic-Linguistic-Capabilities-of-Large-Language-Models"}]]}]]}],["$","article","2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding","children":"[논문리뷰] Agent READMEs: An Empirical Study of Context Files for Agentic Coding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding","children":"Kundjanasith Thonglek이 arXiv에 게시한 'Agent READMEs: An Empirical Study of Context Files for Agentic Coding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Agent-READMEs-An-Empirical-Study-of-Context-Files-for-Agentic-Coding"}]]}]]}],["$","article","2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning","children":"[논문리뷰] Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning","children":"Yucong Luo이 arXiv에 게시한 'Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-Agent-R1-Training-Powerful-LLM-Agents-with-End-to-End-Reinforcement-Learning"}]]}]]}],["$","article","2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning","children":"[논문리뷰] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning","children":"Yuqiang Li이 arXiv에 게시한 'ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-ATLAS-A-High-Difficulty-Multidisciplinary-Benchmark-for-Frontier-Scientific-Reasoning"}]]}]]}],["$","article","2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space","children":"[논문리뷰] A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space","children":"arXiv에 게시된 'A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-A-Style-is-Worth-One-Code-Unlocking-Code-to-Style-Image-Generation-with-Discrete-Style-Space"}]]}]]}],["$","article","2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition","children":"[논문리뷰] A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition","children":"G. Maragatham이 arXiv에 게시한 'A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-19 00:00:00+0900+0900","children":"2025년 11월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-19-A-Brain-Wave-Encodes-a-Thousand-Tokens-Modeling-Inter-Cortical-Neural-Interactions-for-Effective-EEG-based-Emotion-Recognition"}]]}]]}],["$","article","2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data","children":"[논문리뷰] Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data","children":"arXiv에 게시된 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data"}]]}]]}],["$","article","2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity","children":"[논문리뷰] UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity","children":"arXiv에 게시된 'UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity"}]]}]]}],["$","article","2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy","children":"[논문리뷰] UFO^3: Weaving the Digital Agent Galaxy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy","children":"arXiv에 게시된 'UFO^3: Weaving the Digital Agent Galaxy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-UFO3-Weaving-the-Digital-Agent-Galaxy"}]]}]]}],["$","article","2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models","children":"[논문리뷰] TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models","children":"Qingyang Liu이 arXiv에 게시한 'TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models"}]]}]]}],["$","article","2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models","children":"[논문리뷰] Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models","children":"arXiv에 게시된 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models"}]]}]]}],["$","article","2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance","children":"[논문리뷰] Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance","children":"arXiv에 게시된 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance"}]]}]]}],["$","article","2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization","children":"[논문리뷰] SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization","children":"Bo Du이 arXiv에 게시한 'SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-SafeGRPO-Self-Rewarded-Multimodal-Safety-Alignment-via-Rule-Governed-Policy-Optimization"}]]}]]}],["$","article","2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model","children":"[논문리뷰] Part-X-MLLM: Part-aware 3D Multimodal Large Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model","children":"arXiv에 게시된 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model"}]]}]]}],["$","article","2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","children":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","children":"Haiyuan Wan이 arXiv에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"}]]}]]}],["$","article","2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation","children":"[논문리뷰] OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation","children":"arXiv에 게시된 'OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation"}]]}]]}],["$","article","2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards","children":"[논문리뷰] NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards","children":"arXiv에 게시된 'NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-NORA-1-5-A-Vision-Language-Action-Model-Trained-using-World-Model-and-Action-based-Preference-Rewards"}]]}]]}],["$","article","2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling","children":"[논문리뷰] MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling","children":"cyyang822이 arXiv에 게시한 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling"}]]}]]}],["$","article","2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model","children":"[논문리뷰] MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model","children":"Bo Yan이 arXiv에 게시한 'MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model"}]]}]]}],["$","article","2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering","children":"[논문리뷰] LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering","children":"arXiv에 게시된 'LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-LoCoBench-Agent-An-Interactive-Benchmark-for-LLM-Agents-in-Long-Context-Software-Engineering"}]]}]]}],["$","article","2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly","children":"[논문리뷰] Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly","children":"Lingming Zhang이 arXiv에 게시한 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly"}]]}]]}],["$","article","2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners","children":"[논문리뷰] Genomic Next-Token Predictors are In-Context Learners"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners","children":"arXiv에 게시된 'Genomic Next-Token Predictors are In-Context Learners' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Genomic-Next-Token-Predictors-are-In-Context-Learners"}]]}]]}],["$","article","2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing","children":"[논문리뷰] Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing","children":"arXiv에 게시된 'Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-Assessing-LLMs-for-Serendipity-Discovery-in-Knowledge-Graphs-A-Case-for-Drug-Repurposing"}]]}]]}],["$","article","2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing","children":"[논문리뷰] AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing","children":"Hongyu Lin이 arXiv에 게시한 'AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing"}]]}]]}],["$","article","2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain","children":"[논문리뷰] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain","children":"Meng Jiang이 arXiv에 게시한 'A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain"}]]}]]}],["$","article","2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward","children":"[논문리뷰] miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward","children":"Farzan Farnia이 arXiv에 게시한 'miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward"}]]}]]}],["$","article","2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences","children":"[논문리뷰] Workload Schedulers -- Genesis, Algorithms and Differences"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences","children":"Vladimir Getov이 arXiv에 게시한 'Workload Schedulers -- Genesis, Algorithms and Differences' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-Workload-Schedulers-Genesis-Algorithms-and-Differences"}]]}]]}],["$","article","2025-11-17-Virtual-Width-Networks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Virtual-Width-Networks","children":"[논문리뷰] Virtual Width Networks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Virtual-Width-Networks","children":"arXiv에 게시된 'Virtual Width Networks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-Virtual-Width-Networks"}]]}]]}],["$","article","2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation","children":"[논문리뷰] UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation","children":"Weihan Wang이 arXiv에 게시한 'UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-UI2CodeN-A-Visual-Language-Model-for-Test-Time-Scalable-Interactive-UI-to-Code-Generation"}]]}]]}],["$","article","2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap","children":"[논문리뷰] Simulating the Visual World with Artificial Intelligence: A Roadmap"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap","children":"Pengfei Wan이 arXiv에 게시한 'Simulating the Visual World with Artificial Intelligence: A Roadmap' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-Simulating-the-Visual-World-with-Artificial-Intelligence-A-Roadmap"}]]}]]}],["$","article","2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism","children":"[논문리뷰] MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism","children":"arXiv에 게시된 'MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-MarsRL-Advancing-Multi-Agent-Reasoning-System-via-Reinforcement-Learning-with-Agentic-Pipeline-Parallelism"}]]}]]}],["$","article","2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers","children":"[논문리뷰] LiteAttention: A Temporal Sparse Attention for Diffusion Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers","children":"arXiv에 게시된 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-LiteAttention-A-Temporal-Sparse-Attention-for-Diffusion-Transformers"}]]}]]}],["$","article","2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey","children":"[논문리뷰] Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey","children":"Mohammad Hossein Rohban이 arXiv에 게시한 'Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey"}]]}]]}],["$","article","2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant","children":"[논문리뷰] HI-TransPA: Hearing Impairments Translation Personal Assistant"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant","children":"arXiv에 게시된 'HI-TransPA: Hearing Impairments Translation Personal Assistant' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant"}]]}]]}],["$","article","2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models","children":"[논문리뷰] GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models","children":"Siyuan Li이 arXiv에 게시한 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-GGBench-A-Geometric-Generative-Reasoning-Benchmark-for-Unified-Multimodal-Models"}]]}]]}],["$","article","2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models","children":"[논문리뷰] From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models","children":"arXiv에 게시된 'From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-From-Proof-to-Program-Characterizing-Tool-Induced-Reasoning-Hallucinations-in-Large-Language-Models"}]]}]]}],["$","article","2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies","children":"[논문리뷰] Experience-Guided Adaptation of Inference-Time Reasoning Strategies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies","children":"arXiv에 게시된 'Experience-Guided Adaptation of Inference-Time Reasoning Strategies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-Experience-Guided-Adaptation-of-Inference-Time-Reasoning-Strategies"}]]}]]}],["$","article","2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","children":"[논문리뷰] EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation","children":"Zeyu Wang이 arXiv에 게시한 'EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-EmoVid-A-Multimodal-Emotion-Video-Dataset-for-Emotion-Centric-Video-Understanding-and-Generation"}]]}]]}],["$","article","2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding","children":"[논문리뷰] Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding","children":"arXiv에 게시된 'Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-Dont-Waste-It-Guiding-Generative-Recommenders-with-Structured-Human-Priors-via-Multi-head-Decoding"}]]}]]}],["$","article","2025-11-17-DoPE-Denoising-Rotary-Position-Embedding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding","children":"[논문리뷰] DoPE: Denoising Rotary Position Embedding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding","children":"Min Yang이 arXiv에 게시한 'DoPE: Denoising Rotary Position Embedding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding"}]]}]]}],["$","article","2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains","children":"[논문리뷰] DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains","children":"arXiv에 게시된 'DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-DiscoX-Benchmarking-Discourse-Level-Translation-task-in-Expert-Domains"}]]}]]}],["$","article","2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios","children":"[논문리뷰] CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios","children":"Juyoung Oh이 arXiv에 게시한 'CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-CATS-V2V-A-Real-World-Vehicle-to-Vehicle-Cooperative-Perception-Dataset-with-Complex-Adverse-Traffic-Scenarios"}]]}]]}],["$","article","2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems","children":"[논문리뷰] A Meta-Heuristic Load Balancer for Cloud Computing Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems","children":"Vladimir Getov이 arXiv에 게시한 'A Meta-Heuristic Load Balancer for Cloud Computing Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-17 00:00:00+0900+0900","children":"2025년 11월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-17-A-Meta-Heuristic-Load-Balancer-for-Cloud-Computing-Systems"}]]}]]}],["$","article","2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist","children":"[논문리뷰] UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist","children":"arXiv에 게시된 'UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-UniVA-Universal-Video-Agent-towards-Open-Source-Next-Generation-Video-Generalist"}]]}]]}],["$","article","2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training","children":"[논문리뷰] Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training","children":"suayptalha이 arXiv에 게시한 'Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training"}]]}]]}],["$","article","2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control","children":"[논문리뷰] SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control","children":"Ryan Rossi이 arXiv에 게시한 'SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control"}]]}]]}],["$","article","2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following","children":"[논문리뷰] Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following","children":"Karishma Mandyam이 arXiv에 게시한 'Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Rubric-Based-Benchmarking-and-Reinforcement-Learning-for-Advancing-LLM-Instruction-Following"}]]}]]}],["$","article","2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents","children":"[논문리뷰] ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents","children":"arXiv에 게시된 'ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-ResearchRubrics-A-Benchmark-of-Prompts-and-Rubrics-For-Evaluating-Deep-Research-Agents"}]]}]]}],["$","article","2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models","children":"[논문리뷰] One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models","children":"Ilya Makarov이 arXiv에 게시한 'One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-One-Small-Step-in-Latent-One-Giant-Leap-for-Pixels-Fast-Latent-Upscale-Adapter-for-Your-Diffusion-Models"}]]}]]}],["$","article","2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models","children":"[논문리뷰] Music Flamingo: Scaling Music Understanding in Audio Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models","children":"arXiv에 게시된 'Music Flamingo: Scaling Music Understanding in Audio Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models"}]]}]]}],["$","article","2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples","children":"[논문리뷰] MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples","children":"arXiv에 게시된 'MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples"}]]}]]}],["$","article","2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique","children":"[논문리뷰] MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique","children":"arXiv에 게시된 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-MM-CRITIC-A-Holistic-Evaluation-of-Large-Multimodal-Models-as-Multimodal-Critique"}]]}]]}],["$","article","2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO","children":"[논문리뷰] Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO","children":"arXiv에 게시된 'Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Hail-to-the-Thief-Exploring-Attacks-and-Defenses-in-Decentralised-GRPO"}]]}]]}],["$","article","2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views","children":"[논문리뷰] Depth Anything 3: Recovering the Visual Space from Any Views"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views","children":"arXiv에 게시된 'Depth Anything 3: Recovering the Visual Space from Any Views' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Depth-Anything-3-Recovering-the-Visual-Space-from-Any-Views"}]]}]]}],["$","article","2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis","children":"[논문리뷰] CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis","children":"Jian Wu이 arXiv에 게시한 'CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis"}]]}]]}],["$","article","2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models","children":"[논문리뷰] Black-Box On-Policy Distillation of Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models","children":"arXiv에 게시된 'Black-Box On-Policy Distillation of Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Black-Box-On-Policy-Distillation-of-Large-Language-Models"}]]}]]}],["$","article","2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation","children":"[논문리뷰] Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation","children":"arXiv에 게시된 'Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Benchmarking-Diversity-in-Image-Generation-via-Attribute-Conditional-Human-Evaluation"}]]}]]}],["$","article","2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models","children":"[논문리뷰] AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models","children":"Zhen Li이 arXiv에 게시한 'AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation","children":"[논문리뷰] WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation","children":"arXiv에 게시된 'WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation"}]]}]]}],["$","article","2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models","children":"[논문리뷰] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models","children":"arXiv에 게시된 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-WMPO-World-Model-based-Policy-Optimization-for-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance","children":"[논문리뷰] Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance","children":"Kwanyoung Kim이 arXiv에 게시한 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-Toward-the-Frontiers-of-Reliable-Diffusion-Sampling-via-Adversarial-Sinkhorn-Attention-Guidance"}]]}]]}],["$","article","2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression","children":"[논문리뷰] TiDAR: Think in Diffusion, Talk in Autoregression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression","children":"arXiv에 게시된 'TiDAR: Think in Diffusion, Talk in Autoregression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-TiDAR-Think-in-Diffusion-Talk-in-Autoregression"}]]}]]}],["$","article","2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle","children":"[논문리뷰] Stemming Hallucination in Language Models Using a Licensing Oracle"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle","children":"Richard Ackermann이 arXiv에 게시한 'Stemming Hallucination in Language Models Using a Licensing Oracle' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-Stemming-Hallucination-in-Language-Models-Using-a-Licensing-Oracle"}]]}]]}],["$","article","2025-11-13-Motif-2-12-7B-technical-report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Motif-2-12-7B-technical-report","children":"[논문리뷰] Motif 2 12.7B technical report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Motif-2-12-7B-technical-report","children":"arXiv에 게시된 'Motif 2 12.7B technical report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-Motif-2-12-7B-technical-report"}]]}]]}],["$","article","2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning","children":"[논문리뷰] MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning","children":"arXiv에 게시된 'MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-MathSE-Improving-Multimodal-Mathematical-Reasoning-via-Self-Evolving-Iterative-Reflection-and-Reward-Guided-Fine-Tuning"}]]}]]}],["$","article","2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra","children":"[논문리뷰] MADD: Multi-Agent Drug Discovery Orchestra"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra","children":"arXiv에 게시된 'MADD: Multi-Agent Drug Discovery Orchestra' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-MADD-Multi-Agent-Drug-Discovery-Orchestra"}]]}]]}],["$","article","2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds","children":"[논문리뷰] Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds","children":"arXiv에 게시된 'Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds"}]]}]]}],["$","article","2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls","children":"[논문리뷰] LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls","children":"arXiv에 게시된 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-LoopTool-Closing-the-Data-Training-Loop-for-Robust-LLM-Tool-Calls"}]]}]]}],["$","article","2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents","children":"[논문리뷰] Agentic Refactoring: An Empirical Study of AI Coding Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents","children":"Hajimu Iida이 arXiv에 게시한 'Agentic Refactoring: An Empirical Study of AI Coding Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents"}]]}]]}],["$","article","2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision","children":"[논문리뷰] Adapting Web Agents with Synthetic Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision","children":"Siwei Han이 arXiv에 게시한 'Adapting Web Agents with Synthetic Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-13 00:00:00+0900+0900","children":"2025년 11월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-13-Adapting-Web-Agents-with-Synthetic-Supervision"}]]}]]}],["$","article","2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora","children":"[논문리뷰] Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora","children":"Mohamed Motasim Hamed이 arXiv에 게시한 'Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora"}]]}]]}],["$","article","2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective","children":"[논문리뷰] Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective","children":"Christoph Treude이 arXiv에 게시한 'Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective"}]]}]]}],["$","article","2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","children":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","children":"arXiv에 게시된 'VideoSSR: Video Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning"}]]}]]}],["$","article","2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B","children":"[논문리뷰] Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B","children":"arXiv에 게시된 'Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B"}]]}]]}],["$","article","2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning","children":"[논문리뷰] TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning","children":"arXiv에 게시된 'TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning"}]]}]]}],["$","article","2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals","children":"[논문리뷰] The Path Not Taken: RLVR Provably Learns Off the Principals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals","children":"arXiv에 게시된 'The Path Not Taken: RLVR Provably Learns Off the Principals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals"}]]}]]}],["$","article","2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration","children":"[논문리뷰] Optimizing Diversity and Quality through Base-Aligned Model Collaboration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration","children":"Jonathan May이 arXiv에 게시한 'Optimizing Diversity and Quality through Base-Aligned Model Collaboration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration"}]]}]]}],["$","article","2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models","children":"[논문리뷰] KLASS: KL-Guided Fast Inference in Masked Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models","children":"arXiv에 게시된 'KLASS: KL-Guided Fast Inference in Masked Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-KLASS-KL-Guided-Fast-Inference-in-Masked-Diffusion-Models"}]]}]]}],["$","article","2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI","children":"[논문리뷰] Intelligence per Watt: Measuring Intelligence Efficiency of Local AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI","children":"arXiv에 게시된 'Intelligence per Watt: Measuring Intelligence Efficiency of Local AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Intelligence-per-Watt-Measuring-Intelligence-Efficiency-of-Local-AI"}]]}]]}],["$","article","2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations","children":"[논문리뷰] Grounding Computer Use Agents on Human Demonstrations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations","children":"arXiv에 게시된 'Grounding Computer Use Agents on Human Demonstrations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Grounding-Computer-Use-Agents-on-Human-Demonstrations"}]]}]]}],["$","article","2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces","children":"[논문리뷰] DynaAct: Large Language Model Reasoning with Dynamic Action Spaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces","children":"Lingpeng Kong이 arXiv에 게시한 'DynaAct: Large Language Model Reasoning with Dynamic Action Spaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces"}]]}]]}],["$","article","2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives","children":"[논문리뷰] BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives","children":"arXiv에 게시된 'BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-BiCA-Effective-Biomedical-Dense-Retrieval-with-Citation-Aware-Hard-Negatives"}]]}]]}],["$","article","2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces","children":"[논문리뷰] Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces","children":"Vwani Roychowdhury이 arXiv에 게시한 'Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Beyond-Fact-Retrieval-Episodic-Memory-for-RAG-with-Generative-Semantic-Workspaces"}]]}]]}],["$","article","2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs","children":"[논문리뷰] Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs","children":"arXiv에 게시된 'Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs"}]]}]]}],["$","article","2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems","children":"[논문리뷰] Adaptive Multi-Agent Response Refinement in Conversational Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems","children":"arXiv에 게시된 'Adaptive Multi-Agent Response Refinement in Conversational Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems"}]]}]]}],["$","article","2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models","children":"[논문리뷰] VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models","children":"arXiv에 게시된 'VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models"}]]}]]}],["$","article","2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery","children":"[논문리뷰] The Station: An Open-World Environment for AI-Driven Discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery","children":"wydu이 arXiv에 게시한 'The Station: An Open-World Environment for AI-Driven Discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery"}]]}]]}],["$","article","2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence","children":"[논문리뷰] Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence","children":"arXiv에 게시된 'Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Teaching-Pretrained-Language-Models-to-Think-Deeper-with-Retrofitted-Recurrence"}]]}]]}],["$","article","2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization","children":"[논문리뷰] SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization","children":"arXiv에 게시된 'SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via   Gumbel-Reparameterized Soft-Thinking Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-SofT-GRPO-Surpassing-Discrete-Token-LLM-Reinforcement-Learning-via-Gumbel-Reparameterized-Soft-Thinking-Policy-Optimization"}]]}]]}],["$","article","2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads","children":"[논문리뷰] SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads","children":"Ofir Press이 arXiv에 게시한 'SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-SWE-fficiency-Can-Language-Models-Optimize-Real-World-Repositories-on-Real-Workloads"}]]}]]}],["$","article","2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs","children":"[논문리뷰] Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs","children":"Ziyue Li이 arXiv에 게시한 'Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Routing-Manifold-Alignment-Improves-Generalization-of-Mixture-of-Experts-LLMs"}]]}]]}],["$","article","2025-11-11-Robot-Learning-from-a-Physical-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model","children":"[논문리뷰] Robot Learning from a Physical World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model","children":"arXiv에 게시된 'Robot Learning from a Physical World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Robot-Learning-from-a-Physical-World-Model"}]]}]]}],["$","article","2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs","children":"[논문리뷰] Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs","children":"arXiv에 게시된 'Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs"}]]}]]}],["$","article","2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services","children":"[논문리뷰] RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services","children":"Zijie Meng이 arXiv에 게시한 'RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-RedOne-2-0-Rethinking-Domain-specific-LLM-Post-Training-in-Social-Networking-Services"}]]}]]}],["$","article","2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads","children":"[논문리뷰] Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads","children":"Jiaheng Zhang이 arXiv에 게시한 'Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps   via Uncertainty Heads' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Reasoning-with-Confidence-Efficient-Verification-of-LLM-Reasoning-Steps-via-Uncertainty-Heads"}]]}]]}],["$","article","2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization","children":"[논문리뷰] RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization","children":"Wenhao Huang이 arXiv에 게시한 'RLoop: An Self-Improving Framework for Reinforcement Learning with   Iterative Policy Initialization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-RLoop-An-Self-Improving-Framework-for-Reinforcement-Learning-with-Iterative-Policy-Initialization"}]]}]]}],["$","article","2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments","children":"[논문리뷰] RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments","children":"Shuyue Stella Li이 arXiv에 게시한 'RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-RLVE-Scaling-Up-Reinforcement-Learning-for-Language-Models-with-Adaptive-Verifiable-Environments"}]]}]]}],["$","article","2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models","children":"[논문리뷰] Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models","children":"arXiv에 게시된 'Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models"}]]}]]}],["$","article","2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling","children":"[논문리뷰] NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling","children":"arXiv에 게시된 'NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling"}]]}]]}],["$","article","2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","children":"[논문리뷰] MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","children":"arXiv에 게시된 'MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs"}]]}]]}],["$","article","2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings","children":"[논문리뷰] MPJudge: Towards Perceptual Assessment of Music-Induced Paintings"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings","children":"arXiv에 게시된 'MPJudge: Towards Perceptual Assessment of Music-Induced Paintings' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings"}]]}]]}],["$","article","2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale","children":"[논문리뷰] Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale","children":"arXiv에 게시된 'Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale"}]]}]]}],["$","article","2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks","children":"[논문리뷰] Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks","children":"arXiv에 게시된 'Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Llama-Embed-Nemotron-8B-A-Universal-Text-Embedding-Model-for-Multilingual-and-Cross-Lingual-Tasks"}]]}]]}],["$","article","2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs","children":"[논문리뷰] LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs","children":"Jason Cong이 arXiv에 게시한 'LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-LUT-LLM-Efficient-Large-Language-Model-Inference-with-Memory-based-Computations-on-FPGAs"}]]}]]}],["$","article","2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction","children":"[논문리뷰] IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction","children":"Haotian Xu이 arXiv에 게시한 'IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-IterResearch-Rethinking-Long-Horizon-Agents-via-Markovian-State-Reconstruction"}]]}]]}],["$","article","2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents","children":"[논문리뷰] HaluMem: Evaluating Hallucinations in Memory Systems of Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents","children":"arXiv에 게시된 'HaluMem: Evaluating Hallucinations in Memory Systems of Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-HaluMem-Evaluating-Hallucinations-in-Memory-Systems-of-Agents"}]]}]]}],["$","article","2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions","children":"[논문리뷰] Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions","children":"arXiv에 게시된 'Generating an Image From 1,000 Words: Enhancing Text-to-Image With   Structured Captions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Generating-an-Image-From-1000-Words-Enhancing-Text-to-Image-With-Structured-Captions"}]]}]]}],["$","article","2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience","children":"[논문리뷰] FLEX: Continuous Agent Evolution via Forward Learning from Experience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience","children":"Jiangjie Chen이 arXiv에 게시한 'FLEX: Continuous Agent Evolution via Forward Learning from Experience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-FLEX-Continuous-Agent-Evolution-via-Forward-Learning-from-Experience"}]]}]]}],["$","article","2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning","children":"[논문리뷰] Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning","children":"arXiv에 게시된 'Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning"}]]}]]}],["$","article","2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents","children":"[논문리뷰] DigiData: Training and Evaluating General-Purpose Mobile Control Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents","children":"arXiv에 게시된 'DigiData: Training and Evaluating General-Purpose Mobile Control Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-DigiData-Training-and-Evaluating-General-Purpose-Mobile-Control-Agents"}]]}]]}],["$","article","2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models","children":"[논문리뷰] Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models","children":"Zhao Xu이 arXiv에 게시한 'Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion   Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models"}]]}]]}],["$","article","2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation","children":"[논문리뷰] DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation","children":"arXiv에 게시된 'DRIVE: Data Curation Best Practices for Reinforcement Learning with   Verifiable Reward in Competitive Code Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-DRIVE-Data-Curation-Best-Practices-for-Reinforcement-Learning-with-Verifiable-Reward-in-Competitive-Code-Generation"}]]}]]}],["$","article","2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects","children":"[논문리뷰] DIMO: Diverse 3D Motion Generation for Arbitrary Objects"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects","children":"Kostas Daniilidis이 arXiv에 게시한 'DIMO: Diverse 3D Motion Generation for Arbitrary Objects' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-DIMO-Diverse-3D-Motion-Generation-for-Arbitrary-Objects"}]]}]]}],["$","article","2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries","children":"[논문리뷰] Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries","children":"Zhengzhong Tu이 arXiv에 게시한 'Ariadne: A Controllable Framework for Probing and Extending VLM   Reasoning Boundaries' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Ariadne-A-Controllable-Framework-for-Probing-and-Extending-VLM-Reasoning-Boundaries"}]]}]]}],["$","article","2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models","children":"[논문리뷰] 10 Open Challenges Steering the Future of Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models","children":"arXiv에 게시된 '10 Open Challenges Steering the Future of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-10-Open-Challenges-Steering-the-Future-of-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-11-10-Visual-Spatial-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Visual-Spatial-Tuning","children":"[논문리뷰] Visual Spatial Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Visual-Spatial-Tuning","children":"arXiv에 게시된 'Visual Spatial Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-Visual-Spatial-Tuning"}]]}]]}],["$","article","2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks","children":"[논문리뷰] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks","children":"arXiv에 게시된 'VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks"}]]}]]}],["$","article","2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings","children":"[논문리뷰] Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings","children":"Jiaxin Yuan이 arXiv에 게시한 'Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-Towards-Mitigating-Hallucinations-in-Large-Vision-Language-Models-by-Refining-Textual-Embeddings"}]]}]]}],["$","article","2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains","children":"[논문리뷰] Too Good to be Bad: On the Failure of LLMs to Role-Play Villains"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains","children":"arXiv에 게시된 'Too Good to be Bad: On the Failure of LLMs to Role-Play Villains' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-Too-Good-to-be-Bad-On-the-Failure-of-LLMs-to-Role-Play-Villains"}]]}]]}],["$","article","2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments","children":"[논문리뷰] Real-Time Reasoning Agents in Evolving Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments","children":"arXiv에 게시된 'Real-Time Reasoning Agents in Evolving Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-Real-Time-Reasoning-Agents-in-Evolving-Environments"}]]}]]}],["$","article","2025-11-10-Jailbreaking-in-the-Haystack",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Jailbreaking-in-the-Haystack","children":"[논문리뷰] Jailbreaking in the Haystack"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Jailbreaking-in-the-Haystack","children":"Alexander Robey이 arXiv에 게시한 'Jailbreaking in the Haystack' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-Jailbreaking-in-the-Haystack"}]]}]]}],["$","article","2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent","children":"[논문리뷰] HAFixAgent: History-Aware Automated Program Repair Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent","children":"Ahmed E. Hassan이 arXiv에 게시한 'HAFixAgent: History-Aware Automated Program Repair Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent"}]]}]]}],["$","article","2025-11-10-Dense-Motion-Captioning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Dense-Motion-Captioning","children":"[논문리뷰] Dense Motion Captioning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-Dense-Motion-Captioning","children":"Paolo Rota이 arXiv에 게시한 'Dense Motion Captioning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-Dense-Motion-Captioning"}]]}]]}],["$","article","2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model","children":"[논문리뷰] DeepEyesV2: Toward Agentic Multimodal Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model","children":"Guohai Xu이 arXiv에 게시한 'DeepEyesV2: Toward Agentic Multimodal Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-DeepEyesV2-Toward-Agentic-Multimodal-Model"}]]}]]}],["$","article","2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration","children":"[논문리뷰] CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration","children":"Baixuan Xu이 arXiv에 게시한 'CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-CritiCal-Can-Critique-Help-LLM-Uncertainty-or-Confidence-Calibration"}]]}]]}],["$","article","2025-11-7-V-Thinker-Interactive-Thinking-with-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images","children":"[논문리뷰] V-Thinker: Interactive Thinking with Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images","children":"Peiqing Yang이 arXiv에 게시한 'V-Thinker: Interactive Thinking with Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-V-Thinker-Interactive-Thinking-with-Images"}]]}]]}],["$","article","2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm","children":"[논문리뷰] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm","children":"arXiv에 게시된 'Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm"}]]}]]}],["$","article","2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms","children":"[논문리뷰] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms","children":"Susumu Takeuchi이 arXiv에 게시한 'The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-The-Strong-Lottery-Ticket-Hypothesis-for-Multi-Head-Attention-Mechanisms"}]]}]]}],["$","article","2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis","children":"[논문리뷰] Scaling Agent Learning via Experience Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis","children":"arXiv에 게시된 'Scaling Agent Learning via Experience Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Scaling-Agent-Learning-via-Experience-Synthesis"}]]}]]}],["$","article","2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding","children":"[논문리뷰] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding","children":"arXiv에 게시된 'SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding"}]]}]]}],["$","article","2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning","children":"[논문리뷰] SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning","children":"arXiv에 게시된 'SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning"}]]}]]}],["$","article","2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems","children":"[논문리뷰] RDMA Point-to-Point Communication for LLM Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems","children":"arXiv에 게시된 'RDMA Point-to-Point Communication for LLM Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-RDMA-Point-to-Point-Communication-for-LLM-Systems"}]]}]]}],["$","article","2025-11-7-NVIDIA-Nemotron-Nano-V2-VL",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-NVIDIA-Nemotron-Nano-V2-VL","children":"[논문리뷰] NVIDIA Nemotron Nano V2 VL"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-NVIDIA-Nemotron-Nano-V2-VL","children":"arXiv에 게시된 'NVIDIA Nemotron Nano V2 VL' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-NVIDIA-Nemotron-Nano-V2-VL"}]]}]]}],["$","article","2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots","children":"[논문리뷰] Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots","children":"arXiv에 게시된 'Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Learning-Vision-Driven-Reactive-Soccer-Skills-for-Humanoid-Robots"}]]}]]}],["$","article","2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics","children":"[논문리뷰] How to Evaluate Speech Translation with Source-Aware Neural MT Metrics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics","children":"Luisa Bentivogli이 arXiv에 게시한 'How to Evaluate Speech Translation with Source-Aware Neural MT Metrics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-How-to-Evaluate-Speech-Translation-with-Source-Aware-Neural-MT-Metrics"}]]}]]}],["$","article","2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents","children":"[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents","children":"arXiv에 게시된 'GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents"}]]}]]}],["$","article","2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference","children":"[논문리뷰] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference","children":"arXiv에 게시된 'EVTAR: End-to-End Try on with Additional Unpaired Visual Reference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference"}]]}]]}],["$","article","2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation","children":"[논문리뷰] Contamination Detection for VLMs using Multi-Modal Semantic Perturbation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation","children":"arXiv에 게시된 'Contamination Detection for VLMs using Multi-Modal Semantic Perturbation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Contamination-Detection-for-VLMs-using-Multi-Modal-Semantic-Perturbation"}]]}]]}],["$","article","2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video","children":"[논문리뷰] Cambrian-S: Towards Spatial Supersensing in Video"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video","children":"Zihao Yang이 arXiv에 게시한 'Cambrian-S: Towards Spatial Supersensing in Video' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Cambrian-S-Towards-Spatial-Supersensing-in-Video"}]]}]]}],["$","article","2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts","children":"[논문리뷰] Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts","children":"arXiv에 게시된 'Benchmark Designers Should 'Train on the Test Set' to Expose Exploitable Non-Visual Shortcuts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 22:08:24+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-7-Benchmark-Designers-Should-Train-on-the-Test-Set-to-Expose-Exploitable-Non-Visual-Shortcuts"}]]}]]}],["$","article","2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions","children":"[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions","children":"arXiv에 게시된 'UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions"}]]}]]}],["$","article","2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute","children":"[논문리뷰] The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute","children":"arXiv에 게시된 'The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute"}]]}]]}],["$","article","2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models","children":"[논문리뷰] TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models","children":"arXiv에 게시된 'TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-TabTune-A-Unified-Library-for-Inference-and-Fine-Tuning-Tabular-Foundation-Models"}]]}]]}],["$","article","2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning","children":"[논문리뷰] Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning","children":"arXiv에 게시된 'Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-Orion-MSP-Multi-Scale-Sparse-Attention-for-Tabular-In-Context-Learning"}]]}]]}],["$","article","2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity","children":"[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity","children":"arXiv에 게시된 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity"}]]}]]}],["$","article","2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models","children":"[논문리뷰] LiveTradeBench: Seeking Real-World Alpha with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models","children":"Jiaxuan You이 arXiv에 게시한 'LiveTradeBench: Seeking Real-World Alpha with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models"}]]}]]}],["$","article","2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation","children":"[논문리뷰] Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation","children":"Jaehyun Park이 arXiv에 게시한 'Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-Let-Multimodal-Embedders-Learn-When-to-Augment-Query-via-Adaptive-Query-Augmentation"}]]}]]}],["$","article","2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation","children":"[논문리뷰] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation","children":"Soohyun Oh이 arXiv에 게시한 'LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation"}]]}]]}],["$","article","2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects","children":"[논문리뷰] Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects","children":"arXiv에 게시된 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects"}]]}]]}],["$","article","2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper","children":"[논문리뷰] Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper","children":"arXiv에 게시된 'Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-Jr-AI-Scientist-and-Its-Risk-Report-Autonomous-Scientific-Exploration-from-a-Baseline-Paper"}]]}]]}],["$","article","2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask","children":"[논문리뷰] Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask","children":"arXiv에 게시된 'Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask"}]]}]]}],["$","article","2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners","children":"[논문리뷰] Diffusion Language Models are Super Data Learners"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners","children":"arXiv에 게시된 'Diffusion Language Models are Super Data Learners' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-Diffusion-Language-Models-are-Super-Data-Learners"}]]}]]}],["$","article","2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents","children":"[논문리뷰] CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents","children":"Shijue Huang이 arXiv에 게시한 'CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-CostBench-Evaluating-Multi-Turn-Cost-Optimal-Planning-and-Adaptation-in-Dynamic-Environments-for-LLM-Tool-Use-Agents"}]]}]]}],["$","article","2025-11-5-iFlyBot-VLA-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report","children":"[논문리뷰] iFlyBot-VLA Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report","children":"Jiajia wu이 arXiv에 게시한 'iFlyBot-VLA Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-iFlyBot-VLA-Technical-Report"}]]}]]}],["$","article","2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought","children":"[논문리뷰] When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought","children":"arXiv에 게시된 'When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-When-Visualizing-is-the-First-Step-to-Reasoning-MIRA-a-Benchmark-for-Visual-Chain-of-Thought"}]]}]]}],["$","article","2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs","children":"[논문리뷰] When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs","children":"Haotian Wang이 arXiv에 게시한 'When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-When-Modalities-Conflict-How-Unimodal-Reasoning-Uncertainty-Governs-Preference-Dynamics-in-MLLMs"}]]}]]}],["$","article","2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models","children":"[논문리뷰] VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models","children":"Pengfei Wan이 arXiv에 게시한 'VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-VidEmo-Affective-Tree-Reasoning-for-Emotion-Centric-Video-Foundation-Models"}]]}]]}],["$","article","2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation","children":"[논문리뷰] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation","children":"arXiv에 게시된 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-VCode-a-Multimodal-Coding-Benchmark-with-SVG-as-Symbolic-Visual-Representation"}]]}]]}],["$","article","2025-11-5-The-Collaboration-Gap",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-The-Collaboration-Gap","children":"[논문리뷰] The Collaboration Gap"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-The-Collaboration-Gap","children":"arXiv에 게시된 'The Collaboration Gap' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-The-Collaboration-Gap"}]]}]]}],["$","article","2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data","children":"[논문리뷰] TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data","children":"Jin Zeng이 arXiv에 게시한 'TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data"}]]}]]}],["$","article","2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System","children":"[논문리뷰] TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System","children":"Rocky Duan이 arXiv에 게시한 'TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-TWIST2-Scalable-Portable-and-Holistic-Humanoid-Data-Collection-System"}]]}]]}],["$","article","2025-11-5-Step-Audio-EditX-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Step-Audio-EditX-Technical-Report","children":"[논문리뷰] Step-Audio-EditX Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Step-Audio-EditX-Technical-Report","children":"arXiv에 게시된 'Step-Audio-EditX Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Step-Audio-EditX-Technical-Report"}]]}]]}],["$","article","2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR","children":"[논문리뷰] Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR","children":"arXiv에 게시된 'Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Shorter-but-not-Worse-Frugal-Reasoning-via-Easy-Samples-as-Length-Regularizers-in-Math-RLVR"}]]}]]}],["$","article","2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies","children":"[논문리뷰] RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies","children":"arXiv에 게시된 'RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-RoboChallenge-Large-scale-Real-robot-Evaluation-of-Embodied-Policies"}]]}]]}],["$","article","2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs","children":"[논문리뷰] RiddleBench: A New Generative Reasoning Benchmark for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs","children":"arXiv에 게시된 'RiddleBench: A New Generative Reasoning Benchmark for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-RiddleBench-A-New-Generative-Reasoning-Benchmark-for-LLMs"}]]}]]}],["$","article","2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","children":"[논문리뷰] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation","children":"arXiv에 게시된 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation"}]]}]]}],["$","article","2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context","children":"[논문리뷰] LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context","children":"Tianxin Zhang이 arXiv에 게시한 'LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-LiveSecBench-A-Dynamic-and-Culturally-Relevant-AI-Safety-Benchmark-for-LLMs-in-Chinese-Context"}]]}]]}],["$","article","2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw","children":"[논문리뷰] LTD-Bench: Evaluating Large Language Models by Letting Them Draw"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw","children":"arXiv에 게시된 'LTD-Bench: Evaluating Large Language Models by Letting Them Draw' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-LTD-Bench-Evaluating-Large-Language-Models-by-Letting-Them-Draw"}]]}]]}],["$","article","2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs","children":"[논문리뷰] Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs","children":"Bo Bai이 arXiv에 게시한 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs"}]]}]]}],["$","article","2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization","children":"[논문리뷰] Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization","children":"Aleksandr I. Panov이 arXiv에 게시한 'Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Dont-Blind-Your-VLA-Aligning-Visual-Representations-for-OOD-Generalization"}]]}]]}],["$","article","2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning","children":"[논문리뷰] Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning","children":"Zuyi Xiong이 arXiv에 게시한 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning"}]]}]]}],["$","article","2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering","children":"[논문리뷰] CodeClash: Benchmarking Goal-Oriented Software Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering","children":"arXiv에 게시된 'CodeClash: Benchmarking Goal-Oriented Software Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-CodeClash-Benchmarking-Goal-Oriented-Software-Engineering"}]]}]]}],["$","article","2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension","children":"[논문리뷰] ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension","children":"Hao Wang이 arXiv에 게시한 'ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-ChartM3-A-Multi-Stage-Code-Driven-Pipeline-for-Constructing-Multi-Dimensional-and-Multi-Step-Visual-Reasoning-Data-in-Chart-Comprehension"}]]}]]}],["$","article","2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models","children":"[논문리뷰] Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models","children":"Shijie Dong이 arXiv에 게시한 'Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Can-Visual-Input-Be-Compressed-A-Visual-Token-Compression-Benchmark-for-Large-Multimodal-Models"}]]}]]}],["$","article","2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer","children":"[논문리뷰] Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer","children":"arXiv에 게시된 'Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer"}]]}]]}],["$","article","2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring","children":"[논문리뷰] BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring","children":"arXiv에 게시된 'BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-BRAINS-A-Retrieval-Augmented-System-for-Alzheimers-Detection-and-Monitoring"}]]}]]}],["$","article","2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda","children":"[논문리뷰] AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda","children":"arXiv에 게시된 'AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:35:02+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-5-AyurParam-A-State-of-the-Art-Bilingual-Language-Model-for-Ayurveda"}]]}]]}],["$","article","2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles","children":"[논문리뷰] left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles","children":"Deepiha S이 arXiv에 게시한 'left|,circlearrowright,text{BUS},right|: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-leftcirclearrowrighttextBUSright-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles"}]]}]]}],["$","article","2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI","children":"[논문리뷰] World Simulation with Video Foundation Models for Physical AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI","children":"Junjie Bai이 arXiv에 게시한 'World Simulation with Video Foundation Models for Physical AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI"}]]}]]}],["$","article","2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers","children":"[논문리뷰] Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers","children":"arXiv에 게시된 'Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers"}]]}]]}],["$","article","2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process","children":"[논문리뷰] Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process","children":"arXiv에 게시된 'Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process"}]]}]]}],["$","article","2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark","children":"[논문리뷰] UniREditBench: A Unified Reasoning-based Image Editing Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark","children":"arXiv에 게시된 'UniREditBench: A Unified Reasoning-based Image Editing Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark"}]]}]]}],["$","article","2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback","children":"[논문리뷰] UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback","children":"arXiv에 게시된 'UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback"}]]}]]}],["$","article","2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings","children":"[논문리뷰] UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings","children":"Jinsong Su이 arXiv에 게시한 'UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-UME-R1-Exploring-Reasoning-Driven-Generative-Multimodal-Embeddings"}]]}]]}],["$","article","2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval","children":"[논문리뷰] Trove: A Flexible Toolkit for Dense Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval","children":"arXiv에 게시된 'Trove: A Flexible Toolkit for Dense Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Trove-A-Flexible-Toolkit-for-Dense-Retrieval"}]]}]]}],["$","article","2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum","children":"[논문리뷰] Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum","children":"arXiv에 게시된 'Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Towards-Universal-Video-Retrieval-Generalizing-Video-Embedding-via-Synthesized-Multimodal-Pyramid-Curriculum"}]]}]]}],["$","article","2025-11-4-Towards-Robust-Mathematical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Towards-Robust-Mathematical-Reasoning","children":"[논문리뷰] Towards Robust Mathematical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Towards-Robust-Mathematical-Reasoning","children":"Yuri Chervonyi이 arXiv에 게시한 'Towards Robust Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Towards-Robust-Mathematical-Reasoning"}]]}]]}],["$","article","2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use","children":"[논문리뷰] ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use","children":"Guanting Dong이 arXiv에 게시한 'ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-ToolScope-An-Agentic-Framework-for-Vision-Guided-and-Long-Horizon-Tool-Use"}]]}]]}],["$","article","2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding","children":"[논문리뷰] The Underappreciated Power of Vision Models for Graph Structural Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding","children":"Lei Zhang이 arXiv에 게시한 'The Underappreciated Power of Vision Models for Graph Structural Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-The-Underappreciated-Power-of-Vision-Models-for-Graph-Structural-Understanding"}]]}]]}],["$","article","2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning","children":"[논문리뷰] TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning","children":"Shaoheng Lin이 arXiv에 게시한 'TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning"}]]}]]}],["$","article","2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation","children":"[논문리뷰] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation","children":"Feng Li이 arXiv에 게시한 'ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation"}]]}]]}],["$","article","2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset","children":"[논문리뷰] PHUMA: Physically-Grounded Humanoid Locomotion Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset","children":"arXiv에 게시된 'PHUMA: Physically-Grounded Humanoid Locomotion Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-PHUMA-Physically-Grounded-Humanoid-Locomotion-Dataset"}]]}]]}],["$","article","2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner","children":"[논문리뷰] OpenSIR: Open-Ended Self-Improving Reasoner"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner","children":"arXiv에 게시된 'OpenSIR: Open-Ended Self-Improving Reasoner' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner"}]]}]]}],["$","article","2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models","children":"[논문리뷰] NaviTrace: Evaluating Embodied Navigation of Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models","children":"arXiv에 게시된 'NaviTrace: Evaluating Embodied Navigation of Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-NaviTrace-Evaluating-Embodied-Navigation-of-Vision-Language-Models"}]]}]]}],["$","article","2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement","children":"[논문리뷰] Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement","children":"Isabelle Augenstein이 arXiv에 게시한 'Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement"}]]}]]}],["$","article","2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls","children":"[논문리뷰] MotionStream: Real-Time Video Generation with Interactive Motion Controls"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls","children":"arXiv에 게시된 'MotionStream: Real-Time Video Generation with Interactive Motion Controls' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls"}]]}]]}],["$","article","2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models","children":"[논문리뷰] MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models","children":"Bin Yu이 arXiv에 게시한 'MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models"}]]}]]}],["$","article","2025-11-4-LongCat-Flash-Omni-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-LongCat-Flash-Omni-Technical-Report","children":"[논문리뷰] LongCat-Flash-Omni Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-LongCat-Flash-Omni-Technical-Report","children":"Bin Xiao이 arXiv에 게시한 'LongCat-Flash-Omni Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-LongCat-Flash-Omni-Technical-Report"}]]}]]}],["$","article","2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment","children":"[논문리뷰] How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment","children":"Yuhao Zhai이 arXiv에 게시한 'How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment"}]]}]]}],["$","article","2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph","children":"[논문리뷰] Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph","children":"arXiv에 게시된 'Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph"}]]}]]}],["$","article","2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding","children":"[논문리뷰] GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding","children":"Wanrong Zhu이 arXiv에 게시한 'GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding"}]]}]]}],["$","article","2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation","children":"[논문리뷰] Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation","children":"arXiv에 게시된 'Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation"}]]}]]}],["$","article","2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities","children":"[논문리뷰] EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities","children":"Yunxin Liu이 arXiv에 게시한 'EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-EBT-Policy-Energy-Unlocks-Emergent-Physical-Reasoning-Capabilities"}]]}]]}],["$","article","2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench","children":"[논문리뷰] Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench","children":"arXiv에 게시된 'Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Do-Vision-Language-Models-Measure-Up-Benchmarking-Visual-Measurement-Reading-with-MeasureBench"}]]}]]}],["$","article","2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance","children":"[논문리뷰] Data-Efficient RLVR via Off-Policy Influence Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance","children":"Jiale Cheng이 arXiv에 게시한 'Data-Efficient RLVR via Off-Policy Influence Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Data-Efficient-RLVR-via-Off-Policy-Influence-Guidance"}]]}]]}],["$","article","2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence","children":"[논문리뷰] AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence","children":"Peter Worth이 arXiv에 게시한 'AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-AthenaBench-A-Dynamic-Benchmark-for-Evaluating-LLMs-in-Cyber-Threat-Intelligence"}]]}]]}],["$","article","2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models","children":"[논문리뷰] Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models","children":"Changfeng Ma이 arXiv에 게시한 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models","children":"[논문리뷰] π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models","children":"arXiv에 게시된 'π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-pi-RL-Online-RL-Fine-tuning-for-Flow-based-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning","children":"[논문리뷰] Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning","children":"Hanyang Chen이 arXiv에 게시한 'Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Visual-Backdoor-Attacks-on-MLLM-Embodied-Decision-Making-via-Contrastive-Trigger-Learning"}]]}]]}],["$","article","2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training","children":"[논문리뷰] Value Drifts: Tracing Value Alignment During LLM Post-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training","children":"arXiv에 게시된 'Value Drifts: Tracing Value Alignment During LLM Post-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Value-Drifts-Tracing-Value-Alignment-During-LLM-Post-Training"}]]}]]}],["$","article","2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning","children":"[논문리뷰] Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning","children":"arXiv에 게시된 'Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Spatial-SSRL-Enhancing-Spatial-Understanding-via-Self-Supervised-Reinforcement-Learning"}]]}]]}],["$","article","2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens","children":"[논문리뷰] SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens","children":"arXiv에 게시된 'SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-SemCoT-Accelerating-Chain-of-Thought-Reasoning-through-Semantically-Aligned-Implicit-Tokens"}]]}]]}],["$","article","2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models","children":"[논문리뷰] Revisiting Multimodal Positional Encoding in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models","children":"arXiv에 게시된 'Revisiting Multimodal Positional Encoding in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Revisiting-Multimodal-Positional-Encoding-in-Vision-Language-Models"}]]}]]}],["$","article","2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning","children":"[논문리뷰] Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning","children":"arXiv에 게시된 'Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning"}]]}]]}],["$","article","2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals","children":"[논문리뷰] Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals","children":"arXiv에 게시된 'Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Phased-DMD-Few-step-Distribution-Matching-Distillation-via-Score-Matching-within-Subintervals"}]]}]]}],["$","article","2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows","children":"[논문리뷰] OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows","children":"arXiv에 게시된 'OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-OS-Sentinel-Towards-Safety-Enhanced-Mobile-GUI-Agents-via-Hybrid-Validation-in-Realistic-Workflows"}]]}]]}],["$","article","2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games","children":"[논문리뷰] Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games","children":"cavaunpeu이 arXiv에 게시한 'Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Monopoly-Deal-A-Benchmark-Environment-for-Bounded-One-Sided-Response-Games"}]]}]]}],["$","article","2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data","children":"[논문리뷰] MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data","children":"Nadiya Shvai이 arXiv에 게시한 'MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-MisSynth-Improving-MISSCI-Logical-Fallacies-Classification-with-Synthetic-Data"}]]}]]}],["$","article","2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery","children":"[논문리뷰] Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery","children":"Oğuz Hanoğlu이 arXiv에 게시한 'Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Mask-to-Height-A-YOLOv11-Based-Architecture-for-Joint-Building-Instance-Segmentation-and-Height-Classification-from-Satellite-Imagery"}]]}]]}],["$","article","2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning","children":"[논문리뷰] Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning","children":"Nidhi Rastogi이 arXiv에 게시한 'Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning"}]]}]]}],["$","article","2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats","children":"[논문리뷰] INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats","children":"arXiv에 게시된 'INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-INT-v-s-FP-A-Comprehensive-Study-of-Fine-Grained-Low-bit-Quantization-Formats"}]]}]]}],["$","article","2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration","children":"[논문리뷰] HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration","children":"Anan Du이 arXiv에 게시한 'HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-HyperClick-Advancing-Reliable-GUI-Grounding-via-Uncertainty-Calibration"}]]}]]}],["$","article","2025-11-3-Higher-order-Linear-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Higher-order-Linear-Attention","children":"[논문리뷰] Higher-order Linear Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Higher-order-Linear-Attention","children":"arXiv에 게시된 'Higher-order Linear Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Higher-order-Linear-Attention"}]]}]]}],["$","article","2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model","children":"[논문리뷰] Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model","children":"Jinwoo Shin이 arXiv에 게시한 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Dual-Stream-Diffusion-for-World-Model-Augmented-Vision-Language-Action-Model"}]]}]]}],["$","article","2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16","children":"[논문리뷰] Defeating the Training-Inference Mismatch via FP16"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16","children":"arXiv에 게시된 'Defeating the Training-Inference Mismatch via FP16' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Defeating-the-Training-Inference-Mismatch-via-FP16"}]]}]]}],["$","article","2025-11-3-Continuous-Autoregressive-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Continuous-Autoregressive-Language-Models","children":"[논문리뷰] Continuous Autoregressive Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Continuous-Autoregressive-Language-Models","children":"arXiv에 게시된 'Continuous Autoregressive Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Continuous-Autoregressive-Language-Models"}]]}]]}],["$","article","2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification","children":"[논문리뷰] Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification","children":"Olga Russakovsky이 arXiv에 게시한 'Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-Beyond-Objects-Contextual-Synthetic-Data-Generation-for-Fine-Grained-Classification"}]]}]]}],["$","article","2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models","children":"[논문리뷰] A Survey on Efficient Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models","children":"arXiv에 게시된 'A Survey on Efficient Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:01:31+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-3-A-Survey-on-Efficient-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","children":"[논문리뷰] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation","children":"arXiv에 게시된 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-The-Quest-for-Generalizable-Motion-Generation-Data-Model-and-Evaluation"}]]}]]}],["$","article","2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models","children":"[논문리뷰] The Era of Agentic Organization: Learning to Organize with Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models","children":"Xun Wu이 arXiv에 게시한 'The Era of Agentic Organization: Learning to Organize with Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-The-Era-of-Agentic-Organization-Learning-to-Organize-with-Language-Models"}]]}]]}],["$","article","2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models","children":"[논문리뷰] The End of Manual Decoding: Towards Truly End-to-End Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models","children":"arXiv에 게시된 'The End of Manual Decoding: Towards Truly End-to-End Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-The-End-of-Manual-Decoding-Towards-Truly-End-to-End-Language-Models"}]]}]]}],["$","article","2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents","children":"[논문리뷰] Surfer 2: The Next Generation of Cross-Platform Computer Use Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents","children":"arXiv에 게시된 'Surfer 2: The Next Generation of Cross-Platform Computer Use Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents"}]]}]]}],["$","article","2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning","children":"[논문리뷰] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning","children":"arXiv에 게시된 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Supervised-Reinforcement-Learning-From-Expert-Trajectories-to-Step-wise-Reasoning"}]]}]]}],["$","article","2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work","children":"[논문리뷰] Remote Labor Index: Measuring AI Automation of Remote Work"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work","children":"Shivam Singhal이 arXiv에 게시한 'Remote Labor Index: Measuring AI Automation of Remote Work' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Remote-Labor-Index-Measuring-AI-Automation-of-Remote-Work"}]]}]]}],["$","article","2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce","children":"[논문리뷰] Performance Trade-offs of Optimizing Small Language Models for E-Commerce"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce","children":"Nikola Tankovic이 arXiv에 게시한 'Performance Trade-offs of Optimizing Small Language Models for E-Commerce' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Performance-Trade-offs-of-Optimizing-Small-Language-Models-for-E-Commerce"}]]}]]}],["$","article","2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model","children":"[논문리뷰] POWSM: A Phonetic Open Whisper-Style Speech Foundation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model","children":"arXiv에 게시된 'POWSM: A Phonetic Open Whisper-Style Speech Foundation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model"}]]}]]}],["$","article","2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree","children":"[논문리뷰] PORTool: Tool-Use LLM Training with Rewarded Tree"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree","children":"arXiv에 게시된 'PORTool: Tool-Use LLM Training with Rewarded Tree' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-PORTool-Tool-Use-LLM-Training-with-Rewarded-Tree"}]]}]]}],["$","article","2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes","children":"[논문리뷰] OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes","children":"arXiv에 게시된 'OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-OmniX-From-Unified-Panoramic-Generation-and-Perception-to-Graphics-Ready-3D-Scenes"}]]}]]}],["$","article","2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation","children":"[논문리뷰] OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation","children":"Bin Wang이 arXiv에 게시한 'OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-OmniLayout-Enabling-Coarse-to-Fine-Learning-with-LLMs-for-Universal-Document-Layout-Generation"}]]}]]}],["$","article","2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs","children":"[논문리뷰] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs","children":"arXiv에 게시된 'MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-MedVLSynther-Synthesizing-High-Quality-Visual-Question-Answering-from-Medical-Documents-with-Generator-Verifier-LMMs"}]]}]]}],["$","article","2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets","children":"[논문리뷰] Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets","children":"arXiv에 게시된 'Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Magentic-Marketplace-An-Open-Source-Environment-for-Studying-Agentic-Markets"}]]}]]}],["$","article","2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency","children":"[논문리뷰] MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency","children":"David Picard이 arXiv에 게시한 'MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-MIRO-MultI-Reward-cOnditioned-pretraining-improves-T2I-quality-and-efficiency"}]]}]]}],["$","article","2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks","children":"[논문리뷰] L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks","children":"Xenophon Evangelopoulos이 arXiv에 게시한 'L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-L2M3OF-A-Large-Language-Multimodal-Model-for-Metal-Organic-Frameworks"}]]}]]}],["$","article","2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture","children":"[논문리뷰] Kimi Linear: An Expressive, Efficient Attention Architecture"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture","children":"arXiv에 게시된 'Kimi Linear: An Expressive, Efficient Attention Architecture' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Kimi-Linear-An-Expressive-Efficient-Attention-Architecture"}]]}]]}],["$","article","2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution","children":"[논문리뷰] FullPart: Generating each 3D Part at Full Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution","children":"Chenjian Gao이 arXiv에 게시한 'FullPart: Generating each 3D Part at Full Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-FullPart-Generating-each-3D-Part-at-Full-Resolution"}]]}]]}],["$","article","2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control","children":"[논문리뷰] Exploring Conditions for Diffusion models in Robotic Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control","children":"arXiv에 게시된 'Exploring Conditions for Diffusion models in Robotic Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Exploring-Conditions-for-Diffusion-models-in-Robotic-Control"}]]}]]}],["$","article","2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation","children":"[논문리뷰] EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation","children":"arXiv에 게시된 'EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-EnzyControl-Adding-Functional-and-Substrate-Specific-Control-for-Enzyme-Backbone-Generation"}]]}]]}],["$","article","2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners","children":"[논문리뷰] Emu3.5: Native Multimodal Models are World Learners"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners","children":"arXiv에 게시된 'Emu3.5: Native Multimodal Models are World Learners' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Emu3-5-Native-Multimodal-Models-are-World-Learners"}]]}]]}],["$","article","2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis","children":"[논문리뷰] EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis","children":"arXiv에 게시된 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis"}]]}]]}],["$","article","2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing","children":"[논문리뷰] Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing","children":"Xiaowei Shi이 arXiv에 게시한 'Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Counteracting-Matthew-Effect-in-Self-Improvement-of-LVLMs-through-Head-Tail-Re-balancing"}]]}]]}],["$","article","2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning","children":"[논문리뷰] CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning","children":"Yong Li이 arXiv에 게시한 'CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-CityRiSE-Reasoning-Urban-Socio-Economic-Status-in-Vision-Language-Models-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment","children":"[논문리뷰] ChartAB: A Benchmark for Chart Grounding & Dense Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment","children":"arXiv에 게시된 'ChartAB: A Benchmark for Chart Grounding & Dense Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-ChartAB-A-Benchmark-for-Chart-Grounding-Dense-Alignment"}]]}]]}],["$","article","2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games","children":"[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games","children":"Justin Cui이 arXiv에 게시한 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games"}]]}]]}],["$","article","2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark","children":"[논문리뷰] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark","children":"arXiv에 게시된 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-CRAG-MM-Multi-modal-Multi-turn-Comprehensive-RAG-Benchmark"}]]}]]}],["$","article","2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs","children":"[논문리뷰] CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs","children":"arXiv에 게시된 'CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-CLASS-IT-Conversational-and-Lecture-Aligned-Small-Scale-Instruction-Tuning-for-BabyLMs"}]]}]]}],["$","article","2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark","children":"[논문리뷰] Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark","children":"arXiv에 게시된 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-Are-Video-Models-Ready-as-Zero-Shot-Reasoners-An-Empirical-Study-with-the-MME-CoF-Benchmark"}]]}]]}],["$","article","2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions","children":"[논문리뷰] AMO-Bench: Large Language Models Still Struggle in High School Math Competitions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions","children":"arXiv에 게시된 'AMO-Bench: Large Language Models Still Struggle in High School Math Competitions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-31 18:37:31+0900","children":"2025년 10월 31일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-31-AMO-Bench-Large-Language-Models-Still-Struggle-in-High-School-Math-Competitions"}]]}]]}],["$","article","2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning","children":"[논문리뷰] Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning","children":"Runhao Fu이 arXiv에 게시한 'Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","children":"[논문리뷰] VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning","children":"Xiaoyu Shi이 arXiv에 게시한 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-VFXMaster-Unlocking-Dynamic-Visual-Effect-Generation-via-In-Context-Learning"}]]}]]}],["$","article","2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling","children":"[논문리뷰] TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling","children":"Zheng Zhang이 arXiv에 게시한 'TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling"}]]}]]}],["$","article","2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution","children":"[논문리뷰] The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution","children":"Haoze Wu이 arXiv에 게시한 'The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-The-Tool-Decathlon-Benchmarking-Language-Agents-for-Diverse-Realistic-and-Long-Horizon-Task-Execution"}]]}]]}],["$","article","2025-10-30-The-Principles-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models","children":"[논문리뷰] The Principles of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models","children":"Stefano Ermon이 arXiv에 게시한 'The Principles of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-The-Principles-of-Diffusion-Models"}]]}]]}],["$","article","2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs","children":"[논문리뷰] SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs","children":"Jiaxuan You이 arXiv에 게시한 'SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs"}]]}]]}],["$","article","2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models","children":"[논문리뷰] Scaling Latent Reasoning via Looped Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models","children":"arXiv에 게시된 'Scaling Latent Reasoning via Looped Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Scaling-Latent-Reasoning-via-Looped-Language-Models"}]]}]]}],["$","article","2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks","children":"[논문리뷰] Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks","children":"arXiv에 게시된 'Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks"}]]}]]}],["$","article","2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing","children":"[논문리뷰] RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing","children":"Peng Ye이 arXiv에 게시한 'RegionE: Adaptive Region-Aware Generation for Efficient Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-RegionE-Adaptive-Region-Aware-Generation-for-Efficient-Image-Editing"}]]}]]}],["$","article","2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining","children":"[논문리뷰] Reasoning-Aware GRPO using Process Mining"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining","children":"arXiv에 게시된 'Reasoning-Aware GRPO using Process Mining' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining"}]]}]]}],["$","article","2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization","children":"[논문리뷰] ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization","children":"Ruihua Song이 arXiv에 게시한 'ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization"}]]}]]}],["$","article","2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling","children":"[논문리뷰] Parallel Loop Transformer for Efficient Test-Time Computation Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling","children":"arXiv에 게시된 'Parallel Loop Transformer for Efficient Test-Time Computation Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling"}]]}]]}],["$","article","2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models","children":"[논문리뷰] PairUni: Pairwise Training for Unified Multimodal Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models","children":"arXiv에 게시된 'PairUni: Pairwise Training for Unified Multimodal Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-PairUni-Pairwise-Training-for-Unified-Multimodal-Language-Models"}]]}]]}],["$","article","2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design","children":"[논문리뷰] ODesign: A World Model for Biomolecular Interaction Design"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design","children":"Qinghan Wang이 arXiv에 게시한 'ODesign: A World Model for Biomolecular Interaction Design' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-ODesign-A-World-Model-for-Biomolecular-Interaction-Design"}]]}]]}],["$","article","2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks","children":"[논문리뷰] Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks","children":"arXiv에 게시된 'Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks"}]]}]]}],["$","article","2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation","children":"[논문리뷰] Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation","children":"arXiv에 게시된 'Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation"}]]}]]}],["$","article","2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model","children":"[논문리뷰] MASPRM: Multi-Agent System Process Reward Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model","children":"Ying Xiong이 arXiv에 게시한 'MASPRM: Multi-Agent System Process Reward Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-MASPRM-Multi-Agent-System-Process-Reward-Model"}]]}]]}],["$","article","2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence","children":"[논문리뷰] JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence","children":"arXiv에 게시된 'JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence"}]]}]]}],["$","article","2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite","children":"[논문리뷰] Gaperon: A Peppered English-French Generative Language Model Suite"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite","children":"Éric de la Clergerie이 arXiv에 게시한 'Gaperon: A Peppered English-French Generative Language Model Suite' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Gaperon-A-Peppered-English-French-Generative-Language-Model-Suite"}]]}]]}],["$","article","2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus","children":"[논문리뷰] Fortytwo: Swarm Inference with Peer-Ranked Consensus"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus","children":"arXiv에 게시된 'Fortytwo: Swarm Inference with Peer-Ranked Consensus' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Fortytwo-Swarm-Inference-with-Peer-Ranked-Consensus"}]]}]]}],["$","article","2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning","children":"[논문리뷰] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning","children":"Xin Liu이 arXiv에 게시한 'FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning"}]]}]]}],["$","article","2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment","children":"[논문리뷰] Evolving Diagnostic Agents in a Virtual Clinical Environment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment","children":"arXiv에 게시된 'Evolving Diagnostic Agents in a Virtual Clinical Environment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment"}]]}]]}],["$","article","2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks","children":"[논문리뷰] ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks","children":"arXiv에 게시된 'ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-ChronoPlay-A-Framework-for-Modeling-Dual-Dynamics-and-Authenticity-in-Game-RAG-Benchmarks"}]]}]]}],["$","article","2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains","children":"[논문리뷰] BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains","children":"arXiv에 게시된 'BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-BhashaBench-V1-A-Comprehensive-Benchmark-for-the-Quadrant-of-Indic-Domains"}]]}]]}],["$","article","2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking","children":"[논문리뷰] WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking","children":"arXiv에 게시된 'WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-WebLeaper-Empowering-Efficiency-and-Efficacy-in-WebAgent-via-Enabling-Info-Rich-Seeking"}]]}]]}],["$","article","2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations","children":"[논문리뷰] VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations","children":"Jiayi Zhang이 arXiv에 게시한 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-VisJudge-Bench-Aesthetics-and-Quality-Assessment-of-Visualizations"}]]}]]}],["$","article","2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents","children":"[논문리뷰] VisCoder2: Building Multi-Language Visualization Coding Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents","children":"arXiv에 게시된 'VisCoder2: Building Multi-Language Visualization Coding Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents"}]]}]]}],["$","article","2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set","children":"[논문리뷰] VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set","children":"arXiv에 게시된 'VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-VL-SAE-Interpreting-and-Enhancing-Vision-Language-Alignment-with-a-Unified-Concept-Set"}]]}]]}],["$","article","2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation","children":"[논문리뷰] Uniform Discrete Diffusion with Metric Path for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation","children":"arXiv에 게시된 'Uniform Discrete Diffusion with Metric Path for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Uniform-Discrete-Diffusion-with-Metric-Path-for-Video-Generation"}]]}]]}],["$","article","2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset","children":"[논문리뷰] UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset","children":"arXiv에 게시된 'UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset"}]]}]]}],["$","article","2025-10-29-Tongyi-DeepResearch-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Tongyi-DeepResearch-Technical-Report","children":"[논문리뷰] Tongyi DeepResearch Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Tongyi-DeepResearch-Technical-Report","children":"arXiv에 게시된 'Tongyi DeepResearch Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Tongyi-DeepResearch-Technical-Report"}]]}]]}],["$","article","2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence","children":"[논문리뷰] STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence","children":"arXiv에 게시된 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-STAR-Bench-Probing-Deep-Spatio-Temporal-Reasoning-as-Audio-4D-Intelligence"}]]}]]}],["$","article","2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance","children":"[논문리뷰] Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance","children":"arXiv에 게시된 'Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance"}]]}]]}],["$","article","2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context","children":"[논문리뷰] RoboOmni: Proactive Robot Manipulation in Omni-modal Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context","children":"arXiv에 게시된 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-RoboOmni-Proactive-Robot-Manipulation-in-Omni-modal-Context"}]]}]]}],["$","article","2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining","children":"[논문리뷰] Rethinking Visual Intelligence: Insights from Video Pretraining"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining","children":"Ahmad Rahimi이 arXiv에 게시한 'Rethinking Visual Intelligence: Insights from Video Pretraining' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Rethinking-Visual-Intelligence-Insights-from-Video-Pretraining"}]]}]]}],["$","article","2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision","children":"[논문리뷰] Repurposing Synthetic Data for Fine-grained Search Agent Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision","children":"arXiv에 게시된 'Repurposing Synthetic Data for Fine-grained Search Agent Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision"}]]}]]}],["$","article","2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers","children":"[논문리뷰] ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers","children":"Ian L. V. Roque이 arXiv에 게시한 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers"}]]}]]}],["$","article","2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding","children":"[논문리뷰] PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding","children":"Denis Cavallucci이 arXiv에 게시한 'PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-PatenTEB-A-Comprehensive-Benchmark-and-Model-Family-for-Patent-Text-Embedding"}]]}]]}],["$","article","2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding","children":"[논문리뷰] PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding","children":"Lan Xu이 arXiv에 게시한 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding"}]]}]]}],["$","article","2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking","children":"[논문리뷰] ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking","children":"arXiv에 게시된 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-ParallelMuse-Agentic-Parallel-Thinking-for-Deep-Information-Seeking"}]]}]]}],["$","article","2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents","children":"[논문리뷰] OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents","children":"arXiv에 게시된 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-OSWorld-MCP-Benchmarking-MCP-Tool-Invocation-In-Computer-Use-Agents"}]]}]]}],["$","article","2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs","children":"[논문리뷰] Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs","children":"arXiv에 게시된 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs"}]]}]]}],["$","article","2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries","children":"[논문리뷰] InteractComp: Evaluating Search Agents With Ambiguous Queries"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries","children":"Yani Fan이 arXiv에 게시한 'InteractComp: Evaluating Search Agents With Ambiguous Queries' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-InteractComp-Evaluating-Search-Agents-With-Ambiguous-Queries"}]]}]]}],["$","article","2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing","children":"[논문리뷰] Group Relative Attention Guidance for Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing","children":"arXiv에 게시된 'Group Relative Attention Guidance for Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Group-Relative-Attention-Guidance-for-Image-Editing"}]]}]]}],["$","article","2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering","children":"[논문리뷰] Generalization or Memorization: Dynamic Decoding for Mode Steering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering","children":"arXiv에 게시된 'Generalization or Memorization: Dynamic Decoding for Mode Steering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Generalization-or-Memorization-Dynamic-Decoding-for-Mode-Steering"}]]}]]}],["$","article","2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents","children":"[논문리뷰] Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents","children":"arXiv에 게시된 'Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Game-TARS-Pretrained-Foundation-Models-for-Scalable-Generalist-Multimodal-Game-Agents"}]]}]]}],["$","article","2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling","children":"[논문리뷰] FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling","children":"arXiv에 게시된 'FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-FunReason-MT-Technical-Report-Overcoming-the-Complexity-Barrier-in-Multi-Turn-Function-Calling"}]]}]]}],["$","article","2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors","children":"[논문리뷰] From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors","children":"arXiv에 게시된 'From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-From-Spatial-to-Actions-Grounding-Vision-Language-Action-Model-in-Spatial-Foundation-Priors"}]]}]]}],["$","article","2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning","children":"[논문리뷰] Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning","children":"arXiv에 게시된 'Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-Critique-RL-Training-Language-Models-for-Critiquing-through-Two-Stage-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis","children":"[논문리뷰] AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis","children":"arXiv에 게시된 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-AgentFrontier-Expanding-the-Capability-Frontier-of-LLM-Agents-with-ZPD-Guided-Data-Synthesis"}]]}]]}],["$","article","2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management","children":"[논문리뷰] AgentFold: Long-Horizon Web Agents with Proactive Context Management"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management","children":"arXiv에 게시된 'AgentFold: Long-Horizon Web Agents with Proactive Context Management' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-AgentFold-Long-Horizon-Web-Agents-with-Proactive-Context-Management"}]]}]]}],["$","article","2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality","children":"[논문리뷰] ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality","children":"arXiv에 게시된 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-29 13:11:02+0900","children":"2025년 10월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-29-ATLAS-Adaptive-Transfer-Scaling-Laws-for-Multilingual-Pretraining-Finetuning-and-Decoding-the-Curse-of-Multilinguality"}]]}]]}],["$","article","2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields","children":"[논문리뷰] VoMP: Predicting Volumetric Mechanical Property Fields"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields","children":"arXiv에 게시된 'VoMP: Predicting Volumetric Mechanical Property Fields' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-VoMP-Predicting-Volumetric-Mechanical-Property-Fields"}]]}]]}],["$","article","2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting","children":"[논문리뷰] VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting","children":"Haihan Gao이 arXiv에 게시한 'VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-VITA-E-Natural-Embodied-Interaction-with-Concurrent-Seeing-Hearing-Speaking-and-Acting"}]]}]]}],["$","article","2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling","children":"[논문리뷰] Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling","children":"Igor Gilitschenski이 arXiv에 게시한 'Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Track-Inpaint-Resplat-Subject-driven-3D-and-4D-Generation-with-Progressive-Texture-Infilling"}]]}]]}],["$","article","2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation","children":"[논문리뷰] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation","children":"arXiv에 게시된 'The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation"}]]}]]}],["$","article","2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation","children":"[논문리뷰] RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation","children":"Kuan-Hsun Tu이 arXiv에 게시한 'RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-RobotArena-infty-Scalable-Robot-Benchmarking-via-Real-to-Sim-Translation"}]]}]]}],["$","article","2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control","children":"[논문리뷰] ReCode: Unify Plan and Action for Universal Granularity Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control","children":"Yifan Wu이 arXiv에 게시한 'ReCode: Unify Plan and Action for Universal Granularity Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-ReCode-Unify-Plan-and-Action-for-Universal-Granularity-Control"}]]}]]}],["$","article","2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity","children":"[논문리뷰] PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity","children":"Kehan Li이 arXiv에 게시한 'PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-PixelRefer-A-Unified-Framework-for-Spatio-Temporal-Object-Referring-with-Arbitrary-Granularity"}]]}]]}],["$","article","2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences","children":"[논문리뷰] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences","children":"arXiv에 게시된 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Omni-Reward-Towards-Generalist-Omni-Modal-Reward-Modeling-with-Free-Form-Preferences"}]]}]]}],["$","article","2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS","children":"[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS","children":"arXiv에 게시된 'Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS"}]]}]]}],["$","article","2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling","children":"[논문리뷰] Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling","children":"arXiv에 게시된 'Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Memory-based-Language-Models-An-Efficient-Explainable-and-Eco-friendly-Approach-to-Large-Language-Modeling"}]]}]]}],["$","article","2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices","children":"[논문리뷰] MARS-M: When Variance Reduction Meets Matrices"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices","children":"arXiv에 게시된 'MARS-M: When Variance Reduction Meets Matrices' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-MARS-M-When-Variance-Reduction-Meets-Matrices"}]]}]]}],["$","article","2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation","children":"[논문리뷰] Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation","children":"Honglie Chen이 arXiv에 게시한 'Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Lookahead-Anchoring-Preserving-Character-Identity-in-Audio-Driven-Human-Animation"}]]}]]}],["$","article","2025-10-28-LongCat-Video-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LongCat-Video-Technical-Report","children":"[논문리뷰] LongCat-Video Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LongCat-Video-Technical-Report","children":"Hongyu Li이 arXiv에 게시한 'LongCat-Video Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-LongCat-Video-Technical-Report"}]]}]]}],["$","article","2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking","children":"[논문리뷰] LimRank: Less is More for Reasoning-Intensive Information Reranking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking","children":"Arman Cohan이 arXiv에 게시한 'LimRank: Less is More for Reasoning-Intensive Information Reranking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking"}]]}]]}],["$","article","2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation","children":"[논문리뷰] LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation","children":"Chaorui Deng이 arXiv에 게시한 'LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-LightBagel-A-Light-weighted-Double-Fusion-Framework-for-Unified-Multimodal-Understanding-and-Generation"}]]}]]}],["$","article","2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards","children":"[논문리뷰] Language Server CLI Empowers Language Agents with Process Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards","children":"Lanser Contributors이 arXiv에 게시한 'Language Server CLI Empowers Language Agents with Process Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Language-Server-CLI-Empowers-Language-Agents-with-Process-Rewards"}]]}]]}],["$","article","2025-10-28-Knocking-Heads-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Knocking-Heads-Attention","children":"[논문리뷰] Knocking-Heads Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Knocking-Heads-Attention","children":"Jianguo Li이 arXiv에 게시한 'Knocking-Heads Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Knocking-Heads-Attention"}]]}]]}],["$","article","2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction","children":"[논문리뷰] IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction","children":"Fangzhou Hong이 arXiv에 게시한 'IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-IGGT-Instance-Grounded-Geometry-Transformer-for-Semantic-3D-Reconstruction"}]]}]]}],["$","article","2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels","children":"[논문리뷰] FARMER: Flow AutoRegressive Transformer over Pixels"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels","children":"Zhijie Lin이 arXiv에 게시한 'FARMER: Flow AutoRegressive Transformer over Pixels' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-FARMER-Flow-AutoRegressive-Transformer-over-Pixels"}]]}]]}],["$","article","2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization","children":"[논문리뷰] EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization","children":"Yaxing Wang이 arXiv에 게시한 'EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization"}]]}]]}],["$","article","2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker","children":"[논문리뷰] E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker","children":"arXiv에 게시된 'E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-E2Rank-Your-Text-Embedding-can-Also-be-an-Effective-and-Efficient-Listwise-Reranker"}]]}]]}],["$","article","2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation","children":"[논문리뷰] Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation","children":"Guohao Dai이 arXiv에 게시한 'Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Distilled-Decoding-2-One-step-Sampling-of-Image-Auto-regressive-Models-with-Conditional-Score-Distillation"}]]}]]}],["$","article","2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection","children":"[논문리뷰] DiffusionLane: Diffusion Model for Lane Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection","children":"arXiv에 게시된 'DiffusionLane: Diffusion Model for Lane Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-DiffusionLane-Diffusion-Model-for-Lane-Detection"}]]}]]}],["$","article","2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations","children":"[논문리뷰] Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations","children":"arXiv에 게시된 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Concerto-Joint-2D-3D-Self-Supervised-Learning-Emerges-Spatial-Representations"}]]}]]}],["$","article","2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback","children":"[논문리뷰] Code Aesthetics with Agentic Reward Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback","children":"Yupan Huang이 arXiv에 게시한 'Code Aesthetics with Agentic Reward Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback"}]]}]]}],["$","article","2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models","children":"[논문리뷰] ACG: Action Coherence Guidance for Flow-based VLA models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models","children":"arXiv에 게시된 'ACG: Action Coherence Guidance for Flow-based VLA models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-ACG-Action-Coherence-Guidance-for-Flow-based-VLA-models"}]]}]]}],["$","article","2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype","children":"[논문리뷰] A Survey of Data Agents: Emerging Paradigm or Overstated Hype?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype","children":"Boyan Li이 arXiv에 게시한 'A Survey of Data Agents: Emerging Paradigm or Overstated Hype?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-28 13:07:54+0900","children":"2025년 10월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-28-A-Survey-of-Data-Agents-Emerging-Paradigm-or-Overstated-Hype"}]]}]]}],["$","article","2025-10-27-WorldGrow-Generating-Infinite-3D-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-WorldGrow-Generating-Infinite-3D-World","children":"[논문리뷰] WorldGrow: Generating Infinite 3D World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-WorldGrow-Generating-Infinite-3D-World","children":"Jia Lu이 arXiv에 게시한 'WorldGrow: Generating Infinite 3D World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-WorldGrow-Generating-Infinite-3D-World"}]]}]]}],["$","article","2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers","children":"[논문리뷰] Visual Diffusion Models are Geometric Solvers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers","children":"Or Patashnik이 arXiv에 게시한 'Visual Diffusion Models are Geometric Solvers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Visual-Diffusion-Models-are-Geometric-Solvers"}]]}]]}],["$","article","2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation","children":"[논문리뷰] Video-As-Prompt: Unified Semantic Control for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation","children":"arXiv에 게시된 'Video-As-Prompt: Unified Semantic Control for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Video-As-Prompt-Unified-Semantic-Control-for-Video-Generation"}]]}]]}],["$","article","2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning","children":"[논문리뷰] UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning","children":"arXiv에 게시된 'UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning"}]]}]]}],["$","article","2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation","children":"[논문리뷰] Taming Modality Entanglement in Continual Audio-Visual Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation","children":"Zhaojin Fu이 arXiv에 게시한 'Taming Modality Entanglement in Continual Audio-Visual Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Taming-Modality-Entanglement-in-Continual-Audio-Visual-Segmentation"}]]}]]}],["$","article","2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers","children":"[논문리뷰] Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers","children":"arXiv에 게시된 'Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers"}]]}]]}],["$","article","2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation","children":"[논문리뷰] Sparser Block-Sparse Attention via Token Permutation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation","children":"arXiv에 게시된 'Sparser Block-Sparse Attention via Token Permutation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Sparser-Block-Sparse-Attention-via-Token-Permutation"}]]}]]}],["$","article","2025-10-27-Soft-Instruction-De-escalation-Defense",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense","children":"[논문리뷰] Soft Instruction De-escalation Defense"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense","children":"arXiv에 게시된 'Soft Instruction De-escalation Defense' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense"}]]}]]}],["$","article","2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation","children":"[논문리뷰] Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation","children":"arXiv에 게시된 'Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Sample-By-Step-Optimize-By-Chunk-Chunk-Level-GRPO-For-Text-to-Image-Generation"}]]}]]}],["$","article","2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think","children":"[논문리뷰] Reasoning with Sampling: Your Base Model is Smarter Than You Think"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think","children":"arXiv에 게시된 'Reasoning with Sampling: Your Base Model is Smarter Than You Think' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Reasoning-with-Sampling-Your-Base-Model-is-Smarter-Than-You-Think"}]]}]]}],["$","article","2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging","children":"[논문리뷰] RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging","children":"arXiv에 게시된 'RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-RECALL-REpresentation-aligned-Catastrophic-forgetting-ALLeviation-via-Hierarchical-Model-Merging"}]]}]]}],["$","article","2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling","children":"[논문리뷰] RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling","children":"arXiv에 게시된 'RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling"}]]}]]}],["$","article","2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis","children":"[논문리뷰] PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis","children":"Hui Li이 arXiv에 게시한 'PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-PhysWorld-From-Real-Videos-to-World-Models-of-Deformable-Objects-via-Physics-Aware-Demonstration-Synthesis"}]]}]]}],["$","article","2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments","children":"[논문리뷰] PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments","children":"Chaoyang Zhao이 arXiv에 게시한 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments"}]]}]]}],["$","article","2025-10-27-Model-Merging-with-Functional-Dual-Anchors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Model-Merging-with-Functional-Dual-Anchors","children":"[논문리뷰] Model Merging with Functional Dual Anchors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Model-Merging-with-Functional-Dual-Anchors","children":"arXiv에 게시된 'Model Merging with Functional Dual Anchors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Model-Merging-with-Functional-Dual-Anchors"}]]}]]}],["$","article","2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs","children":"[논문리뷰] Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs","children":"Bohyung Han이 arXiv에 게시한 'Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Map-the-Flow-Revealing-Hidden-Pathways-of-Information-in-VideoLLMs"}]]}]]}],["$","article","2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model","children":"[논문리뷰] From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model","children":"arXiv에 게시된 'From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model"}]]}]]}],["$","article","2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video","children":"[논문리뷰] Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video","children":"arXiv에 게시된 'Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Foley-Control-Aligning-a-Frozen-Latent-Text-to-Audio-Model-to-Video"}]]}]]}],["$","article","2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory","children":"[논문리뷰] Document Understanding, Measurement, and Manipulation Using Category Theory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory","children":"arXiv에 게시된 'Document Understanding, Measurement, and Manipulation Using Category Theory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory"}]]}]]}],["$","article","2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets","children":"[논문리뷰] DeepAgent: A General Reasoning Agent with Scalable Toolsets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets","children":"Jiajie Jin이 arXiv에 게시한 'DeepAgent: A General Reasoning Agent with Scalable Toolsets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets"}]]}]]}],["$","article","2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite","children":"[논문리뷰] AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite","children":"Bhavana Dalvi이 arXiv에 게시한 'AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-AstaBench-Rigorous-Benchmarking-of-AI-Agents-with-a-Scientific-Research-Suite"}]]}]]}],["$","article","2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost","children":"[논문리뷰] Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost","children":"Min Yang이 arXiv에 게시한 'Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost"}]]}]]}],["$","article","2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models","children":"[논문리뷰] ARC-Encoder: learning compressed text representations for large language models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models","children":"arXiv에 게시된 'ARC-Encoder: learning compressed text representations for large language models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models"}]]}]]}],["$","article","2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata","children":"[논문리뷰] ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata","children":"José C. Cabaleiro이 arXiv에 게시한 'ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-ALICE-LRI-A-General-Method-for-Lossless-Range-Image-Generation-for-Spinning-LiDAR-Sensors-without-Calibration-Metadata"}]]}]]}],["$","article","2025-10-27-A-Definition-of-AGI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-A-Definition-of-AGI","children":"[논문리뷰] A Definition of AGI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-A-Definition-of-AGI","children":"Yarin Gal이 arXiv에 게시한 'A Definition of AGI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-A-Definition-of-AGI"}]]}]]}],["$","article","2025-10-24-Thought-Communication-in-Multiagent-Collaboration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Thought-Communication-in-Multiagent-Collaboration","children":"[논문리뷰] Thought Communication in Multiagent Collaboration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Thought-Communication-in-Multiagent-Collaboration","children":"Mingze Gao이 arXiv에 게시한 'Thought Communication in Multiagent Collaboration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Thought-Communication-in-Multiagent-Collaboration"}]]}]]}],["$","article","2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB","children":"[논문리뷰] The Massive Legal Embedding Benchmark (MLEB)"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB","children":"arXiv에 게시된 'The Massive Legal Embedding Benchmark (MLEB)' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-The-Massive-Legal-Embedding-Benchmark-MLEB"}]]}]]}],["$","article","2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets","children":"[논문리뷰] Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets","children":"arXiv에 게시된 'Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Seed3D-1-0-From-Images-to-High-Fidelity-Simulation-Ready-3D-Assets"}]]}]]}],["$","article","2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision","children":"[논문리뷰] Search Self-play: Pushing the Frontier of Agent Capability without Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision","children":"arXiv에 게시된 'Search Self-play: Pushing the Frontier of Agent Capability without Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Search-Self-play-Pushing-the-Frontier-of-Agent-Capability-without-Supervision"}]]}]]}],["$","article","2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models","children":"[논문리뷰] SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models","children":"arXiv에 게시된 'SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-SAKE-Towards-Editing-Auditory-Attribute-Knowledge-of-Large-Audio-Language-Models"}]]}]]}],["$","article","2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence","children":"[논문리뷰] Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence","children":"arXiv에 게시된 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence"}]]}]]}],["$","article","2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall","children":"[논문리뷰] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall","children":"Sungjin Ahn이 arXiv에 게시한 'Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Loopholing-Discrete-Diffusion-Deterministic-Bypass-of-the-Sampling-Wall"}]]}]]}],["$","article","2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas","children":"[논문리뷰] LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas","children":"arXiv에 게시된 'LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-LayerComposer-Interactive-Personalized-T2I-via-Spatially-Aware-Layered-Canvas"}]]}]]}],["$","article","2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations","children":"[논문리뷰] Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations","children":"arXiv에 게시된 'Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Investigating-Safety-Vulnerabilities-of-Large-Audio-Language-Models-Under-Speaker-Emotional-Variations"}]]}]]}],["$","article","2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases","children":"[논문리뷰] ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases","children":"Nicholas Carlini이 arXiv에 게시한 'ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-ImpossibleBench-Measuring-LLMs-Propensity-of-Exploiting-Test-Cases"}]]}]]}],["$","article","2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1","children":"[논문리뷰] Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1","children":"arXiv에 게시된 'Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1"}]]}]]}],["$","article","2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives","children":"[논문리뷰] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives","children":"arXiv에 게시된 'HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives"}]]}]]}],["$","article","2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models","children":"[논문리뷰] From Masks to Worlds: A Hitchhiker's Guide to World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models","children":"Shufan Li이 arXiv에 게시한 'From Masks to Worlds: A Hitchhiker's Guide to World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-From-Masks-to-Worlds-A-Hitchhikers-Guide-to-World-Models"}]]}]]}],["$","article","2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values","children":"[논문리뷰] Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values","children":"arXiv에 게시된 'Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values"}]]}]]}],["$","article","2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models","children":"[논문리뷰] Emergence of Linear Truth Encodings in Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models","children":"Alberto Bietti이 arXiv에 게시한 'Emergence of Linear Truth Encodings in Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Emergence-of-Linear-Truth-Encodings-in-Language-Models"}]]}]]}],["$","article","2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion","children":"[논문리뷰] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion","children":"arXiv에 게시된 'DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion"}]]}]]}],["$","article","2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding","children":"[논문리뷰] Diff-XYZ: A Benchmark for Evaluating Diff Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding","children":"arXiv에 게시된 'Diff-XYZ: A Benchmark for Evaluating Diff Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Diff-XYZ-A-Benchmark-for-Evaluating-Diff-Understanding"}]]}]]}],["$","article","2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence","children":"[논문리뷰] Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence","children":"arXiv에 게시된 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence"}]]}]]}],["$","article","2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature","children":"[논문리뷰] ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature","children":"arXiv에 게시된 'ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-ComProScanner-A-multi-agent-based-framework-for-composition-property-structured-data-extraction-from-scientific-literature"}]]}]]}],["$","article","2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models","children":"[논문리뷰] AlphaFlow: Understanding and Improving MeanFlow Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models","children":"arXiv에 게시된 'AlphaFlow: Understanding and Improving MeanFlow Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-AlphaFlow-Understanding-and-Improving-MeanFlow-Models"}]]}]]}],["$","article","2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders","children":"[논문리뷰] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders","children":"arXiv에 게시된 'AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-AdaSPEC-Selective-Knowledge-Distillation-for-Efficient-Speculative-Decoders"}]]}]]}],["$","article","2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model","children":"[논문리뷰] ARGenSeg: Image Segmentation with Autoregressive Image Generation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model","children":"arXiv에 게시된 'ARGenSeg: Image Segmentation with Autoregressive Image Generation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model"}]]}]]}],["$","article","2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR","children":"[논문리뷰] olmOCR 2: Unit Test Rewards for Document OCR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR","children":"arXiv에 게시된 'olmOCR 2: Unit Test Rewards for Document OCR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-olmOCR-2-Unit-Test-Rewards-for-Document-OCR"}]]}]]}],["$","article","2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos","children":"[논문리뷰] VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos","children":"Xinyuan Wang이 arXiv에 게시한 'VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos"}]]}]]}],["$","article","2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models","children":"[논문리뷰] Unified Reinforcement and Imitation Learning for Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models","children":"arXiv에 게시된 'Unified Reinforcement and Imitation Learning for Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Unified-Reinforcement-and-Imitation-Learning-for-Vision-Language-Models"}]]}]]}],["$","article","2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling","children":"[논문리뷰] RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling","children":"Mandip Goswami이 arXiv에 게시한 'RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-RIR-Mega-a-large-scale-simulated-room-impulse-response-dataset-for-machine-learning-and-room-acoustics-modeling"}]]}]]}],["$","article","2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge","children":"[논문리뷰] ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge","children":"arXiv에 게시된 'ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge"}]]}]]}],["$","article","2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing","children":"[논문리뷰] Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing","children":"arXiv에 게시된 'Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Pico-Banana-400K-A-Large-Scale-Dataset-for-Text-Guided-Image-Editing"}]]}]]}],["$","article","2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models","children":"[논문리뷰] OmniNWM: Omniscient Driving Navigation World Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models","children":"Zhujin Liang이 arXiv에 게시한 'OmniNWM: Omniscient Driving Navigation World Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-OmniNWM-Omniscient-Driving-Navigation-World-Models"}]]}]]}],["$","article","2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks","children":"[논문리뷰] Machine Text Detectors are Membership Inference Attacks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks","children":"Naoaki Okazaki이 arXiv에 게시한 'Machine Text Detectors are Membership Inference Attacks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks"}]]}]]}],["$","article","2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models","children":"[논문리뷰] MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models","children":"Yifan Gao이 arXiv에 게시한 'MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-MINED-Probing-and-Updating-with-Multimodal-Time-Sensitive-Knowledge-for-Large-Multimodal-Models"}]]}]]}],["$","article","2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts","children":"[논문리뷰] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts","children":"arXiv에 게시된 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts"}]]}]]}],["$","article","2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection","children":"[논문리뷰] Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection","children":"Yi Cheng이 arXiv에 게시한 'Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection"}]]}]]}],["$","article","2025-10-23-Language-Models-are-Injective-and-Hence-Invertible",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Language-Models-are-Injective-and-Hence-Invertible","children":"[논문리뷰] Language Models are Injective and Hence Invertible"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Language-Models-are-Injective-and-Hence-Invertible","children":"arXiv에 게시된 'Language Models are Injective and Hence Invertible' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Language-Models-are-Injective-and-Hence-Invertible"}]]}]]}],["$","article","2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints","children":"[논문리뷰] KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints","children":"Jinhe Bi이 arXiv에 게시한 'KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-KORE-Enhancing-Knowledge-Injection-for-Large-Multimodal-Models-via-Knowledge-Oriented-Augmentations-and-Constraints"}]]}]]}],["$","article","2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model","children":"[논문리뷰] GigaBrain-0: A World Model-Powered Vision-Language-Action Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model","children":"arXiv에 게시된 'GigaBrain-0: A World Model-Powered Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model"}]]}]]}],["$","article","2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models","children":"[논문리뷰] From Charts to Code: A Hierarchical Benchmark for Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models","children":"Dongxing Mao이 arXiv에 게시한 'From Charts to Code: A Hierarchical Benchmark for Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-From-Charts-to-Code-A-Hierarchical-Benchmark-for-Multimodal-Models"}]]}]]}],["$","article","2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research","children":"[논문리뷰] FinSight: Towards Real-World Financial Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research","children":"Yutao Zhu이 arXiv에 게시한 'FinSight: Towards Real-World Financial Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research"}]]}]]}],["$","article","2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","children":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","children":"arXiv에 게시된 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning"}]]}]]}],["$","article","2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs","children":"[논문리뷰] Directional Reasoning Injection for Fine-Tuning MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs","children":"Jialian Wu이 arXiv에 게시한 'Directional Reasoning Injection for Fine-Tuning MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs"}]]}]]}],["$","article","2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models","children":"[논문리뷰] DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models","children":"Roi Reichart이 arXiv에 게시한 'DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-DeLeaker-Dynamic-Inference-Time-Reweighting-For-Semantic-Leakage-Mitigation-in-Text-to-Image-Models"}]]}]]}],["$","article","2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents","children":"[논문리뷰] DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents","children":"arXiv에 게시된 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-DaMo-Data-Mixing-Optimizer-in-Fine-tuning-Multimodal-LLMs-for-Mobile-Phone-Agents"}]]}]]}],["$","article","2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent","children":"[논문리뷰] ColorAgent: Building A Robust, Personalized, and Interactive OS Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent","children":"Weiming Zhang이 arXiv에 게시한 'ColorAgent: Building A Robust, Personalized, and Interactive OS Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent"}]]}]]}],["$","article","2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping","children":"[논문리뷰] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping","children":"Junrui Shen이 arXiv에 게시한 'BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping"}]]}]]}],["$","article","2025-10-23-Attention-Sinks-in-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Attention-Sinks-in-Diffusion-Language-Models","children":"[논문리뷰] Attention Sinks in Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-Attention-Sinks-in-Diffusion-Language-Models","children":"Simone Scardapane이 arXiv에 게시한 'Attention Sinks in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-Attention-Sinks-in-Diffusion-Language-Models"}]]}]]}],["$","article","2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library","children":"[논문리뷰] AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library","children":"Chonghe Jiang이 arXiv에 게시한 'AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library"}]]}]]}],["$","article","2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World","children":"[논문리뷰] World-in-World: World Models in a Closed-Loop World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World","children":"Arda Uzunoglu이 arXiv에 게시한 'World-in-World: World Models in a Closed-Loop World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-World-in-World-World-Models-in-a-Closed-Loop-World"}]]}]]}],["$","article","2025-10-22-Video-Reasoning-without-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Video-Reasoning-without-Training","children":"[논문리뷰] Video Reasoning without Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Video-Reasoning-without-Training","children":"arXiv에 게시된 'Video Reasoning without Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-Video-Reasoning-without-Training"}]]}]]}],["$","article","2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism","children":"[논문리뷰] Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism","children":"Shuang Gu이 arXiv에 게시한 'Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism"}]]}]]}],["$","article","2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation","children":"[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation","children":"Yujie Zhou이 arXiv에 게시한 'UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation"}]]}]]}],["$","article","2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention","children":"[논문리뷰] UltraGen: High-Resolution Video Generation with Hierarchical Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention","children":"Ran Yi이 arXiv에 게시한 'UltraGen: High-Resolution Video Generation with Hierarchical Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-UltraGen-High-Resolution-Video-Generation-with-Hierarchical-Attention"}]]}]]}],["$","article","2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning","children":"[논문리뷰] Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning","children":"Yuchen Eleanor Jiang이 arXiv에 게시한 'Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder","children":"[논문리뷰] ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder","children":"Zonghao Guo이 arXiv에 게시한 'ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder"}]]}]]}],["$","article","2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold","children":"[논문리뷰] PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold","children":"arXiv에 게시된 'PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold"}]]}]]}],["$","article","2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies","children":"[논문리뷰] PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies","children":"James Glass이 arXiv에 게시한 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-PRISMM-Bench-A-Benchmark-of-Peer-Review-Grounded-Multimodal-Inconsistencies"}]]}]]}],["$","article","2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation","children":"[논문리뷰] MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation","children":"arXiv에 게시된 'MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-MoGA-Mixture-of-Groups-Attention-for-End-to-End-Long-Video-Generation"}]]}]]}],["$","article","2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models","children":"[논문리뷰] MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models","children":"arXiv에 게시된 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models"}]]}]]}],["$","article","2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues","children":"[논문리뷰] MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues","children":"arXiv에 게시된 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-MT-Video-Bench-A-Holistic-Video-Understanding-Benchmark-for-Evaluating-Multimodal-LLMs-in-Multi-Turn-Dialogues"}]]}]]}],["$","article","2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions","children":"[논문리뷰] IF-VidCap: Can Video Caption Models Follow Instructions?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions","children":"arXiv에 게시된 'IF-VidCap: Can Video Caption Models Follow Instructions?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-IF-VidCap-Can-Video-Caption-Models-Follow-Instructions"}]]}]]}],["$","article","2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs","children":"[논문리뷰] Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs","children":"arXiv에 게시된 'Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs"}]]}]]}],["$","article","2025-10-22-Extracting-alignment-data-in-open-models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models","children":"[논문리뷰] Extracting alignment data in open models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models","children":"arXiv에 게시된 'Extracting alignment data in open models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models"}]]}]]}],["$","article","2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning","children":"[논문리뷰] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning","children":"Qipeng Guo이 arXiv에 게시한 'EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning"}]]}]]}],["$","article","2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence","children":"[논문리뷰] DSI-Bench: A Benchmark for Dynamic Spatial Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence","children":"arXiv에 게시된 'DSI-Bench: A Benchmark for Dynamic Spatial Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-DSI-Bench-A-Benchmark-for-Dynamic-Spatial-Intelligence"}]]}]]}],["$","article","2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist","children":"[논문리뷰] Chem-R: Learning to Reason as a Chemist"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist","children":"arXiv에 게시된 'Chem-R: Learning to Reason as a Chemist' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist"}]]}]]}],["$","article","2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading","children":"[논문리뷰] AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading","children":"Jiashu Wang이 arXiv에 게시한 'AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading"}]]}]]}],["$","article","2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling","children":"[논문리뷰] When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling","children":"arXiv에 게시된 'When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-When-to-Ensemble-Identifying-Token-Level-Points-for-Stable-and-Fast-LLM-Ensembling"}]]}]]}],["$","article","2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling","children":"[논문리뷰] Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling","children":"Dim P. Papadopoulos이 arXiv에 게시한 'Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling"}]]}]]}],["$","article","2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback","children":"[논문리뷰] Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback","children":"arXiv에 게시된 'Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback"}]]}]]}],["$","article","2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action","children":"[논문리뷰] UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action","children":"arXiv에 게시된 'UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-UltraCUA-A-Foundation-Model-for-Computer-Use-Agents-with-Hybrid-Action"}]]}]]}],["$","article","2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation","children":"[논문리뷰] Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation","children":"arXiv에 게시된 'Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Towards-Mixed-Modal-Retrieval-for-Universal-Retrieval-Augmented-Generation"}]]}]]}],["$","article","2025-10-21-RL-makes-MLLMs-see-better-than-SFT",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-RL-makes-MLLMs-see-better-than-SFT","children":"[논문리뷰] RL makes MLLMs see better than SFT"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-RL-makes-MLLMs-see-better-than-SFT","children":"arXiv에 게시된 'RL makes MLLMs see better than SFT' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-RL-makes-MLLMs-see-better-than-SFT"}]]}]]}],["$","article","2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems","children":"[논문리뷰] QueST: Incentivizing LLMs to Generate Difficult Problems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems","children":"arXiv에 게시된 'QueST: Incentivizing LLMs to Generate Difficult Problems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-QueST-Incentivizing-LLMs-to-Generate-Difficult-Problems"}]]}]]}],["$","article","2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing","children":"[논문리뷰] PICABench: How Far Are We from Physically Realistic Image Editing?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing","children":"Kaiwen Zhu이 arXiv에 게시한 'PICABench: How Far Are We from Physically Realistic Image Editing?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing"}]]}]]}],["$","article","2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators","children":"[논문리뷰] On Non-interactive Evaluation of Animal Communication Translators"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators","children":"Adam Tauman Kalai이 arXiv에 게시한 'On Non-interactive Evaluation of Animal Communication Translators' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-On-Non-interactive-Evaluation-of-Animal-Communication-Translators"}]]}]]}],["$","article","2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models","children":"[논문리뷰] MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models","children":"arXiv에 게시된 'MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-MultiVerse-A-Multi-Turn-Conversation-Benchmark-for-Evaluating-Large-Vision-and-Language-Models"}]]}]]}],["$","article","2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering","children":"[논문리뷰] Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering","children":"arXiv에 게시된 'Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering"}]]}]]}],["$","article","2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer","children":"[논문리뷰] GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer","children":"arXiv에 게시된 'GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer"}]]}]]}],["$","article","2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression","children":"[논문리뷰] Glyph: Scaling Context Windows via Visual-Text Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression","children":"Wenyi Hong이 arXiv에 게시한 'Glyph: Scaling Context Windows via Visual-Text Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Glyph-Scaling-Context-Windows-via-Visual-Text-Compression"}]]}]]}],["$","article","2025-10-21-FineVision-Open-Data-Is-All-You-Need",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-FineVision-Open-Data-Is-All-You-Need","children":"[논문리뷰] FineVision: Open Data Is All You Need"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-FineVision-Open-Data-Is-All-You-Need","children":"arXiv에 게시된 'FineVision: Open Data Is All You Need' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-FineVision-Open-Data-Is-All-You-Need"}]]}]]}],["$","article","2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research","children":"[논문리뷰] Executable Knowledge Graphs for Replicating AI Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research","children":"arXiv에 게시된 'Executable Knowledge Graphs for Replicating AI Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Executable-Knowledge-Graphs-for-Replicating-AI-Research"}]]}]]}],["$","article","2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics","children":"[논문리뷰] Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics","children":"arXiv에 게시된 'Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics"}]]}]]}],["$","article","2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset","children":"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset","children":"arXiv에 게시된 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset"}]]}]]}],["$","article","2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense","children":"[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense","children":"arXiv에 게시된 'Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense"}]]}]]}],["$","article","2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science","children":"[논문리뷰] DeepAnalyze: Agentic Large Language Models for Autonomous Data Science"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science","children":"arXiv에 게시된 'DeepAnalyze: Agentic Large Language Models for Autonomous Data Science' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-DeepAnalyze-Agentic-Large-Language-Models-for-Autonomous-Data-Science"}]]}]]}],["$","article","2025-10-21-Deep-Self-Evolving-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Deep-Self-Evolving-Reasoning","children":"[논문리뷰] Deep Self-Evolving Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Deep-Self-Evolving-Reasoning","children":"arXiv에 게시된 'Deep Self-Evolving Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Deep-Self-Evolving-Reasoning"}]]}]]}],["$","article","2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing","children":"[논문리뷰] ConsistEdit: Highly Consistent and Precise Training-free Visual Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing","children":"Xili Dai이 arXiv에 게시한 'ConsistEdit: Highly Consistent and Precise Training-free Visual Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-ConsistEdit-Highly-Consistent-and-Precise-Training-free-Visual-Editing"}]]}]]}],["$","article","2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting","children":"[논문리뷰] Chronos-2: From Univariate to Universal Forecasting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting","children":"arXiv에 게시된 'Chronos-2: From Univariate to Universal Forecasting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Chronos-2-From-Univariate-to-Universal-Forecasting"}]]}]]}],["$","article","2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training","children":"[논문리뷰] Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training","children":"Aditya Vir이 arXiv에 게시한 'Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Balanced-Multi-Task-Attention-for-Satellite-Image-Classification-A-Systematic-Approach-to-Achieving-97-23-Accuracy-on-EuroSAT-Without-Pre-Training"}]]}]]}],["$","article","2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning","children":"[논문리뷰] AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning","children":"Nikos Vlassis이 arXiv에 게시한 'AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-AsyncVoice-Agent-Real-Time-Explanation-for-LLM-Planning-and-Reasoning"}]]}]]}],["$","article","2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment","children":"[논문리뷰] Annotation-Efficient Universal Honesty Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment","children":"Jingtong Wu이 arXiv에 게시한 'Annotation-Efficient Universal Honesty Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Annotation-Efficient-Universal-Honesty-Alignment"}]]}]]}],["$","article","2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe","children":"[논문리뷰] Agentic Reinforcement Learning for Search is Unsafe"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe","children":"arXiv에 게시된 'Agentic Reinforcement Learning for Search is Unsafe' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Agentic-Reinforcement-Learning-for-Search-is-Unsafe"}]]}]]}],["$","article","2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent","children":"[논문리뷰] VISTA: A Test-Time Self-Improving Video Generation Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent","children":"Tomas Pfister이 arXiv에 게시한 'VISTA: A Test-Time Self-Improving Video Generation Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-VISTA-A-Test-Time-Self-Improving-Video-Generation-Agent"}]]}]]}],["$","article","2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data","children":"[논문리뷰] Train a Unified Multimodal Data Quality Classifier with Synthetic Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data","children":"Ritesh Sarkhel이 arXiv에 게시한 'Train a Unified Multimodal Data Quality Classifier with Synthetic Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Train-a-Unified-Multimodal-Data-Quality-Classifier-with-Synthetic-Data"}]]}]]}],["$","article","2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery","children":"[논문리뷰] Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery","children":"Chung-Ho Wu이 arXiv에 게시한 'Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Skyfall-GS-Synthesizing-Immersive-3D-Urban-Scenes-from-Satellite-Imagery"}]]}]]}],["$","article","2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset","children":"[논문리뷰] Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset","children":"Hao Ouyang이 arXiv에 게시한 'Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Scaling-Instruction-Based-Video-Editing-with-a-High-Quality-Synthetic-Dataset"}]]}]]}],["$","article","2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning","children":"[논문리뷰] Robust Layerwise Scaling Rules by Proper Weight Decay Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning","children":"arXiv에 게시된 'Robust Layerwise Scaling Rules by Proper Weight Decay Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Robust-Layerwise-Scaling-Rules-by-Proper-Weight-Decay-Tuning"}]]}]]}],["$","article","2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models","children":"[논문리뷰] Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models","children":"Shiwei Liu이 arXiv에 게시한 'Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Rewiring-Experts-on-the-FlyContinuous-Rerouting-for-Better-Online-Adaptation-in-Mixture-of-Expert-models"}]]}]]}],["$","article","2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive","children":"[논문리뷰] Paper2Web: Let's Make Your Paper Alive!"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive","children":"Yao Wan이 arXiv에 게시한 'Paper2Web: Let's Make Your Paper Alive!' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive"}]]}]]}],["$","article","2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM","children":"[논문리뷰] OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM","children":"arXiv에 게시된 'OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM"}]]}]]}],["$","article","2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks","children":"[논문리뷰] NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks","children":"Hongyu Yan이 arXiv에 게시한 'NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-NANO3D-A-Training-Free-Approach-for-Efficient-3D-Editing-Without-Masks"}]]}]]}],["$","article","2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning","children":"[논문리뷰] MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning","children":"arXiv에 게시된 'MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-MorphoBench-A-Benchmark-with-Difficulty-Adaptive-to-Model-Reasoning"}]]}]]}],["$","article","2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal","children":"[논문리뷰] LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal","children":"arXiv에 게시된 'LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal"}]]}]]}],["$","article","2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder","children":"[논문리뷰] Latent Diffusion Model without Variational Autoencoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder","children":"arXiv에 게시된 'Latent Diffusion Model without Variational Autoencoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Latent-Diffusion-Model-without-Variational-Autoencoder"}]]}]]}],["$","article","2025-10-20-Language-Models-Model-Language",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Language-Models-Model-Language","children":"[논문리뷰] Language Models Model Language"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Language-Models-Model-Language","children":"arXiv에 게시된 'Language Models Model Language' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Language-Models-Model-Language"}]]}]]}],["$","article","2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training","children":"[논문리뷰] InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training","children":"Congkai Xie이 arXiv에 게시한 'InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training"}]]}]]}],["$","article","2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation","children":"[논문리뷰] Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation","children":"Junsheng Yu이 arXiv에 게시한 'Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Imaginarium-Vision-guided-High-Quality-3D-Scene-Layout-Generation"}]]}]]}],["$","article","2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition","children":"[논문리뷰] Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition","children":"arXiv에 게시된 'Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Foundation-Models-for-Scientific-Discovery-From-Paradigm-Enhancement-to-Paradigm-Transition"}]]}]]}],["$","article","2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain","children":"[논문리뷰] FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain","children":"Arman Cohan이 arXiv에 게시한 'FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-FinTrust-A-Comprehensive-Benchmark-of-Trustworthiness-Evaluation-in-Finance-Domain"}]]}]]}],["$","article","2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents","children":"[논문리뷰] Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents","children":"Jianshu Zhang이 arXiv에 게시한 'Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Explore-to-Evolve-Scaling-Evolved-Aggregation-Logic-via-Proactive-Online-Exploration-for-Deep-Research-Agents"}]]}]]}],["$","article","2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs","children":"[논문리뷰] Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs","children":"Kevin Zhu이 arXiv에 게시한 'Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Emergent-Misalignment-via-In-Context-Learning-Narrow-in-context-examples-can-produce-broadly-misaligned-LLMs"}]]}]]}],["$","article","2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models","children":"[논문리뷰] ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models","children":"Sean O'Brien이 arXiv에 게시한 'ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-ERGO-Entropy-guided-Resetting-for-Generation-Optimization-in-Multi-turn-Language-Models"}]]}]]}],["$","article","2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion","children":"[논문리뷰] DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion","children":"arXiv에 게시된 'DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-DriveGen3D-Boosting-Feed-Forward-Driving-Scene-Generation-with-Efficient-Video-Diffusion"}]]}]]}],["$","article","2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning","children":"[논문리뷰] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning","children":"arXiv에 게시된 'DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation","children":"[논문리뷰] Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation","children":"Cat Yan이 arXiv에 게시한 'Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-Build-Your-Personalized-Research-Group-A-Multiagent-Framework-for-Continual-and-Interactive-Science-Automation"}]]}]]}],["$","article","2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation","children":"[논문리뷰] BLIP3o-NEXT: Next Frontier of Native Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation","children":"arXiv에 게시된 'BLIP3o-NEXT: Next Frontier of Native Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation"}]]}]]}],["$","article","2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning","children":"[논문리뷰] A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning","children":"arXiv에 게시된 'A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-A2FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning"}]]}]]}],["$","article","2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation","children":"[논문리뷰] pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation","children":"arXiv에 게시된 'pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-pi-Flow-Policy-Based-Few-Step-Generation-via-Imitation-Distillation"}]]}]]}],["$","article","2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation","children":"[논문리뷰] WithAnyone: Towards Controllable and ID Consistent Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation","children":"arXiv에 게시된 'WithAnyone: Towards Controllable and ID Consistent Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-WithAnyone-Towards-Controllable-and-ID-Consistent-Image-Generation"}]]}]]}],["$","article","2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA","children":"[논문리뷰] When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA","children":"Artem Vazhentsev이 arXiv에 게시한 'When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-When-Models-Lie-We-Learn-Multilingual-Span-Level-Hallucination-Detection-with-PsiloQA"}]]}]]}],["$","article","2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning","children":"[논문리뷰] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning","children":"arXiv에 게시된 'VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning"}]]}]]}],["$","article","2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation","children":"[논문리뷰] VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation","children":"arXiv에 게시된 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-VLA2-Empowering-Vision-Language-Action-Models-with-an-Agentic-Framework-for-Unseen-Concept-Manipulation"}]]}]]}],["$","article","2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification","children":"[논문리뷰] VLA-0: Building State-of-the-Art VLAs with Zero Modification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification","children":"arXiv에 게시된 'VLA-0: Building State-of-the-Art VLAs with Zero Modification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification"}]]}]]}],["$","article","2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator","children":"[논문리뷰] VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator","children":"Federico Tombari이 arXiv에 게시한 'VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator"}]]}]]}],["$","article","2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar","children":"[논문리뷰] TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar","children":"arXiv에 게시된 'TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-TokDrift-When-LLM-Speaks-in-Subwords-but-Code-Speaks-in-Grammar"}]]}]]}],["$","article","2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models","children":"[논문리뷰] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models","children":"arXiv에 게시된 'The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models"}]]}]]}],["$","article","2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis","children":"[논문리뷰] SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis","children":"arXiv에 게시된 'SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis"}]]}]]}],["$","article","2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models","children":"[논문리뷰] RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models","children":"arXiv에 게시된 'RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-RefusalBench-Generative-Evaluation-of-Selective-Refusal-in-Grounded-Language-Models"}]]}]]}],["$","article","2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","children":"[논문리뷰] RealDPO: Real or Not Real, that is the Preference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference","children":"Chenyang Si이 arXiv에 게시한 'RealDPO: Real or Not Real, that is the Preference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-RealDPO-Real-or-Not-Real-that-is-the-Preference"}]]}]]}],["$","article","2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems","children":"[논문리뷰] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems","children":"arXiv에 게시된 'RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems"}]]}]]}],["$","article","2025-10-17-Qwen3Guard-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Qwen3Guard-Technical-Report","children":"[논문리뷰] Qwen3Guard Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Qwen3Guard-Technical-Report","children":"arXiv에 게시된 'Qwen3Guard Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Qwen3Guard-Technical-Report"}]]}]]}],["$","article","2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation","children":"[논문리뷰] Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation","children":"arXiv에 게시된 'Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation"}]]}]]}],["$","article","2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model","children":"[논문리뷰] PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model","children":"arXiv에 게시된 'PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-PaddleOCR-VL-Boosting-Multilingual-Document-Parsing-via-a-0-9B-Ultra-Compact-Vision-Language-Model"}]]}]]}],["$","article","2025-10-17-On-Pretraining-for-Project-Level-Code-Completion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-On-Pretraining-for-Project-Level-Code-Completion","children":"[논문리뷰] On Pretraining for Project-Level Code Completion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-On-Pretraining-for-Project-Level-Code-Completion","children":"arXiv에 게시된 'On Pretraining for Project-Level Code Completion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-On-Pretraining-for-Project-Level-Code-Completion"}]]}]]}],["$","article","2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems","children":"[논문리뷰] MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems","children":"Feiyu Xiong이 arXiv에 게시한 'MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-MoM-Mixtures-of-Scenario-Aware-Document-Memories-for-Retrieval-Augmented-Generation-Systems"}]]}]]}],["$","article","2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning","children":"[논문리뷰] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning","children":"Ke Wang이 arXiv에 게시한 'MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning"}]]}]]}],["$","article","2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning","children":"[논문리뷰] LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning","children":"arXiv에 게시된 'LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-LiteStage-Latency-aware-Layer-Skipping-for-Multi-stage-Reasoning"}]]}]]}],["$","article","2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs","children":"[논문리뷰] Learning an Image Editing Model without Image Editing Pairs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs","children":"arXiv에 게시된 'Learning an Image Editing Model without Image Editing Pairs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Learning-an-Image-Editing-Model-without-Image-Editing-Pairs"}]]}]]}],["$","article","2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know","children":"[논문리뷰] Large Language Models Do NOT Really Know What They Don't Know"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know","children":"arXiv에 게시된 'Large Language Models Do NOT Really Know What They Don't Know' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Large-Language-Models-Do-NOT-Really-Know-What-They-Dont-Know"}]]}]]}],["$","article","2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding","children":"[논문리뷰] LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding","children":"arXiv에 게시된 'LaSeR: Reinforcement Learning with Last-Token Self-Rewarding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-LaSeR-Reinforcement-Learning-with-Last-Token-Self-Rewarding"}]]}]]}],["$","article","2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training","children":"[논문리뷰] LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training","children":"arXiv에 게시된 'LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-LLMs-as-Scalable-General-Purpose-Simulators-For-Evolving-Digital-Agent-Training"}]]}]]}],["$","article","2025-10-17-LLM-guided-Hierarchical-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval","children":"[논문리뷰] LLM-guided Hierarchical Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval","children":"arXiv에 게시된 'LLM-guided Hierarchical Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval"}]]}]]}],["$","article","2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents","children":"[논문리뷰] Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents","children":"arXiv에 게시된 'Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents"}]]}]]}],["$","article","2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","children":"[논문리뷰] ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints","children":"arXiv에 게시된 'ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-ImagerySearch-Adaptive-Test-Time-Search-for-Video-Generation-Beyond-Semantic-Dependency-Constraints"}]]}]]}],["$","article","2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale","children":"[논문리뷰] From Pixels to Words -- Towards Native Vision-Language Primitives at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale","children":"arXiv에 게시된 'From Pixels to Words -- Towards Native Vision-Language Primitives at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-From-Pixels-to-Words-Towards-Native-Vision-Language-Primitives-at-Scale"}]]}]]}],["$","article","2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report","children":"[논문리뷰] Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report","children":"arXiv에 게시된 'Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Fantastic-small-Retrievers-and-How-to-Train-Them-mxbai-edge-colbert-v0-Tech-Report"}]]}]]}],["$","article","2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning","children":"[논문리뷰] Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning","children":"Sijia Gu이 arXiv에 게시한 'Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Expertise-need-not-monopolize-Action-Specialized-Mixture-of-Experts-for-Vision-Language-Action-Learning"}]]}]]}],["$","article","2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models","children":"[논문리뷰] Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models","children":"arXiv에 게시된 'Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Efficient-Parallel-Samplers-for-Recurrent-Depth-Models-and-Their-Connection-to-Diffusion-Language-Models"}]]}]]}],["$","article","2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation","children":"[논문리뷰] DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation","children":"arXiv에 게시된 'DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-DialectGen-Benchmarking-and-Improving-Dialect-Robustness-in-Multimodal-Generation"}]]}]]}],["$","article","2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes","children":"[논문리뷰] COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes","children":"arXiv에 게시된 'COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-COIG-Writer-A-High-Quality-Dataset-for-Chinese-Creative-Writing-with-Thought-Processes"}]]}]]}],["$","article","2025-10-17-BitNet-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-BitNet-Distillation","children":"[논문리뷰] BitNet Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-BitNet-Distillation","children":"arXiv에 게시된 'BitNet Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-BitNet-Distillation"}]]}]]}],["$","article","2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts","children":"[논문리뷰] Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts","children":"arXiv에 게시된 'Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts"}]]}]]}],["$","article","2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures","children":"[논문리뷰] Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures","children":"arXiv에 게시된 'Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Beyond-Correctness-Evaluating-Subjective-Writing-Preferences-Across-Cultures"}]]}]]}],["$","article","2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs","children":"[논문리뷰] Attention Is All You Need for KV Cache in Diffusion LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs","children":"arXiv에 게시된 'Attention Is All You Need for KV Cache in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Attention-Is-All-You-Need-for-KV-Cache-in-Diffusion-LLMs"}]]}]]}],["$","article","2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization","children":"[논문리뷰] Agentic Entropy-Balanced Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization","children":"arXiv에 게시된 'Agentic Entropy-Balanced Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization"}]]}]]}],["$","article","2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses","children":"[논문리뷰] AI for Service: Proactive Assistance with AI Glasses"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses","children":"arXiv에 게시된 'AI for Service: Proactive Assistance with AI Glasses' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-AI-for-Service-Proactive-Assistance-with-AI-Glasses"}]]}]]}],["$","article","2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model","children":"[논문리뷰] X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model","children":"Xirui Kang이 arXiv에 게시한 'X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-X-VLA-Soft-Prompted-Transformer-as-Scalable-Cross-Embodiment-Vision-Language-Action-Model"}]]}]]}],["$","article","2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification","children":"[논문리뷰] Universal Image Restoration Pre-training via Masked Degradation Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification","children":"arXiv에 게시된 'Universal Image Restoration Pre-training via Masked Degradation Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification"}]]}]]}],["$","article","2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE","children":"[논문리뷰] UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE","children":"arXiv에 게시된 'UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE"}]]}]]}],["$","article","2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning","children":"[논문리뷰] UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning","children":"Ziyong Feng이 arXiv에 게시한 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-UniME-V2-MLLM-as-a-Judge-for-Universal-Multimodal-Embedding-Learning"}]]}]]}],["$","article","2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark","children":"[논문리뷰] Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark","children":"arXiv에 게시된 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Uni-MMMU-A-Massive-Multi-discipline-Multimodal-Unified-Benchmark"}]]}]]}],["$","article","2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields","children":"[논문리뷰] Trace Anything: Representing Any Video in 4D via Trajectory Fields"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields","children":"arXiv에 게시된 'Trace Anything: Representing Any Video in 4D via Trajectory Fields' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Trace-Anything-Representing-Any-Video-in-4D-via-Trajectory-Fields"}]]}]]}],["$","article","2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research","children":"[논문리뷰] The Role of Computing Resources in Publishing Foundation Model Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research","children":"Zhenwen Liang이 arXiv에 게시한 'The Role of Computing Resources in Publishing Foundation Model Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-The-Role-of-Computing-Resources-in-Publishing-Foundation-Model-Research"}]]}]]}],["$","article","2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs","children":"[논문리뷰] The Art of Scaling Reinforcement Learning Compute for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs","children":"arXiv에 게시된 'The Art of Scaling Reinforcement Learning Compute for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs"}]]}]]}],["$","article","2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs","children":"[논문리뷰] Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs","children":"Hao Zhang이 arXiv에 게시한 'Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Stronger-Together-On-Policy-Reinforcement-Learning-for-Collaborative-LLMs"}]]}]]}],["$","article","2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning","children":"[논문리뷰] Revisiting Model Interpolation for Efficient Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning","children":"arXiv에 게시된 'Revisiting Model Interpolation for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning"}]]}]]}],["$","article","2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World","children":"[논문리뷰] Reasoning in Space via Grounding in the World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World","children":"Li Zhang이 arXiv에 게시한 'Reasoning in Space via Grounding in the World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Reasoning-in-Space-via-Grounding-in-the-World"}]]}]]}],["$","article","2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models","children":"[논문리뷰] Point Prompting: Counterfactual Tracking with Video Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models","children":"Andrew Owens이 arXiv에 게시한 'Point Prompting: Counterfactual Tracking with Video Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Point-Prompting-Counterfactual-Tracking-with-Video-Diffusion-Models"}]]}]]}],["$","article","2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","children":"[논문리뷰] PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning","children":"Hengshuang Zhao이 arXiv에 게시한 'PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-PhysMaster-Mastering-Physical-Representation-for-Video-Generation-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs","children":"[논문리뷰] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs","children":"arXiv에 게시된 'ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-ParallelBench-Understanding-the-Trade-offs-of-Parallel-Decoding-in-Diffusion-LLMs"}]]}]]}],["$","article","2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention","children":"[논문리뷰] NOSA: Native and Offloadable Sparse Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention","children":"Zhiyuan Liu이 arXiv에 게시한 'NOSA: Native and Offloadable Sparse Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-NOSA-Native-and-Offloadable-Sparse-Attention"}]]}]]}],["$","article","2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training","children":"[논문리뷰] MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training","children":"arXiv에 게시된 'MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training"}]]}]]}],["$","article","2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model","children":"[논문리뷰] MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model","children":"Wieland Brendel이 arXiv에 게시한 'MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-MATH-Beyond-A-Benchmark-for-RL-to-Expand-Beyond-the-Base-Model"}]]}]]}],["$","article","2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models","children":"[논문리뷰] LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models","children":"arXiv에 게시된 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-LIBERO-Plus-In-depth-Robustness-Analysis-of-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy","children":"[논문리뷰] InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy","children":"Yilun Chen이 arXiv에 게시한 'InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-InternVLA-M1-A-Spatially-Guided-Vision-Language-Action-Framework-for-Generalist-Robot-Policy"}]]}]]}],["$","article","2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue","children":"[논문리뷰] InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue","children":"Dongchuan Ran이 arXiv에 게시한 'InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue"}]]}]]}],["$","article","2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication","children":"[논문리뷰] HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication","children":"Haochen You이 arXiv에 게시한 'HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-HyperAgent-Leveraging-Hypergraphs-for-Topology-Optimization-in-Multi-Agent-Communication"}]]}]]}],["$","article","2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain","children":"[논문리뷰] Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain","children":"Lingxi Lu이 arXiv에 게시한 'Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain"}]]}]]}],["$","article","2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math","children":"[논문리뷰] Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math","children":"arXiv에 게시된 'Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Hard2Verify-A-Step-Level-Verification-Benchmark-for-Open-Ended-Frontier-Math"}]]}]]}],["$","article","2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search","children":"[논문리뷰] GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search","children":"Zijian Zhang이 arXiv에 게시한 'GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-GraphTracer-Graph-Guided-Failure-Tracing-in-LLM-Agents-for-Robust-Multi-Turn-Deep-Search"}]]}]]}],["$","article","2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner","children":"[논문리뷰] Generative Universal Verifier as Multimodal Meta-Reasoner"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner","children":"arXiv에 게시된 'Generative Universal Verifier as Multimodal Meta-Reasoner' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner"}]]}]]}],["$","article","2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds","children":"[논문리뷰] FlashWorld: High-quality 3D Scene Generation within Seconds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds","children":"Chunchao Guo이 arXiv에 게시한 'FlashWorld: High-quality 3D Scene Generation within Seconds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-FlashWorld-High-quality-3D-Scene-Generation-within-Seconds"}]]}]]}],["$","article","2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model","children":"[논문리뷰] FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model","children":"Dawei Liang이 arXiv에 게시한 'FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-FG-CLIP-2-A-Bilingual-Fine-grained-Vision-Language-Alignment-Model"}]]}]]}],["$","article","2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling","children":"[논문리뷰] EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling","children":"Ahmet Üstün이 arXiv에 게시한 'EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling"}]]}]]}],["$","article","2025-10-16-Direct-Multi-Token-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Direct-Multi-Token-Decoding","children":"[논문리뷰] Direct Multi-Token Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Direct-Multi-Token-Decoding","children":"Xifeng Yan이 arXiv에 게시한 'Direct Multi-Token Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Direct-Multi-Token-Decoding"}]]}]]}],["$","article","2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs","children":"[논문리뷰] Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs","children":"arXiv에 게시된 'Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Deflanderization-for-Game-Dialogue-Balancing-Character-Authenticity-with-Task-Execution-in-LLM-based-NPCs"}]]}]]}],["$","article","2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving","children":"[논문리뷰] CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving","children":"arXiv에 게시된 'CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-CoIRL-AD-Collaborative-Competitive-Imitation-Reinforcement-Learning-in-Latent-World-Models-for-Autonomous-Driving"}]]}]]}],["$","article","2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","children":"[논문리뷰] CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving","children":"Jingcheng Ni이 arXiv에 게시한 'CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-CVD-STORM-Cross-View-Video-Diffusion-with-Spatial-Temporal-Reconstruction-Model-for-Autonomous-Driving"}]]}]]}],["$","article","2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs","children":"[논문리뷰] Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs","children":"arXiv에 게시된 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs"}]]}]]}],["$","article","2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization","children":"[논문리뷰] Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization","children":"arXiv에 게시된 'Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Attention-Illuminates-LLM-Reasoning-The-Preplan-and-Anchor-Rhythm-Enables-Fine-Grained-Policy-Optimization"}]]}]]}],["$","article","2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions","children":"[논문리뷰] What If : Understanding Motion Through Sparse Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions","children":"arXiv에 게시된 'What If : Understanding Motion Through Sparse Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-What-If-Understanding-Motion-Through-Sparse-Interactions"}]]}]]}],["$","article","2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution","children":"[논문리뷰] ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution","children":"arXiv에 게시된 'ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-ViCO-A-Training-Strategy-towards-Semantic-Aware-Dynamic-High-Resolution"}]]}]]}],["$","article","2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation","children":"[논문리뷰] UniFusion: Vision-Language Model as Unified Encoder in Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation","children":"arXiv에 게시된 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-UniFusion-Vision-Language-Model-as-Unified-Encoder-in-Image-Generation"}]]}]]}],["$","article","2025-10-15-Tensor-Logic-The-Language-of-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Tensor-Logic-The-Language-of-AI","children":"[논문리뷰] Tensor Logic: The Language of AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Tensor-Logic-The-Language-of-AI","children":"Pedro Domingos이 arXiv에 게시한 'Tensor Logic: The Language of AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Tensor-Logic-The-Language-of-AI"}]]}]]}],["$","article","2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models","children":"[논문리뷰] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models","children":"arXiv에 게시된 'Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Temporal-Alignment-Guidance-On-Manifold-Sampling-in-Diffusion-Models"}]]}]]}],["$","article","2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale","children":"[논문리뷰] SynthID-Image: Image watermarking at internet scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale","children":"arXiv에 게시된 'SynthID-Image: Image watermarking at internet scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-SynthID-Image-Image-watermarking-at-internet-scale"}]]}]]}],["$","article","2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model","children":"[논문리뷰] Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model","children":"arXiv에 게시된 'Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Spatial-Forcing-Implicit-Spatial-Representation-Alignment-for-Vision-language-action-Model"}]]}]]}],["$","article","2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning","children":"[논문리뷰] Scaling Language-Centric Omnimodal Representation Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning","children":"arXiv에 게시된 'Scaling Language-Centric Omnimodal Representation Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Scaling-Language-Centric-Omnimodal-Representation-Learning"}]]}]]}],["$","article","2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models","children":"[논문리뷰] SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models","children":"arXiv에 게시된 'SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-SRUM-Fine-Grained-Self-Rewarding-for-Unified-Multimodal-Models"}]]}]]}],["$","article","2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model","children":"[논문리뷰] SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model","children":"arXiv에 게시된 'SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-SAIL-Embedding-Technical-Report-Omni-modal-Embedding-Foundation-Model"}]]}]]}],["$","article","2025-10-15-Robot-Learning-A-Tutorial",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial","children":"[논문리뷰] Robot Learning: A Tutorial"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial","children":"arXiv에 게시된 'Robot Learning: A Tutorial' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Robot-Learning-A-Tutorial"}]]}]]}],["$","article","2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability","children":"[논문리뷰] ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability","children":"Tsui-Wei Weng이 arXiv에 게시한 'ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-ReFIne-A-Framework-for-Trustworthy-Large-Reasoning-Models-with-Reliability-Faithfulness-and-Interpretability"}]]}]]}],["$","article","2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration","children":"[논문리뷰] One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration","children":"Mohit Bansal이 arXiv에 게시한 'One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-One-Life-to-Learn-Inferring-Symbolic-World-Models-for-Stochastic-Environments-from-Unguided-Exploration"}]]}]]}],["$","article","2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks","children":"[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks","children":"Xueyuan Lin이 arXiv에 게시한 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks"}]]}]]}],["$","article","2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces","children":"[논문리뷰] MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces","children":"Sungchul Kim이 arXiv에 게시한 'MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-MLLM-as-a-UI-Judge-Benchmarking-Multimodal-LLMs-for-Predicting-Human-Perception-of-User-Interfaces"}]]}]]}],["$","article","2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens","children":"[논문리뷰] LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens","children":"arXiv에 게시된 'LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-LLM-Reasoning-for-Machine-Translation-Synthetic-Data-Generation-over-Thinking-Tokens"}]]}]]}],["$","article","2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation","children":"[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation","children":"arXiv에 게시된 'Information-Preserving Reformulation of Reasoning Traces for Antidistillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation"}]]}]]}],["$","article","2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners","children":"[논문리뷰] HoneyBee: Data Recipes for Vision-Language Reasoners"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners","children":"arXiv에 게시된 'HoneyBee: Data Recipes for Vision-Language Reasoners' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners"}]]}]]}],["$","article","2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution","children":"[논문리뷰] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution","children":"Yihao Liu이 arXiv에 게시한 'FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-FlashVSR-Towards-Real-Time-Diffusion-Based-Streaming-Video-Super-Resolution"}]]}]]}],["$","article","2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning","children":"[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding & Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning","children":"arXiv에 게시된 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-ExpVid-A-Benchmark-for-Experiment-Video-Understanding-Reasoning"}]]}]]}],["$","article","2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning","children":"[논문리뷰] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning","children":"arXiv에 게시된 'ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-ERA-Transforming-VLMs-into-Embodied-Agents-via-Embodied-Prior-Learning-and-Online-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs","children":"[논문리뷰] Dr.LLM: Dynamic Layer Routing in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs","children":"arXiv에 게시된 'Dr.LLM: Dynamic Layer Routing in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Dr-LLM-Dynamic-Layer-Routing-in-LLMs"}]]}]]}],["$","article","2025-10-15-Detect-Anything-via-Next-Point-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Detect-Anything-via-Next-Point-Prediction","children":"[논문리뷰] Detect Anything via Next Point Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Detect-Anything-via-Next-Point-Prediction","children":"arXiv에 게시된 'Detect Anything via Next Point Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Detect-Anything-via-Next-Point-Prediction"}]]}]]}],["$","article","2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search","children":"[논문리뷰] DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search","children":"arXiv에 게시된 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-DeepMMSearch-R1-Empowering-Multimodal-LLMs-in-Multimodal-Web-Search"}]]}]]}],["$","article","2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation","children":"[논문리뷰] DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation","children":"arXiv에 게시된 'DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-DITING-A-Multi-Agent-Evaluation-Framework-for-Benchmarking-Web-Novel-Translation"}]]}]]}],["$","article","2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models","children":"[논문리뷰] Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models","children":"arXiv에 게시된 'Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Boundary-Guided-Policy-Optimization-for-Memory-efficient-RL-of-Diffusion-Large-Language-Models"}]]}]]}],["$","article","2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training","children":"[논문리뷰] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training","children":"arXiv에 게시된 'Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-Advancing-End-to-End-Pixel-Space-Generative-Modeling-via-Self-supervised-Pre-training"}]]}]]}],["$","article","2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models","children":"[논문리뷰] A Survey of Vibe Coding with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models","children":"arXiv에 게시된 'A Survey of Vibe Coding with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models"}]]}]]}],["$","article","2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression","children":"[논문리뷰] Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression","children":"Huan Wang이 arXiv에 게시한 'Which Heads Matter for Reasoning? RL-Guided KV Cache Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Which-Heads-Matter-for-Reasoning-RL-Guided-KV-Cache-Compression"}]]}]]}],["$","article","2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels","children":"[논문리뷰] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels","children":"arXiv에 게시된 'Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels"}]]}]]}],["$","article","2025-10-13-Understanding-DeepResearch-via-Reports",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports","children":"[논문리뷰] Understanding DeepResearch via Reports"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports","children":"Chengen Huang이 arXiv에 게시한 'Understanding DeepResearch via Reports' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Understanding-DeepResearch-via-Reports"}]]}]]}],["$","article","2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation","children":"[논문리뷰] Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation","children":"Linyi Jin이 arXiv에 게시한 'Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Thinking-with-Camera-A-Unified-Multimodal-Model-for-Camera-Centric-Understanding-and-Generation"}]]}]]}],["$","article","2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation","children":"[논문리뷰] Temporal Prompting Matters: Rethinking Referring Video Object Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation","children":"Sifei Liu이 arXiv에 게시한 'Temporal Prompting Matters: Rethinking Referring Video Object Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation"}]]}]]}],["$","article","2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control","children":"[논문리뷰] TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control","children":"Adityan Jothi이 arXiv에 게시한 'TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-TC-LoRA-Temporally-Modulated-Conditional-LoRA-for-Adaptive-Diffusion-Control"}]]}]]}],["$","article","2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams","children":"[논문리뷰] StreamingVLM: Real-Time Understanding for Infinite Video Streams"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams","children":"Kelly Peng이 arXiv에 게시한 'StreamingVLM: Real-Time Understanding for Infinite Video Streams' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams"}]]}]]}],["$","article","2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics","children":"[논문리뷰] StatEval: A Comprehensive Benchmark for Large Language Models in Statistics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics","children":"arXiv에 게시된 'StatEval: A Comprehensive Benchmark for Large Language Models in Statistics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics"}]]}]]}],["$","article","2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation","children":"[논문리뷰] Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation","children":"Han Shi이 arXiv에 게시한 'Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Speculative-Jacobi-Denoising-Decoding-for-Accelerating-Autoregressive-Text-to-image-Generation"}]]}]]}],["$","article","2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km","children":"[논문리뷰] SpaceVista: All-Scale Visual Spatial Reasoning from mm to km"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km","children":"Kaituo Feng이 arXiv에 게시한 'SpaceVista: All-Scale Visual Spatial Reasoning from mm to km' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-SpaceVista-All-Scale-Visual-Spatial-Reasoning-from-mm-to-km"}]]}]]}],["$","article","2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review","children":"[논문리뷰] ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review","children":"Christopher Pal이 arXiv에 게시한 'ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review"}]]}]]}],["$","article","2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth","children":"[논문리뷰] R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth","children":"arXiv에 게시된 'R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth"}]]}]]}],["$","article","2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition","children":"[논문리뷰] Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition","children":"Shang-Tse Chen이 arXiv에 게시한 'Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Pseudo2Real-Task-Arithmetic-for-Pseudo-Label-Correction-in-Automatic-Speech-Recognition"}]]}]]}],["$","article","2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction","children":"[논문리뷰] Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction","children":"danxuhk이 arXiv에 게시한 'Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction"}]]}]]}],["$","article","2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs","children":"[논문리뷰] PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs","children":"Xu Zheng이 arXiv에 게시한 'PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-PhysToolBench-Benchmarking-Physical-Tool-Understanding-for-MLLMs"}]]}]]}],["$","article","2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models","children":"[논문리뷰] Parallel Test-Time Scaling for Latent Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models","children":"arXiv에 게시된 'Parallel Test-Time Scaling for Latent Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Parallel-Test-Time-Scaling-for-Latent-Reasoning-Models"}]]}]]}],["$","article","2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework","children":"[논문리뷰] One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework","children":"Giuseppe Amato이 arXiv에 게시한 'One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-One-Patch-to-Caption-Them-All-A-Unified-Zero-Shot-Captioning-Framework"}]]}]]}],["$","article","2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs","children":"[논문리뷰] Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs","children":"arXiv에 게시된 'Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Multimodal-Prompt-Optimization-Why-Not-Leverage-Multiple-Modalities-for-MLLMs"}]]}]]}],["$","article","2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping","children":"[논문리뷰] Mitigating Overthinking through Reasoning Shaping"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping","children":"Wen Luo이 arXiv에 게시한 'Mitigating Overthinking through Reasoning Shaping' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Mitigating-Overthinking-through-Reasoning-Shaping"}]]}]]}],["$","article","2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval","children":"[논문리뷰] MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval","children":"Tingyu Song이 arXiv에 게시한 'MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-MRMR-A-Realistic-and-Expert-Level-Multidisciplinary-Benchmark-for-Reasoning-Intensive-Multimodal-Retrieval"}]]}]]}],["$","article","2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone","children":"[논문리뷰] KORMo: Korean Open Reasoning Model for Everyone"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone","children":"arXiv에 게시된 'KORMo: Korean Open Reasoning Model for Everyone' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-KORMo-Korean-Open-Reasoning-Model-for-Everyone"}]]}]]}],["$","article","2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes","children":"[논문리뷰] Instant4D: 4D Gaussian Splatting in Minutes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes","children":"Li Lu이 arXiv에 게시한 'Instant4D: 4D Gaussian Splatting in Minutes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Instant4D-4D-Gaussian-Splatting-in-Minutes"}]]}]]}],["$","article","2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation","children":"[논문리뷰] Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation","children":"Zekun Qi이 arXiv에 게시한 'Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Hybrid-grained-Feature-Aggregation-with-Coarse-to-fine-Language-Guidance-for-Self-supervised-Monocular-Depth-Estimation"}]]}]]}],["$","article","2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare","children":"[논문리뷰] GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare","children":"arXiv에 게시된 'GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare"}]]}]]}],["$","article","2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents","children":"[논문리뷰] Dyna-Mind: Learning to Simulate from Experience for Better AI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents","children":"Qianhui Wu이 arXiv에 게시한 'Dyna-Mind: Learning to Simulate from Experience for Better AI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Dyna-Mind-Learning-to-Simulate-from-Experience-for-Better-AI-Agents"}]]}]]}],["$","article","2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting","children":"[논문리뷰] Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting","children":"Julia Kempe이 arXiv에 게시한 'Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting"}]]}]]}],["$","article","2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation","children":"[논문리뷰] DISCO: Diversifying Sample Condensation for Efficient Model Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation","children":"arXiv에 게시된 'DISCO: Diversifying Sample Condensation for Efficient Model Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation"}]]}]]}],["$","article","2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI","children":"[논문리뷰] D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI","children":"Haebin Seong이 arXiv에 게시한 'D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI"}]]}]]}],["$","article","2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization","children":"[논문리뷰] Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization","children":"Mahdi Ghaznavai이 arXiv에 게시한 'Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Bridging-Reasoning-to-Learning-Unmasking-Illusions-using-Complexity-Out-of-Distribution-Generalization"}]]}]]}],["$","article","2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution","children":"[논문리뷰] BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution","children":"Hange Liu이 arXiv에 게시한 'BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution"}]]}]]}],["$","article","2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models","children":"[논문리뷰] Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models","children":"arXiv에 게시된 'Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Better-Together-Leveraging-Unpaired-Multimodal-Data-for-Stronger-Unimodal-Models"}]]}]]}],["$","article","2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion","children":"[논문리뷰] AutoPR: Let's Automate Your Academic Promotion!"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion","children":"Yixin Yuan이 arXiv에 게시한 'AutoPR: Let's Automate Your Academic Promotion!' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion"}]]}]]}],["$","article","2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols","children":"[논문리뷰] Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols","children":"Maksym Andriushchenko이 arXiv에 게시한 'Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-Adaptive-Attacks-on-Trusted-Monitors-Subvert-AI-Control-Protocols"}]]}]]}],["$","article","2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping","children":"[논문리뷰] ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping","children":"Wenbo Hu이 arXiv에 게시한 'ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping"}]]}]]}],["$","article","2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall","children":"[논문리뷰] ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall","children":"Jiaqi Tang이 arXiv에 게시한 'ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-ACE-Attribution-Controlled-Knowledge-Editing-for-Multi-hop-Factual-Recall"}]]}]]}],["$","article","2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks","children":"[논문리뷰] A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks","children":"Fanchao Qi이 arXiv에 게시한 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks"}]]}]]}],["$","article","2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs","children":"[논문리뷰] When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs","children":"arXiv에 게시된 'When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-When-Thoughts-Meet-Facts-Reusable-Reasoning-for-Long-Context-LMs"}]]}]]}],["$","article","2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning","children":"[논문리뷰] VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning","children":"Quande Liu이 arXiv에 게시한 'VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-VideoCanvas-Unified-Video-Completion-from-Arbitrary-Spatiotemporal-Patches-via-In-Context-Conditioning"}]]}]]}],["$","article","2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos","children":"[논문리뷰] UniVideo: Unified Understanding, Generation, and Editing for Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos","children":"Xintao Wang이 arXiv에 게시한 'UniVideo: Unified Understanding, Generation, and Editing for Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos"}]]}]]}],["$","article","2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution","children":"[논문리뷰] UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution","children":"arXiv에 게시된 'UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-UniMMVSR-A-Unified-Multi-Modal-Framework-for-Cascaded-Video-Super-Resolution"}]]}]]}],["$","article","2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections","children":"[논문리뷰] UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections","children":"Boqian Li이 arXiv에 게시한 'UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-UP2You-Fast-Reconstruction-of-Yourself-from-Unconstrained-Photo-Collections"}]]}]]}],["$","article","2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG","children":"[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG","children":"arXiv에 게시된 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG"}]]}]]}],["$","article","2025-10-10-Training-Free-Group-Relative-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization","children":"[논문리뷰] Training-Free Group Relative Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization","children":"arXiv에 게시된 'Training-Free Group Relative Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Training-Free-Group-Relative-Policy-Optimization"}]]}]]}],["$","article","2025-10-10-Towards-Scalable-and-Consistent-3D-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Towards-Scalable-and-Consistent-3D-Editing","children":"[논문리뷰] Towards Scalable and Consistent 3D Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Towards-Scalable-and-Consistent-3D-Editing","children":"Pan Zhou이 arXiv에 게시한 'Towards Scalable and Consistent 3D Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Towards-Scalable-and-Consistent-3D-Editing"}]]}]]}],["$","article","2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety","children":"[논문리뷰] The Alignment Waltz: Jointly Training Agents to Collaborate for Safety"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety","children":"arXiv에 게시된 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-The-Alignment-Waltz-Jointly-Training-Agents-to-Collaborate-for-Safety"}]]}]]}],["$","article","2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction","children":"[논문리뷰] Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction","children":"arXiv에 게시된 'Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Taming-Text-to-Sounding-Video-Generation-via-Advanced-Modality-Condition-and-Interaction"}]]}]]}],["$","article","2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models","children":"[논문리뷰] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models","children":"James Cheng이 arXiv에 게시한 'Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models"}]]}]]}],["$","article","2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models","children":"[논문리뷰] SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models","children":"Mohit Bansal이 arXiv에 게시한 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-SciVideoBench-Benchmarking-Scientific-Video-Reasoning-in-Large-Multimodal-Models"}]]}]]}],["$","article","2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation","children":"[논문리뷰] SViM3D: Stable Video Material Diffusion for Single Image 3D Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation","children":"arXiv에 게시된 'SViM3D: Stable Video Material Diffusion for Single Image 3D Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation"}]]}]]}],["$","article","2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization","children":"[논문리뷰] Reinforcing Diffusion Models by Direct Group Preference Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization","children":"Jing Tang이 arXiv에 게시한 'Reinforcing Diffusion Models by Direct Group Preference Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Reinforcing-Diffusion-Models-by-Direct-Group-Preference-Optimization"}]]}]]}],["$","article","2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training","children":"[논문리뷰] Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training","children":"Peng Cheng이 arXiv에 게시한 'Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training"}]]}]]}],["$","article","2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation","children":"[논문리뷰] R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation","children":"Zheng Zhu이 arXiv에 게시한 'R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-R2RGEN-Real-to-Real-3D-Data-Generation-for-Spatially-Generalized-Manipulation"}]]}]]}],["$","article","2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents","children":"[논문리뷰] NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents","children":"Baixuan Xu이 arXiv에 게시한 'NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-NewtonBench-Benchmarking-Generalizable-Scientific-Law-Discovery-in-LLM-Agents"}]]}]]}],["$","article","2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints","children":"[논문리뷰] NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints","children":"arXiv에 게시된 'NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-NaViL-Rethinking-Scaling-Properties-of-Native-Multimodal-Large-Language-Models-under-Data-Constraints"}]]}]]}],["$","article","2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning","children":"[논문리뷰] Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning","children":"arXiv에 게시된 'Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Meta-Awareness-Enhances-Reasoning-Models-Self-Alignment-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens","children":"[논문리뷰] Memory Retrieval and Consolidation in Large Language Models through Function Tokens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens","children":"arXiv에 게시된 'Memory Retrieval and Consolidation in Large Language Models through Function Tokens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens"}]]}]]}],["$","article","2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model","children":"[논문리뷰] MemMamba: Rethinking Memory Patterns in State Space Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model","children":"Xiao Sun이 arXiv에 게시한 'MemMamba: Rethinking Memory Patterns in State Space Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-MemMamba-Rethinking-Memory-Patterns-in-State-Space-Model"}]]}]]}],["$","article","2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization","children":"[논문리뷰] MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization","children":"vanilla1116이 arXiv에 게시한 'MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-MM-HELIX-Boosting-Multimodal-Long-Chain-Reflective-Reasoning-with-Holistic-Platform-and-Adaptive-Hybrid-Policy-Optimization"}]]}]]}],["$","article","2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward","children":"[논문리뷰] Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward","children":"arXiv에 게시된 'Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward"}]]}]]}],["$","article","2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling","children":"[논문리뷰] LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling","children":"arXiv에 게시된 'LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-LongRM-Revealing-and-Unlocking-the-Context-Boundary-of-Reward-Modeling"}]]}]]}],["$","article","2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs","children":"[논문리뷰] Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs","children":"Franck Dernoncourt이 arXiv에 게시한 'Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Learning-to-Route-LLMs-from-Bandit-Feedback-One-Policy-Many-Trade-offs"}]]}]]}],["$","article","2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks","children":"[논문리뷰] Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks","children":"arXiv에 게시된 'Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Learning-on-the-Job-An-Experience-Driven-Self-Evolving-Agent-for-Long-Horizon-Tasks"}]]}]]}],["$","article","2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency","children":"[논문리뷰] Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency","children":"Jintao Zhang이 arXiv에 게시한 'Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Large-Scale-Diffusion-Distillation-via-Score-Regularized-Continuous-Time-Consistency"}]]}]]}],["$","article","2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions","children":"[논문리뷰] LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions","children":"arXiv에 게시된 'LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-LLMs-Learn-to-Deceive-Unintentionally-Emergent-Misalignment-in-Dishonesty-from-Misaligned-Samples-to-Biased-Human-AI-Interactions"}]]}]]}],["$","article","2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance","children":"[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance","children":"Xinghui Li이 arXiv에 게시한 'InstructX: Towards Unified Visual Editing with MLLM Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance"}]]}]]}],["$","article","2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense","children":"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense","children":"arXiv에 게시된 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense"}]]}]]}],["$","article","2025-10-10-GCPO-When-Contrast-Fails-Go-Gold",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-GCPO-When-Contrast-Fails-Go-Gold","children":"[논문리뷰] GCPO: When Contrast Fails, Go Gold"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-GCPO-When-Contrast-Fails-Go-Gold","children":"arXiv에 게시된 'GCPO: When Contrast Fails, Go Gold' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-GCPO-When-Contrast-Fails-Go-Gold"}]]}]]}],["$","article","2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning","children":"[논문리뷰] From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning","children":"Feiwei Qin이 arXiv에 게시한 'From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning"}]]}]]}],["$","article","2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models","children":"[논문리뷰] First Try Matters: Revisiting the Role of Reflection in Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models","children":"Wee Sun Lee이 arXiv에 게시한 'First Try Matters: Revisiting the Role of Reflection in Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-First-Try-Matters-Revisiting-the-Role-of-Reflection-in-Reasoning-Models"}]]}]]}],["$","article","2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization","children":"[논문리뷰] Fidelity-Aware Data Composition for Robust Robot Generalization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization","children":"Liliang Chen이 arXiv에 게시한 'Fidelity-Aware Data Composition for Robust Robot Generalization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Fidelity-Aware-Data-Composition-for-Robust-Robot-Generalization"}]]}]]}],["$","article","2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints","children":"[논문리뷰] Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints","children":"Huazhe Xu이 arXiv에 게시한 'Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints"}]]}]]}],["$","article","2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model","children":"[논문리뷰] DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model","children":"Li Yi이 arXiv에 게시한 'DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-DexNDM-Closing-the-Reality-Gap-for-Dexterous-In-Hand-Rotation-via-Joint-Wise-Neural-Dynamics-Model"}]]}]]}],["$","article","2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy","children":"[논문리뷰] DeepPrune: Parallel Scaling without Inter-trace Redundancy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy","children":"arXiv에 게시된 'DeepPrune: Parallel Scaling without Inter-trace Redundancy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy"}]]}]]}],["$","article","2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards","children":"[논문리뷰] CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards","children":"Yijiang Li이 arXiv에 게시한 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-CoMAS-Co-Evolving-Multi-Agent-Systems-via-Interaction-Rewards"}]]}]]}],["$","article","2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window","children":"[논문리뷰] Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window","children":"Yaojie Lu이 arXiv에 게시한 'Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Beyond-Turn-Limits-Training-Deep-Search-Agents-with-Dynamic-Context-Window"}]]}]]}],["$","article","2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization","children":"[논문리뷰] Beyond Outliers: A Study of Optimizers Under Quantization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization","children":"arXiv에 게시된 'Beyond Outliers: A Study of Optimizers Under Quantization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Beyond-Outliers-A-Study-of-Optimizers-Under-Quantization"}]]}]]}],["$","article","2025-10-10-Agent-Learning-via-Early-Experience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Agent-Learning-via-Early-Experience","children":"[논문리뷰] Agent Learning via Early Experience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Agent-Learning-via-Early-Experience","children":"arXiv에 게시된 'Agent Learning via Early Experience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Agent-Learning-via-Early-Experience"}]]}]]}],["$","article","2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation","children":"[논문리뷰] ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation","children":"arXiv에 게시된 'ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-ARTDECO-Towards-Efficient-and-High-Fidelity-On-the-Fly-3D-Reconstruction-with-Structured-Scene-Representation"}]]}]]}],["$","article","2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning","children":"[논문리뷰] A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning","children":"arXiv에 게시된 'A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","children":"[논문리뷰] WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation","children":"arXiv에 게시된 'WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation"}]]}]]}],["$","article","2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention","children":"[논문리뷰] Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention","children":"arXiv에 게시된 'Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Why-Low-Precision-Transformer-Training-Fails-An-Analysis-on-Flash-Attention"}]]}]]}],["$","article","2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation","children":"[논문리뷰] When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation","children":"arXiv에 게시된 'When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-When-Benchmarks-Age-Temporal-Misalignment-through-Large-Language-Model-Factuality-Evaluation"}]]}]]}],["$","article","2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference","children":"[논문리뷰] Vibe Checker: Aligning Code Evaluation with Human Preference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference","children":"arXiv에 게시된 'Vibe Checker: Aligning Code Evaluation with Human Preference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference"}]]}]]}],["$","article","2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking","children":"[논문리뷰] U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking","children":"Heqin Zhu이 arXiv에 게시한 'U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking"}]]}]]}],["$","article","2025-10-9-The-Markovian-Thinker",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-The-Markovian-Thinker","children":"[논문리뷰] The Markovian Thinker"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-The-Markovian-Thinker","children":"arXiv에 게시된 'The Markovian Thinker' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-The-Markovian-Thinker"}]]}]]}],["$","article","2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP","children":"[논문리뷰] The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP","children":"arXiv에 게시된 'The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-The-African-Languages-Lab-A-Collaborative-Approach-to-Advancing-Low-Resource-African-NLP"}]]}]]}],["$","article","2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models","children":"[논문리뷰] TTRV: Test-Time Reinforcement Learning for Vision Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models","children":"Serena Yeung-Levy이 arXiv에 게시한 'TTRV: Test-Time Reinforcement Learning for Vision Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models"}]]}]]}],["$","article","2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation","children":"[논문리뷰] StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation","children":"arXiv에 게시된 'StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-StaMo-Unsupervised-Learning-of-Generalizable-Robot-Motion-from-Compact-State-Representation"}]]}]]}],["$","article","2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models","children":"[논문리뷰] SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models","children":"Kevin Lin이 arXiv에 게시한 'SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models"}]]}]]}],["$","article","2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces","children":"[논문리뷰] Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces","children":"arXiv에 게시된 'Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces"}]]}]]}],["$","article","2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective","children":"[논문리뷰] Revisiting Long-context Modeling from Context Denoising Perspective"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective","children":"arXiv에 게시된 'Revisiting Long-context Modeling from Context Denoising Perspective' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Revisiting-Long-context-Modeling-from-Context-Denoising-Perspective"}]]}]]}],["$","article","2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training","children":"[논문리뷰] RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training","children":"arXiv에 게시된 'RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLARL-Training"}]]}]]}],["$","article","2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought","children":"[논문리뷰] Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought","children":"arXiv에 게시된 'Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Pushing-on-Multilingual-Reasoning-Models-with-Language-Mixed-Chain-of-Thought"}]]}]]}],["$","article","2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs","children":"[논문리뷰] Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs","children":"Jingyi Liao이 arXiv에 게시한 'Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Patch-as-Decodable-Token-Towards-Unified-Multi-Modal-Vision-Tasks-in-MLLMs"}]]}]]}],["$","article","2025-10-9-Online-Generic-Event-Boundary-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Online-Generic-Event-Boundary-Detection","children":"[논문리뷰] Online Generic Event Boundary Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Online-Generic-Event-Boundary-Detection","children":"Jonghyun Choi이 arXiv에 게시한 'Online Generic Event Boundary Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Online-Generic-Event-Boundary-Detection"}]]}]]}],["$","article","2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot","children":"[논문리뷰] OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot","children":"arXiv에 게시된 'OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot"}]]}]]}],["$","article","2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable","children":"[논문리뷰] NorMuon: Making Muon more efficient and scalable"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable","children":"Tuo Zhao이 arXiv에 게시한 'NorMuon: Making Muon more efficient and scalable' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-NorMuon-Making-Muon-more-efficient-and-scalable"}]]}]]}],["$","article","2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling","children":"[논문리뷰] Native Hybrid Attention for Efficient Sequence Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling","children":"Yu Cheng이 arXiv에 게시한 'Native Hybrid Attention for Efficient Sequence Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling"}]]}]]}],["$","article","2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization","children":"[논문리뷰] Multi-Agent Tool-Integrated Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization","children":"Lidong Bing이 arXiv에 게시한 'Multi-Agent Tool-Integrated Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Multi-Agent-Tool-Integrated-Policy-Optimization"}]]}]]}],["$","article","2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer","children":"[논문리뷰] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer","children":"arXiv에 게시된 'Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer"}]]}]]}],["$","article","2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline","children":"[논문리뷰] MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline","children":"arXiv에 게시된 'MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-MLE-Smith-Scaling-MLE-Tasks-with-Automated-Multi-Agent-Pipeline"}]]}]]}],["$","article","2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation","children":"[논문리뷰] MATRIX: Mask Track Alignment for Interaction-aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation","children":"Hyunwook Choi이 arXiv에 게시한 'MATRIX: Mask Track Alignment for Interaction-aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation"}]]}]]}],["$","article","2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding","children":"[논문리뷰] Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding","children":"arXiv에 게시된 'Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding"}]]}]]}],["$","article","2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals","children":"[논문리뷰] Heptapod: Language Modeling on Visual Signals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals","children":"arXiv에 게시된 'Heptapod: Language Modeling on Visual Signals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Heptapod-Language-Modeling-on-Visual-Signals"}]]}]]}],["$","article","2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models","children":"[논문리뷰] G^2RPO: Granular GRPO for Precise Reward in Flow Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models","children":"arXiv에 게시된 'G^2RPO: Granular GRPO for Precise Reward in Flow Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-G2RPO-Granular-GRPO-for-Precise-Reward-in-Flow-Models"}]]}]]}],["$","article","2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents","children":"[논문리뷰] DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents","children":"arXiv에 게시된 'DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents"}]]}]]}],["$","article","2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection","children":"[논문리뷰] D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection","children":"Yueqi Duan이 arXiv에 게시한 'D^3QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-D3QE-Learning-Discrete-Distribution-Discrepancy-aware-Quantization-Error-for-Autoregressive-Generated-Image-Detection"}]]}]]}],["$","article","2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models","children":"[논문리뷰] Cache-to-Cache: Direct Semantic Communication Between Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models","children":"arXiv에 게시된 'Cache-to-Cache: Direct Semantic Communication Between Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models"}]]}]]}],["$","article","2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling","children":"[논문리뷰] CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling","children":"Chengpeng Li이 arXiv에 게시한 'CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling"}]]}]]}],["$","article","2025-10-9-Bridging-Text-and-Video-Generation-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey","children":"[논문리뷰] Bridging Text and Video Generation: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey","children":"G. Maragatham이 arXiv에 게시한 'Bridging Text and Video Generation: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Bridging-Text-and-Video-Generation-A-Survey"}]]}]]}],["$","article","2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models","children":"[논문리뷰] Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models","children":"arXiv에 게시된 'Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models"}]]}]]}],["$","article","2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling","children":"[논문리뷰] Artificial Hippocampus Networks for Efficient Long-Context Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling","children":"arXiv에 게시된 'Artificial Hippocampus Networks for Efficient Long-Context Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling"}]]}]]}],["$","article","2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods","children":"[논문리뷰] Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods","children":"Yiyu Wang이 arXiv에 게시한 'Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods"}]]}]]}],["$","article","2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning","children":"[논문리뷰] AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning","children":"Zongze Li이 arXiv에 게시한 'AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning"}]]}]]}],["$","article","2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation","children":"[논문리뷰] VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation","children":"arXiv에 게시된 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-VeriGuard-Enhancing-LLM-Agent-Safety-via-Verified-Code-Generation"}]]}]]}],["$","article","2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness","children":"[논문리뷰] Training Dynamics Impact Post-Training Quantization Robustness"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness","children":"Jonas Geiping이 arXiv에 게시한 'Training Dynamics Impact Post-Training Quantization Robustness' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness"}]]}]]}],["$","article","2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation","children":"[논문리뷰] TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation","children":"arXiv에 게시된 'TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-TensorBLEU-Vectorized-GPU-based-BLEU-Score-Implementation-for-Per-Sentence-In-Training-Evaluation"}]]}]]}],["$","article","2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning","children":"[논문리뷰] TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning","children":"arXiv에 게시된 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning"}]]}]]}],["$","article","2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos","children":"[논문리뷰] ShapeGen4D: Towards High Quality 4D Shape Generation from Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos","children":"Sergey Tulyakov이 arXiv에 게시한 'ShapeGen4D: Towards High Quality 4D Shape Generation from Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-ShapeGen4D-Towards-High-Quality-4D-Shape-Generation-from-Videos"}]]}]]}],["$","article","2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning","children":"[논문리뷰] Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning","children":"Zhuoshi Pan이 arXiv에 게시한 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Scaling-Code-Assisted-Chain-of-Thoughts-and-Instructions-for-Model-Reasoning"}]]}]]}],["$","article","2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions","children":"[논문리뷰] Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions","children":"arXiv에 게시된 'Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Revisiting-Modeling-and-Evaluation-Approaches-in-Speech-Emotion-Recognition-Considering-Subjectivity-of-Annotators-and-Ambiguity-of-Emotions"}]]}]]}],["$","article","2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning","children":"[논문리뷰] Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning","children":"arXiv에 게시된 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Refusal-Falls-off-a-Cliff-How-Safety-Alignment-Fails-in-Reasoning"}]]}]]}],["$","article","2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations","children":"[논문리뷰] Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations","children":"arXiv에 게시된 'Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Presenting-a-Paper-is-an-Art-Self-Improvement-Aesthetic-Agents-for-Academic-Presentations"}]]}]]}],["$","article","2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows","children":"[논문리뷰] OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows","children":"arXiv에 게시된 'OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-OneFlow-Concurrent-Mixed-Modal-and-Interleaved-Generation-with-Edit-Flows"}]]}]]}],["$","article","2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models","children":"[논문리뷰] No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models","children":"Xiao Xiao Sun이 arXiv에 게시한 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-No-Tokens-Wasted-Leveraging-Long-Context-in-Biomedical-Vision-Language-Models"}]]}]]}],["$","article","2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context","children":"[논문리뷰] Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context","children":"arXiv에 게시된 'Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Mixing-Mechanisms-How-Language-Models-Retrieve-Bound-Entities-In-Context"}]]}]]}],["$","article","2025-10-8-MixReasoning-Switching-Modes-to-Think",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think","children":"[논문리뷰] MixReasoning: Switching Modes to Think"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think","children":"arXiv에 게시된 'MixReasoning: Switching Modes to Think' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think"}]]}]]}],["$","article","2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization","children":"[논문리뷰] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization","children":"sirano1004이 arXiv에 게시한 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization"}]]}]]}],["$","article","2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","children":"[논문리뷰] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation","children":"Zheng Zhan이 arXiv에 게시한 'LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-LightCache-Memory-Efficient-Training-Free-Acceleration-for-Video-Generation"}]]}]]}],["$","article","2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks","children":"[논문리뷰] Less is More: Recursive Reasoning with Tiny Networks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks","children":"arXiv에 게시된 'Less is More: Recursive Reasoning with Tiny Networks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Less-is-More-Recursive-Reasoning-with-Tiny-Networks"}]]}]]}],["$","article","2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use","children":"[논문리뷰] In-the-Flow Agentic System Optimization for Effective Planning and Tool Use"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use","children":"arXiv에 게시된 'In-the-Flow Agentic System Optimization for Effective Planning and Tool Use' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use"}]]}]]}],["$","article","2025-10-8-Human3R-Everyone-Everywhere-All-at-Once",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Human3R-Everyone-Everywhere-All-at-Once","children":"[논문리뷰] Human3R: Everyone Everywhere All at Once"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Human3R-Everyone-Everywhere-All-at-Once","children":"Yuliang Xiu이 arXiv에 게시한 'Human3R: Everyone Everywhere All at Once' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Human3R-Everyone-Everywhere-All-at-Once"}]]}]]}],["$","article","2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video","children":"[논문리뷰] HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video","children":"Katelyn Gao이 arXiv에 게시한 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-HoloScene-Simulation-Ready-Interactive-3D-Worlds-from-a-Single-Video"}]]}]]}],["$","article","2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation","children":"[논문리뷰] HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation","children":"Radu State이 arXiv에 게시한 'HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-HalluGuard-Evidence-Grounded-Small-Reasoning-Models-to-Mitigate-Hallucinations-in-Retrieval-Augmented-Generation"}]]}]]}],["$","article","2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs","children":"[논문리뷰] Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs","children":"arXiv에 게시된 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Fathom-DeepResearch-Unlocking-Long-Horizon-Information-Retrieval-and-Synthesis-for-SLMs"}]]}]]}],["$","article","2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM","children":"[논문리뷰] Fast-dLLM v2: Efficient Block-Diffusion LLM"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM","children":"arXiv에 게시된 'Fast-dLLM v2: Efficient Block-Diffusion LLM' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Fast-dLLM-v2-Efficient-Block-Diffusion-LLM"}]]}]]}],["$","article","2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models","children":"[논문리뷰] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models","children":"arXiv에 게시된 'Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Equilibrium-Matching-Generative-Modeling-with-Implicit-Energy-Based-Models"}]]}]]}],["$","article","2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark","children":"[논문리뷰] EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark","children":"Tianwen Qian이 arXiv에 게시한 'EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark"}]]}]]}],["$","article","2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching","children":"[논문리뷰] Drax: Speech Recognition with Discrete Flow Matching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching","children":"arXiv에 게시된 'Drax: Speech Recognition with Discrete Flow Matching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Drax-Speech-Recognition-with-Discrete-Flow-Matching"}]]}]]}],["$","article","2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models","children":"[논문리뷰] Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models","children":"Jacobo Azcona이 arXiv에 게시한 'Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Distributional-Semantics-Tracing-A-Framework-for-Explaining-Hallucinations-in-Large-Language-Models"}]]}]]}],["$","article","2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation","children":"[논문리뷰] Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation","children":"arXiv에 게시된 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Discrete-Diffusion-Models-with-MLLMs-for-Unified-Medical-Multimodal-Generation"}]]}]]}],["$","article","2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics","children":"[논문리뷰] Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics","children":"arXiv에 게시된 'Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Demystifying-deep-search-a-holistic-evaluation-with-hint-free-multi-hop-questions-and-factorised-metrics"}]]}]]}],["$","article","2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","children":"[논문리뷰] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","children":"Chengzu Li이 arXiv에 게시한 'Deforming Videos to Masks: Flow Matching for Referring Video Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation"}]]}]]}],["$","article","2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","children":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","children":"Zheli Liu이 arXiv에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning"}]]}]]}],["$","article","2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation","children":"[논문리뷰] CoDA: Coding LM via Diffusion Adaptation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation","children":"arXiv에 게시된 'CoDA: Coding LM via Diffusion Adaptation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation"}]]}]]}],["$","article","2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding","children":"[논문리뷰] CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding","children":"arXiv에 게시된 'CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding"}]]}]]}],["$","article","2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation","children":"[논문리뷰] CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation","children":"arXiv에 게시된 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation"}]]}]]}],["$","article","2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks","children":"[논문리뷰] Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks","children":"Pedro Bizarro이 arXiv에 게시한 'Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-Benchmark-It-Yourself-BIY-Preparing-a-Dataset-and-Benchmarking-AI-Models-for-Scatterplot-Related-Tasks"}]]}]]}],["$","article","2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions","children":"[논문리뷰] BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions","children":"Shipei Lin이 arXiv에 게시한 'BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-BIRD-INTERACT-Re-imagining-Text-to-SQL-Evaluation-for-Large-Language-Models-via-Lens-of-Dynamic-Interactions"}]]}]]}],["$","article","2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization","children":"[논문리뷰] ASPO: Asymmetric Importance Sampling Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization","children":"Xiu Li이 arXiv에 게시한 'ASPO: Asymmetric Importance Sampling Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization"}]]}]]}],["$","article","2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems","children":"[논문리뷰] AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems","children":"Jose Dolz이 arXiv에 게시한 'AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-AInstein-Assessing-the-Feasibility-of-AI-Generated-Approaches-to-Research-Problems"}]]}]]}],["$","article","2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling","children":"[논문리뷰] A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling","children":"sirano1004이 arXiv에 게시한 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-A-Contextual-Quality-Reward-Model-for-Reliable-and-Efficient-Best-of-N-Sampling"}]]}]]}],["$","article","2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos","children":"[논문리뷰] Watch and Learn: Learning to Use Computers from Online Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos","children":"Oriana Riva이 arXiv에 게시한 'Watch and Learn: Learning to Use Computers from Online Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos"}]]}]]}],["$","article","2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models","children":"[논문리뷰] Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models","children":"zeliang0426이 arXiv에 게시한 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models"}]]}]]}],["$","article","2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"[논문리뷰] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"Paul Debevec이 arXiv에 게시한 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation"}]]}]]}],["$","article","2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents","children":"[논문리뷰] Utility-Learning Tension in Self-Modifying Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents","children":"Peter Jin이 arXiv에 게시한 'Utility-Learning Tension in Self-Modifying Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Utility-Learning-Tension-in-Self-Modifying-Agents"}]]}]]}],["$","article","2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents","children":"[논문리뷰] Thai Semantic End-of-Turn Detection for Real-Time Voice Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents","children":"Monthol Charattrakool이 arXiv에 게시한 'Thai Semantic End-of-Turn Detection for Real-Time Voice Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Thai-Semantic-End-of-Turn-Detection-for-Real-Time-Voice-Agents"}]]}]]}],["$","article","2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs","children":"[논문리뷰] SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs","children":"arXiv에 게시된 'SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-SwiReasoning-Switch-Thinking-in-Latent-and-Explicit-for-Pareto-Superior-Reasoning-LLMs"}]]}]]}],["$","article","2025-10-7-Self-Reflective-Generation-at-Test-Time",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time","children":"[논문리뷰] Self-Reflective Generation at Test Time"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time","children":"Shuang Qiu이 arXiv에 게시한 'Self-Reflective Generation at Test Time' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time"}]]}]]}],["$","article","2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder","children":"[논문리뷰] SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder","children":"Or Patashnik이 arXiv에 게시한 'SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-SAEdit-Token-level-control-for-continuous-image-editing-via-Sparse-AutoEncoder"}]]}]]}],["$","article","2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training","children":"[논문리뷰] Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training","children":"arXiv에 게시된 'Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Reinforce-Ada-An-Adaptive-Sampling-Framework-for-Reinforce-Style-LLM-Training"}]]}]]}],["$","article","2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models","children":"[논문리뷰] Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models","children":"arXiv에 게시된 'Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Reactive-Transformer-RxT-Stateful-Real-Time-Processing-for-Event-Driven-Reactive-Language-Models"}]]}]]}],["$","article","2025-10-7-Optimal-Scaling-Needs-Optimal-Norm",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm","children":"[논문리뷰] Optimal Scaling Needs Optimal Norm"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm","children":"Stefan Kesselheim이 arXiv에 게시한 'Optimal Scaling Needs Optimal Norm' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm"}]]}]]}],["$","article","2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition","children":"[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition","children":"arXiv에 게시된 'MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition"}]]}]]}],["$","article","2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information","children":"[논문리뷰] MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information","children":"arXiv에 게시된 'MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-MITS-Enhanced-Tree-Search-Reasoning-for-LLMs-via-Pointwise-Mutual-Information"}]]}]]}],["$","article","2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning","children":"[논문리뷰] Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning","children":"arXiv에 게시된 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL","children":"[논문리뷰] LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL","children":"arXiv에 게시된 'LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-LLMSQL-Upgrading-WikiSQL-for-the-LLM-Era-of-Text-to-SQL"}]]}]]}],["$","article","2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions","children":"[논문리뷰] Judging with Confidence: Calibrating Autoraters to Preference Distributions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions","children":"arXiv에 게시된 'Judging with Confidence: Calibrating Autoraters to Preference Distributions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions"}]]}]]}],["$","article","2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models","children":"[논문리뷰] Imperceptible Jailbreaking against Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models","children":"arXiv에 게시된 'Imperceptible Jailbreaking against Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models"}]]}]]}],["$","article","2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights","children":"[논문리뷰] Hybrid Architectures for Language Models: Systematic Analysis and Design Insights"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights","children":"arXiv에 게시된 'Hybrid Architectures for Language Models: Systematic Analysis and Design Insights' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Hybrid-Architectures-for-Language-Models-Systematic-Analysis-and-Design-Insights"}]]}]]}],["$","article","2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition","children":"[논문리뷰] HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition","children":"arXiv에 게시된 'HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-HiKE-Hierarchical-Evaluation-Framework-for-Korean-English-Code-Switching-Speech-Recognition"}]]}]]}],["$","article","2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs","children":"[논문리뷰] Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs","children":"Zeyi Liao이 arXiv에 게시한 'Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Graph2Eval-Automatic-Multimodal-Task-Generation-for-Agents-via-Knowledge-Graphs"}]]}]]}],["$","article","2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where","children":"[논문리뷰] Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where","children":"Denis Peskoff이 arXiv에 게시한 'Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Good-Intentions-Beyond-ACL-Who-Does-NLP-for-Social-Good-and-Where"}]]}]]}],["$","article","2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data","children":"[논문리뷰] Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data","children":"arXiv에 게시된 'Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data"}]]}]]}],["$","article","2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","children":"[논문리뷰] Factuality Matters: When Image Generation and Editing Meet Structured Visuals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","children":"Boxiang Qiu이 arXiv에 게시한 'Factuality Matters: When Image Generation and Editing Meet Structured Visuals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals"}]]}]]}],["$","article","2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty","children":"[논문리뷰] EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty","children":"Xuanwu Wang이 arXiv에 게시한 'EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty"}]]}]]}],["$","article","2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models","children":"[논문리뷰] Epistemic Diversity and Knowledge Collapse in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models","children":"arXiv에 게시된 'Epistemic Diversity and Knowledge Collapse in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models"}]]}]]}],["$","article","2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform","children":"[논문리뷰] Code4MeV2: a Research-oriented Code-completion Platform"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform","children":"arXiv에 게시된 'Code4MeV2: a Research-oriented Code-completion Platform' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Code4MeV2-a-Research-oriented-Code-completion-Platform"}]]}]]}],["$","article","2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","children":"[논문리뷰] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation","children":"arXiv에 게시된 'ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-ChronoEdit-Towards-Temporal-Reasoning-for-Image-Editing-and-World-Simulation"}]]}]]}],["$","article","2025-10-7-Character-Mixing-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation","children":"[논문리뷰] Character Mixing for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation","children":"arXiv에 게시된 'Character Mixing for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Character-Mixing-for-Video-Generation"}]]}]]}],["$","article","2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails","children":"[논문리뷰] Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails","children":"Xinyuan Liu이 arXiv에 게시한 'Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Alignment-Tipping-Process-How-Self-Evolution-Pushes-LLM-Agents-Off-the-Rails"}]]}]]}],["$","article","2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models","children":"[논문리뷰] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models","children":"Fenglu Hong이 arXiv에 게시한 'Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Agentic-Context-Engineering-Evolving-Contexts-for-Self-Improving-Language-Models"}]]}]]}],["$","article","2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning","children":"[논문리뷰] AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning","children":"Zeliang Zhang이 arXiv에 게시한 'AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-AdvEvo-MARL-Shaping-Internalized-Safety-through-Adversarial-Co-Evolution-in-Multi-Agent-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents","children":"[논문리뷰] Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents","children":"Boyi Wei이 arXiv에 게시한 'Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Your-Agent-May-Misevolve-Emergent-Risks-in-Self-evolving-LLM-Agents"}]]}]]}],["$","article","2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents","children":"[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents","children":"Neil Zhenqiang Gong이 arXiv에 게시한 'WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents"}]]}]]}],["$","article","2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles","children":"[논문리뷰] Triangle Splatting+: Differentiable Rendering with Opaque Triangles"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles","children":"Matheus Gadelha이 arXiv에 게시한 'Triangle Splatting+: Differentiable Rendering with Opaque Triangles' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Triangle-Splatting-Differentiable-Rendering-with-Opaque-Triangles"}]]}]]}],["$","article","2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling","children":"[논문리뷰] TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling","children":"Juhan Nam이 arXiv에 게시한 'TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-TalkPlay-Tools-Conversational-Music-Recommendation-with-LLM-Tool-Calling"}]]}]]}],["$","article","2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys","children":"[논문리뷰] SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys","children":"Shuo Wang이 arXiv에 게시한 'SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-SurveyBench-How-Well-Can-LLM-Agents-Write-Academic-Surveys"}]]}]]}],["$","article","2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus","children":"[논문리뷰] SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus","children":"Zhonghao Zhang이 arXiv에 게시한 'SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus"}]]}]]}],["$","article","2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey","children":"[논문리뷰] Self-Improvement in Multimodal Large Language Models: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey","children":"Yapeng Tian이 arXiv에 게시한 'Self-Improvement in Multimodal Large Language Models: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Self-Improvement-in-Multimodal-Large-Language-Models-A-Survey"}]]}]]}],["$","article","2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces","children":"[논문리뷰] Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces","children":"arXiv에 게시된 'Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Scaling-Policy-Compliance-Assessment-in-Language-Models-with-Policy-Reasoning-Traces"}]]}]]}],["$","article","2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration","children":"[논문리뷰] REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration","children":"arXiv에 게시된 'REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-REPAIR-Robust-Editing-via-Progressive-Adaptive-Intervention-and-Reintegration"}]]}]]}],["$","article","2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features","children":"[논문리뷰] OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features","children":"Elena Tutubalina이 arXiv에 게시한 'OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-OrtSAE-Orthogonal-Sparse-Autoencoders-Uncover-Atomic-Features"}]]}]]}],["$","article","2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving","children":"[논문리뷰] NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving","children":"arXiv에 게시된 'NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-NuRisk-A-Visual-Question-Answering-Dataset-for-Agent-Level-Risk-Assessment-in-Autonomous-Driving"}]]}]]}],["$","article","2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning","children":"[논문리뷰] LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning","children":"arXiv에 게시된 'LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-LSPO-Length-aware-Dynamic-Sampling-for-Policy-Optimization-in-LLM-Reasoning"}]]}]]}],["$","article","2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models","children":"[논문리뷰] LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models","children":"Yu-Chiang Frank Wang이 arXiv에 게시한 'LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping","children":"[논문리뷰] Improving GUI Grounding with Explicit Position-to-Coordinate Mapping"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping","children":"Spandana Gella이 arXiv에 게시한 'Improving GUI Grounding with Explicit Position-to-Coordinate Mapping' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Improving-GUI-Grounding-with-Explicit-Position-to-Coordinate-Mapping"}]]}]]}],["$","article","2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty","children":"[논문리뷰] How Confident are Video Models? Empowering Video Models to Express their Uncertainty"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty","children":"Anirudha Majumdar이 arXiv에 게시한 'How Confident are Video Models? Empowering Video Models to Express their Uncertainty' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-How-Confident-are-Video-Models-Empowering-Video-Models-to-Express-their-Uncertainty"}]]}]]}],["$","article","2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs","children":"[논문리뷰] Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs","children":"arXiv에 게시된 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs"}]]}]]}],["$","article","2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents","children":"[논문리뷰] FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents","children":"Léo Boisvert이 arXiv에 게시한 'FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-FocusAgent-Simple-Yet-Effective-Ways-of-Trimming-the-Large-Context-of-Web-Agents"}]]}]]}],["$","article","2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation","children":"[논문리뷰] Efficient Multi-modal Large Language Models via Progressive Consistency Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation","children":"arXiv에 게시된 'Efficient Multi-modal Large Language Models via Progressive Consistency Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Efficient-Multi-modal-Large-Language-Models-via-Progressive-Consistency-Distillation"}]]}]]}],["$","article","2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern","children":"[논문리뷰] DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern","children":"Jia Li이 arXiv에 게시한 'DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-DiffTester-Accelerating-Unit-Test-Generation-for-Diffusion-LLMs-via-Repetitive-Pattern"}]]}]]}],["$","article","2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition","children":"[논문리뷰] Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition","children":"arXiv에 게시된 'Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Compose-Your-Policies-Improving-Diffusion-based-or-Flow-based-Robot-Policies-via-Test-time-Distribution-level-Composition"}]]}]]}],["$","article","2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization","children":"[논문리뷰] CoDA: Agentic Systems for Collaborative Data Visualization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization","children":"arXiv에 게시된 'CoDA: Agentic Systems for Collaborative Data Visualization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-CoDA-Agentic-Systems-for-Collaborative-Data-Visualization"}]]}]]}],["$","article","2025-10-6-Apriel-1-5-15b-Thinker",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Apriel-1-5-15b-Thinker","children":"[논문리뷰] Apriel-1.5-15b-Thinker"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Apriel-1-5-15b-Thinker","children":"arXiv에 게시된 'Apriel-1.5-15b-Thinker' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Apriel-1-5-15b-Thinker"}]]}]]}],["$","article","2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents","children":"[논문리뷰] Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents","children":"Jong Chul Ye이 arXiv에 게시한 'Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-Align-Your-Tangent-Training-Better-Consistency-Models-via-Manifold-Aligned-Tangents"}]]}]]}],["$","article","2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning","children":"[논문리뷰] A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning","children":"arXiv에 게시된 'A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-06 13:29:11+0900","children":"2025년 10월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-6-A-Practitioners-Guide-to-Multi-turn-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls","children":"[논문리뷰] Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls","children":"Stuart Shieber이 arXiv에 게시한 'Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Why-Cant-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls"}]]}]]}],["$","article","2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs","children":"[논문리뷰] VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs","children":"arXiv에 게시된 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-VLM-FO1-Bridging-the-Gap-Between-High-Level-Reasoning-and-Fine-Grained-Perception-in-VLMs"}]]}]]}],["$","article","2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators","children":"[논문리뷰] VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators","children":"Zirui Ge이 arXiv에 게시한 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-VLA-RFT-Vision-Language-Action-Reinforcement-Fine-tuning-with-Verified-Rewards-in-World-Simulators"}]]}]]}],["$","article","2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned","children":"[논문리뷰] Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned","children":"arXiv에 게시된 'Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Training-Vision-Language-Process-Reward-Models-for-Test-Time-Scaling-in-Multimodal-Reasoning-Key-Insights-and-Lessons-Learned"}]]}]]}],["$","article","2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction","children":"[논문리뷰] ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction","children":"arXiv에 게시된 'ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-ReSWD-ReSTIRd-not-shaken-Combining-Reservoir-Sampling-and-Sliced-Wasserstein-Distance-for-Variance-Reduction"}]]}]]}],["$","article","2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning","children":"[논문리뷰] PIPer: On-Device Environment Setup via Online Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning","children":"arXiv에 게시된 'PIPer: On-Device Environment Setup via Online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-PIPer-On-Device-Environment-Setup-via-Online-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models","children":"[논문리뷰] On Predictability of Reinforcement Learning Dynamics for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models","children":"Yuqing Huang이 arXiv에 게시한 'On Predictability of Reinforcement Learning Dynamics for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models"}]]}]]}],["$","article","2025-10-2-Making-not-Taking-the-Best-of-N",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Making-not-Taking-the-Best-of-N","children":"[논문리뷰] Making, not Taking, the Best of N"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Making-not-Taking-the-Best-of-N","children":"arXiv에 게시된 'Making, not Taking, the Best of N' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Making-not-Taking-the-Best-of-N"}]]}]]}],["$","article","2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation","children":"[논문리뷰] Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation","children":"arXiv에 게시된 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Knapsack-RL-Unlocking-Exploration-of-LLMs-via-Optimizing-Budget-Allocation"}]]}]]}],["$","article","2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA","children":"[논문리뷰] JoyAgent-JDGenie: Technical Report on the GAIA"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA","children":"arXiv에 게시된 'JoyAgent-JDGenie: Technical Report on the GAIA' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-JoyAgent-JDGenie-Technical-Report-on-the-GAIA"}]]}]]}],["$","article","2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents","children":"[논문리뷰] Infusing Theory of Mind into Socially Intelligent LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents","children":"arXiv에 게시된 'Infusing Theory of Mind into Socially Intelligent LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents"}]]}]]}],["$","article","2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning","children":"[논문리뷰] In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning","children":"Chaehyeon Chung이 arXiv에 게시한 'In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-In-Place-Feedback-A-New-Paradigm-for-Guiding-LLMs-in-Multi-Turn-Reasoning"}]]}]]}],["$","article","2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures","children":"[논문리뷰] Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures","children":"Andrea Passerini이 arXiv에 게시한 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Hyperdimensional-Probe-Decoding-LLM-Representations-via-Vector-Symbolic-Architectures"}]]}]]}],["$","article","2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness","children":"[논문리뷰] GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness","children":"Chien-Sheng Wu이 arXiv에 게시한 'GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-GUI-KV-Efficient-GUI-Agents-via-KV-Cache-with-Spatio-Temporal-Awareness"}]]}]]}],["$","article","2025-10-2-GEM-A-Gym-for-Agentic-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-GEM-A-Gym-for-Agentic-LLMs","children":"[논문리뷰] GEM: A Gym for Agentic LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-GEM-A-Gym-for-Agentic-LLMs","children":"arXiv에 게시된 'GEM: A Gym for Agentic LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-GEM-A-Gym-for-Agentic-LLMs"}]]}]]}],["$","article","2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution","children":"[논문리뷰] Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution","children":"arXiv에 게시된 'Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Flash-Searcher-Fast-and-Effective-Web-Agents-via-DAG-Based-Parallel-Execution"}]]}]]}],["$","article","2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models","children":"[논문리뷰] Eliciting Secret Knowledge from Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models","children":"Neel Nanda이 arXiv에 게시한 'Eliciting Secret Knowledge from Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Eliciting-Secret-Knowledge-from-Language-Models"}]]}]]}],["$","article","2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search","children":"[논문리뷰] DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search","children":"arXiv에 게시된 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-DeepSearch-Overcome-the-Bottleneck-of-Reinforcement-Learning-with-Verifiable-Rewards-via-Monte-Carlo-Tree-Search"}]]}]]}],["$","article","2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs","children":"[논문리뷰] CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs","children":"Hengyi Cai이 arXiv에 게시한 'CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-CurES-From-Gradient-Analysis-to-Efficient-Curriculum-Learning-for-Reasoning-LLMs"}]]}]]}],["$","article","2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation","children":"[논문리뷰] Code2Video: A Code-centric Paradigm for Educational Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation","children":"arXiv에 게시된 'Code2Video: A Code-centric Paradigm for Educational Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Code2Video-A-Code-centric-Paradigm-for-Educational-Video-Generation"}]]}]]}],["$","article","2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration","children":"[논문리뷰] BroRL: Scaling Reinforcement Learning via Broadened Exploration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration","children":"arXiv에 게시된 'BroRL: Scaling Reinforcement Learning via Broadened Exploration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-BroRL-Scaling-Reinforcement-Learning-via-Broadened-Exploration"}]]}]]}],["$","article","2025-10-2-Boolean-Satisfiability-via-Imitation-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Boolean-Satisfiability-via-Imitation-Learning","children":"[논문리뷰] Boolean Satisfiability via Imitation Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Boolean-Satisfiability-via-Imitation-Learning","children":"Xiangyu Xu이 arXiv에 게시한 'Boolean Satisfiability via Imitation Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Boolean-Satisfiability-via-Imitation-Learning"}]]}]]}],["$","article","2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","children":"[논문리뷰] BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration","children":"Xiangyang Xia이 arXiv에 게시한 'BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-BindWeave-Subject-Consistent-Video-Generation-via-Cross-Modal-Integration"}]]}]]}],["$","article","2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses","children":"[논문리뷰] BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses","children":"Julian McAuley이 arXiv에 게시한 'BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-BiasFreeBench-a-Benchmark-for-Mitigating-Bias-in-Large-Language-Model-Responses"}]]}]]}],["$","article","2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum","children":"[논문리뷰] Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum","children":"Hanghang Tong이 arXiv에 게시한 'Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-Beyond-Log-Likelihood-Probability-Based-Objectives-for-Supervised-Fine-Tuning-across-the-Model-Capability-Continuum"}]]}]]}],["$","article","2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications","children":"[논문리뷰] An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications","children":"Bram Adams이 arXiv에 게시한 'An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-An-Empirical-Study-of-Testing-Practices-in-Open-Source-AI-Agent-Frameworks-and-Agentic-Applications"}]]}]]}],["$","article","2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents","children":"[논문리뷰] ACON: Optimizing Context Compression for Long-horizon LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents","children":"arXiv에 게시된 'ACON: Optimizing Context Compression for Long-horizon LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-02 13:30:22+0900","children":"2025년 10월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-2-ACON-Optimizing-Context-Compression-for-Long-horizon-LLM-Agents"}]]}]]}],["$","article","2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking","children":"[논문리뷰] jina-reranker-v3: Last but Not Late Interaction for Document Reranking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking","children":"arXiv에 게시된 'jina-reranker-v3: Last but Not Late Interaction for Document Reranking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-jina-reranker-v3-Last-but-Not-Late-Interaction-for-Document-Reranking"}]]}]]}],["$","article","2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs","children":"[논문리뷰] dParallel: Learnable Parallel Decoding for dLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs","children":"arXiv에 게시된 'dParallel: Learnable Parallel Decoding for dLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-dParallel-Learnable-Parallel-Decoding-for-dLLMs"}]]}]]}],["$","article","2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching","children":"[논문리뷰] d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching","children":"Jiarui Wang이 arXiv에 게시한 'd^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-d2Cache-Accelerating-Diffusion-Based-LLMs-via-Dual-Adaptive-Caching"}]]}]]}],["$","article","2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning","children":"[논문리뷰] Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning","children":"Yue Min이 arXiv에 게시한 'Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning"}]]}]]}],["$","article","2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments","children":"[논문리뷰] Who's Your Judge? On the Detectability of LLM-Generated Judgments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments","children":"arXiv에 게시된 'Who's Your Judge? On the Detectability of LLM-Generated Judgments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Whos-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments"}]]}]]}],["$","article","2025-10-1-Who-invented-deep-residual-learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Who-invented-deep-residual-learning","children":"[논문리뷰] Who invented deep residual learning?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Who-invented-deep-residual-learning","children":"Juergen Schmidhuber이 arXiv에 게시한 'Who invented deep residual learning?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Who-invented-deep-residual-learning"}]]}]]}],["$","article","2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap","children":"[논문리뷰] Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap","children":"Hengfan Zhang이 arXiv에 게시한 'Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap"}]]}]]}],["$","article","2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications","children":"[논문리뷰] VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications","children":"arXiv에 게시된 'VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications"}]]}]]}],["$","article","2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes","children":"[논문리뷰] VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes","children":"Muhammad Huzaifa이 arXiv에 게시한 'VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-VisualOverload-Probing-Visual-Understanding-of-VLMs-in-Really-Dense-Scenes"}]]}]]}],["$","article","2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play","children":"[논문리뷰] Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play","children":"Jing Shi이 arXiv에 게시한 'Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play"}]]}]]}],["$","article","2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning","children":"[논문리뷰] TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning","children":"arXiv에 게시된 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training","children":"[논문리뷰] Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training","children":"arXiv에 게시된 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training"}]]}]]}],["$","article","2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain","children":"[논문리뷰] The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain","children":"arXiv에 게시된 'The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain"}]]}]]}],["$","article","2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs","children":"[논문리뷰] Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs","children":"Yao Shu이 arXiv에 게시한 'Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs"}]]}]]}],["$","article","2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training","children":"[논문리뷰] TTT3R: 3D Reconstruction as Test-Time Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training","children":"Anpei Chen이 arXiv에 게시한 'TTT3R: 3D Reconstruction as Test-Time Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-TTT3R-3D-Reconstruction-as-Test-Time-Training"}]]}]]}],["$","article","2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics","children":"[논문리뷰] TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics","children":"Szu-Chi Chen이 arXiv에 게시한 'TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-TAU-A-Benchmark-for-Cultural-Sound-Understanding-Beyond-Semantics"}]]}]]}],["$","article","2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation","children":"[논문리뷰] Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation","children":"arXiv에 게시된 'Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation"}]]}]]}],["$","article","2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models","children":"[논문리뷰] Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models","children":"arXiv에 게시된 'Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Specialization-after-Generalization-Towards-Understanding-Test-Time-Training-in-Foundation-Models"}]]}]]}],["$","article","2025-10-1-Regression-Language-Models-for-Code",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Regression-Language-Models-for-Code","children":"[논문리뷰] Regression Language Models for Code"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Regression-Language-Models-for-Code","children":"arXiv에 게시된 'Regression Language Models for Code' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Regression-Language-Models-for-Code"}]]}]]}],["$","article","2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation","children":"[논문리뷰] ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation","children":"Antonio Liotta이 arXiv에 게시한 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-ProfVLM-A-Lightweight-Video-Language-Model-for-Multi-View-Proficiency-Estimation"}]]}]]}],["$","article","2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark","children":"[논문리뷰] Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark","children":"Penghao Zhu이 arXiv에 게시한 'Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark"}]]}]]}],["$","article","2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always","children":"[논문리뷰] OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always","children":"arXiv에 게시된 'OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-OffTopicEval-When-Large-Language-Models-Enter-the-Wrong-Chat-Almost-Always"}]]}]]}],["$","article","2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents","children":"[논문리뷰] OceanGym: A Benchmark Environment for Underwater Embodied Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents","children":"arXiv에 게시된 'OceanGym: A Benchmark Environment for Underwater Embodied Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents"}]]}]]}],["$","article","2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation","children":"[논문리뷰] MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation","children":"Limin Wang이 arXiv에 게시한 'MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation"}]]}]]}],["$","article","2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models","children":"[논문리뷰] More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models","children":"Fabian Waschkowski이 arXiv에 게시한 'More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models"}]]}]]}],["$","article","2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning","children":"[논문리뷰] Mem-α: Learning Memory Construction via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning","children":"Yuzhen Mao이 arXiv에 게시한 'Mem-α: Learning Memory Construction via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Mem-a-Learning-Memory-Construction-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use","children":"[논문리뷰] MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use","children":"arXiv에 게시된 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use"}]]}]]}],["$","article","2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification","children":"[논문리뷰] MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification","children":"Zhiming Luo이 arXiv에 게시한 'MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-MANI-Pure-Magnitude-Adaptive-Noise-Injection-for-Adversarial-Purification"}]]}]]}],["$","article","2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training","children":"[논문리뷰] Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training","children":"Koustuv Sinha이 arXiv에 게시한 'Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training"}]]}]]}],["$","article","2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs","children":"[논문리뷰] Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs","children":"arXiv에 게시된 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs"}]]}]]}],["$","article","2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers","children":"[논문리뷰] LayerD: Decomposing Raster Graphic Designs into Layers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers","children":"Kota Yamaguchi이 arXiv에 게시한 'LayerD: Decomposing Raster Graphic Designs into Layers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-LayerD-Decomposing-Raster-Graphic-Designs-into-Layers"}]]}]]}],["$","article","2025-10-1-Knowledge-Homophily-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Knowledge-Homophily-in-Large-Language-Models","children":"[논문리뷰] Knowledge Homophily in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Knowledge-Homophily-in-Large-Language-Models","children":"Nedim Lipka이 arXiv에 게시한 'Knowledge Homophily in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Knowledge-Homophily-in-Large-Language-Models"}]]}]]}],["$","article","2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents","children":"[논문리뷰] InfoAgent: Advancing Autonomous Information-Seeking Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents","children":"arXiv에 게시된 'InfoAgent: Advancing Autonomous Information-Seeking Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-InfoAgent-Advancing-Autonomous-Information-Seeking-Agents"}]]}]]}],["$","article","2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance","children":"[논문리뷰] IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance","children":"arXiv에 게시된 'IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance"}]]}]]}],["$","article","2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss","children":"[논문리뷰] Humanline: Online Alignment as Perceptual Loss"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss","children":"arXiv에 게시된 'Humanline: Online Alignment as Perceptual Loss' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Humanline-Online-Alignment-as-Perceptual-Loss"}]]}]]}],["$","article","2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents","children":"[논문리뷰] Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents","children":"arXiv에 게시된 'Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Ferret-UI-Lite-Lessons-from-Building-Small-On-Device-GUI-Agents"}]]}]]}],["$","article","2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning","children":"[논문리뷰] Estimating Time Series Foundation Model Transferability via In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning","children":"Jun Qi이 arXiv에 게시한 'Estimating Time Series Foundation Model Transferability via In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Estimating-Time-Series-Foundation-Model-Transferability-via-In-Context-Learning"}]]}]]}],["$","article","2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting","children":"[논문리뷰] EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting","children":"arXiv에 게시된 'EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting"}]]}]]}],["$","article","2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","children":"[논문리뷰] Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","children":"arXiv에 게시된 'Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention"}]]}]]}],["$","article","2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively","children":"[논문리뷰] DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively","children":"arXiv에 게시된 'DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively"}]]}]]}],["$","article","2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","children":"[논문리뷰] DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder","children":"arXiv에 게시된 'DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder"}]]}]]}],["$","article","2025-10-1-DA2-Depth-Anything-in-Any-Direction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DA2-Depth-Anything-in-Any-Direction","children":"[논문리뷰] DA^2: Depth Anything in Any Direction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-DA2-Depth-Anything-in-Any-Direction","children":"arXiv에 게시된 'DA^2: Depth Anything in Any Direction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-DA2-Depth-Anything-in-Any-Direction"}]]}]]}],["$","article","2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs","children":"[논문리뷰] Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs","children":"normanpaulsen이 arXiv에 게시한 'Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs"}]]}]]}],["$","article","2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software","children":"[논문리뷰] BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software","children":"arXiv에 게시된 'BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-BuildBench-Benchmarking-LLM-Agents-on-Compiling-Real-World-Open-Source-Software"}]]}]]}],["$","article","2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective","children":"[논문리뷰] Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective","children":"arXiv에 게시된 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective"}]]}]]}],["$","article","2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models","children":"[논문리뷰] Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models","children":"arXiv에 게시된 'Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models"}]]}]]}],["$","article","2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects","children":"[논문리뷰] A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects","children":"Jennifer Ding이 arXiv에 게시한 'A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-1-A-Cartography-of-Open-Collaboration-in-Open-Source-AI-Mapping-Practices-Motivations-and-Governance-in-14-Open-Large-Language-Model-Projects"}]]}]]}],["$","article","2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs","children":"[논문리뷰] Visual Jigsaw Post-Training Improves MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs","children":"Lewei Lu이 arXiv에 게시한 'Visual Jigsaw Post-Training Improves MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-Visual-Jigsaw-Post-Training-Improves-MLLMs"}]]}]]}],["$","article","2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs","children":"[논문리뷰] StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs","children":"Wei Jia이 arXiv에 게시한 'StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-StableToken-A-Noise-Robust-Semantic-Speech-Tokenizer-for-Resilient-SpeechLLMs"}]]}]]}],["$","article","2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention","children":"[논문리뷰] SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention","children":"arXiv에 게시된 'SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-SLA-Beyond-Sparsity-in-Diffusion-Transformers-via-Fine-Tunable-Sparse-Linear-Attention"}]]}]]}],["$","article","2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer","children":"[논문리뷰] SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer","children":"arXiv에 게시된 'SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-SANA-Video-Efficient-Video-Generation-with-Block-Linear-Diffusion-Transformer"}]]}]]}],["$","article","2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark","children":"[논문리뷰] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark","children":"Yuran Wang이 arXiv에 게시한 'RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-RealUnify-Do-Unified-Models-Truly-Benefit-from-Unification-A-Comprehensive-Benchmark"}]]}]]}],["$","article","2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards","children":"[논문리뷰] Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards","children":"Binxing Jiao이 arXiv에 게시한 'Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-Random-Policy-Valuation-is-Enough-for-LLM-Reasoning-with-Verifiable-Rewards"}]]}]]}],["$","article","2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing","children":"[논문리뷰] OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing","children":"Huanyu Zhang이 arXiv에 게시한 'OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-OpenGPT-4o-Image-A-Comprehensive-Dataset-for-Advanced-Image-Generation-and-Editing"}]]}]]}],["$","article","2025-9-30-Multiplayer-Nash-Preference-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-Multiplayer-Nash-Preference-Optimization","children":"[논문리뷰] Multiplayer Nash Preference Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-Multiplayer-Nash-Preference-Optimization","children":"arXiv에 게시된 'Multiplayer Nash Preference Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-Multiplayer-Nash-Preference-Optimization"}]]}]]}],["$","article","2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling","children":"[논문리뷰] EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling","children":"arXiv에 게시된 'EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-EditScore-Unlocking-Online-RL-for-Image-Editing-via-High-Fidelity-Reward-Modeling"}]]}]]}],["$","article","2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering","children":"[논문리뷰] EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering","children":"arXiv에 게시된 'EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-30 13:52:24+0900","children":"2025년 9월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-30-EasySteer-A-Unified-Framework-for-High-Performance-and-Extensible-LLM-Steering"}]]}]]}],["$","article","2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","children":"[논문리뷰] X-Streamer: Unified Human World Modeling with Audiovisual Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction","children":"Guoxian Song이 arXiv에 게시한 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-X-Streamer-Unified-Human-World-Modeling-with-Audiovisual-Interaction"}]]}]]}],["$","article","2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning","children":"[논문리뷰] X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning","children":"Raghuveer Rao이 arXiv에 게시한 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning"}]]}]]}],["$","article","2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction","children":"[논문리뷰] WoW: Towards a World omniscient World model Through Embodied Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction","children":"Weishi Mi이 arXiv에 게시한 'WoW: Towards a World omniscient World model Through Embodied Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-WoW-Towards-a-World-omniscient-World-model-Through-Embodied-Interaction"}]]}]]}],["$","article","2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation","children":"[논문리뷰] Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation","children":"Shiming Liu이 arXiv에 게시한 'Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Where-MLLMs-Attend-and-What-They-Rely-On-Explaining-Autoregressive-Token-Generation"}]]}]]}],["$","article","2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning","children":"[논문리뷰] WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning","children":"Zhuofan Zong이 arXiv에 게시한 'WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing","children":"[논문리뷰] VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing","children":"arXiv에 게시된 'VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing"}]]}]]}],["$","article","2025-9-29-Variational-Reasoning-for-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Variational-Reasoning-for-Language-Models","children":"[논문리뷰] Variational Reasoning for Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Variational-Reasoning-for-Language-Models","children":"arXiv에 게시된 'Variational Reasoning for Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Variational-Reasoning-for-Language-Models"}]]}]]}],["$","article","2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models","children":"[논문리뷰] UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models","children":"Yuchao Gu이 arXiv에 게시한 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-UniVid-Unifying-Vision-Tasks-with-Pre-trained-Video-Generation-Models"}]]}]]}],["$","article","2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios","children":"[논문리뷰] UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios","children":"Zeyu Qin이 arXiv에 게시한 'UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-UltraHorizon-Benchmarking-Agent-Capabilities-in-Ultra-Long-Horizon-Scenarios"}]]}]]}],["$","article","2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval","children":"[논문리뷰] Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval","children":"arXiv에 게시된 'Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Think-on-Graph-3-0-Efficient-and-Adaptive-LLM-Reasoning-on-Heterogeneous-Graphs-via-Multi-Agent-Dual-Evolving-Context-Retrieval"}]]}]]}],["$","article","2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images","children":"[논문리뷰] TUN3D: Towards Real-World Scene Understanding from Unposed Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images","children":"Anna Vorontsova이 arXiv에 게시한 'TUN3D: Towards Real-World Scene Understanding from Unposed Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-TUN3D-Towards-Real-World-Scene-Understanding-from-Unposed-Images"}]]}]]}],["$","article","2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion","children":"[논문리뷰] StateX: Enhancing RNN Recall via Post-training State Expansion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion","children":"Zhiyuan Liu이 arXiv에 게시한 'StateX: Enhancing RNN Recall via Post-training State Expansion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-StateX-Enhancing-RNN-Recall-via-Post-training-State-Expansion"}]]}]]}],["$","article","2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation","children":"[논문리뷰] See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation","children":"Chih-Hai Su이 arXiv에 게시한 'See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-See-Point-Fly-A-Learning-Free-VLM-Framework-for-Universal-Unmanned-Aerial-Navigation"}]]}]]}],["$","article","2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework","children":"[논문리뷰] SPARK: Synergistic Policy And Reward Co-Evolving Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework","children":"arXiv에 게시된 'SPARK: Synergistic Policy And Reward Co-Evolving Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-SPARK-Synergistic-Policy-And-Reward-Co-Evolving-Framework"}]]}]]}],["$","article","2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models","children":"[논문리뷰] ReviewScore: Misinformed Peer Review Detection with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models","children":"arXiv에 게시된 'ReviewScore: Misinformed Peer Review Detection with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-ReviewScore-Misinformed-Peer-Review-Detection-with-Large-Language-Models"}]]}]]}],["$","article","2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation","children":"[논문리뷰] RefAM: Attention Magnets for Zero-Shot Referral Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation","children":"Federico Tombari이 arXiv에 게시한 'RefAM: Attention Magnets for Zero-Shot Referral Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation"}]]}]]}],["$","article","2025-9-29-Real-Time-Object-Detection-Meets-DINOv3",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Real-Time-Object-Detection-Meets-DINOv3","children":"[논문리뷰] Real-Time Object Detection Meets DINOv3"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Real-Time-Object-Detection-Meets-DINOv3","children":"Xi Shen이 arXiv에 게시한 'Real-Time Object Detection Meets DINOv3' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Real-Time-Object-Detection-Meets-DINOv3"}]]}]]}],["$","article","2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning","children":"[논문리뷰] Quantile Advantage Estimation for Entropy-Safe Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning","children":"An Zhang이 arXiv에 게시한 'Quantile Advantage Estimation for Entropy-Safe Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Quantile-Advantage-Estimation-for-Entropy-Safe-Reasoning"}]]}]]}],["$","article","2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning","children":"[논문리뷰] PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning","children":"Lingpeng Kong이 arXiv에 게시한 'PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning"}]]}]]}],["$","article","2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping","children":"[논문리뷰] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping","children":"arXiv에 게시된 'No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-No-Prompt-Left-Behind-Exploiting-Zero-Variance-Prompts-in-LLM-Reinforcement-Learning-via-Entropy-Guided-Advantage-Shaping"}]]}]]}],["$","article","2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing","children":"[논문리뷰] MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing","children":"SunYuefeng이 arXiv에 게시한 'MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-MinerU2-5-A-Decoupled-Vision-Language-Model-for-Efficient-High-Resolution-Document-Parsing"}]]}]]}],["$","article","2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation","children":"[논문리뷰] Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation","children":"Peter Wonka이 arXiv에 게시한 'Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation"}]]}]]}],["$","article","2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning","children":"[논문리뷰] MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning","children":"Weipeng Zhong이 arXiv에 게시한 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning"}]]}]]}],["$","article","2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer","children":"[논문리뷰] LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer","children":"arXiv에 게시된 'LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-LucidFlux-Caption-Free-Universal-Image-Restoration-via-a-Large-Scale-Diffusion-Transformer"}]]}]]}],["$","article","2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation","children":"[논문리뷰] LongLive: Real-time Interactive Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation","children":"arXiv에 게시된 'LongLive: Real-time Interactive Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-LongLive-Real-time-Interactive-Long-Video-Generation"}]]}]]}],["$","article","2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning","children":"[논문리뷰] Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning","children":"Gang Li이 arXiv에 게시한 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Learn-the-Ropes-Then-Trust-the-Wins-Self-imitation-with-Progressive-Exploration-for-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards","children":"[논문리뷰] Language Models Can Learn from Verbal Feedback Without Scalar Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards","children":"arXiv에 게시된 'Language Models Can Learn from Verbal Feedback Without Scalar Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards"}]]}]]}],["$","article","2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models","children":"[논문리뷰] Instruction-Following Evaluation in Function Calling for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models","children":"NikolaiSkripko이 arXiv에 게시한 'Instruction-Following Evaluation in Function Calling for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Instruction-Following-Evaluation-in-Function-Calling-for-Large-Language-Models"}]]}]]}],["$","article","2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models","children":"[논문리뷰] HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models","children":"Romann M. Weber이 arXiv에 게시한 'HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models"}]]}]]}],["$","article","2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing","children":"[논문리뷰] FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing","children":"Linghe Kong이 arXiv에 게시한 'FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing"}]]}]]}],["$","article","2025-9-29-Fine-tuning-Done-Right-in-Model-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing","children":"[논문리뷰] Fine-tuning Done Right in Model Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing","children":"Du Su이 arXiv에 게시한 'Fine-tuning Done Right in Model Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing"}]]}]]}],["$","article","2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences","children":"[논문리뷰] Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences","children":"Eija Honkavaara이 arXiv에 게시한 'Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Finding-3D-Positions-of-Distant-Objects-from-Noisy-Camera-Movement-and-Semantic-Segmentation-Sequences"}]]}]]}],["$","article","2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models","children":"[논문리뷰] ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models","children":"Ki-Ung Song이 arXiv에 게시한 'ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-ERGO-Efficient-High-Resolution-Visual-Understanding-for-Vision-Language-Models"}]]}]]}],["$","article","2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning","children":"[논문리뷰] EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning","children":"Li Yu-Jhe이 arXiv에 게시한 'EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-EPO-Entropy-regularized-Policy-Optimization-for-LLM-Agents-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents","children":"[논문리뷰] D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents","children":"Jinyuan Li이 arXiv에 게시한 'D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-D-Artemis-A-Deliberative-Cognitive-Framework-for-Mobile-GUI-Multi-Agents"}]]}]]}],["$","article","2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training","children":"[논문리뷰] Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training","children":"arXiv에 게시된 'Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-Chasing-the-Tail-Effective-Rubric-based-Reward-Modeling-for-Large-Language-Model-Post-Training"}]]}]]}],["$","article","2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning","children":"[논문리뷰] CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning","children":"arXiv에 게시된 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-CapRL-Stimulating-Dense-Image-Caption-Capabilities-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition","children":"[논문리뷰] CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition","children":"arXiv에 게시된 'CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-CHURRO-Making-History-Readable-with-an-Open-Weight-Large-Vision-Language-Model-for-High-Accuracy-Low-Cost-Historical-Text-Recognition"}]]}]]}],["$","article","2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity","children":"[논문리뷰] When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity","children":"John P Dickerson이 arXiv에 게시한 'When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity"}]]}]]}],["$","article","2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models","children":"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models","children":"Yuewei Zhang이 arXiv에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models"}]]}]]}],["$","article","2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models","children":"[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models","children":"Shawn Guo이 arXiv에 게시한 'V-GameGym: Visual Game Generation for Code Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models"}]]}]]}],["$","article","2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory","children":"[논문리뷰] Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory","children":"Yanbin Fu이 arXiv에 게시한 'Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory"}]]}]]}],["$","article","2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them","children":"[논문리뷰] TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them","children":"Zhuohao Yu이 arXiv에 게시한 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them"}]]}]]}],["$","article","2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning","children":"[논문리뷰] Tree Search for LLM Agent Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning","children":"Xiangxiang Chu이 arXiv에 게시한 'Tree Search for LLM Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Tree-Search-for-LLM-Agent-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification","children":"[논문리뷰] Thinking While Listening: Simple Test Time Scaling For Audio Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification","children":"Mert Pilanci이 arXiv에 게시한 'Thinking While Listening: Simple Test Time Scaling For Audio Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Thinking-While-Listening-Simple-Test-Time-Scaling-For-Audio-Classification"}]]}]]}],["$","article","2025-9-26-Thinking-Augmented-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Thinking-Augmented-Pre-training","children":"[논문리뷰] Thinking Augmented Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Thinking-Augmented-Pre-training","children":"Furu Wei이 arXiv에 게시한 'Thinking Augmented Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Thinking-Augmented-Pre-training"}]]}]]}],["$","article","2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment","children":"[논문리뷰] The Unanticipated Asymmetry Between Perceptual Optimization and Assessment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment","children":"Du Chen이 arXiv에 게시한 'The Unanticipated Asymmetry Between Perceptual Optimization and Assessment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-The-Unanticipated-Asymmetry-Between-Perceptual-Optimization-and-Assessment"}]]}]]}],["$","article","2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models","children":"[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models","children":"Javad Lavaei이 arXiv에 게시한 'StyleBench: Evaluating thinking styles in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models"}]]}]]}],["$","article","2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation","children":"[논문리뷰] Seedream 4.0: Toward Next-generation Multimodal Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation","children":"Yunpeng Chen이 arXiv에 게시한 'Seedream 4.0: Toward Next-generation Multimodal Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Seedream-4-0-Toward-Next-generation-Multimodal-Image-Generation"}]]}]]}],["$","article","2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines","children":"[논문리뷰] SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines","children":"Jiabei Xiao이 arXiv에 게시한 'SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines"}]]}]]}],["$","article","2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent","children":"[논문리뷰] SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent","children":"Siyuan Huang이 arXiv에 게시한 'SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-SceneWeaver-All-in-One-3D-Scene-Synthesis-with-an-Extensible-and-Self-Reflective-Agent"}]]}]]}],["$","article","2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning","children":"[논문리뷰] ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning","children":"Yu Li이 arXiv에 게시한 'ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning"}]]}]]}],["$","article","2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows","children":"[논문리뷰] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows","children":"Yi-Zhe Song이 arXiv에 게시한 'SD3.5-Flash: Distribution-Guided Distillation of Generative Flows' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-SD3-5-Flash-Distribution-Guided-Distillation-of-Generative-Flows"}]]}]]}],["$","article","2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies","children":"[논문리뷰] Residual Off-Policy RL for Finetuning Behavior Cloning Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies","children":"Pieter Abbeel이 arXiv에 게시한 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Residual-Off-Policy-RL-for-Finetuning-Behavior-Cloning-Policies"}]]}]]}],["$","article","2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution","children":"[논문리뷰] Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution","children":"Jinjie Gu이 arXiv에 게시한 'Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Recon-Act-A-Self-Evolving-Multi-Agent-Browser-Use-System-via-Web-Reconnaissance-Tool-Generation-and-Task-Execution"}]]}]]}],["$","article","2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer","children":"[논문리뷰] Quantized Visual Geometry Grounded Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer","children":"Yuqi Li이 arXiv에 게시한 'Quantized Visual Geometry Grounded Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Quantized-Visual-Geometry-Grounded-Transformer"}]]}]]}],["$","article","2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning","children":"[논문리뷰] MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning","children":"Junyan Zhang이 arXiv에 게시한 'MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-MOSS-ChatV-Reinforcement-Learning-with-Process-Reasoning-Reward-for-Video-Temporal-Reasoning"}]]}]]}],["$","article","2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources","children":"[논문리뷰] MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources","children":"Jing Wang이 arXiv에 게시한 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources"}]]}]]}],["$","article","2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model","children":"[논문리뷰] MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model","children":"Hung-yi Lee이 arXiv에 게시한 'MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-MI-Fuse-Label-Fusion-for-Unsupervised-Domain-Adaptation-with-Closed-Source-Large-Audio-Language-Model"}]]}]]}],["$","article","2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands","children":"[논문리뷰] Interactive Recommendation Agent with Active User Commands"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands","children":"Xueyang Feng이 arXiv에 게시한 'Interactive Recommendation Agent with Active User Commands' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands"}]]}]]}],["$","article","2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets","children":"[논문리뷰] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets","children":"Bowen Zhang이 arXiv에 게시한 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Hunyuan3D-Omni-A-Unified-Framework-for-Controllable-Generation-of-3D-Assets"}]]}]]}],["$","article","2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition","children":"[논문리뷰] Does FLUX Already Know How to Perform Physically Plausible Image Composition?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition","children":"Chen Zhao이 arXiv에 게시한 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition"}]]}]]}],["$","article","2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving","children":"[논문리뷰] Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving","children":"Hang Zhao이 arXiv에 게시한 'Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Discrete-Diffusion-for-Reflective-Vision-Language-Action-Models-in-Autonomous-Driving"}]]}]]}],["$","article","2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling","children":"[논문리뷰] CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling","children":"Yushi Bai이 arXiv에 게시한 'CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-CHARM-Control-point-based-3D-Anime-Hairstyle-Auto-Regressive-Modeling"}]]}]]}],["$","article","2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning","children":"[논문리뷰] CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning","children":"Wenping Hu이 arXiv에 게시한 'CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance","children":"[논문리뷰] Blueprints of Trust: AI System Cards for End to End Transparency and Governance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance","children":"Roman Zhukov이 arXiv에 게시한 'Blueprints of Trust: AI System Cards for End to End Transparency and Governance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Blueprints-of-Trust-AI-System-Cards-for-End-to-End-Transparency-and-Governance"}]]}]]}],["$","article","2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information","children":"[논문리뷰] Behind RoPE: How Does Causal Mask Encode Positional Information?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information","children":"Yeyun Gong이 arXiv에 게시한 'Behind RoPE: How Does Causal Mask Encode Positional Information?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information"}]]}]]}],["$","article","2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback","children":"[논문리뷰] BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback","children":"Dongha Lee이 arXiv에 게시한 'BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-BESPOKE-Benchmark-for-Search-Augmented-Large-Language-Model-Personalization-via-Diagnostic-Feedback"}]]}]]}],["$","article","2025-9-26-AutoIntent-AutoML-for-Text-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-AutoIntent-AutoML-for-Text-Classification","children":"[논문리뷰] AutoIntent: AutoML for Text Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-AutoIntent-AutoML-for-Text-Classification","children":"Denis Kuznetsov이 arXiv에 게시한 'AutoIntent: AutoML for Text Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-AutoIntent-AutoML-for-Text-Classification"}]]}]]}],["$","article","2025-9-25-Video-models-are-zero-shot-learners-and-reasoners",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Video-models-are-zero-shot-learners-and-reasoners","children":"[논문리뷰] Video models are zero-shot learners and reasoners"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Video-models-are-zero-shot-learners-and-reasoners","children":"rgeirhos이 arXiv에 게시한 'Video models are zero-shot learners and reasoners' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-Video-models-are-zero-shot-learners-and-reasoners"}]]}]]}],["$","article","2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought","children":"[논문리뷰] SIM-CoT: Supervised Implicit Chain-of-Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought","children":"Yuhang Cao이 arXiv에 게시한 'SIM-CoT: Supervised Implicit Chain-of-Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought"}]]}]]}],["$","article","2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","children":"[논문리뷰] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation","children":"Yiming Huang이 arXiv에 게시한 'PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation"}]]}]]}],["$","article","2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub","children":"[논문리뷰] On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub","children":"Hajimu Iida이 arXiv에 게시한 'On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub"}]]}]]}],["$","article","2025-9-25-Logics-Parsing-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Logics-Parsing-Technical-Report","children":"[논문리뷰] Logics-Parsing Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Logics-Parsing-Technical-Report","children":"Fan Yang이 arXiv에 게시한 'Logics-Parsing Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-Logics-Parsing-Technical-Report"}]]}]]}],["$","article","2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation","children":"[논문리뷰] Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation","children":"Zhe Lin이 arXiv에 게시한 'Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation"}]]}]]}],["$","article","2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines","children":"[논문리뷰] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines","children":"Yanfang이 arXiv에 게시한 'LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines"}]]}]]}],["$","article","2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations","children":"[논문리뷰] EmbeddingGemma: Powerful and Lightweight Text Representations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations","children":"Marksherwood이 arXiv에 게시한 'EmbeddingGemma: Powerful and Lightweight Text Representations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-EmbeddingGemma-Powerful-and-Lightweight-Text-Representations"}]]}]]}],["$","article","2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning","children":"[논문리뷰] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning","children":"Tianyu Wang이 arXiv에 게시한 'EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-EditVerse-Unifying-Image-and-Video-Editing-and-Generation-with-In-Context-Learning"}]]}]]}],["$","article","2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO","children":"[논문리뷰] Advancing Speech Understanding in Speech-Aware Language Models with GRPO"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO","children":"Avihu이 arXiv에 게시한 'Advancing Speech Understanding in Speech-Aware Language Models with GRPO' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-Advancing-Speech-Understanding-in-Speech-Aware-Language-Models-with-GRPO"}]]}]]}],["$","article","2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications","children":"[논문리뷰] Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications","children":"Genady Beryozkin이 arXiv에 게시한 'Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications"}]]}]]}],["$","article","2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT","children":"[논문리뷰] What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT","children":"Anthony Hartshorn이 arXiv에 게시한 'What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT"}]]}]]}],["$","article","2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction","children":"[논문리뷰] VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction","children":"Haoxiao Wang이 arXiv에 게시한 'VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction"}]]}]]}],["$","article","2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction","children":"[논문리뷰] VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction","children":"So Fukuda이 arXiv에 게시한 'VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction"}]]}]]}],["$","article","2025-9-24-Reinforcement-Learning-on-Pre-Training-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data","children":"[논문리뷰] Reinforcement Learning on Pre-Training Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data","children":"Evander Yang이 arXiv에 게시한 'Reinforcement Learning on Pre-Training Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data"}]]}]]}],["$","article","2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation","children":"[논문리뷰] OpenGVL - Benchmarking Visual Temporal Progress for Data Curation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation","children":"Viktor Petrenko이 arXiv에 게시한 'OpenGVL - Benchmarking Visual Temporal Progress for Data Curation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation"}]]}]]}],["$","article","2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe","children":"[논문리뷰] MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe","children":"Wenshuo Ma이 arXiv에 게시한 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe"}]]}]]}],["$","article","2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization","children":"[논문리뷰] MAPO: Mixed Advantage Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization","children":"Xuankun Rong이 arXiv에 게시한 'MAPO: Mixed Advantage Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-MAPO-Mixed-Advantage-Policy-Optimization"}]]}]]}],["$","article","2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation","children":"[논문리뷰] Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation","children":"Yifeng Jiang이 arXiv에 게시한 'Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation"}]]}]]}],["$","article","2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects","children":"[논문리뷰] Large Language Models Discriminate Against Speakers of German Dialects"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects","children":"Katharina von der Wense이 arXiv에 게시한 'Large Language Models Discriminate Against Speakers of German Dialects' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects"}]]}]]}],["$","article","2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation","children":"[논문리뷰] Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation","children":"Jianbin Zheng이 arXiv에 게시한 'Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation"}]]}]]}],["$","article","2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis","children":"[논문리뷰] HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis","children":"Dan Xu이 arXiv에 게시한 'HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis"}]]}]]}],["$","article","2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction","children":"[논문리뷰] GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction","children":"Jin Zheng이 arXiv에 게시한 'GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction"}]]}]]}],["$","article","2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies","children":"[논문리뷰] Do You Need Proprioceptive States in Visuomotor Policies?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies","children":"Yushen Liang이 arXiv에 게시한 'Do You Need Proprioceptive States in Visuomotor Policies?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies"}]]}]]}],["$","article","2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching","children":"[논문리뷰] CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching","children":"Rui Qian이 arXiv에 게시한 'CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching"}]]}]]}],["$","article","2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR","children":"[논문리뷰] Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR","children":"Zeina Aldallal이 arXiv에 게시한 'Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR"}]]}]]}],["$","article","2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs","children":"[논문리뷰] When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs","children":"Anand Mishra이 arXiv에 게시한 'When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-When-Big-Models-Train-Small-Ones-Label-Free-Model-Parity-Alignment-for-Efficient-Visual-Question-Answering-using-Small-VLMs"}]]}]]}],["$","article","2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models","children":"[논문리뷰] VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models","children":"Sunghyun Cho이 arXiv에 게시한 'VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-VideoFrom3D-3D-Scene-Video-Generation-via-Complementary-Image-and-Video-Diffusion-Models"}]]}]]}],["$","article","2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery","children":"[논문리뷰] VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery","children":"Shiya Huang이 arXiv에 게시한 'VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-VaseVQA-Multimodal-Agent-and-Benchmark-for-Ancient-Greek-Pottery"}]]}]]}],["$","article","2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering","children":"[논문리뷰] Understanding Embedding Scaling in Collaborative Filtering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering","children":"Yonghui Yang이 arXiv에 게시한 'Understanding Embedding Scaling in Collaborative Filtering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Understanding-Embedding-Scaling-in-Collaborative-Filtering"}]]}]]}],["$","article","2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications","children":"[논문리뷰] Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications","children":"Fatma Betül Terzioğlu이 arXiv에 게시한 'Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications"}]]}]]}],["$","article","2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs","children":"[논문리뷰] TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs","children":"Shaohui Jiao이 arXiv에 게시한 'TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"}]]}]]}],["$","article","2025-9-23-Synthetic-bootstrapped-pretraining",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Synthetic-bootstrapped-pretraining","children":"[논문리뷰] Synthetic bootstrapped pretraining"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Synthetic-bootstrapped-pretraining","children":"Emmanuel Candès이 arXiv에 게시한 'Synthetic bootstrapped pretraining' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Synthetic-bootstrapped-pretraining"}]]}]]}],["$","article","2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks","children":"[논문리뷰] SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks","children":"Yannis Yiming He이 arXiv에 게시한 'SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-SWE-Bench-Pro-Can-AI-Agents-Solve-Long-Horizon-Software-Engineering-Tasks"}]]}]]}],["$","article","2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning","children":"[논문리뷰] SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning","children":"Zhaopeng Tu이 arXiv에 게시한 'SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning"}]]}]]}],["$","article","2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning","children":"[논문리뷰] Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning","children":"Damien Sileo이 arXiv에 게시한 'Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Reasoning-Core-A-Scalable-RL-Environment-for-LLM-Symbolic-Reasoning"}]]}]]}],["$","article","2025-9-23-Qwen3-Omni-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Qwen3-Omni-Technical-Report","children":"[논문리뷰] Qwen3-Omni Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Qwen3-Omni-Technical-Report","children":"Lhma-aslp이 arXiv에 게시한 'Qwen3-Omni Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Qwen3-Omni-Technical-Report"}]]}]]}],["$","article","2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models","children":"[논문리뷰] QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models","children":"Jae-Joon Kim이 arXiv에 게시한 'QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-QWHA-Quantization-Aware-Walsh-Hadamard-Adaptation-for-Parameter-Efficient-Fine-Tuning-on-Large-Language-Models"}]]}]]}],["$","article","2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","children":"[논문리뷰] OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models","children":"Pengze Zhang이 arXiv에 게시한 'OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-OmniInsert-Mask-Free-Video-Insertion-of-Any-Reference-via-Diffusion-Transformer-Models"}]]}]]}],["$","article","2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction","children":"[논문리뷰] MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction","children":"Xintao Chen이 arXiv에 게시한 'MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-MetaEmbed-Scaling-Multimodal-Retrieval-at-Test-Time-with-Flexible-Late-Interaction"}]]}]]}],["$","article","2025-9-23-Mano-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Mano-Report","children":"[논문리뷰] Mano Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Mano-Report","children":"Minghui Wu이 arXiv에 게시한 'Mano Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Mano-Report"}]]}]]}],["$","article","2025-9-23-LIMI-Less-is-More-for-Agency",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency","children":"[논문리뷰] LIMI: Less is More for Agency"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency","children":"happyZYM이 arXiv에 게시한 'LIMI: Less is More for Agency' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency"}]]}]]}],["$","article","2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning","children":"[논문리뷰] GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning","children":"Hou Pong Chan이 arXiv에 게시한 'GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-GeoPQA-Bridging-the-Visual-Perception-Gap-in-MLLMs-for-Geometric-Reasoning"}]]}]]}],["$","article","2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature","children":"[논문리뷰] From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature","children":"Bin Cui이 arXiv에 게시한 'From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-From-Uniform-to-Heterogeneous-Tailoring-Policy-Optimization-to-Every-Tokens-Nature"}]]}]]}],["$","article","2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem","children":"[논문리뷰] From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem","children":"Ahmed E. Hassan이 arXiv에 게시한 'From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-From-Hugging-Face-to-GitHub-Tracing-License-Drift-in-the-Open-Source-AI-Ecosystem"}]]}]]}],["$","article","2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions","children":"[논문리뷰] FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions","children":"tengdai722이 arXiv에 게시한 'FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-FlagEval-Findings-Report-A-Preliminary-Evaluation-of-Large-Reasoning-Models-on-Automatically-Verifiable-Textual-and-Visual-Questions"}]]}]]}],["$","article","2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering","children":"[논문리뷰] EpiCache: Episodic KV Cache Management for Long Conversational Question Answering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering","children":"Minsik Cho이 arXiv에 게시한 'EpiCache: Episodic KV Cache Management for Long Conversational Question Answering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-EpiCache-Episodic-KV-Cache-Management-for-Long-Conversational-Question-Answering"}]]}]]}],["$","article","2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process","children":"[논문리뷰] DiffusionNFT: Online Diffusion Reinforcement with Forward Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process","children":"Qinsheng Zhang이 arXiv에 게시한 'DiffusionNFT: Online Diffusion Reinforcement with Forward Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-DiffusionNFT-Online-Diffusion-Reinforcement-with-Forward-Process"}]]}]]}],["$","article","2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context","children":"[논문리뷰] DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context","children":"Maunendra Sankar Desarkar이 arXiv에 게시한 'DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context"}]]}]]}],["$","article","2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models","children":"[논문리뷰] Cross-Attention is Half Explanation in Speech-to-Text Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models","children":"Luisa Bentivogli이 arXiv에 게시한 'Cross-Attention is Half Explanation in Speech-to-Text Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Cross-Attention-is-Half-Explanation-in-Speech-to-Text-Models"}]]}]]}],["$","article","2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment","children":"[논문리뷰] ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment","children":"Yue Ma이 arXiv에 게시한 'ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-ContextFlow-Training-Free-Video-Object-Editing-via-Adaptive-Context-Enrichment"}]]}]]}],["$","article","2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects","children":"[논문리뷰] CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects","children":"Hang Yu이 arXiv에 게시한 'CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects"}]]}]]}],["$","article","2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces","children":"[논문리뷰] ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces","children":"Jiafeng Xu이 arXiv에 게시한 'ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-ByteWrist-A-Parallel-Robotic-Wrist-Enabling-Flexible-and-Anthropomorphic-Motion-for-Confined-Spaces"}]]}]]}],["$","article","2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing","children":"[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing","children":"Jaeho Lee이 arXiv에 게시한 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing"}]]}]]}],["$","article","2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels","children":"[논문리뷰] Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels","children":"Qi Zhang이 arXiv에 게시한 'Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-Analyzing-the-Effects-of-Supervised-Fine-Tuning-on-Model-Knowledge-from-Token-and-Parameter-Levels"}]]}]]}],["$","article","2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations","children":"[논문리뷰] ARE: Scaling Up Agent Environments and Evaluations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations","children":"Matteo Bettini이 arXiv에 게시한 'ARE: Scaling Up Agent Environments and Evaluations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-ARE-Scaling-Up-Agent-Environments-and-Evaluations"}]]}]]}],["$","article","2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers","children":"[논문리뷰] WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers","children":"Karun Kumar이 arXiv에 게시한 'WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-WhisTLE-Deeply-Supervised-Text-Only-Domain-Adaptation-for-Pretrained-Speech-Recognition-Transformers"}]]}]]}],["$","article","2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents","children":"[논문리뷰] Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents","children":"Chao Zhang이 arXiv에 게시한 'Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-Video2Roleplay-A-Multimodal-Dataset-and-Framework-for-Video-Guided-Role-playing-Agents"}]]}]]}],["$","article","2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation","children":"[논문리뷰] SPATIALGEN: Layout-guided 3D Indoor Scene Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation","children":"Yongsen Mao이 arXiv에 게시한 'SPATIALGEN: Layout-guided 3D Indoor Scene Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-SPATIALGEN-Layout-guided-3D-Indoor-Scene-Generation"}]]}]]}],["$","article","2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation","children":"[논문리뷰] RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation","children":"Steven Liu이 arXiv에 게시한 'RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-RPG-A-Repository-Planning-Graph-for-Unified-and-Scalable-Codebase-Generation"}]]}]]}],["$","article","2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes","children":"[논문리뷰] RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes","children":"Narendra Ahuja이 arXiv에 게시한 'RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes"}]]}]]}],["$","article","2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer","children":"[논문리뷰] MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer","children":"jialingt이 arXiv에 게시한 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-MANZANO-A-Simple-and-Scalable-Unified-Multimodal-Model-with-a-Hybrid-Vision-Tokenizer"}]]}]]}],["$","article","2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation","children":"[논문리뷰] Lynx: Towards High-Fidelity Personalized Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation","children":"Linjie Luo이 arXiv에 게시한 'Lynx: Towards High-Fidelity Personalized Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-Lynx-Towards-High-Fidelity-Personalized-Video-Generation"}]]}]]}],["$","article","2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification","children":"[논문리뷰] Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification","children":"Wenyu Wang이 arXiv에 게시한 'Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-Latent-Zoning-Network-A-Unified-Principle-for-Generative-Modeling-Representation-Learning-and-Classification"}]]}]]}],["$","article","2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems","children":"[논문리뷰] Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems","children":"Hung-yi Lee이 arXiv에 게시한 'Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems"}]]}]]}],["$","article","2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model","children":"[논문리뷰] BaseReward: A Strong Baseline for Multimodal Reward Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model","children":"jianfeipan이 arXiv에 게시한 'BaseReward: A Strong Baseline for Multimodal Reward Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-BaseReward-A-Strong-Baseline-for-Multimodal-Reward-Model"}]]}]]}],["$","article","2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent","children":"[논문리뷰] BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent","children":"Jiahui Yang이 arXiv에 게시한 'BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-BTL-UI-Blink-Think-Link-Reasoning-Model-for-GUI-Agent"}]]}]]}],["$","article","2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue","children":"[논문리뷰] Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue","children":"Hui Zhang이 arXiv에 게시한 'Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue"}]]}]]}],["$","article","2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning","children":"[논문리뷰] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning","children":"Jiangmiao이 arXiv에 게시한 'A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-22 13:11:29+0900","children":"2025년 9월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-22-A-Vision-Language-Action-Critic-Model-for-Robotic-Real-World-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance","children":"[논문리뷰] WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance","children":"Ruibo Li이 arXiv에 게시한 'WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-WorldForge-Unlocking-Emergent-3D4D-Generation-in-Video-Diffusion-Model-via-Training-Free-Guidance"}]]}]]}],["$","article","2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding","children":"[논문리뷰] Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding","children":"Rynson W. H. Lau이 arXiv에 게시한 'Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding"}]]}]]}],["$","article","2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation","children":"[논문리뷰] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation","children":"Xihui Liu이 arXiv에 게시한 'Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-Understand-Before-You-Generate-Self-Guided-Training-for-Autoregressive-Image-Generation"}]]}]]}],["$","article","2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data","children":"[논문리뷰] ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data","children":"Zehao Li이 arXiv에 게시한 'ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data"}]]}]]}],["$","article","2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation","children":"[논문리뷰] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation","children":"SpaceProduct이 arXiv에 게시한 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-RynnVLA-001-Using-Human-Demonstrations-to-Improve-Robot-Manipulation"}]]}]]}],["$","article","2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems","children":"[논문리뷰] RecoWorld: Building Simulated Environments for Agentic Recommender Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems","children":"Mingyuan Wu이 arXiv에 게시한 'RecoWorld: Building Simulated Environments for Agentic Recommender Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-RecoWorld-Building-Simulated-Environments-for-Agentic-Recommender-Systems"}]]}]]}],["$","article","2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration","children":"[논문리뷰] Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration","children":"Zhilin Wang이 arXiv에 게시한 'Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-Reasoning-over-Boundaries-Enhancing-Specification-Alignment-via-Test-time-Delibration"}]]}]]}],["$","article","2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks","children":"[논문리뷰] MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks","children":"Xijun Gu이 arXiv에 게시한 'MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-MultiEdit-Advancing-Instruction-based-Image-Editing-on-Diverse-and-Challenging-Tasks"}]]}]]}],["$","article","2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs","children":"[논문리뷰] Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs","children":"Katharina von der Wense이 arXiv에 게시한 'Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-Mind-the-Gap-A-Closer-Look-at-Tokenization-for-Multiple-Choice-Question-Answering-with-LLMs"}]]}]]}],["$","article","2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning","children":"[논문리뷰] FlowRL: Matching Reward Distributions for LLM Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning","children":"Hengli Li이 arXiv에 게시한 'FlowRL: Matching Reward Distributions for LLM Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning"}]]}]]}],["$","article","2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning","children":"[논문리뷰] FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning","children":"Jiashuo Liu이 arXiv에 게시한 'FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-FinSearchComp-Towards-a-Realistic-Expert-Level-Evaluation-of-Financial-Search-and-Reasoning"}]]}]]}],["$","article","2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection","children":"[논문리뷰] FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection","children":"Zhewei Zhang이 arXiv에 게시한 'FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-FSG-Net-Frequency-Spatial-Synergistic-Gated-Network-for-High-Resolution-Remote-Sensing-Change-Detection"}]]}]]}],["$","article","2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation","children":"[논문리뷰] Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation","children":"Kishan Panaganti이 arXiv에 게시한 'Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation"}]]}]]}],["$","article","2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence","children":"[논문리뷰] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence","children":"Qinghua Huang이 arXiv에 게시한 'EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-EchoVLM-Dynamic-Mixture-of-Experts-Vision-Language-Model-for-Universal-Ultrasound-Intelligence"}]]}]]}],["$","article","2025-9-19-AToken-A-Unified-Tokenizer-for-Vision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision","children":"[논문리뷰] AToken: A Unified Tokenizer for Vision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision","children":"Mingze Xu이 arXiv에 게시한 'AToken: A Unified Tokenizer for Vision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-19 13:12:21+0900","children":"2025년 9월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-19-AToken-A-Unified-Tokenizer-for-Vision"}]]}]]}],["$","article","2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication","children":"[논문리뷰] Wan-Animate: Unified Character Animation and Replacement with Holistic Replication"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication","children":"Mingyang Huang이 arXiv에 게시한 'Wan-Animate: Unified Character Animation and Replacement with Holistic Replication' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication"}]]}]]}],["$","article","2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning","children":"[논문리뷰] THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning","children":"Yicheng Pan이 arXiv에 게시한 'THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning"}]]}]]}],["$","article","2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs","children":"[논문리뷰] SteeringControl: Holistic Evaluation of Alignment Steering in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs","children":"Zhun Wang이 arXiv에 게시한 'SteeringControl: Holistic Evaluation of Alignment Steering in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs"}]]}]]}],["$","article","2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning","children":"[논문리뷰] Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning","children":"Zhou Yang이 arXiv에 게시한 'Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning"}]]}]]}],["$","article","2025-9-18-SAIL-VL2-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-SAIL-VL2-Technical-Report","children":"[논문리뷰] SAIL-VL2 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-SAIL-VL2-Technical-Report","children":"Zijian Kang이 arXiv에 게시한 'SAIL-VL2 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-SAIL-VL2-Technical-Report"}]]}]]}],["$","article","2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era","children":"[논문리뷰] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era","children":"Zihao Dongfang이 arXiv에 게시한 'PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era"}]]}]]}],["$","article","2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook","children":"[논문리뷰] MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook","children":"Bowen Zhou이 arXiv에 게시한 'MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook"}]]}]]}],["$","article","2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning","children":"[논문리뷰] Improving Context Fidelity via Native Retrieval-Augmented Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning","children":"Xiangru Tang이 arXiv에 게시한 'Improving Context Fidelity via Native Retrieval-Augmented Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning"}]]}]]}],["$","article","2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale","children":"[논문리뷰] Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale","children":"Bernard Ghanem이 arXiv에 게시한 'Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale"}]]}]]}],["$","article","2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam","children":"[논문리뷰] GenExam: A Multidisciplinary Text-to-Image Exam"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam","children":"Yu Qiao이 arXiv에 게시한 'GenExam: A Multidisciplinary Text-to-Image Exam' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-18 13:07:00+0900","children":"2025년 9월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-18-GenExam-A-Multidisciplinary-Text-to-Image-Exam"}]]}]]}],["$","article","2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research","children":"[논문리뷰] WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research","children":"Houquan Zhou이 arXiv에 게시한 'WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research"}]]}]]}],["$","article","2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning","children":"[논문리뷰] WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning","children":"Huifeng Yin이 arXiv에 게시한 'WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents","children":"[논문리뷰] WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents","children":"Wenbiao Yin이 arXiv에 게시한 'WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents"}]]}]]}],["$","article","2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling","children":"[논문리뷰] Towards General Agentic Intelligence via Environment Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling","children":"Guangyu Li이 arXiv에 게시한 'Towards General Agentic Intelligence via Environment Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling"}]]}]]}],["$","article","2025-9-17-Single-stream-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Single-stream-Policy-Optimization","children":"[논문리뷰] Single-stream Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Single-stream-Policy-Optimization","children":"Zihan Ding이 arXiv에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Single-stream-Policy-Optimization"}]]}]]}],["$","article","2025-9-17-Scaling-Agents-via-Continual-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Scaling-Agents-via-Continual-Pre-training","children":"[논문리뷰] Scaling Agents via Continual Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Scaling-Agents-via-Continual-Pre-training","children":"Guangyu Li이 arXiv에 게시한 'Scaling Agents via Continual Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Scaling-Agents-via-Continual-Pre-training"}]]}]]}],["$","article","2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization","children":"[논문리뷰] ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization","children":"Litu Ou이 arXiv에 게시한 'ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization"}]]}]]}],["$","article","2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs","children":"[논문리뷰] Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs","children":"Luca Benini이 arXiv에 게시한 'Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs"}]]}]]}],["$","article","2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis","children":"[논문리뷰] Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis","children":"Bo Liu이 arXiv에 게시한 'Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis"}]]}]]}],["$","article","2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge","children":"[논문리뷰] Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge","children":"Wentao Zhang이 arXiv에 게시한 'Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge"}]]}]]}],["$","article","2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation","children":"[논문리뷰] Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation","children":"Lixin Xu이 arXiv에 게시한 'Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation"}]]}]]}],["$","article","2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms","children":"[논문리뷰] Exact Coset Sampling for Quantum Lattice Algorithms"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms","children":"Yifan Zhang이 arXiv에 게시한 'Exact Coset Sampling for Quantum Lattice Algorithms' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms"}]]}]]}],["$","article","2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving","children":"[논문리뷰] EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving","children":"Shansan Gong이 arXiv에 게시한 'EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving"}]]}]]}],["$","article","2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model","children":"[논문리뷰] 3D Aware Region Prompted Vision Language Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model","children":"Xiaolong Li이 arXiv에 게시한 '3D Aware Region Prompted Vision Language Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-3D-Aware-Region-Prompted-Vision-Language-Model"}]]}]]}],["$","article","2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning","children":"[논문리뷰] UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning","children":"Yongliang Shen이 arXiv에 게시한 'UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation","children":"[논문리뷰] SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation","children":"Heshaam Faili이 arXiv에 게시한 'SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-SearchInstruct-Enhancing-Domain-Adaptation-via-Retrieval-Based-Instruction-Dataset-Creation"}]]}]]}],["$","article","2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits","children":"[논문리뷰] PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits","children":"Zhenhao Chen이 arXiv에 게시한 'PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-PersonaX-Multimodal-Datasets-with-LLM-Inferred-Behavior-Traits"}]]}]]}],["$","article","2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling","children":"[논문리뷰] OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling","children":"Yang Zhou이 arXiv에 게시한 'OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-OmniWorld-A-Multi-Domain-and-Multi-Modal-Dataset-for-4D-World-Modeling"}]]}]]}],["$","article","2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models","children":"[논문리뷰] Measuring Epistemic Humility in Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models","children":"Kaiyang Zhou이 arXiv에 게시한 'Measuring Epistemic Humility in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-Measuring-Epistemic-Humility-in-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models","children":"[논문리뷰] Lost in Embeddings: Information Loss in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models","children":"Ivan Vulić이 arXiv에 게시한 'Lost in Embeddings: Information Loss in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-Lost-in-Embeddings-Information-Loss-in-Vision-Language-Models"}]]}]]}],["$","article","2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models","children":"[논문리뷰] Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models","children":"Shuo Ren이 arXiv에 게시한 'Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models"}]]}]]}],["$","article","2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics","children":"[논문리뷰] Locality in Image Diffusion Models Emerges from Data Statistics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics","children":"Vincent Sitzmann이 arXiv에 게시한 'Locality in Image Diffusion Models Emerges from Data Statistics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-Locality-in-Image-Diffusion-Models-Emerges-from-Data-Statistics"}]]}]]}],["$","article","2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting","children":"[논문리뷰] Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting","children":"Changlong Yu이 arXiv에 게시한 'Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-Learning-to-Optimize-Multi-Objective-Alignment-Through-Dynamic-Reward-Weighting"}]]}]]}],["$","article","2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence","children":"[논문리뷰] LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence","children":"Lionel M. Ni이 arXiv에 게시한 'LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence"}]]}]]}],["$","article","2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts","children":"[논문리뷰] InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts","children":"Wenzhe Cai이 arXiv에 게시한 'InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-InternScenes-A-Large-scale-Simulatable-Indoor-Scene-Dataset-with-Realistic-Layouts"}]]}]]}],["$","article","2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings","children":"[논문리뷰] GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings","children":"Yixuan Tang이 arXiv에 게시한 'GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-GAPrune-Gradient-Alignment-Pruning-for-Domain-Aware-Embeddings"}]]}]]}],["$","article","2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI","children":"[논문리뷰] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI","children":"UVSKKR이 arXiv에 게시한 'EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI"}]]}]]}],["$","article","2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding","children":"[논문리뷰] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding","children":"Li Zheng이 arXiv에 게시한 'Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-Dr-V-A-Hierarchical-Perception-Temporal-Cognition-Framework-to-Diagnose-Video-Hallucination-by-Fine-grained-Spatial-Temporal-Grounding"}]]}]]}],["$","article","2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media","children":"[논문리뷰] CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media","children":"Subasish Das이 arXiv에 게시한 'CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-16 13:16:41+0900","children":"2025년 9월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-16-CognitiveSky-Scalable-Sentiment-and-Narrative-Analysis-for-Decentralized-Social-Media"}]]}]]}],["$","article","2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition","children":"[논문리뷰] X-Part: high fidelity and structure coherent shape decomposition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition","children":"Yunhan Yang이 arXiv에 게시한 'X-Part: high fidelity and structure coherent shape decomposition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-X-Part-high-fidelity-and-structure-coherent-shape-decomposition"}]]}]]}],["$","article","2025-9-15-Virtual-Agent-Economies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-Virtual-Agent-Economies","children":"[논문리뷰] Virtual Agent Economies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-Virtual-Agent-Economies","children":"William A. Cunningham이 arXiv에 게시한 'Virtual Agent Economies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-Virtual-Agent-Economies"}]]}]]}],["$","article","2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions","children":"[논문리뷰] VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions","children":"Dong Zhang이 arXiv에 게시한 'VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-VStyle-A-Benchmark-for-Voice-Style-Adaptation-with-Spoken-Instructions"}]]}]]}],["$","article","2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs","children":"[논문리뷰] The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs","children":"Jonas Geiping이 arXiv에 게시한 'The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs"}]]}]]}],["$","article","2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading","children":"[논문리뷰] QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading","children":"Chenyu You이 arXiv에 게시한 'QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading"}]]}]]}],["$","article","2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools","children":"[논문리뷰] MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools","children":"Xiaorui Wang이 arXiv에 게시한 'MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-MCP-AgentBench-Evaluating-Real-World-Language-Agent-Performance-with-MCP-Mediated-Tools"}]]}]]}],["$","article","2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios","children":"[논문리뷰] LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios","children":"Bing Su이 arXiv에 게시한 'LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-LoFT-Parameter-Efficient-Fine-Tuning-for-Long-tailed-Semi-Supervised-Learning-in-Open-World-Scenarios"}]]}]]}],["$","article","2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations","children":"[논문리뷰] IntrEx: A Dataset for Modeling Engagement in Educational Conversations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations","children":"Gabriele Pergola이 arXiv에 게시한 'IntrEx: A Dataset for Modeling Engagement in Educational Conversations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-IntrEx-A-Dataset-for-Modeling-Engagement-in-Educational-Conversations"}]]}]]}],["$","article","2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models","children":"[논문리뷰] Inpainting-Guided Policy Optimization for Diffusion Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models","children":"Chenyu Wang이 arXiv에 게시한 'Inpainting-Guided Policy Optimization for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models"}]]}]]}],["$","article","2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis","children":"[논문리뷰] InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis","children":"Song Guo이 arXiv에 게시한 'InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis"}]]}]]}],["$","article","2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering","children":"[논문리뷰] HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering","children":"Zhehao Tan이 arXiv에 게시한 'HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-HANRAG-Heuristic-Accurate-Noise-resistant-Retrieval-Augmented-Generation-for-Multi-hop-Question-Answering"}]]}]]}],["$","article","2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies","children":"[논문리뷰] FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies","children":"Fabian Otto이 arXiv에 게시한 'FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-FLOWER-Democratizing-Generalist-Robot-Policies-with-Efficient-Vision-Language-Action-Flow-Policies"}]]}]]}],["$","article","2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China","children":"[논문리뷰] CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China","children":"XU Han이 arXiv에 게시한 'CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-15 13:12:08+0900","children":"2025년 9월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-15-CMHG-A-Dataset-and-Benchmark-for-Headline-Generation-of-Minority-Languages-in-China"}]]}]]}],["$","article","2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding","children":"[논문리뷰] Visual Programmability: A Guide for Code-as-Thought in Chart Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding","children":"Ethan Chern이 arXiv에 게시한 'Visual Programmability: A Guide for Code-as-Thought in Chart Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Visual-Programmability-A-Guide-for-Code-as-Thought-in-Chart-Understanding"}]]}]]}],["$","article","2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model","children":"[논문리뷰] VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model","children":"Zirui Ge이 arXiv에 게시한 'VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-VLA-Adapter-An-Effective-Paradigm-for-Tiny-Scale-Vision-Language-Action-Model"}]]}]]}],["$","article","2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward","children":"[논문리뷰] The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward","children":"Xiaoyu Tan이 arXiv에 게시한 'The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-The-Choice-of-Divergence-A-Neglected-Key-to-Mitigating-Diversity-Collapse-in-Reinforcement-Learning-with-Verifiable-Reward"}]]}]]}],["$","article","2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations","children":"[논문리뷰] SpatialVID: A Large-Scale Video Dataset with Spatial Annotations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations","children":"Jian Gao이 arXiv에 게시한 'SpatialVID: A Large-Scale Video Dataset with Spatial Annotations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-SpatialVID-A-Large-Scale-Video-Dataset-with-Spatial-Annotations"}]]}]]}],["$","article","2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning","children":"[논문리뷰] SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning","children":"Zhaohui Yang이 arXiv에 게시한 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-SimpleVLA-RL-Scaling-VLA-Training-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated","children":"[논문리뷰] Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated","children":"Jamie Hayes이 arXiv에 게시한 'Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated"}]]}]]}],["$","article","2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning","children":"[논문리뷰] OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning","children":"Yuzheng Zhuang이 arXiv에 게시한 'OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning"}]]}]]}],["$","article","2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation","children":"[논문리뷰] Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation","children":"Dong-Ho Lee이 arXiv에 게시한 'Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Modality-Alignment-with-Multi-scale-Bilateral-Attention-for-Multimodal-Recommendation"}]]}]]}],["$","article","2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering","children":"[논문리뷰] LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering","children":"Jianguo Zhang이 arXiv에 게시한 'LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-LoCoBench-A-Benchmark-for-Long-Context-Large-Language-Models-in-Complex-Software-Engineering"}]]}]]}],["$","article","2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis","children":"[논문리뷰] Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis","children":"Wentao Hu이 arXiv에 게시한 'Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Kling-Avatar-Grounding-Multimodal-Instructions-for-Cascaded-Long-Duration-Avatar-Animation-Synthesis"}]]}]]}],["$","article","2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning","children":"[논문리뷰] HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning","children":"Zhuowei Chen이 arXiv에 게시한 'HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-HuMo-Human-Centric-Video-Generation-via-Collaborative-Multi-Modal-Conditioning"}]]}]]}],["$","article","2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents","children":"[논문리뷰] Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents","children":"Xintao Wang이 arXiv에 게시한 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Harnessing-Uncertainty-Entropy-Modulated-Policy-Gradients-for-Long-Horizon-LLM-Agents"}]]}]]}],["$","article","2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval","children":"[논문리뷰] Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval","children":"Kaicheng Yang이 arXiv에 게시한 'Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Gradient-Attention-Guided-Dual-Masking-Synergetic-Framework-for-Robust-Text-based-Person-Retrieval"}]]}]]}],["$","article","2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark","children":"[논문리뷰] FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark","children":"Shuai Bai이 arXiv에 게시한 'FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-FLUX-Reason-6M-PRISM-Bench-A-Million-Scale-Text-to-Image-Reasoning-Dataset-and-Comprehensive-Benchmark"}]]}]]}],["$","article","2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs","children":"[논문리뷰] EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs","children":"Kaiqi Kou이 arXiv에 게시한 'EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-EchoX-Towards-Mitigating-Acoustic-Semantic-Gap-via-Echo-Training-for-Speech-to-Speech-LLMs"}]]}]]}],["$","article","2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist","children":"[논문리뷰] Can Understanding and Generation Truly Benefit Together -- or Just Coexist?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist","children":"Hui Han이 arXiv에 게시한 'Can Understanding and Generation Truly Benefit Together -- or Just Coexist?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Can-Understanding-and-Generation-Truly-Benefit-Together-or-Just-Coexist"}]]}]]}],["$","article","2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting","children":"[논문리뷰] 2D Gaussian Splatting with Semantic Alignment for Image Inpainting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting","children":"Guangming Lu이 arXiv에 게시한 '2D Gaussian Splatting with Semantic Alignment for Image Inpainting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-2D-Gaussian-Splatting-with-Semantic-Alignment-for-Image-Inpainting"}]]}]]}],["$","article","2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs","children":"[논문리뷰] <think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs","children":"Alexander Panchenko이 arXiv에 게시한 '<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-think-So-lets-replace-this-phrase-with-insult-think-Lessons-learned-from-generation-of-toxic-texts-with-LLMs"}]]}]]}],["$","article","2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation","children":"[논문리뷰] RewardDance: Reward Scaling in Visual Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation","children":"Liang Li이 arXiv에 게시한 'RewardDance: Reward Scaling in Visual Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-RewardDance-Reward-Scaling-in-Visual-Generation"}]]}]]}],["$","article","2025-9-11-P3-SAM-Native-3D-Part-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-P3-SAM-Native-3D-Part-Segmentation","children":"[논문리뷰] P3-SAM: Native 3D Part Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-P3-SAM-Native-3D-Part-Segmentation","children":"Yunhan Yang이 arXiv에 게시한 'P3-SAM: Native 3D Part Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-P3-SAM-Native-3D-Part-Segmentation"}]]}]]}],["$","article","2025-9-11-Hunyuan-MT-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-Hunyuan-MT-Technical-Report","children":"[논문리뷰] Hunyuan-MT Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-Hunyuan-MT-Technical-Report","children":"Yang Du이 arXiv에 게시한 'Hunyuan-MT Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-Hunyuan-MT-Technical-Report"}]]}]]}],["$","article","2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants","children":"[논문리뷰] HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants","children":"Jacy Reese Anthis이 arXiv에 게시한 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-HumanAgencyBench-Scalable-Evaluation-of-Human-Agency-Support-in-AI-Assistants"}]]}]]}],["$","article","2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI","children":"[논문리뷰] EnvX: Agentize Everything with Agentic AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI","children":"Wenzheng Tom Tang이 arXiv에 게시한 'EnvX: Agentize Everything with Agentic AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI"}]]}]]}],["$","article","2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","children":"[논문리뷰] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","children":"Honglin Guo이 arXiv에 게시한 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models","children":"[논문리뷰] A Survey of Reinforcement Learning for Large Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models","children":"Runze Liu이 arXiv에 게시한 'A Survey of Reinforcement Learning for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models"}]]}]]}],["$","article","2025-9-11-3D-and-4D-World-Modeling-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey","children":"[논문리뷰] 3D and 4D World Modeling: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey","children":"Ao Liang이 arXiv에 게시한 '3D and 4D World Modeling: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-11-3D-and-4D-World-Modeling-A-Survey"}]]}]]}],["$","article","2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR","children":"[논문리뷰] ΔL Normalization: Rethink Loss Aggregation in RLVR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR","children":"Lili Qiu이 arXiv에 게시한 'ΔL Normalization: Rethink Loss Aggregation in RLVR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-delta-L-Normalization-Rethink-Loss-Aggregation-in-RLVR"}]]}]]}],["$","article","2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models","children":"[논문리뷰] Visual Representation Alignment for Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models","children":"Heeseong Shin이 arXiv에 게시한 'Visual Representation Alignment for Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Visual-Representation-Alignment-for-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward","children":"[논문리뷰] UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward","children":"Fei Ding이 arXiv에 게시한 'UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-UMO-Scaling-Multi-Identity-Consistency-for-Image-Customization-via-Matching-Reward"}]]}]]}],["$","article","2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding","children":"[논문리뷰] Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding","children":"Yongcheng Zeng이 arXiv에 게시한 'Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Staying-in-the-Sweet-Spot-Responsive-Reasoning-Evolution-via-Capability-Adaptive-Hint-Scaffolding"}]]}]]}],["$","article","2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge","children":"[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge","children":"Dipanjan Das이 arXiv에 게시한 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge"}]]}]]}],["$","article","2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models","children":"[논문리뷰] Reconstruction Alignment Improves Unified Multimodal Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models","children":"XuDong Wang이 arXiv에 게시한 'Reconstruction Alignment Improves Unified Multimodal Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Reconstruction-Alignment-Improves-Unified-Multimodal-Models"}]]}]]}],["$","article","2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling","children":"[논문리뷰] Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling","children":"Diana Marculescu이 arXiv에 게시한 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Q-Sched-Pushing-the-Boundaries-of-Few-Step-Diffusion-Models-with-Quantization-Aware-Scheduling"}]]}]]}],["$","article","2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning","children":"[논문리뷰] Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning","children":"Xinyu Yang이 arXiv에 게시한 'Parallel-R1: Towards Parallel Thinking via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search","children":"[논문리뷰] Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search","children":"Tianjian Li이 arXiv에 게시한 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search"}]]}]]}],["$","article","2025-9-10-Language-Self-Play-For-Data-Free-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training","children":"[논문리뷰] Language Self-Play For Data-Free Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training","children":"Vijai Mohan이 arXiv에 게시한 'Language Self-Play For Data-Free Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training"}]]}]]}],["$","article","2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions","children":"[논문리뷰] F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions","children":"Zherui Qiu이 arXiv에 게시한 'F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-F1-A-Vision-Language-Action-Model-Bridging-Understanding-and-Generation-to-Actions"}]]}]]}],["$","article","2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference","children":"[논문리뷰] Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference","children":"Yingfang Zhang이 arXiv에 게시한 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Directly-Aligning-the-Full-Diffusion-Trajectory-with-Fine-Grained-Human-Preference"}]]}]]}],["$","article","2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology","children":"[논문리뷰] Curia: A Multi-Modal Foundation Model for Radiology"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology","children":"Elodie Ferreres이 arXiv에 게시한 'Curia: A Multi-Modal Foundation Model for Radiology' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Curia-A-Multi-Modal-Foundation-Model-for-Radiology"}]]}]]}],["$","article","2025-9-10-Causal-Attention-with-Lookahead-Keys",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Causal-Attention-with-Lookahead-Keys","children":"[논문리뷰] Causal Attention with Lookahead Keys"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-10-Causal-Attention-with-Lookahead-Keys","children":"Quanquan Gu이 arXiv에 게시한 'Causal Attention with Lookahead Keys' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-10 13:11:01+0900","children":"2025년 9월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-10-Causal-Attention-with-Lookahead-Keys"}]]}]]}],["$","article","2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents","children":"[논문리뷰] WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents","children":"Aili Chen이 arXiv에 게시한 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-WebExplorer-Explore-and-Evolve-for-Training-Long-Horizon-Web-Agents"}]]}]]}],["$","article","2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts","children":"[논문리뷰] UniVerse-1: Unified Audio-Video Generation via Stitching of Experts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts","children":"Xinyao Liao이 arXiv에 게시한 'UniVerse-1: Unified Audio-Video Generation via Stitching of Experts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-UniVerse-1-Unified-Audio-Video-Generation-via-Stitching-of-Experts"}]]}]]}],["$","article","2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet","children":"[논문리뷰] Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet","children":"See-Kiong Ng이 arXiv에 게시한 'Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet"}]]}]]}],["$","article","2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers","children":"[논문리뷰] Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers","children":"Xia Xiao이 arXiv에 게시한 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Scaling-up-Multi-Turn-Off-Policy-RL-and-Multi-Agent-Tree-Search-for-LLM-Step-Provers"}]]}]]}],["$","article","2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem","children":"[논문리뷰] Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem","children":"Damien Sileo이 arXiv에 게시한 'Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Saturation-Driven-Dataset-Generation-for-LLM-Mathematical-Reasoning-in-the-TPTP-Ecosystem"}]]}]]}],["$","article","2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World","children":"[논문리뷰] R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World","children":"Bowen Zhou이 arXiv에 게시한 'R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Rtextbf2AI-Towards-Resistant-and-Resilient-AI-in-an-Evolving-World"}]]}]]}],["$","article","2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models","children":"[논문리뷰] Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models","children":"Ke Shen이 arXiv에 게시한 'Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models"}]]}]]}],["$","article","2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation","children":"[논문리뷰] Reverse-Engineered Reasoning for Open-Ended Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation","children":"Wangchunshu Zhou이 arXiv에 게시한 'Reverse-Engineered Reasoning for Open-Ended Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Reverse-Engineered-Reasoning-for-Open-Ended-Generation"}]]}]]}],["$","article","2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey","children":"[논문리뷰] Reinforcement Learning Foundations for Deep Research Systems: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey","children":"Wei Han이 arXiv에 게시한 'Reinforcement Learning Foundations for Deep Research Systems: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Reinforcement-Learning-Foundations-for-Deep-Research-Systems-A-Survey"}]]}]]}],["$","article","2025-9-9-Reinforced-Visual-Perception-with-Tools",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Reinforced-Visual-Perception-with-Tools","children":"[논문리뷰] Reinforced Visual Perception with Tools"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Reinforced-Visual-Perception-with-Tools","children":"Mingyang Fu이 arXiv에 게시한 'Reinforced Visual Perception with Tools' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Reinforced-Visual-Perception-with-Tools"}]]}]]}],["$","article","2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents","children":"[논문리뷰] Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents","children":"James Zou이 arXiv에 게시한 'Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Paper2Agent-Reimagining-Research-Papers-As-Interactive-and-Reliable-AI-Agents"}]]}]]}],["$","article","2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents","children":"[논문리뷰] MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents","children":"Zhengxi Lu이 arXiv에 게시한 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-MAS-Bench-A-Unified-Benchmark-for-Shortcut-Augmented-Hybrid-Mobile-GUI-Agents"}]]}]]}],["$","article","2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian","children":"[논문리뷰] Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian","children":"Hoi-Fong Mak이 arXiv에 게시한 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Llama-GENBA-10B-A-Trilingual-Large-Language-Model-for-German-English-and-Bavarian"}]]}]]}],["$","article","2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation","children":"[논문리뷰] Interleaving Reasoning for Better Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation","children":"Shixiang Tang이 arXiv에 게시한 'Interleaving Reasoning for Better Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Interleaving-Reasoning-for-Better-Text-to-Image-Generation"}]]}]]}],["$","article","2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning","children":"[논문리뷰] Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning","children":"Baolong Bi이 arXiv에 게시한 'Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Focusing-by-Contrastive-Attention-Enhancing-VLMs-Visual-Reasoning"}]]}]]}],["$","article","2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play","children":"[논문리뷰] Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play","children":"Rui Chen이 arXiv에 게시한 'Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Easier-Painting-Than-Thinking-Can-Text-to-Image-Models-Set-the-Stage-but-Not-Direct-the-Play"}]]}]]}],["$","article","2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard","children":"[논문리뷰] Does DINOv3 Set a New Medical Vision Standard?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard","children":"Bailiang Jian이 arXiv에 게시한 'Does DINOv3 Set a New Medical Vision Standard?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Does-DINOv3-Set-a-New-Medical-Vision-Standard"}]]}]]}],["$","article","2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning","children":"[논문리뷰] D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning","children":"Dhanvin Sanjay Namboodiri이 arXiv에 게시한 'D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-D-HUMOR-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning"}]]}]]}],["$","article","2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool","children":"[논문리뷰] WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool","children":"Wenzheng Chang이 arXiv에 게시한 'WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-WinT3R-Window-Based-Streaming-Reconstruction-with-Camera-Token-Pool"}]]}]]}],["$","article","2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning","children":"[논문리뷰] WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning","children":"Amit Namburi이 arXiv에 게시한 'WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning"}]]}]]}],["$","article","2025-9-8-Why-Language-Models-Hallucinate",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Why-Language-Models-Hallucinate","children":"[논문리뷰] Why Language Models Hallucinate"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Why-Language-Models-Hallucinate","children":"Edwin Zhang이 arXiv에 게시한 'Why Language Models Hallucinate' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-Why-Language-Models-Hallucinate"}]]}]]}],["$","article","2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation","children":"[논문리뷰] U-ARM : Ultra low-cost general teleoperation interface for robot manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation","children":"Junda Huang이 arXiv에 게시한 'U-ARM : Ultra low-cost general teleoperation interface for robot manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-U-ARM-Ultra-low-cost-general-teleoperation-interface-for-robot-manipulation"}]]}]]}],["$","article","2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models","children":"[논문리뷰] Symbolic Graphics Programming with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models","children":"Kaipeng Zhang이 arXiv에 게시한 'Symbolic Graphics Programming with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models"}]]}]]}],["$","article","2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator","children":"[논문리뷰] Set Block Decoding is a Language Model Inference Accelerator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator","children":"Jeremy Reizenstein이 arXiv에 게시한 'Set Block Decoding is a Language Model Inference Accelerator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-Set-Block-Decoding-is-a-Language-Model-Inference-Accelerator"}]]}]]}],["$","article","2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs","children":"[논문리뷰] On Robustness and Reliability of Benchmark-Based Evaluation of LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs","children":"Kevin Roitero이 arXiv에 게시한 'On Robustness and Reliability of Benchmark-Based Evaluation of LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-On-Robustness-and-Reliability-of-Benchmark-Based-Evaluation-of-LLMs"}]]}]]}],["$","article","2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting","children":"[논문리뷰] MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting","children":"Vanessa Wildman이 arXiv에 게시한 'MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting"}]]}]]}],["$","article","2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer","children":"[논문리뷰] LuxDiT: Lighting Estimation with Video Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer","children":"Sanja Fidler이 arXiv에 게시한 'LuxDiT: Lighting Estimation with Video Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-LuxDiT-Lighting-Estimation-with-Video-Diffusion-Transformer"}]]}]]}],["$","article","2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation","children":"[논문리뷰] LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation","children":"Zhan Zhao이 arXiv에 게시한 'LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-LatticeWorld-A-Multimodal-Large-Language-Model-Empowered-Framework-for-Interactive-Complex-World-Generation"}]]}]]}],["$","article","2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement","children":"[논문리뷰] Bootstrapping Task Spaces for Self-Improvement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement","children":"Yoram Bachrach이 arXiv에 게시한 'Bootstrapping Task Spaces for Self-Improvement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-Bootstrapping-Task-Spaces-for-Self-Improvement"}]]}]]}],["$","article","2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models","children":"[논문리뷰] Behavioral Fingerprinting of Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models","children":"Xing Li이 arXiv에 게시한 'Behavioral Fingerprinting of Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-08 13:10:18+0900","children":"2025년 9월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models"}]]}]]}],["$","article","2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding","children":"[논문리뷰] Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding","children":"Lionel Ni이 arXiv에 게시한 'Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding"}]]}]]}],["$","article","2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective","children":"[논문리뷰] Transition Models: Rethinking the Generative Learning Objective"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective","children":"Yangguang Li이 arXiv에 게시한 'Transition Models: Rethinking the Generative Learning Objective' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Transition-Models-Rethinking-the-Generative-Learning-Objective"}]]}]]}],["$","article","2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training","children":"[논문리뷰] Towards a Unified View of Large Language Model Post-Training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training","children":"Hongyi Liu이 arXiv에 게시한 'Towards a Unified View of Large Language Model Post-Training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Towards-a-Unified-View-of-Large-Language-Model-Post-Training"}]]}]]}],["$","article","2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings","children":"[논문리뷰] NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings","children":"Oren Glickman이 arXiv에 게시한 'NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings"}]]}]]}],["$","article","2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions","children":"[논문리뷰] Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions","children":"Yu Fu이 arXiv에 게시한 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions"}]]}]]}],["$","article","2025-9-5-From-Editor-to-Dense-Geometry-Estimator",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-From-Editor-to-Dense-Geometry-Estimator","children":"[논문리뷰] From Editor to Dense Geometry Estimator"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-From-Editor-to-Dense-Geometry-Estimator","children":"Lang Nie이 arXiv에 게시한 'From Editor to Dense Geometry Estimator' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-From-Editor-to-Dense-Geometry-Estimator"}]]}]]}],["$","article","2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation","children":"[논문리뷰] Few-step Flow for 3D Generation via Marginal-Data Transport Distillation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation","children":"Lingxi Xie이 arXiv에 게시한 'Few-step Flow for 3D Generation via Marginal-Data Transport Distillation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation"}]]}]]}],["$","article","2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize","children":"[논문리뷰] False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize","children":"Muhao Chen이 arXiv에 게시한 'False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize"}]]}]]}],["$","article","2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer","children":"[논문리뷰] Durian: Dual Reference-guided Portrait Animation with Attribute Transfer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer","children":"Hanbyul Joo이 arXiv에 게시한 'Durian: Dual Reference-guided Portrait Animation with Attribute Transfer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer"}]]}]]}],["$","article","2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth","children":"[논문리뷰] Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth","children":"Chi-Li Chen이 arXiv에 게시한 'Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth"}]]}]]}],["$","article","2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings","children":"[논문리뷰] Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings","children":"Meie Fang이 arXiv에 게시한 'Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings"}]]}]]}],["$","article","2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models","children":"[논문리뷰] Delta Activations: A Representation for Finetuned Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models","children":"Ser-Nam Lim이 arXiv에 게시한 'Delta Activations: A Representation for Finetuned Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models"}]]}]]}],["$","article","2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks","children":"[논문리뷰] DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks","children":"Jiaxuan Lu이 arXiv에 게시한 'DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-05 13:07:20+0900","children":"2025년 9월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-5-DeepResearch-Arena-The-First-Exam-of-LLMs-Research-Abilities-via-Seminar-Grounded-Tasks"}]]}]]}],["$","article","2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning","children":"[논문리뷰] Robix: A Unified Model for Robot Interaction, Reasoning and Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning","children":"Zixuan Wang이 arXiv에 게시한 'Robix: A Unified Model for Robot Interaction, Reasoning and Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-04 12:56:15+0900","children":"2025년 9월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-4-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning"}]]}]]}],["$","article","2025-9-4-Open-Data-Synthesis-For-Deep-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research","children":"[논문리뷰] Open Data Synthesis For Deep Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research","children":"Zheng Liu이 arXiv에 게시한 'Open Data Synthesis For Deep Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-04 12:56:15+0900","children":"2025년 9월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research"}]]}]]}],["$","article","2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation","children":"[논문리뷰] Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation","children":"Kai Li이 arXiv에 게시한 'Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-04 12:56:15+0900","children":"2025년 9월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-4-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation"}]]}]]}],["$","article","2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement","children":"[논문리뷰] MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement","children":"Hualiang Wang이 arXiv에 게시한 'MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-04 12:56:15+0900","children":"2025년 9월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement"}]]}]]}],["$","article","2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations","children":"[논문리뷰] LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations","children":"Yoav Gur-Arieh이 arXiv에 게시한 'LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-04 12:56:15+0900","children":"2025년 9월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-4-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations"}]]}]]}],["$","article","2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association","children":"[논문리뷰] ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association","children":"Daniel Cremers이 arXiv에 게시한 'ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-ViSTA-SLAM-Visual-SLAM-with-Symmetric-Two-view-Association"}]]}]]}],["$","article","2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use","children":"[논문리뷰] VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use","children":"Zhiheng Lyu이 arXiv에 게시한 'VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use"}]]}]]}],["$","article","2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy","children":"[논문리뷰] Universal Deep Research: Bring Your Own Model and Strategy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy","children":"Pavlo Molchanov이 arXiv에 게시한 'Universal Deep Research: Bring Your Own Model and Strategy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Universal-Deep-Research-Bring-Your-Own-Model-and-Strategy"}]]}]]}],["$","article","2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning","children":"[논문리뷰] UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning","children":"Haoyang Zou이 arXiv에 게시한 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning"}]]}]]}],["$","article","2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views","children":"[논문리뷰] Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views","children":"Junchi Yan이 arXiv에 게시한 'Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Towards-More-Diverse-and-Challenging-Pre-training-for-Point-Cloud-Learning-Self-Supervised-Cross-Reconstruction-with-Decoupled-Views"}]]}]]}],["$","article","2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey","children":"[논문리뷰] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey","children":"Hejia Geng이 arXiv에 게시한 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey"}]]}]]}],["$","article","2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang","children":"[논문리뷰] The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang","children":"Solomon Tsai이 arXiv에 게시한 'The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-The-Gold-Medals-in-an-Empty-Room-Diagnosing-Metalinguistic-Reasoning-in-LLMs-with-Camlang"}]]}]]}],["$","article","2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning","children":"[논문리뷰] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning","children":"Qian Liu이 arXiv에 게시한 'SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning"}]]}]]}],["$","article","2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction","children":"[논문리뷰] SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction","children":"bindsch이 arXiv에 게시한 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction"}]]}]]}],["$","article","2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic","children":"[논문리뷰] Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic","children":"Bernard Ghanem이 arXiv에 게시한 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic"}]]}]]}],["$","article","2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion","children":"[논문리뷰] POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion","children":"Haicheng Wang이 arXiv에 게시한 'POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-POINTS-Reader-Distillation-Free-Adaptation-of-Vision-Language-Models-for-Document-Conversion"}]]}]]}],["$","article","2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning","children":"[논문리뷰] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning","children":"Zirui Wang이 arXiv에 게시한 'OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning"}]]}]]}],["$","article","2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents","children":"[논문리뷰] MobiAgent: A Systematic Framework for Customizable Mobile Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents","children":"Wangbo Gong이 arXiv에 게시한 'MobiAgent: A Systematic Framework for Customizable Mobile Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-MobiAgent-A-Systematic-Framework-for-Customizable-Mobile-Agents"}]]}]]}],["$","article","2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization","children":"[논문리뷰] Metis: Training Large Language Models with Advanced Low-Bit Quantization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization","children":"Hengjie Cao이 arXiv에 게시한 'Metis: Training Large Language Models with Advanced Low-Bit Quantization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Metis-Training-Large-Language-Models-with-Advanced-Low-Bit-Quantization"}]]}]]}],["$","article","2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation","children":"[논문리뷰] MedDINOv3: How to adapt vision foundation models for medical image segmentation?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation","children":"Xiaofeng Yang이 arXiv에 게시한 'MedDINOv3: How to adapt vision foundation models for medical image segmentation?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation"}]]}]]}],["$","article","2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision","children":"[논문리뷰] M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision","children":"Yan-Jie Zhou이 arXiv에 게시한 'M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision"}]]}]]}],["$","article","2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model","children":"[논문리뷰] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model","children":"Jianwei Yang이 arXiv에 게시한 'LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-LLaVA-Critic-R1-Your-Critic-Model-is-Secretly-a-Strong-Policy-Model"}]]}]]}],["$","article","2025-9-3-Kwai-Keye-VL-1-5-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report","children":"[논문리뷰] Kwai Keye-VL 1.5 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report","children":"SXxtyz이 arXiv에 게시한 'Kwai Keye-VL 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report"}]]}]]}],["$","article","2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations","children":"[논문리뷰] Jointly Reinforcing Diversity and Quality in Language Model Generations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations","children":"Tianlu이 arXiv에 게시한 'Jointly Reinforcing Diversity and Quality in Language Model Generations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations"}]]}]]}],["$","article","2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers","children":"[논문리뷰] Improving Large Vision and Language Models by Learning from a Panel of Peers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers","children":"Simon Jenni이 arXiv에 게시한 'Improving Large Vision and Language Models by Learning from a Panel of Peers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Improving-Large-Vision-and-Language-Models-by-Learning-from-a-Panel-of-Peers"}]]}]]}],["$","article","2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR","children":"[논문리뷰] Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR","children":"Lu Wang이 arXiv에 게시한 'Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR"}]]}]]}],["$","article","2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer","children":"[논문리뷰] GenCompositor: Generative Video Compositing with Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer","children":"Lingen Li이 arXiv에 게시한 'GenCompositor: Generative Video Compositing with Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-GenCompositor-Generative-Video-Compositing-with-Diffusion-Transformer"}]]}]]}],["$","article","2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games","children":"[논문리뷰] FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games","children":"Dongmin Park이 arXiv에 게시한 'FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-FlashAdventure-A-Benchmark-for-GUI-Agents-Solving-Full-Story-Arcs-in-Diverse-Adventure-Games"}]]}]]}],["$","article","2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models","children":"[논문리뷰] FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models","children":"Zhen Wang이 arXiv에 게시한 'FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-FastFit-Accelerating-Multi-Reference-Virtual-Try-On-via-Cacheable-Diffusion-Models"}]]}]]}],["$","article","2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them","children":"[논문리뷰] Fantastic Pretraining Optimizers and Where to Find Them"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them","children":"Percy Liang이 arXiv에 게시한 'Fantastic Pretraining Optimizers and Where to Find Them' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them"}]]}]]}],["$","article","2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding","children":"[논문리뷰] ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding","children":"Xuanyu Zheng이 arXiv에 게시한 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-ELV-Halluc-Benchmarking-Semantic-Aggregation-Hallucinations-in-Long-Video-Understanding"}]]}]]}],["$","article","2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing","children":"[논문리뷰] Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing","children":"Amin Heyrani Nobar이 arXiv에 게시한 'Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Discrete-Noise-Inversion-for-Next-scale-Autoregressive-Text-based-Image-Editing"}]]}]]}],["$","article","2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization","children":"[논문리뷰] DCPO: Dynamic Clipping Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization","children":"Kai Lu이 arXiv에 게시한 'DCPO: Dynamic Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-DCPO-Dynamic-Clipping-Policy-Optimization"}]]}]]}],["$","article","2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection","children":"[논문리뷰] C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection","children":"Vito Renó이 arXiv에 게시한 'C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-C-DiffDet-Fusing-Global-Scene-Context-with-Generative-Denoising-for-High-Fidelity-Object-Detection"}]]}]]}],["$","article","2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining","children":"[논문리뷰] Benchmarking Optimizers for Large Language Model Pretraining"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining","children":"mjaggi이 arXiv에 게시한 'Benchmarking Optimizers for Large Language Model Pretraining' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Benchmarking-Optimizers-for-Large-Language-Model-Pretraining"}]]}]]}],["$","article","2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System","children":"[논문리뷰] Baichuan-M2: Scaling Medical Capability with Large Verifier System"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System","children":"Jayok6이 arXiv에 게시한 'Baichuan-M2: Scaling Medical Capability with Large Verifier System' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System"}]]}]]}],["$","article","2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation","children":"[논문리뷰] Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation","children":"Xiaolei Huang이 arXiv에 게시한 'Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Attributes-as-Textual-Genes-Leveraging-LLMs-as-Genetic-Algorithm-Simulators-for-Conditional-Synthetic-Data-Generation"}]]}]]}],["$","article","2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models","children":"[논문리뷰] AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models","children":"Rahul Karthikeyan이 arXiv에 게시한 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models"}]]}]]}],["$","article","2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat","children":"[논문리뷰] UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat","children":"Omartificial-Intelligence-Space이 arXiv에 게시한 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-02 13:01:41+0900","children":"2025년 9월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-2-UI-Level-Evaluation-of-ALLaM-34B-Measuring-an-Arabic-Centric-LLM-via-HUMAIN-Chat"}]]}]]}],["$","article","2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables","children":"[논문리뷰] T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables","children":"Yu Zhao이 arXiv에 게시한 'T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-02 13:01:41+0900","children":"2025년 9월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-2-T2R-bench-A-Benchmark-for-Generating-Article-Level-Reports-from-Real-World-Industrial-Tables"}]]}]]}],["$","article","2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning","children":"[논문리뷰] PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning","children":"Yuewei Zhang이 arXiv에 게시한 'PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-02 13:01:41+0900","children":"2025년 9월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-2-PVPO-Pre-Estimated-Value-Based-Policy-Optimization-for-Agentic-Reasoning"}]]}]]}],["$","article","2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes","children":"[논문리뷰] No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes","children":"Danijel Skočaj이 arXiv에 게시한 'No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-02 13:01:41+0900","children":"2025년 9월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-2-No-Label-Left-Behind-A-Unified-Surface-Defect-Detection-Model-for-all-Supervision-Regimes"}]]}]]}],["$","article","2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench","children":"[논문리뷰] How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench","children":"Jayanth Srinivasa이 arXiv에 게시한 'How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on τ-bench' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-02 13:01:41+0900","children":"2025년 9월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-2-How-Can-Input-Reformulation-Improve-Tool-Usage-Accuracy-in-a-Complex-Dynamic-Environment-A-Study-on-τ-bench"}]]}]]}],["$","article","2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents","children":"[논문리뷰] From reactive to cognitive: brain-inspired spatial intelligence for embodied agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents","children":"Songming Liu이 arXiv에 게시한 'From reactive to cognitive: brain-inspired spatial intelligence for embodied agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-02 13:01:41+0900","children":"2025년 9월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-2-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agents"}]]}]]}],["$","article","2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning","children":"[논문리뷰] UItron: Foundational GUI Agent with Advanced Perception and Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning","children":"Yufeng Zhong이 arXiv에 게시한 'UItron: Foundational GUI Agent with Advanced Perception and Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-UItron-Foundational-GUI-Agent-with-Advanced-Perception-and-Planning"}]]}]]}],["$","article","2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training","children":"[논문리뷰] TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training","children":"Jiyao Deng이 arXiv에 게시한 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-TiKMiX-Take-Data-Influence-into-Dynamic-Mixture-for-Language-Model-Pre-training"}]]}]]}],["$","article","2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models","children":"[논문리뷰] Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models","children":"Yifan Lu이 arXiv에 게시한 'Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models"}]]}]]}],["$","article","2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis","children":"[논문리뷰] TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis","children":"Pengcheng Chen이 arXiv에 게시한 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-TalkVid-A-Large-Scale-Diversified-Dataset-for-Audio-Driven-Talking-Head-Synthesis"}]]}]]}],["$","article","2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning","children":"[논문리뷰] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning","children":"Han Hu이 arXiv에 게시한 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning"}]]}]]}],["$","article","2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices","children":"[논문리뷰] Morae: Proactively Pausing UI Agents for User Choices"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices","children":"Amy Pavel이 arXiv에 게시한 'Morae: Proactively Pausing UI Agents for User Choices' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-Morae-Proactively-Pausing-UI-Agents-for-User-Choices"}]]}]]}],["$","article","2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery","children":"[논문리뷰] Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery","children":"Wenjie Zhou이 arXiv에 게시한 'Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-Mimicking-the-Physicists-EyeA-VLM-centric-Approach-for-Physics-Formula-Discovery"}]]}]]}],["$","article","2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation","children":"[논문리뷰] HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation","children":"Tianhai Liang이 arXiv에 게시한 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation"}]]}]]}],["$","article","2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control","children":"[논문리뷰] EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control","children":"Zhaoqing Chen이 arXiv에 게시한 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-EmbodiedOneVision-Interleaved-Vision-Text-Action-Pretraining-for-General-Robot-Control"}]]}]]}],["$","article","2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models","children":"[논문리뷰] Efficient Code Embeddings from Code Generation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models","children":"Han Xiao이 arXiv에 게시한 'Efficient Code Embeddings from Code Generation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-Efficient-Code-Embeddings-from-Code-Generation-Models"}]]}]]}],["$","article","2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation","children":"[논문리뷰] Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation","children":"Qi Jia이 arXiv에 게시한 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-Droplet3D-Commonsense-Priors-from-Videos-Facilitate-3D-Generation"}]]}]]}],["$","article","2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP","children":"[논문리뷰] CLIPSym: Delving into Symmetry Detection with CLIP"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP","children":"Raymond A. Yeh이 arXiv에 게시한 'CLIPSym: Delving into Symmetry Detection with CLIP' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-CLIPSym-Delving-into-Symmetry-Detection-with-CLIP"}]]}]]}],["$","article","2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models","children":"[논문리뷰] AHELM: A Holistic Evaluation of Audio-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models","children":"Siwei Yang이 arXiv에 게시한 'AHELM: A Holistic Evaluation of Audio-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-AHELM-A-Holistic-Evaluation-of-Audio-Language-Models"}]]}]]}],["$","article","2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers","children":"[논문리뷰] A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers","children":"Jiamin Wu이 arXiv에 게시한 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-A-Survey-of-Scientific-Large-Language-Models-From-Data-Foundations-to-Agent-Frontiers"}]]}]]}],["$","article","2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code","children":"[논문리뷰] A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code","children":"Libo Chen이 arXiv에 게시한 'A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-01 13:14:34+0900","children":"2025년 9월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-1-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Code"}]]}]]}],["$","article","2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report","children":"[논문리뷰] rStar2-Agent: Agentic Reasoning Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report","children":"Weijiang Xu이 arXiv에 게시한 'rStar2-Agent: Agentic Reasoning Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-rStar2-Agent-Agentic-Reasoning-Technical-Report"}]]}]]}],["$","article","2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning","children":"[논문리뷰] USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning","children":"Jiahe Tian이 arXiv에 게시한 'USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-USO-Unified-Style-and-Subject-Driven-Generation-via-Disentangled-and-Reward-Learning"}]]}]]}],["$","article","2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection","children":"[논문리뷰] Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection","children":"Bernard Ghanem이 arXiv에 게시한 'Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Turning-the-Spell-Around-Lightweight-Alignment-Amplification-via-Rank-One-Safety-Injection"}]]}]]}],["$","article","2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning","children":"[논문리뷰] TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning","children":"Simin Ma이 arXiv에 게시한 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning"}]]}]]}],["$","article","2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos","children":"[논문리뷰] ROSE: Remove Objects with Side Effects in Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos","children":"Hantang Liu이 arXiv에 게시한 'ROSE: Remove Objects with Side Effects in Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-ROSE-Remove-Objects-with-Side-Effects-in-Videos"}]]}]]}],["$","article","2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models","children":"[논문리뷰] Provable Benefits of In-Tool Learning for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models","children":"Vivien Cabannes이 arXiv에 게시한 'Provable Benefits of In-Tool Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models"}]]}]]}],["$","article","2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning","children":"[논문리뷰] Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning","children":"Jiazi Bu이 arXiv에 게시한 'Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Pref-GRPO-Pairwise-Preference-Reward-based-GRPO-for-Stable-Text-to-Image-Reinforcement-Learning"}]]}]]}],["$","article","2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD","children":"[논문리뷰] Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD","children":"Roy Ka-Wei Lee이 arXiv에 게시한 'Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Persuasion-Dynamics-in-LLMs-Investigating-Robustness-and-Adaptability-in-Knowledge-and-Safety-with-DuET-PD"}]]}]]}],["$","article","2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning","children":"[논문리뷰] OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning","children":"Yitong Wang이 arXiv에 게시한 'OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-OneReward-Unified-Mask-Guided-Image-Generation-via-Multi-Task-Human-Preference-Learning"}]]}]]}],["$","article","2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models","children":"[논문리뷰] OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models","children":"Alex Endert이 arXiv에 게시한 'OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-OnGoal-Tracking-and-Visualizing-Conversational-Goals-in-Multi-Turn-Dialogue-with-Large-Language-Models"}]]}]]}],["$","article","2025-8-29-Multi-View-3D-Point-Tracking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Multi-View-3D-Point-Tracking","children":"[논문리뷰] Multi-View 3D Point Tracking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Multi-View-3D-Point-Tracking","children":"Irem Demir이 arXiv에 게시한 'Multi-View 3D Point Tracking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Multi-View-3D-Point-Tracking"}]]}]]}],["$","article","2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation","children":"[논문리뷰] Mixture of Contexts for Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation","children":"Junfei Xiao이 arXiv에 게시한 'Mixture of Contexts for Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Mixture-of-Contexts-for-Long-Video-Generation"}]]}]]}],["$","article","2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers","children":"[논문리뷰] MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers","children":"Shashank Biju이 arXiv에 게시한 'MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-MCP-Bench-Benchmarking-Tool-Using-LLM-Agents-with-Complex-Real-World-Tasks-via-MCP-Servers"}]]}]]}],["$","article","2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes","children":"[논문리뷰] FakeParts: a New Family of AI-Generated DeepFakes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes","children":"Xi Wang이 arXiv에 게시한 'FakeParts: a New Family of AI-Generated DeepFakes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-FakeParts-a-New-Family-of-AI-Generated-DeepFakes"}]]}]]}],["$","article","2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview","children":"[논문리뷰] Dress&Dance: Dress up and Dance as You Like It - Technical Preview"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview","children":"Yu-Xiong Wang이 arXiv에 게시한 'Dress&Dance: Dress up and Dance as You Like It - Technical Preview' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-DressDance-Dress-up-and-Dance-as-You-Like-It-Technical-Preview"}]]}]]}],["$","article","2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation","children":"[논문리뷰] Collaborative Multi-Modal Coding for High-Quality 3D Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation","children":"Ziwei Liu이 arXiv에 게시한 'Collaborative Multi-Modal Coding for High-Quality 3D Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation"}]]}]]}],["$","article","2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification","children":"[논문리뷰] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification","children":"Liqiang Nie이 arXiv에 게시한 'CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-CogVLA-Cognition-Aligned-Vision-Language-Action-Model-via-Instruction-Driven-Routing-Sparsification"}]]}]]}],["$","article","2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI","children":"[논문리뷰] AWorld: Orchestrating the Training Recipe for Agentic AI"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI","children":"Qintong Wu이 arXiv에 게시한 'AWorld: Orchestrating the Training Recipe for Agentic AI' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-29 13:14:44+0900","children":"2025년 8월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-29-AWorld-Orchestrating-the-Training-Recipe-for-Agentic-AI"}]]}]]}],["$","article","2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference","children":"[논문리뷰] Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference","children":"Chunlei Han이 arXiv에 게시한 'Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Taming-the-Chaos-Coordinated-Autoscaling-for-Heterogeneous-and-Disaggregated-LLM-Inference"}]]}]]}],["$","article","2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning","children":"[논문리뷰] StepWiser: Stepwise Generative Judges for Wiser Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning","children":"Olga Golovneva이 arXiv에 게시한 'StepWiser: Stepwise Generative Judges for Wiser Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning"}]]}]]}],["$","article","2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition","children":"[논문리뷰] Self-Rewarding Vision-Language Model via Reasoning Decomposition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition","children":"Zhenwen Liang이 arXiv에 게시한 'Self-Rewarding Vision-Language Model via Reasoning Decomposition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Self-Rewarding-Vision-Language-Model-via-Reasoning-Decomposition"}]]}]]}],["$","article","2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling","children":"[논문리뷰] Predicting the Order of Upcoming Tokens Improves Language Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling","children":"Alham Fikri Aji이 arXiv에 게시한 'Predicting the Order of Upcoming Tokens Improves Language Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling"}]]}]]}],["$","article","2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment","children":"[논문리뷰] MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment","children":"An-An Liu이 arXiv에 게시한 'MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-MotionFlux-Efficient-Text-Guided-Motion-Generation-through-Rectified-Flow-Matching-and-Preference-Alignment"}]]}]]}],["$","article","2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents","children":"[논문리뷰] Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents","children":"Yue Yao이 arXiv에 게시한 'Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Mind-the-Third-Eye-Benchmarking-Privacy-Awareness-in-MLLM-powered-Smartphone-Agents"}]]}]]}],["$","article","2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation","children":"[논문리뷰] MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation","children":"Yan Zhou이 arXiv에 게시한 'MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-MIDAS-Multimodal-Interactive-Digital-human-Synthesis-via-Real-time-Autoregressive-Video-Generation"}]]}]]}],["$","article","2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation","children":"[논문리뷰] Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation","children":"Anton Ivaschenko이 arXiv에 게시한 'Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimation"}]]}]]}],["$","article","2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies","children":"[논문리뷰] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies","children":"Sitong Mao이 arXiv에 게시한 'Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Discrete-Diffusion-VLA-Bringing-Discrete-Diffusion-to-Action-Decoding-in-Vision-Language-Action-Policies"}]]}]]}],["$","article","2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding","children":"[논문리뷰] Diffusion Language Models Know the Answer Before Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding","children":"Shilin Yan이 arXiv에 게시한 'Diffusion Language Models Know the Answer Before Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Diffusion-Language-Models-Know-the-Answer-Before-Decoding"}]]}]]}],["$","article","2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis","children":"[논문리뷰] DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis","children":"Ion Stoica이 arXiv에 게시한 'DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-DeepScholar-Bench-A-Live-Benchmark-and-Automated-Evaluation-for-Generative-Research-Synthesis"}]]}]]}],["$","article","2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning","children":"[논문리뷰] CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning","children":"Jianze Liang이 arXiv에 게시한 'CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-CODA-Coordinating-the-Cerebrum-and-Cerebellum-for-a-Dual-Brain-Computer-Use-Agent-with-Decoupled-Reinforcement-Learning"}]]}]]}],["$","article","2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR","children":"[논문리뷰] Beyond Transcription: Mechanistic Interpretability in ASR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR","children":"Aviv Shamsian이 arXiv에 게시한 'Beyond Transcription: Mechanistic Interpretability in ASR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-Beyond-Transcription-Mechanistic-Interpretability-in-ASR"}]]}]]}],["$","article","2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models","children":"[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models","children":"Yixiao Ge이 arXiv에 게시한 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models"}]]}]]}],["$","article","2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation","children":"[논문리뷰] Wan-S2V: Audio-Driven Cinematic Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation","children":"Chaonan Ji이 arXiv에 게시한 'Wan-S2V: Audio-Driven Cinematic Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Wan-S2V-Audio-Driven-Cinematic-Video-Generation"}]]}]]}],["$","article","2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space","children":"[논문리뷰] VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space","children":"Rui Chen이 arXiv에 게시한 'VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-VoxHammer-Training-Free-Precise-and-Coherent-3D-Editing-in-Native-3D-Space"}]]}]]}],["$","article","2025-8-27-VibeVoice-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-VibeVoice-Technical-Report","children":"[논문리뷰] VibeVoice Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-VibeVoice-Technical-Report","children":"Yaoyao Chang이 arXiv에 게시한 'VibeVoice Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-VibeVoice-Technical-Report"}]]}]]}],["$","article","2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities","children":"[논문리뷰] Unraveling the cognitive patterns of Large Language Models through module communities"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities","children":"Jianxi Gao이 arXiv에 게시한 'Unraveling the cognitive patterns of Large Language Models through module communities' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities"}]]}]]}],["$","article","2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning","children":"[논문리뷰] UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning","children":"Ran Guo이 arXiv에 게시한 'UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-UltraMemV2-Memory-Networks-Scaling-to-120B-Parameters-with-Superior-Long-Context-Learning"}]]}]]}],["$","article","2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling","children":"[논문리뷰] TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling","children":"Zhoufutu Wen이 arXiv에 게시한 'TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling"}]]}]]}],["$","article","2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo","children":"[논문리뷰] Training Language Model Agents to Find Vulnerabilities with CTF-Dojo"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo","children":"Zijian Wang이 arXiv에 게시한 'Training Language Model Agents to Find Vulnerabilities with CTF-Dojo' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Training-Language-Model-Agents-to-Find-Vulnerabilities-with-CTF-Dojo"}]]}]]}],["$","article","2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models","children":"[논문리뷰] ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models","children":"Jiangjie Chen이 arXiv에 게시한 'ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models"}]]}]]}],["$","article","2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration","children":"[논문리뷰] Spacer: Towards Engineered Scientific Inspiration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration","children":"zerojun48이 arXiv에 게시한 'Spacer: Towards Engineered Scientific Inspiration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Spacer-Towards-Engineered-Scientific-Inspiration"}]]}]]}],["$","article","2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks","children":"[논문리뷰] ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks","children":"Kai Jia이 arXiv에 게시한 'ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-ReportBench-Evaluating-Deep-Research-Agents-via-Academic-Survey-Tasks"}]]}]]}],["$","article","2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting","children":"[논문리뷰] QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting","children":"Manuela Veloso이 arXiv에 게시한 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting"}]]}]]}],["$","article","2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels","children":"[논문리뷰] Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels","children":"Dinesh Jayaraman이 arXiv에 게시한 'Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Pixie-Fast-and-Generalizable-Supervised-Learning-of-3D-Physics-from-Pixels"}]]}]]}],["$","article","2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks","children":"[논문리뷰] Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks","children":"Daisuke Nohara이 arXiv에 게시한 'Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks"}]]}]]}],["$","article","2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation","children":"[논문리뷰] OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation","children":"Jiaqi Yang이 arXiv에 게시한 'OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation"}]]}]]}],["$","article","2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models","children":"[논문리뷰] ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models","children":"Beiqi Chen이 arXiv에 게시한 'ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-ObjFiller-3D-Consistent-Multi-view-3D-Inpainting-via-Video-Diffusion-Models"}]]}]]}],["$","article","2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies","children":"[논문리뷰] MovieCORE: COgnitive REasoning in Movies"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies","children":"Hung-Ting Su이 arXiv에 게시한 'MovieCORE: COgnitive REasoning in Movies' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-MovieCORE-COgnitive-REasoning-in-Movies"}]]}]]}],["$","article","2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling","children":"[논문리뷰] FastMesh:Efficient Artistic Mesh Generation via Component Decoupling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling","children":"Xingang Pan이 arXiv에 게시한 'FastMesh:Efficient Artistic Mesh Generation via Component Decoupling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-FastMeshEfficient-Artistic-Mesh-Generation-via-Component-Decoupling"}]]}]]}],["$","article","2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning","children":"[논문리뷰] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning","children":"Arman Cohan이 arXiv에 게시한 'Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning"}]]}]]}],["$","article","2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation","children":"[논문리뷰] ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation","children":"Kun Kuang이 arXiv에 게시한 'ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation"}]]}]]}],["$","article","2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","children":"[논문리뷰] CineScale: Free Lunch in High-Resolution Cinematic Visual Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation","children":"Ziwei Liu이 arXiv에 게시한 'CineScale: Free Lunch in High-Resolution Cinematic Visual Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation"}]]}]]}],["$","article","2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics","children":"[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics","children":"Dongchen Huang이 arXiv에 게시한 'CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics"}]]}]]}],["$","article","2025-8-27-Autoregressive-Universal-Video-Segmentation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Autoregressive-Universal-Video-Segmentation-Model","children":"[논문리뷰] Autoregressive Universal Video Segmentation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Autoregressive-Universal-Video-Segmentation-Model","children":"Albert Gu이 arXiv에 게시한 'Autoregressive Universal Video Segmentation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Autoregressive-Universal-Video-Segmentation-Model"}]]}]]}],["$","article","2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation","children":"[논문리뷰] Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation","children":"Haoxiang Shi이 arXiv에 게시한 'Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generation"}]]}]]}],["$","article","2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions","children":"[논문리뷰] UQ: Assessing Language Models on Unsolved Questions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions","children":"Wei Liu이 arXiv에 게시한 'UQ: Assessing Language Models on Unsolved Questions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-UQ-Assessing-Language-Models-on-Unsolved-Questions"}]]}]]}],["$","article","2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling","children":"[논문리뷰] TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling","children":"Jiaqi Li이 arXiv에 게시한 'TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-TaDiCodec-Text-aware-Diffusion-Speech-Tokenizer-for-Speech-Language-Modeling"}]]}]]}],["$","article","2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation","children":"[논문리뷰] T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation","children":"Xihui Liu이 arXiv에 게시한 'T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-T2I-ReasonBench-Benchmarking-Reasoning-Informed-Text-to-Image-Generation"}]]}]]}],["$","article","2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods","children":"[논문리뷰] SpotEdit: Evaluating Visually-Guided Image Editing Methods"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods","children":"Ersin Yumer이 arXiv에 게시한 'SpotEdit: Evaluating Visually-Guided Image Editing Methods' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-SpotEdit-Evaluating-Visually-Guided-Image-Editing-Methods"}]]}]]}],["$","article","2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering","children":"[논문리뷰] ST-Raptor: LLM-Powered Semi-Structured Table Question Answering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering","children":"Wei Zhou이 arXiv에 게시한 'ST-Raptor: LLM-Powered Semi-Structured Table Question Answering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answering"}]]}]]}],["$","article","2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs","children":"[논문리뷰] PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs","children":"Chenyu You이 arXiv에 게시한 'PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMs"}]]}]]}],["$","article","2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges","children":"[논문리뷰] Neither Valid nor Reliable? Investigating the Use of LLMs as Judges"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges","children":"Golnoosh Farnadi이 arXiv에 게시한 'Neither Valid nor Reliable? Investigating the Use of LLMs as Judges' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Neither-Valid-nor-Reliable-Investigating-the-Use-of-LLMs-as-Judges"}]]}]]}],["$","article","2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting","children":"[논문리뷰] MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting","children":"Yanzhe Liang이 arXiv에 게시한 'MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-MeshSplat-Generalizable-Sparse-View-Surface-Reconstruction-via-Gaussian-Splatting"}]]}]]}],["$","article","2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion","children":"[논문리뷰] MV-RAG: Retrieval Augmented Multiview Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion","children":"sagiebenaim이 arXiv에 게시한 'MV-RAG: Retrieval Augmented Multiview Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-MV-RAG-Retrieval-Augmented-Multiview-Diffusion"}]]}]]}],["$","article","2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment","children":"[논문리뷰] MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment","children":"Doratossadat Dastgheib이 arXiv에 게시한 'MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-MEENA-PersianMMMU-Multimodal-Multilingual-Educational-Exams-for-N-level-Assessment"}]]}]]}],["$","article","2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism","children":"[논문리뷰] Limitations of Normalization in Attention Mechanism"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism","children":"Radu State이 arXiv에 게시한 'Limitations of Normalization in Attention Mechanism' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Limitations-of-Normalization-in-Attention-Mechanism"}]]}]]}],["$","article","2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency","children":"[논문리뷰] InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency","children":"jinglinglin이 arXiv에 게시한 'InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiency"}]]}]]}],["$","article","2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German","children":"[논문리뷰] German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German","children":"Cristian-George Craciun이 arXiv에 게시한 'German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-German4All-A-Dataset-and-Model-for-Readability-Controlled-Paraphrasing-in-German"}]]}]]}],["$","article","2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning","children":"[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning","children":"Xin Zheng이 arXiv에 게시한 'Explain Before You Answer: A Survey on Compositional Visual Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning"}]]}]]}],["$","article","2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning","children":"[논문리뷰] Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning","children":"Jiale Zhao이 arXiv에 게시한 'Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning"}]]}]]}],["$","article","2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling","children":"[논문리뷰] Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling","children":"Daniil Orel이 arXiv에 게시한 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling"}]]}]]}],["$","article","2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference","children":"[논문리뷰] TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference","children":"Di Yin이 arXiv에 게시한 'TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill & Decode Inference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-TPLA-Tensor-Parallel-Latent-Attention-for-Efficient-Disaggregated-Prefill-Decode-Inference"}]]}]]}],["$","article","2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding","children":"[논문리뷰] Selective Contrastive Learning for Weakly Supervised Affordance Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding","children":"Jae-Pil Heo이 arXiv에 게시한 'Selective Contrastive Learning for Weakly Supervised Affordance Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-Selective-Contrastive-Learning-for-Weakly-Supervised-Affordance-Grounding"}]]}]]}],["$","article","2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics","children":"[논문리뷰] Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics","children":"Xiao Sun이 arXiv에 게시한 'Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-Learnable-SMPLify-A-Neural-Solution-for-Optimization-Free-Human-Pose-Inverse-Kinematics"}]]}]]}],["$","article","2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts","children":"[논문리뷰] Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts","children":"Liming Fang이 arXiv에 게시한 'Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-Jailbreaking-Commercial-Black-Box-LLMs-with-Explicitly-Harmful-Prompts"}]]}]]}],["$","article","2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles","children":"[논문리뷰] InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles","children":"Diping Song이 arXiv에 게시한 'InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles"}]]}]]}],["$","article","2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning","children":"[논문리뷰] End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning","children":"Pengcheng Qiu이 arXiv에 게시한 'End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"}]]}]]}],["$","article","2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person","children":"[논문리뷰] EgoTwin: Dreaming Body and View in First Person"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person","children":"Wentao Wang이 arXiv에 게시한 'EgoTwin: Dreaming Body and View in First Person' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-EgoTwin-Dreaming-Body-and-View-in-First-Person"}]]}]]}],["$","article","2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible","children":"[논문리뷰] Do What? Teaching Vision-Language-Action Models to Reject the Impossible"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible","children":"Roei Herzig이 arXiv에 게시한 'Do What? Teaching Vision-Language-Action Models to Reject the Impossible' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible"}]]}]]}],["$","article","2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders","children":"[논문리뷰] CRISP: Persistent Concept Unlearning via Sparse Autoencoders"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders","children":"Yonatan Belinkov이 arXiv에 게시한 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-CRISP-Persistent-Concept-Unlearning-via-Sparse-Autoencoders"}]]}]]}],["$","article","2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning","children":"[논문리뷰] CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning","children":"Yulun Zhang이 arXiv에 게시한 'CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning"}]]}]]}],["$","article","2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR","children":"[논문리뷰] Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR","children":"Ying Nian Wu이 arXiv에 게시한 'Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR"}]]}]]}],["$","article","2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications","children":"[논문리뷰] AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications","children":"Liuyi Yao이 arXiv에 게시한 'AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications"}]]}]]}],["$","article","2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions","children":"[논문리뷰] AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions","children":"Yidi Du이 arXiv에 게시한 'AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-25 13:13:07+0900","children":"2025년 8월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-25-AetherCode-Evaluating-LLMs-Ability-to-Win-In-Premier-Programming-Competitions"}]]}]]}],["$","article","2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists","children":"[논문리뷰] aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists","children":"Heng Zhang이 arXiv에 게시한 'aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-aiXiv-A-Next-Generation-Open-Access-Ecosystem-for-Scientific-Discovery-Generated-by-AI-Scientists"}]]}]]}],["$","article","2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding","children":"[논문리뷰] When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding","children":"Rui Guo이 arXiv에 게시한 'When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding"}]]}]]}],["$","article","2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation","children":"[논문리뷰] Waver: Wave Your Way to Lifelike Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation","children":"Yifu Zhang이 arXiv에 게시한 'Waver: Wave Your Way to Lifelike Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Waver-Wave-Your-Way-to-Lifelike-Video-Generation"}]]}]]}],["$","article","2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds","children":"[논문리뷰] Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds","children":"Chuiyun Wu이 arXiv에 게시한 'Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds"}]]}]]}],["$","article","2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass","children":"[논문리뷰] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass","children":"Ya Zhang이 arXiv에 게시한 'SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass"}]]}]]}],["$","article","2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation","children":"[논문리뷰] Mobile-Agent-v3: Foundamental Agents for GUI Automation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation","children":"Haowei Liu이 arXiv에 게시한 'Mobile-Agent-v3: Foundamental Agents for GUI Automation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation"}]]}]]}],["$","article","2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries","children":"[논문리뷰] LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries","children":"huuuyeah이 arXiv에 게시한 'LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-enabled-Agents-on-Challenging-Queries"}]]}]]}],["$","article","2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model","children":"[논문리뷰] Intern-S1: A Scientific Multimodal Foundation Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model","children":"xuhuang87이 arXiv에 게시한 'Intern-S1: A Scientific Multimodal Foundation Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Intern-S1-A-Scientific-Multimodal-Foundation-Model"}]]}]]}],["$","article","2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior","children":"[논문리뷰] INTIMA: A Benchmark for Human-AI Companionship Behavior"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior","children":"Yacine Jernite이 arXiv에 게시한 'INTIMA: A Benchmark for Human-AI Companionship Behavior' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-INTIMA-A-Benchmark-for-Human-AI-Companionship-Behavior"}]]}]]}],["$","article","2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models","children":"[논문리뷰] Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models","children":"Lifan Guo이 arXiv에 게시한 'Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models"}]]}]]}],["$","article","2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries","children":"[논문리뷰] 'Does the cafe entrance look accessible? Where is the door?' Towards Geospatial AI Agents for Visual Inquiries"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries","children":"Xia Su이 arXiv에 게시한 'Does the cafe entrance look accessible? Where is the door? Towards Geospatial AI Agents for Visual Inquiries' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Does-the-cafe-entrance-look-accessible-Where-is-the-door-Towards-Geospatial-AI-Agents-for-Visual-Inquiries"}]]}]]}],["$","article","2025-8-22-Deep-Think-with-Confidence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Deep-Think-with-Confidence","children":"[논문리뷰] Deep Think with Confidence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-Deep-Think-with-Confidence","children":"Xuewei Wang이 arXiv에 게시한 'Deep Think with Confidence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-Deep-Think-with-Confidence"}]]}]]}],["$","article","2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling","children":"[논문리뷰] ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling","children":"Shunsuke Saito이 arXiv에 게시한 'ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-ATLAS-Decoupling-Skeletal-and-Shape-Parameters-for-Expressive-Parametric-Human-Modeling"}]]}]]}],["$","article","2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks","children":"[논문리뷰] A Survey on Large Language Model Benchmarks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks","children":"Siyi Li이 arXiv에 게시한 'A Survey on Large Language Model Benchmarks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-22 13:10:52+0900","children":"2025년 8월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-22-A-Survey-on-Large-Language-Model-Benchmarks"}]]}]]}],["$","article","2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning","children":"[논문리뷰] mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning","children":"anoperson이 arXiv에 게시한 'mSCoRe: a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-mSCoRe-a-Multilingual-and-Scalable-Benchmark-for-Skill-based-Commonsense-Reasoning"}]]}]]}],["$","article","2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions","children":"[논문리뷰] ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions","children":"Daeyoung Kim이 arXiv에 게시한 'ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions"}]]}]]}],["$","article","2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization","children":"[논문리뷰] Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization","children":"Hao Chen이 arXiv에 게시한 'Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-Tinker-Diffusions-Gift-to-3D-Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization"}]]}]]}],["$","article","2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World","children":"[논문리뷰] RynnEC: Bringing MLLMs into Embodied World"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World","children":"jiangpinliu이 arXiv에 게시한 'RynnEC: Bringing MLLMs into Embodied World' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-RynnEC-Bringing-MLLMs-into-Embodied-World"}]]}]]}],["$","article","2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation","children":"[논문리뷰] Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation","children":"Shiqing Wu이 arXiv에 게시한 'Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-Refining-Contrastive-Learning-and-Homography-Relations-for-Multi-Modal-Recommendation"}]]}]]}],["$","article","2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs","children":"[논문리뷰] Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs","children":"Haobo Xu이 arXiv에 게시한 'Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs"}]]}]]}],["$","article","2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting","children":"[논문리뷰] On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting","children":"Guoyin Wang이 arXiv에 게시한 'On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting"}]]}]]}],["$","article","2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model","children":"[논문리뷰] NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model","children":"abercovich이 arXiv에 게시한 'NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"}]]}]]}],["$","article","2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds","children":"[논문리뷰] MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds","children":"Jiangmiao이 arXiv에 게시한 'MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds"}]]}]]}],["$","article","2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers","children":"[논문리뷰] MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers","children":"Prathyusha Jwalapuram이 arXiv에 게시한 'MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers"}]]}]]}],["$","article","2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer","children":"[논문리뷰] Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer","children":"Jeremiah Jiang이 arXiv에 게시한 'Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-Local-Scale-Equivariance-with-Latent-Deep-Equilibrium-Canonicalizer"}]]}]]}],["$","article","2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell","children":"[논문리뷰] Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell","children":"Ingrid Verbauwhede이 arXiv에 게시한 'Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-Leuvenshtein-Efficient-FHE-based-Edit-Distance-Computation-with-Single-Bootstrap-per-Cell"}]]}]]}],["$","article","2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction","children":"[논문리뷰] FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction","children":"tianlecai이 arXiv에 게시한 'FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-FutureX-An-Advanced-Live-Benchmark-for-LLM-Agents-in-Future-Prediction"}]]}]]}],["$","article","2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models","children":"[논문리뷰] From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models","children":"Ziyan Kuang이 arXiv에 게시한 'From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-From-Scores-to-Skills-A-Cognitive-Diagnosis-Framework-for-Evaluating-Financial-Large-Language-Models"}]]}]]}],["$","article","2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery","children":"[논문리뷰] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery","children":"zijieqiu이 arXiv에 게시한 'From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery"}]]}]]}],["$","article","2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization","children":"[논문리뷰] DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization","children":"Yu Lu이 arXiv에 게시한 'DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-21 13:15:28+0900","children":"2025년 8월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-21-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization"}]]}]]}],["$","article","2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents","children":"[논문리뷰] ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents","children":"Flora D. Salim이 arXiv에 게시한 'ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-ZARA-Zero-shot-Motion-Time-Series-Analysis-via-Knowledge-and-Retrieval-Driven-LLM-Agents"}]]}]]}],["$","article","2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer","children":"[논문리뷰] Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer","children":"Deyu Zhou이 arXiv에 게시한 'Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer"}]]}]]}],["$","article","2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models","children":"[논문리뷰] TempFlow-GRPO: When Timing Matters for GRPO in Flow Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models","children":"Jian Yang이 arXiv에 게시한 'TempFlow-GRPO: When Timing Matters for GRPO in Flow Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-TempFlow-GRPO-When-Timing-Matters-for-GRPO-in-Flow-Models"}]]}]]}],["$","article","2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation","children":"[논문리뷰] Semantic IDs for Joint Generative Search and Recommendation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation","children":"Enrico Palumbo이 arXiv에 게시한 'Semantic IDs for Joint Generative Search and Recommendation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Semantic-IDs-for-Joint-Generative-Search-and-Recommendation"}]]}]]}],["$","article","2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research","children":"[논문리뷰] Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research","children":"Susanne Schmidt이 arXiv에 게시한 'Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Radiance-Fields-in-XR-A-Survey-on-How-Radiance-Fields-are-Envisioned-and-Addressed-for-XR-Research"}]]}]]}],["$","article","2025-8-20-Prompt-Orchestration-Markup-Language",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language","children":"[논문리뷰] Prompt Orchestration Markup Language"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language","children":"Yuqing Yang이 arXiv에 게시한 'Prompt Orchestration Markup Language' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language"}]]}]]}],["$","article","2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks","children":"[논문리뷰] OmniTry: Virtual Try-On Anything without Masks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks","children":"Xiaoduan Feng이 arXiv에 게시한 'OmniTry: Virtual Try-On Anything without Masks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-OmniTry-Virtual-Try-On-Anything-without-Masks"}]]}]]}],["$","article","2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References","children":"[논문리뷰] MultiRef: Controllable Image Generation with Multiple Visual References"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References","children":"Shiyun Lang이 arXiv에 게시한 'MultiRef: Controllable Image Generation with Multiple Visual References' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-MultiRef-Controllable-Image-Generation-with-Multiple-Visual-References"}]]}]]}],["$","article","2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence","children":"[논문리뷰] Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence","children":"Xin Chen이 arXiv에 게시한 'Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Motion2Motion-Cross-topology-Motion-Transfer-with-Sparse-Correspondence"}]]}]]}],["$","article","2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation","children":"[논문리뷰] Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation","children":"Xinyi Wang이 arXiv에 게시한 'Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation"}]]}]]}],["$","article","2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation","children":"[논문리뷰] MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation","children":"Jonas Geiping이 arXiv에 게시한 'MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-MedSAMix-A-Training-Free-Model-Merging-Approach-for-Medical-Image-Segmentation"}]]}]]}],["$","article","2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence","children":"[논문리뷰] MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence","children":"Fernando López이 arXiv에 게시한 'MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-MMAU-Pro-A-Challenging-and-Comprehensive-Benchmark-for-Holistic-Evaluation-of-Audio-General-Intelligence"}]]}]]}],["$","article","2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents","children":"[논문리뷰] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents","children":"Jun Dong이 arXiv에 게시한 'MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-MM-BrowseComp-A-Comprehensive-Benchmark-for-Multimodal-Browsing-Agents"}]]}]]}],["$","article","2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos","children":"[논문리뷰] LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos","children":"Yen-Yu Lin이 arXiv에 게시한 'LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-LongSplat-Robust-Unposed-3D-Gaussian-Splatting-for-Casual-Long-Videos"}]]}]]}],["$","article","2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery","children":"[논문리뷰] Leveraging Large Language Models for Predictive Analysis of Human Misery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery","children":"Abhilash Nandy이 arXiv에 게시한 'Leveraging Large Language Models for Predictive Analysis of Human Misery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Leveraging-Large-Language-Models-for-Predictive-Analysis-of-Human-Misery"}]]}]]}],["$","article","2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge","children":"[논문리뷰] Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge","children":"Alice Wang이 arXiv에 게시한 'Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge"}]]}]]}],["$","article","2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation","children":"[논문리뷰] Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation","children":"Fei Ni이 arXiv에 게시한 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation"}]]}]]}],["$","article","2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","children":"[논문리뷰] Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","children":"Mounia Lalmas이 arXiv에 게시한 'Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations"}]]}]]}],["$","article","2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection","children":"[논문리뷰] CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection","children":"Adriano Koshiyama이 arXiv에 게시한 'CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection"}]]}]]}],["$","article","2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends","children":"[논문리뷰] Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends","children":"Xixiang Zhao이 arXiv에 게시한 'Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Copyright-Protection-for-Large-Language-Models-A-Survey-of-Methods-Challenges-and-Trends"}]]}]]}],["$","article","2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL","children":"[논문리뷰] Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL","children":"Liam-Liu이 arXiv에 게시한 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Chain-of-Agents-End-to-End-Agent-Foundation-Models-via-Multi-Agent-Distillation-and-Agentic-RL"}]]}]]}],["$","article","2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing","children":"[논문리뷰] CAMAR: Continuous Actions Multi-Agent Routing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing","children":"Alexey Skrynnik이 arXiv에 게시한 'CAMAR: Continuous Actions Multi-Agent Routing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-CAMAR-Continuous-Actions-Multi-Agent-Routing"}]]}]]}],["$","article","2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding","children":"[논문리뷰] Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding","children":"Alina Landowska이 arXiv에 게시한 'Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding"}]]}]]}],["$","article","2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends","children":"[논문리뷰] Advances in Speech Separation: Techniques, Challenges, and Future Trends"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends","children":"Zhuo Chen이 arXiv에 게시한 'Advances in Speech Separation: Techniques, Challenges, and Future Trends' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-Advances-in-Speech-Separation-Techniques-Challenges-and-Future-Trends"}]]}]]}],["$","article","2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models","children":"[논문리뷰] A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models","children":"Zishang Jiang이 arXiv에 게시한 'A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-20-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models"}]]}]]}],["$","article","2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs","children":"[논문리뷰] When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs","children":"Elena Tutubalina이 arXiv에 게시한 'When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs"}]]}]]}],["$","article","2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models","children":"[논문리뷰] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models","children":"Jusen Du이 arXiv에 게시한 'Speed Always Wins: A Survey on Efficient Architectures for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models"}]]}]]}],["$","article","2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models","children":"[논문리뷰] S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models","children":"Meiqi Wu이 arXiv에 게시한 'S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-S2-Guidance-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models"}]]}]]}],["$","article","2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens","children":"[논문리뷰] Representing Speech Through Autoregressive Prediction of Cochlear Tokens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens","children":"Daniel L. K. Yamins이 arXiv에 게시한 'Representing Speech Through Autoregressive Prediction of Cochlear Tokens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Representing-Speech-Through-Autoregressive-Prediction-of-Cochlear-Tokens"}]]}]]}],["$","article","2025-8-19-Reinforcement-Learning-with-Rubric-Anchors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors","children":"[논문리뷰] Reinforcement Learning with Rubric Anchors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors","children":"Haokai Xu이 arXiv에 게시한 'Reinforcement Learning with Rubric Anchors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors"}]]}]]}],["$","article","2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts","children":"[논문리뷰] Precise Action-to-Video Generation Through Visual Action Prompts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts","children":"Minghan Qin이 arXiv에 게시한 'Precise Action-to-Video Generation Through Visual Action Prompts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Precise-Action-to-Video-Generation-Through-Visual-Action-Prompts"}]]}]]}],["$","article","2025-8-19-Ovis2-5-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Ovis2-5-Technical-Report","children":"[논문리뷰] Ovis2.5 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Ovis2-5-Technical-Report","children":"Yang Li이 arXiv에 게시한 'Ovis2.5 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Ovis2-5-Technical-Report"}]]}]]}],["$","article","2025-8-19-Next-Visual-Granularity-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Next-Visual-Granularity-Generation","children":"[논문리뷰] Next Visual Granularity Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Next-Visual-Granularity-Generation","children":"Kang Liao이 arXiv에 게시한 'Next Visual Granularity Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Next-Visual-Granularity-Generation"}]]}]]}],["$","article","2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model","children":"[논문리뷰] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model","children":"Yifan Zhang이 arXiv에 게시한 'Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model"}]]}]]}],["$","article","2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models","children":"[논문리뷰] Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models","children":"Zixiang Gao이 arXiv에 게시한 'Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Lumen-Consistent-Video-Relighting-and-Harmonious-Background-Replacement-with-Video-Generative-Models"}]]}]]}],["$","article","2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping","children":"[논문리뷰] Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping","children":"Tyler Derr이 arXiv에 게시한 'Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping"}]]}]]}],["$","article","2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds","children":"[논문리뷰] HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds","children":"Artyom Sorokin이 arXiv에 게시한 'HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds"}]]}]]}],["$","article","2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study","children":"[논문리뷰] Has GPT-5 Achieved Spatial Intelligence? An Empirical Study"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study","children":"Ruisi Wang이 arXiv에 게시한 'Has GPT-5 Achieved Spatial Intelligence? An Empirical Study' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Has-GPT-5-Achieved-Spatial-Intelligence-An-Empirical-Study"}]]}]]}],["$","article","2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration","children":"[논문리뷰] G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration","children":"Evgeny Burnaev이 arXiv에 게시한 'G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration"}]]}]]}],["$","article","2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning","children":"[논문리뷰] ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning","children":"Yufeng Wang이 arXiv에 게시한 'ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-ComoRAG-A-Cognitive-Inspired-Memory-Organized-RAG-for-Stateful-Long-Narrative-Reasoning"}]]}]]}],["$","article","2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information","children":"[논문리뷰] Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information","children":"Xi Yang이 arXiv에 게시한 'Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-Beyond-Solving-Math-Quiz-Evaluating-the-Ability-of-Large-Reasoning-Models-to-Ask-for-Information"}]]}]]}],["$","article","2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy","children":"[논문리뷰] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy","children":"Zeng Tao이 arXiv에 게시한 '4DNeX: Feed-Forward 4D Generative Modeling Made Easy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-19 13:15:01+0900","children":"2025년 8월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-19-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy"}]]}]]}],["$","article","2025-8-18-X-Node-Self-Explanation-is-All-We-Need",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-X-Node-Self-Explanation-is-All-We-Need","children":"[논문리뷰] X-Node: Self-Explanation is All We Need"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-X-Node-Self-Explanation-is-All-We-Need","children":"Islem Rekik이 arXiv에 게시한 'X-Node: Self-Explanation is All We Need' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-X-Node-Self-Explanation-is-All-We-Need"}]]}]]}],["$","article","2025-8-18-Thyme-Think-Beyond-Images",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-Thyme-Think-Beyond-Images","children":"[논문리뷰] Thyme: Think Beyond Images"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-Thyme-Think-Beyond-Images","children":"Wei Chen이 arXiv에 게시한 'Thyme: Think Beyond Images' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-Thyme-Think-Beyond-Images"}]]}]]}],["$","article","2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures","children":"[논문리뷰] TexVerse: A Universe of 3D Objects with High-Resolution Textures"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures","children":"Nan Cao이 arXiv에 게시한 'TexVerse: A Universe of 3D Objects with High-Resolution Textures' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-TexVerse-A-Universe-of-3D-Objects-with-High-Resolution-Textures"}]]}]]}],["$","article","2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation","children":"[논문리뷰] StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation","children":"Junyong Noh이 arXiv에 게시한 'StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation"}]]}]]}],["$","article","2025-8-18-SSRL-Self-Search-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning","children":"[논문리뷰] SSRL: Self-Search Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning","children":"Yanxu Chen이 arXiv에 게시한 'SSRL: Self-Search Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning"}]]}]]}],["$","article","2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation","children":"[논문리뷰] SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation","children":"Paolo Soda이 arXiv에 게시한 'SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation"}]]}]]}],["$","article","2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing","children":"[논문리뷰] PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing","children":"Xianpei Han이 arXiv에 게시한 'PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-PaperRegister-Boosting-Flexible-grained-Paper-Search-via-Hierarchical-Register-Indexing"}]]}]]}],["$","article","2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data","children":"[논문리뷰] MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data","children":"Nicolas Gonthier이 arXiv에 게시한 'MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data"}]]}]]}],["$","article","2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation","children":"[논문리뷰] FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation","children":"Mu Xu이 arXiv에 게시한 'FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation"}]]}]]}],["$","article","2025-8-18-DINOv3",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-DINOv3","children":"[논문리뷰] DINOv3"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-DINOv3","children":"Maxime Oquab이 arXiv에 게시한 'DINOv3' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-DINOv3"}]]}]]}],["$","article","2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding","children":"[논문리뷰] Controlling Multimodal LLMs via Reward-guided Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding","children":"Michal Drozdzal이 arXiv에 게시한 'Controlling Multimodal LLMs via Reward-guided Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-18 13:14:38+0900","children":"2025년 8월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-18-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding"}]]}]]}],["$","article","2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning","children":"[논문리뷰] We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning","children":"Xiaowan Wang이 arXiv에 게시한 'We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-We-Math-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning"}]]}]]}],["$","article","2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT","children":"[논문리뷰] UI-Venus Technical Report: Building High-performance UI Agents with RFT"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT","children":"Shuheng Shen이 arXiv에 게시한 'UI-Venus Technical Report: Building High-performance UI Agents with RFT' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT"}]]}]]}],["$","article","2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing","children":"[논문리뷰] ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing","children":"Xiaoyu Li이 arXiv에 게시한 'ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing"}]]}]]}],["$","article","2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer","children":"[논문리뷰] STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer","children":"Honghua Chen이 arXiv에 게시한 'STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer"}]]}]]}],["$","article","2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera","children":"[논문리뷰] Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera","children":"Giorgos Tolias이 arXiv에 게시한 'Processing and acquisition traces in visual encoders: What does CLIP know about your camera?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera"}]]}]]}],["$","article","2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models","children":"[논문리뷰] Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models","children":"Qinghao Ye이 arXiv에 게시한 'Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models"}]]}]]}],["$","article","2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts","children":"[논문리뷰] PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts","children":"Rui Lu이 arXiv에 게시한 'PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts"}]]}]]}],["$","article","2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale","children":"[논문리뷰] NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale","children":"Quan Sun이 arXiv에 게시한 'NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale"}]]}]]}],["$","article","2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs","children":"[논문리뷰] HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs","children":"Yi Yuan이 arXiv에 게시한 'HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs"}]]}]]}],["$","article","2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms","children":"[논문리뷰] From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms","children":"Ziyin Zhang이 arXiv에 게시한 'From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms"}]]}]]}],["$","article","2025-8-15-A-Survey-on-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models","children":"[논문리뷰] A Survey on Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models","children":"Zhiqiang Shen이 arXiv에 게시한 'A Survey on Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-A-Survey-on-Diffusion-Language-Models"}]]}]]}],["$","article","2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP","children":"[논문리뷰] When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP","children":"Gjergji Kasneci이 arXiv에 게시한 'When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-15 13:09:31+0900","children":"2025년 8월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-15-2025-8-15-Explainability-and-Privacy-in-NLP"}]]}]]}],["$","article","2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models","children":"[논문리뷰] VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models","children":"Dongdong Zhang이 arXiv에 게시한 'VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models"}]]}]]}],["$","article","2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation","children":"[논문리뷰] Story2Board: A Training-Free Approach for Expressive Storyboard Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation","children":"Dani Lischinski이 arXiv에 게시한 'Story2Board: A Training-Free Approach for Expressive Storyboard Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation"}]]}]]}],["$","article","2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","children":"[논문리뷰] Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation","children":"Chen Li이 arXiv에 게시한 'Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Stand-In-A-Lightweight-and-Plug-and-Play-Identity-Control-for-Video-Generation"}]]}]]}],["$","article","2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory","children":"[논문리뷰] Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory","children":"Yuan Lin이 arXiv에 게시한 'Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory"}]]}]]}],["$","article","2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models","children":"[논문리뷰] Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models","children":"Zeynep Akata이 arXiv에 게시한 'Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models"}]]}]]}],["$","article","2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery","children":"[논문리뷰] Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery","children":"Di Zhang이 arXiv에 게시한 'Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery"}]]}]]}],["$","article","2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models","children":"[논문리뷰] MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models","children":"Zhihan Zhou이 arXiv에 게시한 'MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models"}]]}]]}],["$","article","2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment","children":"[논문리뷰] Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment","children":"Lei Fan이 arXiv에 게시한 'Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment"}]]}]]}],["$","article","2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding","children":"[논문리뷰] IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding","children":"Di Zhang이 arXiv에 게시한 'IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-IAG-Input-aware-Backdoor-Attack-on-VLMs-for-Visual-Grounding"}]]}]]}],["$","article","2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors","children":"[논문리뷰] GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors","children":"Qingnan Fan이 arXiv에 게시한 'GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors"}]]}]]}],["$","article","2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation","children":"[논문리뷰] Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation","children":"Zhenghao Hu이 arXiv에 게시한 'Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation"}]]}]]}],["$","article","2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing","children":"[논문리뷰] Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing","children":"Hao Zhang이 arXiv에 게시한 'Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing"}]]}]]}],["$","article","2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models","children":"[논문리뷰] Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models","children":"Guiyang Hou이 arXiv에 게시한 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models"}]]}]]}],["$","article","2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study","children":"[논문리뷰] Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study","children":"Gjergji Kasneci이 arXiv에 게시한 'Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study"}]]}]]}],["$","article","2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving","children":"[논문리뷰] AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving","children":"Jinjie Gu이 arXiv에 게시한 'AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving"}]]}]]}],["$","article","2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance","children":"[논문리뷰] AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance","children":"Yong Li이 arXiv에 게시한 'AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance"}]]}]]}],["$","article","2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion","children":"[논문리뷰] WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion","children":"Rachid Nedjai이 arXiv에 게시한 'WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion"}]]}]]}],["$","article","2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail","children":"[논문리뷰] VertexRegen: Mesh Generation with Continuous Level of Detail"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail","children":"Jakob Engel이 arXiv에 게시한 'VertexRegen: Mesh Generation with Continuous Level of Detail' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-VertexRegen-Mesh-Generation-with-Continuous-Level-of-Detail"}]]}]]}],["$","article","2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation","children":"[논문리뷰] UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation","children":"Kevin Galim이 arXiv에 게시한 'UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation"}]]}]]}],["$","article","2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning","children":"[논문리뷰] Train Long, Think Short: Curriculum Learning for Efficient Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning","children":"Marzyeh Ghassemi이 arXiv에 게시한 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning"}]]}]]}],["$","article","2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors","children":"[논문리뷰] Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors","children":"Haoran Xu이 arXiv에 게시한 'Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors"}]]}]]}],["$","article","2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation","children":"[논문리뷰] TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation","children":"Rachel Bawden이 arXiv에 게시한 'TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-TopXGen-Topic-Diverse-Parallel-Data-Generation-for-Low-Resource-Machine-Translation"}]]}]]}],["$","article","2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models","children":"[논문리뷰] Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models","children":"Chenchen Jing이 arXiv에 게시한 'Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models"}]]}]]}],["$","article","2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency","children":"[논문리뷰] Test-Time Reinforcement Learning for GUI Grounding via Region Consistency"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency","children":"Zhengxi Lu이 arXiv에 게시한 'Test-Time Reinforcement Learning for GUI Grounding via Region Consistency' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency"}]]}]]}],["$","article","2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents","children":"[논문리뷰] OpenCUA: Open Foundations for Computer-Use Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents","children":"Tianbao Xie이 arXiv에 게시한 'OpenCUA: Open Foundations for Computer-Use Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-OpenCUA-Open-Foundations-for-Computer-Use-Agents"}]]}]]}],["$","article","2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations","children":"[논문리뷰] NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations","children":"Haoyue Zhan이 arXiv에 게시한 'NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations"}]]}]]}],["$","article","2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation","children":"[논문리뷰] Matrix-3D: Omnidirectional Explorable 3D World Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation","children":"Yuqi Li이 arXiv에 게시한 'Matrix-3D: Omnidirectional Explorable 3D World Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation"}]]}]]}],["$","article","2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches","children":"[논문리뷰] HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches","children":"Qiang Ju이 arXiv에 게시한 'HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches"}]]}]]}],["$","article","2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay","children":"[논문리뷰] GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay","children":"Yang Fan이 arXiv에 게시한 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay"}]]}]]}],["$","article","2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments","children":"[논문리뷰] Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments","children":"Xuesong Yao이 arXiv에 게시한 'Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments"}]]}]]}],["$","article","2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy","children":"[논문리뷰] Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy","children":"Elizabeth Karpinski이 arXiv에 게시한 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy"}]]}]]}],["$","article","2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition","children":"[논문리뷰] DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition","children":"Lukáš Burget이 arXiv에 게시한 'DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-DeCRED-Decoder-Centric-Regularization-for-Encoder-Decoder-Based-Speech-Recognition"}]]}]]}],["$","article","2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning","children":"[논문리뷰] Cut2Next: Generating Next Shot via In-Context Tuning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning","children":"Yu Qiao이 arXiv에 게시한 'Cut2Next: Generating Next Shot via In-Context Tuning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning"}]]}]]}],["$","article","2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation","children":"[논문리뷰] CharacterShot: Controllable and Consistent 4D Character Animation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation","children":"Fei Shen이 arXiv에 게시한 'CharacterShot: Controllable and Consistent 4D Character Animation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-CharacterShot-Controllable-and-Consistent-4D-Character-Animation"}]]}]]}],["$","article","2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware","children":"[논문리뷰] Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware","children":"Jhon Alejandro Andrade이 arXiv에 게시한 'Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Bridging-Theory-and-Practice-in-Quantum-Game-Theory-Optimized-Implementation-of-the-Battle-of-the-Sexes-with-Error-Mitigation-on-NISQ-Hardware"}]]}]]}],["$","article","2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them","children":"[논문리뷰] BiasGym: Fantastic Biases and How to Find (and Remove) Them"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them","children":"Arnav Arora이 arXiv에 게시한 'BiasGym: Fantastic Biases and How to Find (and Remove) Them' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them"}]]}]]}],["$","article","2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL","children":"[논문리뷰] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL","children":"Chuyi He이 arXiv에 게시한 'Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL"}]]}]]}],["$","article","2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators","children":"[논문리뷰] AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators","children":"Tao Zhang이 arXiv에 게시한 'AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-AutoCodeBench-Large-Language-Models-are-Automatic-Code-Benchmark-Generators"}]]}]]}],["$","article","2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math","children":"[논문리뷰] Aryabhata: An exam-focused language model for JEE Math"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math","children":"Sandeep Varma이 arXiv에 게시한 'Aryabhata: An exam-focused language model for JEE Math' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math"}]]}]]}],["$","article","2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval","children":"[논문리뷰] Adversarial Video Promotion Against Text-to-Video Retrieval"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval","children":"Shuai Liu이 arXiv에 게시한 'Adversarial Video Promotion Against Text-to-Video Retrieval' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Adversarial-Video-Promotion-Against-Text-to-Video-Retrieval"}]]}]]}],["$","article","2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking","children":"[논문리뷰] WideSearch: Benchmarking Agentic Broad Info-Seeking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking","children":"Yan Gao이 arXiv에 게시한 'WideSearch: Benchmarking Agentic Broad Info-Seeking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-WideSearch-Benchmarking-Agentic-Broad-Info-Seeking"}]]}]]}],["$","article","2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs","children":"[논문리뷰] When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs","children":"Dasol Choi이 arXiv에 게시한 'When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-When-Good-Sounds-Go-Adversarial-Jailbreaking-Audio-Language-Models-with-Benign-Inputs"}]]}]]}],["$","article","2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding","children":"[논문리뷰] VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding","children":"Tong Yu이 arXiv에 게시한 'VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding"}]]}]]}],["$","article","2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents","children":"[논문리뷰] UserBench: An Interactive Gym Environment for User-Centric Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents","children":"Jianguo Zhang이 arXiv에 게시한 'UserBench: An Interactive Gym Environment for User-Centric Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-UserBench-An-Interactive-Gym-Environment-for-User-Centric-Agents"}]]}]]}],["$","article","2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future","children":"[논문리뷰] Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future","children":"Qiufeng Wang이 arXiv에 게시한 'Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future"}]]}]]}],["$","article","2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences","children":"[논문리뷰] Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences","children":"Matvey Skripkin이 arXiv에 게시한 'Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Speech-to-LaTeX-New-Models-and-Datasets-for-Converting-Spoken-Equations-and-Sentences"}]]}]]}],["$","article","2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation","children":"[논문리뷰] Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation","children":"Hengtao Shen이 arXiv에 게시한 'Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation"}]]}]]}],["$","article","2025-8-12-Reinforcement-Learning-in-Vision-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey","children":"[논문리뷰] Reinforcement Learning in Vision: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey","children":"Qingwei Meng이 arXiv에 게시한 'Reinforcement Learning in Vision: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Reinforcement-Learning-in-Vision-A-Survey"}]]}]]}],["$","article","2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability","children":"[논문리뷰] ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability","children":"Yuchen Li이 arXiv에 게시한 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability"}]]}]]}],["$","article","2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning","children":"[논문리뷰] Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning","children":"Jiaheng Liu이 arXiv에 게시한 'Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning"}]]}]]}],["$","article","2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks","children":"[논문리뷰] OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks","children":"Hongxing Li이 arXiv에 게시한 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-OmniEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks"}]]}]]}],["$","article","2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","children":"[논문리뷰] Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation","children":"Xiaokun Feng이 arXiv에 게시한 'Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation"}]]}]]}],["$","article","2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space","children":"[논문리뷰] MolmoAct: Action Reasoning Models that can Reason in Space"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space","children":"Shuo Liu이 arXiv에 게시한 'MolmoAct: Action Reasoning Models that can Reason in Space' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"}]]}]]}],["$","article","2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs","children":"[논문리뷰] MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs","children":"Jianguo Li이 arXiv에 게시한 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs"}]]}]]}],["$","article","2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning","children":"[논문리뷰] Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning","children":"Baihong Yuan이 arXiv에 게시한 'Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Less-Is-More-Training-Free-Sparse-Attention-with-Global-Locality-for-Efficient-Reasoning"}]]}]]}],["$","article","2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization","children":"[논문리뷰] Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization","children":"Guanting Dong이 arXiv에 게시한 'Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"}]]}]]}],["$","article","2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts","children":"[논문리뷰] Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts","children":"Tieyuan Chen이 arXiv에 게시한 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts"}]]}]]}],["$","article","2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks","children":"[논문리뷰] GLiClass: Generalist Lightweight Model for Sequence Classification Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks","children":"Alexander Yavorskyi이 arXiv에 게시한 'GLiClass: Generalist Lightweight Model for Sequence Classification Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks"}]]}]]}],["$","article","2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control","children":"[논문리뷰] Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control","children":"Hongyu Liu이 arXiv에 게시한 'Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control"}]]}]]}],["$","article","2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System","children":"[논문리뷰] Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System","children":"Reynold Cheng이 arXiv에 게시한 'Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Fact2Fiction-Targeted-Poisoning-Attack-to-Agentic-Fact-checking-System"}]]}]]}],["$","article","2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs","children":"[논문리뷰] Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs","children":"Robert Kirk이 arXiv에 게시한 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Deep-Ignorance-Filtering-Pretraining-Data-Builds-Tamper-Resistant-Safeguards-into-Open-Weight-LLMs"}]]}]]}],["$","article","2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy","children":"[논문리뷰] Compressing Chain-of-Thought in LLMs via Step Entropy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy","children":"Zhijian Xu이 arXiv에 게시한 'Compressing Chain-of-Thought in LLMs via Step Entropy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy"}]]}]]}],["$","article","2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent","children":"[논문리뷰] BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent","children":"Kai Zou이 arXiv에 게시한 'BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-BrowseComp-Plus-A-More-Fair-and-Transparent-Evaluation-Benchmark-of-Deep-Research-Agent"}]]}]]}],["$","article","2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents","children":"[논문리뷰] Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents","children":"Mohit Bansal이 arXiv에 게시한 'Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents"}]]}]]}],["$","article","2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems","children":"[논문리뷰] A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems","children":"Xinhao Yi이 arXiv에 게시한 'A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems"}]]}]]}],["$","article","2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off","children":"[논문리뷰] Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off","children":"jgkwak이 arXiv에 게시한 'Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-Voost-A-Unified-and-Scalable-Diffusion-Transformer-for-Bidirectional-Virtual-Try-On-and-Try-Off"}]]}]]}],["$","article","2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding","children":"[논문리뷰] UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding","children":"Bingqi Chen이 arXiv에 게시한 'UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding"}]]}]]}],["$","article","2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal","children":"[논문리뷰] Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal","children":"Chengcheng Wan이 arXiv에 게시한 'Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal"}]]}]]}],["$","article","2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh","children":"[논문리뷰] MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh","children":"Yi Yang이 arXiv에 게시한 'MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-MeshLLM-Empowering-Large-Language-Models-to-Progressively-Understand-and-Generate-3D-Mesh"}]]}]]}],["$","article","2025-8-11-Memp-Exploring-Agent-Procedural-Memory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory","children":"[논문리뷰] Memp: Exploring Agent Procedural Memory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory","children":"Shuofei Qiao이 arXiv에 게시한 'Memp: Exploring Agent Procedural Memory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-Memp-Exploring-Agent-Procedural-Memory"}]]}]]}],["$","article","2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs","children":"[논문리뷰] MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs","children":"Guohang Yan이 arXiv에 게시한 'MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs"}]]}]]}],["$","article","2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion","children":"[논문리뷰] LightSwitch: Multi-view Relighting with Material-guided Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion","children":"Shubham Tulsiani이 arXiv에 게시한 'LightSwitch: Multi-view Relighting with Material-guided Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-LightSwitch-Multi-view-Relighting-with-Material-guided-Diffusion"}]]}]]}],["$","article","2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization","children":"[논문리뷰] InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization","children":"Pengxiang Li이 arXiv에 게시한 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-InfiGUI-G1-Advancing-GUI-Grounding-with-Adaptive-Exploration-Policy-Optimization"}]]}]]}],["$","article","2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models","children":"[논문리뷰] GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models","children":"GLM-4. 5 Team이 arXiv에 게시한 'GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models"}]]}]]}],["$","article","2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing","children":"[논문리뷰] GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing","children":"Przemysław Spurek이 arXiv에 게시한 'GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing"}]]}]]}],["$","article","2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey","children":"[논문리뷰] Adapting Vision-Language Models Without Labels: A Comprehensive Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey","children":"Eleni Chatzi이 arXiv에 게시한 'Adapting Vision-Language Models Without Labels: A Comprehensive Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-11 13:13:28+0900","children":"2025년 8월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-11-Adapting-Vision-Language-Models-Without-Labels-A-Comprehensive-Survey"}]]}]]}],["$","article","2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling","children":"[논문리뷰] Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling","children":"Ruolin Shen이 arXiv에 게시한 'Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling"}]]}]]}],["$","article","2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance","children":"[논문리뷰] StrandDesigner: Towards Practical Strand Generation with Sketch Guidance"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance","children":"Xiaobin Hu이 arXiv에 게시한 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance"}]]}]]}],["$","article","2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression","children":"[논문리뷰] Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression","children":"Yifei Ji이 arXiv에 게시한 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Steering-One-Step-Diffusion-Model-with-Fidelity-Rich-Decoder-for-Fast-Image-Compression"}]]}]]}],["$","article","2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation","children":"[논문리뷰] RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation","children":"Jian Yang이 arXiv에 게시한 'RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-RPCANet-Deep-Interpretable-Robust-PCA-for-Sparse-Object-Segmentation"}]]}]]}],["$","article","2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation","children":"[논문리뷰] REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation","children":"Xiao Yu이 arXiv에 게시한 'REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-REINA-Regularized-Entropy-Information-Based-Loss-for-Efficient-Simultaneous-Speech-Translation"}]]}]]}],["$","article","2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data","children":"[논문리뷰] R-Zero: Self-Evolving Reasoning LLM from Zero Data"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data","children":"Zongxia Li이 arXiv에 게시한 'R-Zero: Self-Evolving Reasoning LLM from Zero Data' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data"}]]}]]}],["$","article","2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction","children":"[논문리뷰] PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction","children":"Prajit Das이 arXiv에 게시한 'PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction"}]]}]]}],["$","article","2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification","children":"[논문리뷰] On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification","children":"Xinyu Ye이 arXiv에 게시한 'On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"}]]}]]}],["$","article","2025-8-8-Marco-Voice-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Marco-Voice-Technical-Report","children":"[논문리뷰] Marco-Voice Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Marco-Voice-Technical-Report","children":"Qingjuan Li이 arXiv에 게시한 'Marco-Voice Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Marco-Voice-Technical-Report"}]]}]]}],["$","article","2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes","children":"[논문리뷰] MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes","children":"Xudong Jiang이 arXiv에 게시한 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-MOSEv2-A-More-Challenging-Dataset-for-Video-Object-Segmentation-in-Complex-Scenes"}]]}]]}],["$","article","2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities","children":"[논문리뷰] InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities","children":"Zhijie Sang이 arXiv에 게시한 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities"}]]}]]}],["$","article","2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking","children":"[논문리뷰] I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking","children":"Chao Wang이 arXiv에 게시한 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking"}]]}]]}],["$","article","2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations","children":"[논문리뷰] I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations","children":"Chirag Shah이 arXiv에 게시한 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-I-Think-Therefore-I-Am-Under-Qualified-A-Benchmark-for-Evaluating-Linguistic-Shibboleth-Detection-in-LLM-Hiring-Evaluations"}]]}]]}],["$","article","2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis","children":"[논문리뷰] Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis","children":"Reshmi Ghosh이 arXiv에 게시한 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis"}]]}]]}],["$","article","2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity","children":"[논문리뷰] Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity","children":"Zhibing Li이 arXiv에 게시한 'Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity"}]]}]]}],["$","article","2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation","children":"[논문리뷰] Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation","children":"Shengcong Chen이 arXiv에 게시한 'Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation"}]]}]]}],["$","article","2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation","children":"[논문리뷰] Evaluating, Synthesizing, and Enhancing for Customer Support Conversation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation","children":"Feng Chen이 arXiv에 게시한 'Evaluating, Synthesizing, and Enhancing for Customer Support Conversation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation"}]]}]]}],["$","article","2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models","children":"[논문리뷰] Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models","children":"Fangzhou Yao이 arXiv에 게시한 'Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models"}]]}]]}],["$","article","2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning","children":"[논문리뷰] DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning","children":"Ziming Wang이 arXiv에 게시한 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning"}]]}]]}],["$","article","2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions","children":"[논문리뷰] CoAct-1: Computer-using Agents with Coding as Actions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions","children":"Taiwei Shi이 arXiv에 게시한 'CoAct-1: Computer-using Agents with Coding as Actions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-CoAct-1-Computer-using-Agents-with-Coding-as-Actions"}]]}]]}],["$","article","2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability","children":"[논문리뷰] Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability","children":"Yuan Wu이 arXiv에 게시한 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Can-Large-Multimodal-Models-Actively-Recognize-Faulty-Inputs-A-Systematic-Evaluation-Framework-of-Their-Input-Scrutiny-Ability"}]]}]]}],["$","article","2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation","children":"[논문리뷰] Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation","children":"Junjie Yang이 arXiv에 게시한 'Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Are-We-on-the-Right-Way-for-Assessing-Document-Retrieval-Augmented-Generation"}]]}]]}],["$","article","2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts","children":"[논문리뷰] Are Today's LLMs Ready to Explain Well-Being Concepts?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts","children":"Huan Liu이 arXiv에 게시한 'Are Today's LLMs Ready to Explain Well-Being Concepts?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts"}]]}]]}],["$","article","2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents","children":"[논문리뷰] Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents","children":"Xinyu Yang이 arXiv에 게시한 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents"}]]}]]}],["$","article","2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning","children":"[논문리뷰] Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning","children":"Maksim Nekrashevich이 arXiv에 게시한 'Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning"}]]}]]}],["$","article","2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models","children":"[논문리뷰] The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models","children":"Elisabetta Rocchetti이 arXiv에 게시한 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models"}]]}]]}],["$","article","2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence","children":"[논문리뷰] Sotopia-RL: Reward Design for Social Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence","children":"Keyang Xuan이 arXiv에 게시한 'Sotopia-RL: Reward Design for Social Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence"}]]}]]}],["$","article","2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering","children":"[논문리뷰] SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering","children":"Ambuj Mehrish이 arXiv에 게시한 'SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-SonicMaster-Towards-Controllable-All-in-One-Music-Restoration-and-Mastering"}]]}]]}],["$","article","2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation","children":"[논문리뷰] Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation","children":"Hao Huang이 arXiv에 게시한 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Sel3DCraft-Interactive-Visual-Prompts-for-User-Friendly-Text-to-3D-Generation"}]]}]]}],["$","article","2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management","children":"[논문리뷰] Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management","children":"Yunxin Liu이 arXiv에 게시한 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management"}]]}]]}],["$","article","2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience","children":"[논문리뷰] SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience","children":"Xiaoyi Dong이 arXiv에 게시한 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-SEAgent-Self-Evolving-Computer-Use-Agent-with-Autonomous-Learning-from-Experience"}]]}]]}],["$","article","2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks","children":"[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks","children":"Haozhe Zhang이 arXiv에 게시한 'Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks"}]]}]]}],["$","article","2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","children":"[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","children":"Kechi Zhang이 arXiv에 게시한 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization"}]]}]]}],["$","article","2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference","children":"[논문리뷰] Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference","children":"Jiaying Wu이 arXiv에 게시한 'Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Position-The-Current-AI-Conference-Model-is-Unsustainable-Diagnosing-the-Crisis-of-Centralized-AI-Conference"}]]}]]}],["$","article","2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets","children":"[논문리뷰] OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets","children":"MaziyarPanahi이 arXiv에 게시한 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets"}]]}]]}],["$","article","2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions","children":"[논문리뷰] MiDashengLM: Efficient Audio Understanding with General Audio Captions"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions","children":"Yadong Niu이 arXiv에 게시한 'MiDashengLM: Efficient Audio Understanding with General Audio Captions' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions"}]]}]]}],["$","article","2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following","children":"[논문리뷰] Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following","children":"Liang Xu이 arXiv에 게시한 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Light-IF-Endowing-LLMs-with-Generalizable-Reasoning-via-Preview-and-Self-Checking-for-Complex-Instruction-Following"}]]}]]}],["$","article","2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding","children":"[논문리뷰] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding","children":"Yuqing Yang이 arXiv에 게시한 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-LeanK-Learnable-K-Cache-Channel-Pruning-for-Efficient-Decoding"}]]}]]}],["$","article","2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought","children":"[논문리뷰] LaTCoder: Converting Webpage Design to Code with Layout-as-Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought","children":"Tianpeng Lv이 arXiv에 게시한 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-LaTCoder-Converting-Webpage-Design-to-Code-with-Layout-as-Thought"}]]}]]}],["$","article","2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens","children":"[논문리뷰] Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens","children":"Zhen Tan이 arXiv에 게시한 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens"}]]}]]}],["$","article","2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards","children":"[논문리뷰] IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards","children":"Ling-I Wu이 arXiv에 게시한 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-IFDECORATOR-Wrapping-Instruction-Following-Reinforcement-Learning-with-Verifiable-Rewards"}]]}]]}],["$","article","2025-8-7-IAUNet-Instance-Aware-U-Net",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-IAUNet-Instance-Aware-U-Net","children":"[논문리뷰] IAUNet: Instance-Aware U-Net"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-IAUNet-Instance-Aware-U-Net","children":"Dmytro Fishman이 arXiv에 게시한 'IAUNet: Instance-Aware U-Net' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-IAUNet-Instance-Aware-U-Net"}]]}]]}],["$","article","2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score","children":"[논문리뷰] HPSv3: Towards Wide-Spectrum Human Preference Score"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score","children":"Hongsheng Li이 arXiv에 게시한 'HPSv3: Towards Wide-Spectrum Human Preference Score' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score"}]]}]]}],["$","article","2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis","children":"[논문리뷰] Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis","children":"Feng Zhao이 arXiv에 게시한 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis"}]]}]]}],["$","article","2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success","children":"[논문리뷰] Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success","children":"Ruslan Rakhimov이 arXiv에 게시한 'Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Enhancing-Vision-Language-Model-Training-with-Reinforcement-Learning-in-Synthetic-Worlds-for-Real-World-Success"}]]}]]}],["$","article","2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost","children":"[논문리뷰] Efficient Agents: Building Effective Agents While Reducing Cost"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost","children":"Yue Hou이 arXiv에 게시한 'Efficient Agents: Building Effective Agents While Reducing Cost' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Efficient-Agents-Building-Effective-Agents-While-Reducing-Cost"}]]}]]}],["$","article","2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation","children":"[논문리뷰] EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation","children":"Dong Chen이 arXiv에 게시한 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation"}]]}]]}],["$","article","2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework","children":"[논문리뷰] DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework","children":"Chao Liang이 arXiv에 게시한 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework"}]]}]]}],["$","article","2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction","children":"[논문리뷰] CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction","children":"Donghyeon Lee이 arXiv에 게시한 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction"}]]}]]}],["$","article","2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor","children":"[논문리뷰] C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor","children":"Jinbao Wang이 arXiv에 게시한 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-C3D-AD-Toward-Continual-3D-Anomaly-Detection-via-Kernel-Attention-with-Learnable-Advisor"}]]}]]}],["$","article","2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning","children":"[논문리뷰] Agent Lightning: Train ANY AI Agents with Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning","children":"Zilong Wang이 arXiv에 게시한 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning"}]]}]]}],["$","article","2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding","children":"[논문리뷰] A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding","children":"Jianke Zhu이 arXiv에 게시한 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-A-Coarse-to-Fine-Approach-to-Multi-Modality-3D-Occupancy-Grounding"}]]}]]}],["$","article","2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search","children":"[논문리뷰] Tool-integrated Reinforcement Learning for Repo Deep Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search","children":"Yanzhen Zou이 arXiv에 게시한 'Tool-integrated Reinforcement Learning for Repo Deep Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-Tool-integrated-Reinforcement-Learning-for-Repo-Deep-Search"}]]}]]}],["$","article","2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs","children":"[논문리뷰] TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs","children":"Aman Chadha이 arXiv에 게시한 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-TRACEALIGN-Tracing-the-Drift-Attributing-Alignment-Failures-to-Training-Time-Belief-Sources-in-LLMs"}]]}]]}],["$","article","2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation","children":"[논문리뷰] Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation","children":"Tianyidan Xie이 arXiv에 게시한 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-Skywork-UniPic-Unified-Autoregressive-Modeling-for-Visual-Understanding-and-Generation"}]]}]]}],["$","article","2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference","children":"[논문리뷰] Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference","children":"Fan Xia이 arXiv에 게시한 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-Seed-Diffusion-A-Large-Scale-Diffusion-Language-Model-with-High-Speed-Inference"}]]}]]}],["$","article","2025-8-6-Multi-human-Interactive-Talking-Dataset",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset","children":"[논문리뷰] Multi-human Interactive Talking Dataset"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset","children":"Mike Zheng Shou이 arXiv에 게시한 'Multi-human Interactive Talking Dataset' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-Multi-human-Interactive-Talking-Dataset"}]]}]]}],["$","article","2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation","children":"[논문리뷰] LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation","children":"Chenyang Si이 arXiv에 게시한 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-LongVie-Multimodal-Guided-Controllable-Ultra-Long-Video-Generation"}]]}]]}],["$","article","2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools","children":"[논문리뷰] LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools","children":"Yaojie Lu이 arXiv에 게시한 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-LiveMCPBench-Can-Agents-Navigate-an-Ocean-of-MCP-Tools"}]]}]]}],["$","article","2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer","children":"[논문리뷰] LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer","children":"Shunyu Yao이 arXiv에 게시한 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer"}]]}]]}],["$","article","2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction","children":"[논문리뷰] Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction","children":"Jui-Hui Chung이 arXiv에 게시한 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-Goedel-Prover-V2-Scaling-Formal-Theorem-Proving-with-Scaffolded-Data-Synthesis-and-Self-Correction"}]]}]]}],["$","article","2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward","children":"[논문리뷰] CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward","children":"Songyang Gao이 arXiv에 게시한 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-CompassVerifier-A-Unified-and-Robust-Verifier-for-LLMs-Evaluation-and-Outcome-Reward"}]]}]]}],["$","article","2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning","children":"[논문리뷰] ChartCap: Mitigating Hallucination of Dense Chart Captioning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning","children":"Gunhee Kim이 arXiv에 게시한 'ChartCap: Mitigating Hallucination of Dense Chart Captioning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-ChartCap-Mitigating-Hallucination-of-Dense-Chart-Captioning"}]]}]]}],["$","article","2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search","children":"[논문리뷰] CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search","children":"Jiwei Li이 arXiv에 게시한 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search"}]]}]]}],["$","article","2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization","children":"[논문리뷰] AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization","children":"Aman Chadha이 arXiv에 게시한 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-06 13:46:36+0900","children":"2025년 8월 6일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-6-AlignGuard-LoRA-Alignment-Preserving-Fine-Tuning-via-Fisher-Guided-Decomposition-and-Riemannian-Geodesic-Collision-Regularization"}]]}]]}],["$","article","2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo","children":"[논문리뷰] VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo","children":"Bin Jia이 arXiv에 게시한 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-VeOmni-Scaling-Any-Modality-Model-Training-with-Model-Centric-Distributed-Recipe-Zoo"}]]}]]}],["$","article","2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension","children":"[논문리뷰] SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension","children":"Liyan Xu이 arXiv에 게시한 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-SitEmb-v1-5-Improved-Context-Aware-Dense-Retrieval-for-Semantic-Association-and-Long-Story-Comprehension"}]]}]]}],["$","article","2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems","children":"[논문리뷰] RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems","children":"Junkun Hong이 arXiv에 게시한 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems"}]]}]]}],["$","article","2025-8-5-Qwen-Image-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Qwen-Image-Technical-Report","children":"[논문리뷰] Qwen-Image Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Qwen-Image-Technical-Report","children":"Kaiyuan Gao이 arXiv에 게시한 'Qwen-Image Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-Qwen-Image-Technical-Report"}]]}]]}],["$","article","2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models","children":"[논문리뷰] Personalized Safety Alignment for Text-to-Image Diffusion Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models","children":"Kaidong Yu이 arXiv에 게시한 'Personalized Safety Alignment for Text-to-Image Diffusion Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models"}]]}]]}],["$","article","2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report","children":"[논문리뷰] Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report","children":"Anu Vellore이 arXiv에 게시한 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-Llama-3-1-FoundationAI-SecurityLLM-8B-Instruct-Technical-Report"}]]}]]}],["$","article","2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation","children":"[논문리뷰] InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation","children":"Yang Tian이 arXiv에 게시한 'InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation"}]]}]]}],["$","article","2025-8-5-Exploitation-Is-All-You-Need-for-Exploration",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Exploitation-Is-All-You-Need-for-Exploration","children":"[논문리뷰] Exploitation Is All You Need... for Exploration"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Exploitation-Is-All-You-Need-for-Exploration","children":"Jesse Roberts이 arXiv에 게시한 'Exploitation Is All You Need... for Exploration' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-Exploitation-Is-All-You-Need-for-Exploration"}]]}]]}],["$","article","2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime","children":"[논문리뷰] Cyber-Zero: Training Cybersecurity Agents without Runtime"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime","children":"Zijian Wang이 arXiv에 게시한 'Cyber-Zero: Training Cybersecurity Agents without Runtime' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-Cyber-Zero-Training-Cybersecurity-Agents-without-Runtime"}]]}]]}],["$","article","2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models","children":"[논문리뷰] CellForge: Agentic Design of Virtual Cell Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models","children":"Daniel Shao이 arXiv에 게시한 'CellForge: Agentic Design of Virtual Cell Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-CellForge-Agentic-Design-of-Virtual-Cell-Models"}]]}]]}],["$","article","2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following","children":"[논문리뷰] Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following","children":"Jiaqing Liang이 arXiv에 게시한 'Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following"}]]}]]}],["$","article","2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks","children":"[논문리뷰] AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks","children":"Zhiwei Zhang이 arXiv에 게시한 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks"}]]}]]}],["$","article","2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models","children":"[논문리뷰] A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models","children":"Zuxuan Wu이 arXiv에 게시한 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-05 11:40:52+0900","children":"2025년 8월 5일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-5-A-Glimpse-to-Compress-Dynamic-Visual-Token-Pruning-for-Large-Vision-Language-Models"}]]}]]}],["$","article","2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation","children":"[논문리뷰] SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation","children":"Long Chen이 arXiv에 게시한 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-SpA2V-Harnessing-Spatial-Auditory-Cues-for-Audio-driven-Spatially-aware-Video-Generation"}]]}]]}],["$","article","2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution","children":"[논문리뷰] SWE-Exp: Experience-Driven Software Issue Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution","children":"Heng Lian이 arXiv에 게시한 'SWE-Exp: Experience-Driven Software Issue Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-SWE-Exp-Experience-Driven-Software-Issue-Resolution"}]]}]]}],["$","article","2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution","children":"[논문리뷰] SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution","children":"Heng Lian이 arXiv에 게시한 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution"}]]}]]}],["$","article","2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion","children":"[논문리뷰] PixNerd: Pixel Neural Field Diffusion"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion","children":"Limin Wang이 arXiv에 게시한 'PixNerd: Pixel Neural Field Diffusion' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-PixNerd-Pixel-Neural-Field-Diffusion"}]]}]]}],["$","article","2025-8-4-Multimodal-Referring-Segmentation-A-Survey",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Multimodal-Referring-Segmentation-A-Survey","children":"[논문리뷰] Multimodal Referring Segmentation: A Survey"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Multimodal-Referring-Segmentation-A-Survey","children":"Zuxuan Wu이 arXiv에 게시한 'Multimodal Referring Segmentation: A Survey' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-Multimodal-Referring-Segmentation-A-Survey"}]]}]]}],["$","article","2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges","children":"[논문리뷰] Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges","children":"Chengfei Lv이 arXiv에 게시한 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges"}]]}]]}],["$","article","2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages","children":"[논문리뷰] Investigating Hallucination in Conversations for Low Resource Languages"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages","children":"Fatemeh Jamshidi이 arXiv에 게시한 'Investigating Hallucination in Conversations for Low Resource Languages' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-Investigating-Hallucination-in-Conversations-for-Low-Resource-Languages"}]]}]]}],["$","article","2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation","children":"[논문리뷰] IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation","children":"Jianjiang Feng이 arXiv에 게시한 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-IGL-Nav-Incremental-3D-Gaussian-Localization-for-Image-goal-Navigation"}]]}]]}],["$","article","2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models","children":"[논문리뷰] Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models","children":"Jiaqi Wang이 arXiv에 게시한 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-Beyond-Fixed-Variable-Length-Denoising-for-Diffusion-Large-Language-Models"}]]}]]}],["$","article","2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding","children":"[논문리뷰] 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding","children":"Hao Tang이 arXiv에 게시한 '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding"}]]}]]}],["$","article","2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models","children":"[논문리뷰] villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models","children":"Kaixin Wang이 arXiv에 게시한 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-villa-X-Enhancing-Latent-Action-Modeling-in-Vision-Language-Action-Models"}]]}]]}],["$","article","2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model","children":"[논문리뷰] iLRM: An Iterative Large 3D Reconstruction Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model","children":"Abdelrahman Mohamed이 arXiv에 게시한 'iLRM: An Iterative Large 3D Reconstruction Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-iLRM-An-Iterative-Large-3D-Reconstruction-Model"}]]}]]}],["$","article","2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs","children":"[논문리뷰] TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs","children":"Jiasheng Tang이 arXiv에 게시한 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-TARS-MinMax-Token-Adaptive-Preference-Strategy-for-Hallucination-Reduction-in-MLLMs"}]]}]]}],["$","article","2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving","children":"[논문리뷰] Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving","children":"Zhicheng Jiang이 arXiv에 게시한 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving"}]]}]]}],["$","article","2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents","children":"[논문리뷰] Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents","children":"Anji Liu이 arXiv에 게시한 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Scalable-Multi-Task-Reinforcement-Learning-for-Generalizable-Spatial-Intelligence-in-Visuomotor-Agents"}]]}]]}],["$","article","2025-8-3-RecGPT-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-RecGPT-Technical-Report","children":"[논문리뷰] RecGPT Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-RecGPT-Technical-Report","children":"Jian Wu이 arXiv에 게시한 'RecGPT Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-RecGPT-Technical-Report"}]]}]]}],["$","article","2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding","children":"[논문리뷰] Phi-Ground Tech Report: Advancing Perception in GUI Grounding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding","children":"Kai Qiu이 arXiv에 게시한 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Phi-Ground-Tech-Report-Advancing-Perception-in-GUI-Grounding"}]]}]]}],["$","article","2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models","children":"[논문리뷰] Persona Vectors: Monitoring and Controlling Character Traits in Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models","children":"Jack Lindsey이 arXiv에 게시한 'Persona Vectors: Monitoring and Controlling Character Traits in Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Persona-Vectors-Monitoring-and-Controlling-Character-Traits-in-Language-Models"}]]}]]}],["$","article","2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective","children":"[논문리뷰] On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective","children":"Eric C. Larson이 arXiv에 게시한 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective"}]]}]]}],["$","article","2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting","children":"[논문리뷰] NeRF Is a Valuable Assistant for 3D Gaussian Splatting"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting","children":"ZeSheng Wang이 arXiv에 게시한 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting"}]]}]]}],["$","article","2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks","children":"[논문리뷰] Flow Equivariant Recurrent Neural Networks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks","children":"T. Anderson Keller이 arXiv에 게시한 'Flow Equivariant Recurrent Neural Networks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Flow-Equivariant-Recurrent-Neural-Networks"}]]}]]}],["$","article","2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring","children":"[논문리뷰] Enhanced Arabic Text Retrieval with Attentive Relevance Scoring"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring","children":"Abdenour Hadid이 arXiv에 게시한 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Enhanced-Arabic-Text-Retrieval-with-Attentive-Relevance-Scoring"}]]}]]}],["$","article","2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation","children":"[논문리뷰] Efficient Machine Unlearning via Influence Approximation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation","children":"Enhong Chen이 arXiv에 게시한 'Efficient Machine Unlearning via Influence Approximation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Efficient-Machine-Unlearning-via-Influence-Approximation"}]]}]]}],["$","article","2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations","children":"[논문리뷰] C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations","children":"Yiwen Guo이 arXiv에 게시한 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-C3-A-Bilingual-Benchmark-for-Spoken-Dialogue-Models-Exploring-Challenges-in-Complex-Conversations"}]]}]]}],["$","article","2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification","children":"[논문리뷰] Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification","children":"Abdelmalik Taleb-Ahmed이 arXiv에 게시한 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Beyond-Linear-Bottlenecks-Spline-Based-Knowledge-Distillation-for-Culturally-Diverse-Art-Style-Classification"}]]}]]}],["$","article","2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture","children":"[논문리뷰] AgroBench: Vision-Language Model Benchmark in Agriculture"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture","children":"Yoshitaka Ushiku이 arXiv에 게시한 'AgroBench: Vision-Language Model Benchmark in Agriculture' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-AgroBench-Vision-Language-Model-Benchmark-in-Agriculture"}]]}]]}]]}]]}]]}]}]]}]],null],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children","$6","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L8",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L3",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L9",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L9",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$La"]]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"#Review - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"Review 태그가 포함된 포스트 목록"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/tags/Review"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"#Review - secrett2633's blog"}],["$","meta","14",{"property":"og:description","content":"Review 태그가 포함된 포스트 목록"}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/tags/Review"}],["$","meta","16",{"property":"og:type","content":"website"}],["$","meta","17",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","18",{"name":"twitter:title","content":"#Review - secrett2633's blog"}],["$","meta","19",{"name":"twitter:description","content":"Review 태그가 포함된 포스트 목록"}],["$","link","20",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
