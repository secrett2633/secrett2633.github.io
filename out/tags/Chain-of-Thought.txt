2:I[9038,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
3:I[231,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],""]
4:I[227,["231","static/chunks/231-ee5764c1002761f9.js","605","static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js"],"default"]
5:I[9275,[],""]
7:I[1343,[],""]
8:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
9:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
6:["tag","Chain-of-Thought","d"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":["tags",{"children":[["tag","Chain-of-Thought","d"],{"children":["__PAGE__?{\"tag\":\"Chain-of-Thought\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["tags",{"children":[["tag","Chain-of-Thought","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"CollectionPage\",\"name\":\"#Chain-of-Thought - secrett2633's blog\",\"description\":\"Chain-of-Thought 태그가 포함된 포스트 목록\",\"url\":\"https://blog.secrett2633.cloud/tags/Chain-of-Thought\",\"isPartOf\":{\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\"},\"inLanguage\":\"ko\"}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"#Chain-of-Thought\",\"item\":\"https://blog.secrett2633.cloud/tags/Chain-of-Thought\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L3",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L3",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L3",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L3",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L3",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L3",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L3",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L3",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L3",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L3",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L3",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L3",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L3",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L3",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/tags/Chain-of-Thought",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"#Chain-of-Thought"}]]}]]]}]}],["$","h1",null,{"className":"page__title mb-6","children":["#","Chain-of-Thought"]}],["$","p",null,{"className":"text-gray-500 mb-6","children":[96,"개의 포스트"]}],["$","div",null,{"className":"entries-list","children":[["$","article","2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling","children":"[논문리뷰] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling","children":"Animesh Sinha이 arXiv에 게시한 'UniT: Unified Multimodal Chain-of-Thought Test-time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-18 00:00:00+0900+0900","children":"2026년 2월 18일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-18-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling"}]]}]]}],["$","article","2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs","children":"[논문리뷰] On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs","children":"arXiv에 게시된 'On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs"}]]}]]}],["$","article","2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics","children":"[논문리뷰] GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics","children":"MingMing Cheng이 arXiv에 게시한 'GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-16-GeoAgent-Learning-to-Geolocate-Everywhere-with-Reinforced-Geographic-Characteristics"}]]}]]}],["$","article","2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces","children":"[논문리뷰] ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces","children":"Julian McAuley이 arXiv에 게시한 'ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces"}]]}]]}],["$","article","2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning","children":"[논문리뷰] Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning","children":"arXiv에 게시된 'Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-13 00:00:00+0900+0900","children":"2026년 2월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning","children":"[논문리뷰] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning","children":"arXiv에 게시된 'InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-09 00:00:00+0900+0900","children":"2026년 2월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning"}]]}]]}],["$","article","2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs","children":"[논문리뷰] No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs","children":"arXiv에 게시된 'No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-04-No-Global-Plan-in-Chain-of-Thought-Uncover-the-Latent-Planning-Horizon-of-LLMs"}]]}]]}],["$","article","2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models","children":"[논문리뷰] THINKSAFE: Self-Generated Safety Alignment for Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models","children":"Minki Kang이 arXiv에 게시한 'THINKSAFE: Self-Generated Safety Alignment for Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-THINKSAFE-Self-Generated-Safety-Alignment-for-Reasoning-Models"}]]}]]}],["$","article","2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought","children":"[논문리뷰] ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought","children":"Zhifeng Gao이 arXiv에 게시한 'ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-02-02 00:00:00+0900+0900","children":"2026년 2월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-02-02-ReGuLaR-Variational-Latent-Reasoning-Guided-by-Rendered-Chain-of-Thought"}]]}]]}],["$","article","2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods","children":"[논문리뷰] MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods","children":"arXiv에 게시된 'MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-30 00:00:00+0900+0900","children":"2026년 1월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods"}]]}]]}],["$","article","2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge","children":"[논문리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge","children":"arXiv에 게시된 'Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-20 00:00:00+0900+0900","children":"2026년 1월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge"}]]}]]}],["$","article","2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning","children":"[논문리뷰] Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning","children":"arXiv에 게시된 'Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-15 00:00:00+0900+0900","children":"2026년 1월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-15-Fast-ThinkAct-Efficient-Vision-Language-Action-Reasoning-via-Verbalizable-Latent-Planning"}]]}]]}],["$","article","2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","children":"[논문리뷰] TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning","children":"Hao Wang이 arXiv에 게시한 'TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-13 00:00:00+0900+0900","children":"2026년 1월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-13-TourPlanner-A-Competitive-Consensus-Framework-with-Constraint-Gated-Reinforcement-Learning-for-Travel-Planning"}]]}]]}],["$","article","2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","children":"[논문리뷰] Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing","children":"Yu Xu이 arXiv에 게시한 'Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-Re-Align-Structured-Reasoning-guided-Alignment-for-In-Context-Image-Generation-and-Editing"}]]}]]}],["$","article","2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","children":"[논문리뷰] DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs","children":"Jing Ma이 arXiv에 게시한 'DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-09 00:00:00+0900+0900","children":"2026년 1월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs"}]]}]]}],["$","article","2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing","children":"[논문리뷰] ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing","children":"arXiv에 게시된 'ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-ThinkRL-Edit-Thinking-in-Reinforcement-Learning-for-Reasoning-Centric-Image-Editing"}]]}]]}],["$","article","2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning","children":"[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning","children":"Guanchen Wu이 arXiv에 게시한 'EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-08 00:00:00+0900+0900","children":"2026년 1월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning"}]]}]]}],["$","article","2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process","children":"[논문리뷰] Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process","children":"arXiv에 게시된 'Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2026-01-01 00:00:00+0900+0900","children":"2026년 1월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2026-01-01-Fantastic-Reasoning-Behaviors-and-Where-to-Find-Them-Unsupervised-Discovery-of-the-Reasoning-Process"}]]}]]}],["$","article","2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding","children":"[논문리뷰] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding","children":"Yixin Chen이 arXiv에 게시한 'Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-29 00:00:00+0900+0900","children":"2025년 12월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-29-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding"}]]}]]}],["$","article","2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models","children":"[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models","children":"arXiv에 게시된 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models"}]]}]]}],["$","article","2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos","children":"[논문리뷰] AdaTooler-V: Adaptive Tool-Use for Images and Videos"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos","children":"Zhixun Li이 arXiv에 게시한 'AdaTooler-V: Adaptive Tool-Use for Images and Videos' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-19 00:00:00+0900+0900","children":"2025년 12월 19일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-19-AdaTooler-V-Adaptive-Tool-Use-for-Images-and-Videos"}]]}]]}],["$","article","2025-12-15-Sliding-Window-Attention-Adaptation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation","children":"[논문리뷰] Sliding Window Attention Adaptation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation","children":"arXiv에 게시된 'Sliding Window Attention Adaptation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-15 00:00:00+0900+0900","children":"2025년 12월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation"}]]}]]}],["$","article","2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification","children":"[논문리뷰] OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification","children":"arXiv에 게시된 'OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-12 00:00:00+0900+0900","children":"2025년 12월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-12-OPV-Outcome-based-Process-Verifier-for-Efficient-Long-Chain-of-Thought-Verification"}]]}]]}],["$","article","2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","children":"[논문리뷰] UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving","children":"arXiv에 게시된 'UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-UniUGP-Unifying-Understanding-Generation-and-Planing-For-End-to-end-Autonomous-Driving"}]]}]]}],["$","article","2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing","children":"[논문리뷰] EtCon: Edit-then-Consolidate for Reliable Knowledge Editing"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing","children":"Chenglin Li이 arXiv에 게시한 'EtCon: Edit-then-Consolidate for Reliable Knowledge Editing' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-11 00:00:00+0900+0900","children":"2025년 12월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing"}]]}]]}],["$","article","2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models","children":"[논문리뷰] ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models","children":"Xiuyu Li이 arXiv에 게시한 'ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-ThreadWeaver-Adaptive-Threading-for-Efficient-Parallel-Reasoning-in-Language-Models"}]]}]]}],["$","article","2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning","children":"[논문리뷰] LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning","children":"arXiv에 게시된 'LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-10 00:00:00+0900+0900","children":"2025년 12월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-10-LYNX-Learning-Dynamic-Exits-for-Confidence-Controlled-Reasoning"}]]}]]}],["$","article","2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning","children":"[논문리뷰] VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning","children":"Yansong Tang이 arXiv에 게시한 'VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning"}]]}]]}],["$","article","2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","children":"[논문리뷰] Unified Video Editing with Temporal Reasoner"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner","children":"arXiv에 게시된 'Unified Video Editing with Temporal Reasoner' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-09-Unified-Video-Editing-with-Temporal-Reasoner"}]]}]]}],["$","article","2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence","children":"[논문리뷰] COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence","children":"Jiawei Sheng이 arXiv에 게시한 'COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-08 00:00:00+0900+0900","children":"2025년 12월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence"}]]}]]}],["$","article","2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization","children":"[논문리뷰] Rectifying LLM Thought from Lens of Optimization"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization","children":"Kai Chen이 arXiv에 게시한 'Rectifying LLM Thought from Lens of Optimization' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-Rectifying-LLM-Thought-from-Lens-of-Optimization"}]]}]]}],["$","article","2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic","children":"[논문리뷰] OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic","children":"arXiv에 게시된 'OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-OpenREAD-Reinforced-Open-Ended-Reasoing-for-End-to-End-Autonomous-Driving-with-LLM-as-Critic"}]]}]]}],["$","article","2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling","children":"[논문리뷰] LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling","children":"arXiv에 게시된 'LongVT: Incentivizing 'Thinking with Long Videos' via Native Tool Calling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-02 00:00:00+0900+0900","children":"2025년 12월 2일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-02-LongVT-Incentivizing-Thinking-with-Long-Videos-via-Native-Tool-Calling"}]]}]]}],["$","article","2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information","children":"[논문리뷰] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information","children":"Kristian Kersting이 arXiv에 게시한 'Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-12-01 00:00:00+0900+0900","children":"2025년 12월 1일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-12-01-Focused-Chain-of-Thought-Efficient-LLM-Reasoning-via-Structured-Input-Information"}]]}]]}],["$","article","2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward","children":"[논문리뷰] Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward","children":"arXiv에 게시된 'Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-26 00:00:00+0900+0900","children":"2025년 11월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward"}]]}]]}],["$","article","2025-11-21-Step-Audio-R1-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report","children":"[논문리뷰] Step-Audio-R1 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report","children":"arXiv에 게시된 'Step-Audio-R1 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-21 00:00:00+0900+0900","children":"2025년 11월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-21-Step-Audio-R1-Technical-Report"}]]}]]}],["$","article","2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models","children":"[논문리뷰] Music Flamingo: Scaling Music Understanding in Audio Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models","children":"arXiv에 게시된 'Music Flamingo: Scaling Music Understanding in Audio Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-14 00:00:00+0900+0900","children":"2025년 11월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-14-Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models"}]]}]]}],["$","article","2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale","children":"[논문리뷰] Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale","children":"arXiv에 게시된 'Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-11-Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale"}]]}]]}],["$","article","2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks","children":"[논문리뷰] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks","children":"arXiv에 게시된 'VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-10 00:00:00+0900+0900","children":"2025년 11월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks"}]]}]]}],["$","article","2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity","children":"[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity","children":"arXiv에 게시된 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 21:54:30+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-6-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity"}]]}]]}],["$","article","2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement","children":"[논문리뷰] Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement","children":"Isabelle Augenstein이 arXiv에 게시한 'Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement"}]]}]]}],["$","article","2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models","children":"[논문리뷰] MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models","children":"Bin Yu이 arXiv에 게시한 'MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-11-09 19:22:42+0900","children":"2025년 11월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-11-4-MR-Align-Meta-Reasoning-Informed-Factuality-Alignment-for-Large-Reasoning-Models"}]]}]]}],["$","article","2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning","children":"[논문리뷰] Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning","children":"Runhao Fu이 arXiv에 게시한 'Video-Thinker: Sparking 'Thinking with Videos' via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-30 13:06:06+0900","children":"2025년 10월 30일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-30-Video-Thinker-Sparking-Thinking-with-Videos-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments","children":"[논문리뷰] PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments","children":"Chaoyang Zhao이 arXiv에 게시한 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-27 13:07:36+0900","children":"2025년 10월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-27-PhysVLM-AVR-Active-Visual-Reasoning-for-Multimodal-Large-Language-Models-in-Physical-Environments"}]]}]]}],["$","article","2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence","children":"[논문리뷰] Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence","children":"arXiv에 게시된 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-24 13:04:16+0900","children":"2025년 10월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-24-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence"}]]}]]}],["$","article","2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts","children":"[논문리뷰] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts","children":"arXiv에 게시된 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts"}]]}]]}],["$","article","2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model","children":"[논문리뷰] GigaBrain-0: A World Model-Powered Vision-Language-Action Model"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model","children":"arXiv에 게시된 'GigaBrain-0: A World Model-Powered Vision-Language-Action Model' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-23-GigaBrain-0-A-World-Model-Powered-Vision-Language-Action-Model"}]]}]]}],["$","article","2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist","children":"[논문리뷰] Chem-R: Learning to Reason as a Chemist"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist","children":"arXiv에 게시된 'Chem-R: Learning to Reason as a Chemist' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-22 13:07:20+0900","children":"2025년 10월 22일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist"}]]}]]}],["$","article","2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense","children":"[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense","children":"arXiv에 게시된 'Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-21 13:08:30+0900","children":"2025년 10월 21일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense"}]]}]]}],["$","article","2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning","children":"[논문리뷰] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning","children":"arXiv에 게시된 'DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-20 13:04:24+0900","children":"2025년 10월 20일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning"}]]}]]}],["$","article","2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning","children":"[논문리뷰] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning","children":"arXiv에 게시된 'VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning"}]]}]]}],["$","article","2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts","children":"[논문리뷰] Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts","children":"arXiv에 게시된 'Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-17 13:09:57+0900","children":"2025년 10월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-17-Beyond-One-World-Benchmarking-Super-Heros-in-Role-Playing-Across-Multiversal-Contexts"}]]}]]}],["$","article","2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning","children":"[논문리뷰] Revisiting Model Interpolation for Efficient Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning","children":"arXiv에 게시된 'Revisiting Model Interpolation for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning"}]]}]]}],["$","article","2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling","children":"[논문리뷰] EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling","children":"Ahmet Üstün이 arXiv에 게시한 'EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-EAGER-Entropy-Aware-GEneRation-for-Adaptive-Inference-Time-Scaling"}]]}]]}],["$","article","2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs","children":"[논문리뷰] Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs","children":"arXiv에 게시된 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-16 13:09:51+0900","children":"2025년 10월 16일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-16-Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs"}]]}]]}],["$","article","2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners","children":"[논문리뷰] HoneyBee: Data Recipes for Vision-Language Reasoners"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners","children":"arXiv에 게시된 'HoneyBee: Data Recipes for Vision-Language Reasoners' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-15 13:01:40+0900","children":"2025년 10월 15일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-15-HoneyBee-Data-Recipes-for-Vision-Language-Reasoners"}]]}]]}],["$","article","2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth","children":"[논문리뷰] R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth","children":"arXiv에 게시된 'R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth"}]]}]]}],["$","article","2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping","children":"[논문리뷰] ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping","children":"Wenbo Hu이 arXiv에 게시한 'ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-13 13:44:18+0900","children":"2025년 10월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-13-ARES-Multimodal-Adaptive-Reasoning-via-Difficulty-Aware-Token-Level-Entropy-Shaping"}]]}]]}],["$","article","2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models","children":"[논문리뷰] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models","children":"James Cheng이 arXiv에 게시한 'Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models"}]]}]]}],["$","article","2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy","children":"[논문리뷰] DeepPrune: Parallel Scaling without Inter-trace Redundancy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy","children":"arXiv에 게시된 'DeepPrune: Parallel Scaling without Inter-trace Redundancy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-10 13:53:45+0900","children":"2025년 10월 10일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-10-DeepPrune-Parallel-Scaling-without-Inter-trace-Redundancy"}]]}]]}],["$","article","2025-10-9-The-Markovian-Thinker",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-The-Markovian-Thinker","children":"[논문리뷰] The Markovian Thinker"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-The-Markovian-Thinker","children":"arXiv에 게시된 'The Markovian Thinker' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-The-Markovian-Thinker"}]]}]]}],["$","article","2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models","children":"[논문리뷰] SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models","children":"Kevin Lin이 arXiv에 게시한 'SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models"}]]}]]}],["$","article","2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces","children":"[논문리뷰] Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces","children":"arXiv에 게시된 'Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-09 13:45:06+0900","children":"2025년 10월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-9-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces"}]]}]]}],["$","article","2025-10-8-MixReasoning-Switching-Modes-to-Think",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think","children":"[논문리뷰] MixReasoning: Switching Modes to Think"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think","children":"arXiv에 게시된 'MixReasoning: Switching Modes to Think' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-8-MixReasoning-Switching-Modes-to-Think"}]]}]]}],["$","article","2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"[논문리뷰] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation","children":"Paul Debevec이 arXiv에 게시한 'VChain: Chain-of-Visual-Thought for Reasoning in Video Generation' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-VChain-Chain-of-Visual-Thought-for-Reasoning-in-Video-Generation"}]]}]]}],["$","article","2025-10-7-Self-Reflective-Generation-at-Test-Time",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time","children":"[논문리뷰] Self-Reflective Generation at Test Time"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time","children":"Shuang Qiu이 arXiv에 게시한 'Self-Reflective Generation at Test Time' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time"}]]}]]}],["$","article","2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","children":"[논문리뷰] Factuality Matters: When Image Generation and Editing Meet Structured Visuals"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals","children":"Boxiang Qiu이 arXiv에 게시한 'Factuality Matters: When Image Generation and Editing Meet Structured Visuals' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-10-07 13:36:57+0900","children":"2025년 10월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-10-7-Factuality-Matters-When-Image-Generation-and-Editing-Meet-Structured-Visuals"}]]}]]}],["$","article","2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning","children":"[논문리뷰] X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning","children":"Raghuveer Rao이 arXiv에 게시한 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-29 13:47:46+0900","children":"2025년 9월 29일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-29-X-CoT-Explainable-Text-to-Video-Retrieval-via-LLM-based-Chain-of-Thought-Reasoning"}]]}]]}],["$","article","2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory","children":"[논문리뷰] Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory","children":"Yanbin Fu이 arXiv에 게시한 'Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-Understanding-the-Thinking-Process-of-Reasoning-Models-A-Perspective-from-Schoenfelds-Episode-Theory"}]]}]]}],["$","article","2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines","children":"[논문리뷰] SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines","children":"Jiabei Xiao이 arXiv에 게시한 'SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines"}]]}]]}],["$","article","2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources","children":"[논문리뷰] MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources","children":"Jing Wang이 arXiv에 게시한 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-26 13:35:32+0900","children":"2025년 9월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-26-MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources"}]]}]]}],["$","article","2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought","children":"[논문리뷰] SIM-CoT: Supervised Implicit Chain-of-Thought"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought","children":"Yuhang Cao이 arXiv에 게시한 'SIM-CoT: Supervised Implicit Chain-of-Thought' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-25 13:08:16+0900","children":"2025년 9월 25일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-25-SIM-CoT-Supervised-Implicit-Chain-of-Thought"}]]}]]}],["$","article","2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT","children":"[논문리뷰] What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT","children":"Anthony Hartshorn이 arXiv에 게시한 'What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-24 13:14:19+0900","children":"2025년 9월 24일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-24-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT"}]]}]]}],["$","article","2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs","children":"[논문리뷰] TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs","children":"Shaohui Jiao이 arXiv에 게시한 'TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"}]]}]]}],["$","article","2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing","children":"[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing","children":"Jaeho Lee이 arXiv에 게시한 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-23 13:36:03+0900","children":"2025년 9월 23일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing"}]]}]]}],["$","article","2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving","children":"[논문리뷰] EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving","children":"Shansan Gong이 arXiv에 게시한 'EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-17 13:16:01+0900","children":"2025년 9월 17일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-17-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving"}]]}]]}],["$","article","2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated","children":"[논문리뷰] Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated","children":"Jamie Hayes이 arXiv에 게시한 'Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-12 13:12:46+0900","children":"2025년 9월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-12-Reasoning-Introduces-New-Poisoning-Attacks-Yet-Makes-Them-More-Complicated"}]]}]]}],["$","article","2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet","children":"[논문리뷰] Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet","children":"See-Kiong Ng이 arXiv에 게시한 'Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-09 13:19:09+0900","children":"2025년 9월 9일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet"}]]}]]}],["$","article","2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction","children":"[논문리뷰] SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction","children":"bindsch이 arXiv에 게시한 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction"}]]}]]}],["$","article","2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic","children":"[논문리뷰] Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic","children":"Bernard Ghanem이 arXiv에 게시한 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Reasoning-Vectors-Transferring-Chain-of-Thought-Capabilities-via-Task-Arithmetic"}]]}]]}],["$","article","2025-9-3-Kwai-Keye-VL-1-5-Technical-Report",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report","children":"[논문리뷰] Kwai Keye-VL 1.5 Technical Report"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report","children":"SXxtyz이 arXiv에 게시한 'Kwai Keye-VL 1.5 Technical Report' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-09-03 13:36:21+0900","children":"2025년 9월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-9-3-Kwai-Keye-VL-1-5-Technical-Report"}]]}]]}],["$","article","2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning","children":"[논문리뷰] StepWiser: Stepwise Generative Judges for Wiser Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning","children":"Olga Golovneva이 arXiv에 게시한 'StepWiser: Stepwise Generative Judges for Wiser Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-28 13:10:39+0900","children":"2025년 8월 28일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-28-StepWiser-Stepwise-Generative-Judges-for-Wiser-Reasoning"}]]}]]}],["$","article","2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning","children":"[논문리뷰] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning","children":"Arman Cohan이 arXiv에 게시한 'Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-27 13:22:18+0900","children":"2025년 8월 27일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning"}]]}]]}],["$","article","2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning","children":"[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning","children":"Xin Zheng이 arXiv에 게시한 'Explain Before You Answer: A Survey on Compositional Visual Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning"}]]}]]}],["$","article","2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling","children":"[논문리뷰] Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling","children":"Daniil Orel이 arXiv에 게시한 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-26 13:21:57+0900","children":"2025년 8월 26일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-26-Beyond-Memorization-Extending-Reasoning-Depth-with-Recurrence-Memory-and-Test-Time-Compute-Scaling"}]]}]]}],["$","article","2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery","children":"[논문리뷰] Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery","children":"Di Zhang이 arXiv에 게시한 'Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-14 13:19:02+0900","children":"2025년 8월 14일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery"}]]}]]}],["$","article","2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning","children":"[논문리뷰] Train Long, Think Short: Curriculum Learning for Efficient Reasoning"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning","children":"Marzyeh Ghassemi이 arXiv에 게시한 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning"}]]}]]}],["$","article","2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math","children":"[논문리뷰] Aryabhata: An exam-focused language model for JEE Math"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math","children":"Sandeep Varma이 arXiv에 게시한 'Aryabhata: An exam-focused language model for JEE Math' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-13 13:29:23+0900","children":"2025년 8월 13일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-13-Aryabhata-An-exam-focused-language-model-for-JEE-Math"}]]}]]}],["$","article","2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy","children":"[논문리뷰] Compressing Chain-of-Thought in LLMs via Step Entropy"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy","children":"Zhijian Xu이 arXiv에 게시한 'Compressing Chain-of-Thought in LLMs via Step Entropy' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-12 13:29:09+0900","children":"2025년 8월 12일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-12-Compressing-Chain-of-Thought-in-LLMs-via-Step-Entropy"}]]}]]}],["$","article","2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models","children":"[논문리뷰] Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models","children":"Fangzhou Yao이 arXiv에 게시한 'Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-08 13:32:22+0900","children":"2025년 8월 8일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-8-Dont-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models"}]]}]]}],["$","article","2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks","children":"[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks","children":"Haozhe Zhang이 arXiv에 게시한 'Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks"}]]}]]}],["$","article","2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens","children":"[논문리뷰] Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens","children":"Zhen Tan이 arXiv에 게시한 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-Is-Chain-of-Thought-Reasoning-of-LLMs-a-Mirage-A-Data-Distribution-Lens"}]]}]]}],["$","article","2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score","children":"[논문리뷰] HPSv3: Towards Wide-Spectrum Human Preference Score"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score","children":"Hongsheng Li이 arXiv에 게시한 'HPSv3: Towards Wide-Spectrum Human Preference Score' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-HPSv3-Towards-Wide-Spectrum-Human-Preference-Score"}]]}]]}],["$","article","2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction","children":"[논문리뷰] CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction","children":"Donghyeon Lee이 arXiv에 게시한 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-7-CoTox-Chain-of-Thought-Based-Molecular-Toxicity-Reasoning-and-Prediction"}]]}]]}],["$","article","2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding","children":"[논문리뷰] 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding","children":"Hao Tang이 arXiv에 게시한 '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-04 12:17:01+0900","children":"2025년 8월 4일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-4-3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding"}]]}]]}],["$","article","2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving",{"className":"archive__item","children":[["$","h2",null,{"className":"archive__item-title","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving","children":"[논문리뷰] Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving"}]}],["$","div",null,{"className":"archive__item-excerpt","children":["$","$L3",null,{"href":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving","children":"Zhicheng Jiang이 arXiv에 게시한 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다."}]}],["$","div",null,{"className":"archive__item-meta","children":[["$","time",null,{"dateTime":"2025-08-03 07:35:17+0900","children":"2025년 8월 3일"}],["$","$L4",null,{"postPermalink":"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving"}]]}]]}]]}]]}]]}]}]]}]],null],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children","$6","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L8",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L3",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L9",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L9",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$La"]]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"#Chain-of-Thought - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"Chain-of-Thought 태그가 포함된 포스트 목록"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/tags/Chain-of-Thought"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"#Chain-of-Thought - secrett2633's blog"}],["$","meta","14",{"property":"og:description","content":"Chain-of-Thought 태그가 포함된 포스트 목록"}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/tags/Chain-of-Thought"}],["$","meta","16",{"property":"og:type","content":"website"}],["$","meta","17",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","18",{"name":"twitter:title","content":"#Chain-of-Thought - secrett2633's blog"}],["$","meta","19",{"name":"twitter:description","content":"Chain-of-Thought 태그가 포함된 포스트 목록"}],["$","link","20",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
