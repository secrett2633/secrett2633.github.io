<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/edb8d4ad4fe2f3b0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-4b0d66bdf1ba1813.js" async=""></script><script src="/_next/static/chunks/23-41c976638cd1a58c.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-ee5764c1002761f9.js" async=""></script><script src="/_next/static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js" async=""></script><script src="/_next/static/chunks/132-273e49420772df1e.js" async=""></script><script src="/_next/static/chunks/app/layout-d443cbc354279241.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><title>#Large Language Models - secrett2633&#x27;s blog</title><meta name="description" content="Large Language Models 태그가 포함된 포스트 목록"/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/tags/Large%20Language%20Models"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="#Large Language Models - secrett2633&#x27;s blog"/><meta property="og:description" content="Large Language Models 태그가 포함된 포스트 목록"/><meta property="og:url" content="https://blog.secrett2633.cloud/tags/Large%20Language%20Models"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="#Large Language Models - secrett2633&#x27;s blog"/><meta name="twitter:description" content="Large Language Models 태그가 포함된 포스트 목록"/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"CollectionPage","name":"#Large Language Models - secrett2633's blog","description":"Large Language Models 태그가 포함된 포스트 목록","url":"https://blog.secrett2633.cloud/tags/Large%20Language%20Models","isPartOf":{"@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud"},"inLanguage":"ko"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"#Large Language Models","item":"https://blog.secrett2633.cloud/tags/Large%20Language%20Models"}]}</script><div class="space-y-6"><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2741<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">#Large Language Models</span></li></ol></nav><h1 class="page__title mb-6">#<!-- -->Large Language Models</h1><p class="text-gray-500 mb-6">319<!-- -->개의 포스트</p><div class="entries-list"><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens">[논문리뷰] STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens">Zhilong Zheng이 arXiv에 게시한 &#x27;STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-18 00:00:00+0900+0900">2026년 2월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model">[논문리뷰] Query as Anchor: Scenario-Adaptive User Representation via Large Language Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model">arXiv에 게시된 &#x27;Query as Anchor: Scenario-Adaptive User Representation via Large Language Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-17 00:00:00+0900+0900">2026년 2월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks">[논문리뷰] Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks">arXiv에 게시된 &#x27;Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-17 00:00:00+0900+0900">2026년 2월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation">[논문리뷰] Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation">Ryan Rossi이 arXiv에 게시한 &#x27;Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-17 00:00:00+0900+0900">2026년 2월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub">[논문리뷰] AIDev: Studying AI Coding Agents on GitHub</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub">Ahmed E. Hassan이 arXiv에 게시한 &#x27;AIDev: Studying AI Coding Agents on GitHub&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-17 00:00:00+0900+0900">2026년 2월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models">[논문리뷰] BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models">arXiv에 게시된 &#x27;BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-16 00:00:00+0900+0900">2026년 2월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning">[논문리뷰] Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning">arXiv에 게시된 &#x27;Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-13 00:00:00+0900+0900">2026년 2월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments">[논문리뷰] LawThinker: A Deep Research Legal Agent in Dynamic Environments</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments">arXiv에 게시된 &#x27;LawThinker: A Deep Research Legal Agent in Dynamic Environments&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-13 00:00:00+0900+0900">2026년 2월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models">[논문리뷰] Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models">arXiv에 게시된 &#x27;Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-13 00:00:00+0900+0900">2026년 2월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research">[논문리뷰] Towards Autonomous Mathematics Research</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research">arXiv에 게시된 &#x27;Towards Autonomous Mathematics Research&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions">[논문리뷰] TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions">arXiv에 게시된 &#x27;TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models">[논문리뷰] Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models">Zhen Fang이 arXiv에 게시한 &#x27;Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models">[논문리뷰] Free(): Learning to Forget in Malloc-Only Reasoning Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models">arXiv에 게시된 &#x27;Free(): Learning to Forget in Malloc-Only Reasoning Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-12 00:00:00+0900+0900">2026년 2월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth">[논문리뷰] LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth">arXiv에 게시된 &#x27;LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-10 00:00:00+0900+0900">2026년 2월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning">[논문리뷰] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning">arXiv에 게시된 &#x27;InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-09 00:00:00+0900+0900">2026년 2월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight">[논문리뷰] Steering LLMs via Scalable Interactive Oversight</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight">arXiv에 게시된 &#x27;Steering LLMs via Scalable Interactive Oversight&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities">[논문리뷰] Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities">arXiv에 게시된 &#x27;Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments">[논문리뷰] ProAct: Agentic Lookahead in Interactive Environments</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments">arXiv에 게시된 &#x27;ProAct: Agentic Lookahead in Interactive Environments&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark">[논문리뷰] BABE: Biology Arena BEnchmark</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark">arXiv에 게시된 &#x27;BABE: Biology Arena BEnchmark&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning">[논문리뷰] WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning">arXiv에 게시된 &#x27;WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-05 00:00:00+0900+0900">2026년 2월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning">[논문리뷰] Self-Hinting Language Models Enhance Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning">arXiv에 게시된 &#x27;Self-Hinting Language Models Enhance Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-05 00:00:00+0900+0900">2026년 2월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR">[논문리뷰] PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR">Alejandro Lozano이 arXiv에 게시한 &#x27;PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-05 00:00:00+0900+0900">2026년 2월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models">[논문리뷰] OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models">Yiyan Ji이 arXiv에 게시한 &#x27;OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-05 00:00:00+0900+0900">2026년 2월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy">[논문리뷰] SimpleGPT: Improving GPT via A Simple Normalization Strategy</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy">Rong Xiao이 arXiv에 게시한 &#x27;SimpleGPT: Improving GPT via A Simple Normalization Strategy&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-04 00:00:00+0900+0900">2026년 2월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration">[논문리뷰] AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration">Zhaoyang Yu이 arXiv에 게시한 &#x27;AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-04 00:00:00+0900+0900">2026년 2월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System">[논문리뷰] RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System">arXiv에 게시된 &#x27;RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-03 00:00:00+0900+0900">2026년 2월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation">[논문리뷰] RM -RF: Reward Model for Run-Free Unit Test Evaluation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation">Vadim Alperovich이 arXiv에 게시한 &#x27;RM -RF: Reward Model for Run-Free Unit Test Evaluation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-02 00:00:00+0900+0900">2026년 2월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning">[논문리뷰] MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning">Yuxin Chen이 arXiv에 게시한 &#x27;MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-02 00:00:00+0900+0900">2026년 2월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience">[논문리뷰] Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience">arXiv에 게시된 &#x27;Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-02-02 00:00:00+0900+0900">2026년 2월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods">[논문리뷰] MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods">arXiv에 게시된 &#x27;MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience">[논문리뷰] Language-based Trial and Error Falls Behind in the Era of Experience</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience">arXiv에 게시된 &#x27;Language-based Trial and Error Falls Behind in the Era of Experience&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories">[논문리뷰] Discovering Hidden Gems in Model Repositories</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories">Yedid Hoshen이 arXiv에 게시한 &#x27;Discovering Hidden Gems in Model Repositories&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-30 00:00:00+0900+0900">2026년 1월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection">[논문리뷰] GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection">arXiv에 게시된 &#x27;GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-29 00:00:00+0900+0900">2026년 1월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep">[논문리뷰] Post-LayerNorm Is Back: Stable, ExpressivE, and Deep</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep">arXiv에 게시된 &#x27;Post-LayerNorm Is Back: Stable, ExpressivE, and Deep&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-28 00:00:00+0900+0900">2026년 1월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering">[논문리뷰] daVinci-Dev: Agent-native Mid-training for Software Engineering</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering">arXiv에 게시된 &#x27;daVinci-Dev: Agent-native Mid-training for Software Engineering&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-27 00:00:00+0900+0900">2026년 1월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report">[논문리뷰] VIBEVOICE-ASR Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report">arXiv에 게시된 &#x27;VIBEVOICE-ASR Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-27 00:00:00+0900+0900">2026년 1월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion">[논문리뷰] STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion">arXiv에 게시된 &#x27;STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-27 00:00:00+0900+0900">2026년 1월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences">[논문리뷰] MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences">Jianwen Sun이 arXiv에 게시한 &#x27;MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-26 00:00:00+0900+0900">2026년 1월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization">[논문리뷰] Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization">Gabriele Bavota이 arXiv에 게시한 &#x27;Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-26 00:00:00+0900+0900">2026년 1월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind">[논문리뷰] Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind">Yi R Fung이 arXiv에 게시한 &#x27;Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-26 00:00:00+0900+0900">2026년 1월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs">[논문리뷰] Towards Automated Kernel Generation in the Era of LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs">Yixin Shen이 arXiv에 게시한 &#x27;Towards Automated Kernel Generation in the Era of LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-23 00:00:00+0900+0900">2026년 1월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model">[논문리뷰] Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model">arXiv에 게시된 &#x27;Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-23 00:00:00+0900+0900">2026년 1월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models">[논문리뷰] Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models">arXiv에 게시된 &#x27;Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-22 00:00:00+0900+0900">2026년 1월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents">[논문리뷰] ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents">arXiv에 게시된 &#x27;ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-21 00:00:00+0900+0900">2026년 1월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing">[논문리뷰] On the Evidentiary Limits of Membership Inference for Copyright Auditing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing">Marten van Dijk이 arXiv에 게시한 &#x27;On the Evidentiary Limits of Membership Inference for Copyright Auditing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-21 00:00:00+0900+0900">2026년 1월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search">[논문리뷰] Agentic-R: Learning to Retrieve for Agentic Search</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search">Daiting Shi이 arXiv에 게시한 &#x27;Agentic-R: Learning to Retrieve for Agentic Search&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-21 00:00:00+0900+0900">2026년 1월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge">[논문리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge">arXiv에 게시된 &#x27;Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-20 00:00:00+0900+0900">2026년 1월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models">[논문리뷰] Language of Thought Shapes Output Diversity in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models">arXiv에 게시된 &#x27;Language of Thought Shapes Output Diversity in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-19 00:00:00+0900+0900">2026년 1월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning">[논문리뷰] Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning">arXiv에 게시된 &#x27;Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-16 00:00:00+0900+0900">2026년 1월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning">[논문리뷰] Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning">arXiv에 게시된 &#x27;Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-16 00:00:00+0900+0900">2026년 1월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5">[논문리뷰] A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5">Yutao Wu이 arXiv에 게시한 &#x27;A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-16 00:00:00+0900+0900">2026년 1월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-15-TranslateGemma-Technical-Report">[논문리뷰] TranslateGemma Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-15-TranslateGemma-Technical-Report">arXiv에 게시된 &#x27;TranslateGemma Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-15 00:00:00+0900+0900">2026년 1월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization">[논문리뷰] Controlled Self-Evolution for Algorithmic Code Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization">arXiv에 게시된 &#x27;Controlled Self-Evolution for Algorithmic Code Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-15 00:00:00+0900+0900">2026년 1월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity">[논문리뷰] Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity">Chi Zhang이 arXiv에 게시한 &#x27;Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-15 00:00:00+0900+0900">2026년 1월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-14-Solar-Open-Technical-Report">[논문리뷰] Solar Open Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-14-Solar-Open-Technical-Report">arXiv에 게시된 &#x27;Solar Open Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-14 00:00:00+0900+0900">2026년 1월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-14-Ministral-3">[논문리뷰] Ministral 3</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-14-Ministral-3">arXiv에 게시된 &#x27;Ministral 3&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-14 00:00:00+0900+0900">2026년 1월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents">[논문리뷰] SmartSearch: Process Reward-Guided Query Refinement for Search Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents">Guanting Dong이 arXiv에 게시한 &#x27;SmartSearch: Process Reward-Guided Query Refinement for Search Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-12 00:00:00+0900+0900">2026년 1월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers">[논문리뷰] Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers">arXiv에 게시된 &#x27;Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-09 00:00:00+0900+0900">2026년 1월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs">[논문리뷰] DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs">Jing Ma이 arXiv에 게시한 &#x27;DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-09 00:00:00+0900+0900">2026년 1월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics">[논문리뷰] MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics">arXiv에 게시된 &#x27;MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-08 00:00:00+0900+0900">2026년 1월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents">[논문리뷰] MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents">Bingzhe Li이 arXiv에 게시한 &#x27;MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-08 00:00:00+0900+0900">2026년 1월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning">[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning">Guanchen Wu이 arXiv에 게시한 &#x27;EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-08 00:00:00+0900+0900">2026년 1월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-06-Recursive-Language-Models">[논문리뷰] Recursive Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-06-Recursive-Language-Models">arXiv에 게시된 &#x27;Recursive Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-06 00:00:00+0900+0900">2026년 1월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction">[논문리뷰] Diversity or Precision? A Deep Dive into Next Token Prediction</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction">arXiv에 게시된 &#x27;Diversity or Precision? A Deep Dive into Next Token Prediction&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-05 00:00:00+0900+0900">2026년 1월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections">[논문리뷰] mHC: Manifold-Constrained Hyper-Connections</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections">arXiv에 게시된 &#x27;mHC: Manifold-Constrained Hyper-Connections&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-01 00:00:00+0900+0900">2026년 1월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem">[논문리뷰] Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem">Wei Gao이 arXiv에 게시한 &#x27;Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2026-01-01 00:00:00+0900+0900">2026년 1월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization">[논문리뷰] GraphLocator: Graph-guided Causal Reasoning for Issue Localization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization">Wei Zhang이 arXiv에 게시한 &#x27;GraphLocator: Graph-guided Causal Reasoning for Issue Localization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-31 00:00:00+0900+0900">2025년 12월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers">[논문리뷰] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers">arXiv에 게시된 &#x27;SlideTailor: Personalized Presentation Slide Generation for Scientific Papers&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-29 00:00:00+0900+0900">2025년 12월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation">[논문리뷰] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen&#x27;s Kappa and Semantic Similarity for Qualitative Research Validation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation">arXiv에 게시된 &#x27;Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen&#x27;s Kappa and Semantic Similarity for Qualitative Research Validation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-24 00:00:00+0900+0900">2025년 12월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos">[논문리뷰] LongVideoAgent: Multi-Agent Reasoning with Long Videos</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos">Renjie Pi이 arXiv에 게시한 &#x27;LongVideoAgent: Multi-Agent Reasoning with Long Videos&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-24 00:00:00+0900+0900">2025년 12월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-24-INTELLECT-3-Technical-Report">[논문리뷰] INTELLECT-3: Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-24-INTELLECT-3-Technical-Report">arXiv에 게시된 &#x27;INTELLECT-3: Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-24 00:00:00+0900+0900">2025년 12월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies">[논문리뷰] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies">arXiv에 게시된 &#x27;Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-24 00:00:00+0900+0900">2025년 12월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction">[논문리뷰] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction">Hong Jiao이 arXiv에 게시한 &#x27;Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-23 00:00:00+0900+0900">2025년 12월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience">[논문리뷰] Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience">arXiv에 게시된 &#x27;Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-22 00:00:00+0900+0900">2025년 12월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward">[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward">arXiv에 게시된 &#x27;Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-19 00:00:00+0900+0900">2025년 12월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning">[논문리뷰] SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning">arXiv에 게시된 &#x27;SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-18 00:00:00+0900+0900">2025년 12월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning">[논문리뷰] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning">arXiv에 게시된 &#x27;Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-18 00:00:00+0900+0900">2025년 12월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-17-RecGPT-V2-Technical-Report">[논문리뷰] RecGPT-V2 Technical Report</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-17-RecGPT-V2-Technical-Report">Dian Chen이 arXiv에 게시한 &#x27;RecGPT-V2 Technical Report&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-17 00:00:00+0900+0900">2025년 12월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-17-Olmo-3">[논문리뷰] Olmo 3</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-17-Olmo-3">arXiv에 게시된 &#x27;Olmo 3&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-17 00:00:00+0900+0900">2025년 12월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation">[논문리뷰] Sliding Window Attention Adaptation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation">arXiv에 게시된 &#x27;Sliding Window Attention Adaptation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-15 00:00:00+0900+0900">2025년 12월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing">[논문리뷰] EtCon: Edit-then-Consolidate for Reliable Knowledge Editing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing">Chenglin Li이 arXiv에 게시한 &#x27;EtCon: Edit-then-Consolidate for Reliable Knowledge Editing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-11 00:00:00+0900+0900">2025년 12월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning">[논문리뷰] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning">Jiacheng Chen이 arXiv에 게시한 &#x27;Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-09 00:00:00+0900+0900">2025년 12월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows">[논문리뷰] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows">arXiv에 게시된 &#x27;TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-08 00:00:00+0900+0900">2025년 12월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks">[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks">Yang Li이 arXiv에 게시한 &#x27;From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-08 00:00:00+0900+0900">2025년 12월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning">[논문리뷰] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning">Zijia Lin이 arXiv에 게시한 &#x27;Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-08 00:00:00+0900+0900">2025년 12월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining">[논문리뷰] PretrainZero: Reinforcement Active Pretraining</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining">Guoqi Li이 arXiv에 게시한 &#x27;PretrainZero: Reinforcement Active Pretraining&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-04 00:00:00+0900+0900">2025년 12월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models">[논문리뷰] The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models">arXiv에 게시된 &#x27;The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models">[논문리뷰] DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models">arXiv에 게시된 &#x27;DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models">[논문리뷰] C^2DLM: Causal Concept-Guided Diffusion Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models">Xinpeng Dong이 arXiv에 게시한 &#x27;C^2DLM: Causal Concept-Guided Diffusion Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-03 00:00:00+0900+0900">2025년 12월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models">[논문리뷰] Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models">Mikhail Burtsev이 arXiv에 게시한 &#x27;Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models">[논문리뷰] PromptBridge: Cross-Model Prompt Transfer for Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models">Wei Wei이 arXiv에 게시한 &#x27;PromptBridge: Cross-Model Prompt Transfer for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion">[논문리뷰] OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion">arXiv에 게시된 &#x27;OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks">[논문리뷰] Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks">arXiv에 게시된 &#x27;Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution">[논문리뷰] Agentic Policy Optimization via Instruction-Policy Co-Evolution</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution">arXiv에 게시된 &#x27;Agentic Policy Optimization via Instruction-Policy Co-Evolution&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-02 00:00:00+0900+0900">2025년 12월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models">[논문리뷰] Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models">Wei Wu이 arXiv에 게시한 &#x27;Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-12-01 00:00:00+0900+0900">2025년 12월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-28-What-does-it-mean-to-understand-language">[논문리뷰] What does it mean to understand language?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-28-What-does-it-mean-to-understand-language">arXiv에 게시된 &#x27;What does it mean to understand language?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-28 00:00:00+0900+0900">2025년 11월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems">[논문리뷰] Latent Collaboration in Multi-Agent Systems</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems">arXiv에 게시된 &#x27;Latent Collaboration in Multi-Agent Systems&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-27 00:00:00+0900+0900">2025년 11월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion">[논문리뷰] Yo&#x27;City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion">Zhifei Yang이 arXiv에 게시한 &#x27;Yo&#x27;City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-26 00:00:00+0900+0900">2025년 11월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization">[논문리뷰] Soft Adaptive Policy Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization">arXiv에 게시된 &#x27;Soft Adaptive Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-26 00:00:00+0900+0900">2025년 11월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System">[논문리뷰] SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System">arXiv에 게시된 &#x27;SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-26 00:00:00+0900+0900">2025년 11월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking">[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking">Elias Stengel-Eskin이 arXiv에 게시한 &#x27;PRInTS: Reward Modeling for Long-Horizon Information Seeking&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-25 00:00:00+0900+0900">2025년 11월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser">[논문리뷰] AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser">arXiv에 게시된 &#x27;AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-25 00:00:00+0900+0900">2025년 11월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries">[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries">arXiv에 게시된 &#x27;ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-20 00:00:00+0900+0900">2025년 11월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models">[논문리뷰] OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models">Jian liu이 arXiv에 게시한 &#x27;OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-19 00:00:00+0900+0900">2025년 11월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models">[논문리뷰] Mitigating Label Length Bias in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models">Katharina von der Wense이 arXiv에 게시한 &#x27;Mitigating Label Length Bias in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-19 00:00:00+0900+0900">2025년 11월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost">[논문리뷰] LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost">Kengo Tajiri이 arXiv에 게시한 &#x27;LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-19 00:00:00+0900+0900">2025년 11월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance">[논문리뷰] Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance">arXiv에 게시된 &#x27;Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-18 00:00:00+0900+0900">2025년 11월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning">[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning">Haiyuan Wan이 arXiv에 게시한 &#x27;P1: Mastering Physics Olympiads with Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-18 00:00:00+0900+0900">2025년 11월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling">[논문리뷰] MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling">cyyang822이 arXiv에 게시한 &#x27;MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-18 00:00:00+0900+0900">2025년 11월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing">[논문리뷰] AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing">Hongyu Lin이 arXiv에 게시한 &#x27;AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-18 00:00:00+0900+0900">2025년 11월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain">[논문리뷰] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain">Meng Jiang이 arXiv에 게시한 &#x27;A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-18 00:00:00+0900+0900">2025년 11월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward">[논문리뷰] miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward">Farzan Farnia이 arXiv에 게시한 &#x27;miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-17 00:00:00+0900+0900">2025년 11월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey">[논문리뷰] Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey">Mohammad Hossein Rohban이 arXiv에 게시한 &#x27;Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-17 00:00:00+0900+0900">2025년 11월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding">[논문리뷰] DoPE: Denoising Rotary Position Embedding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding">Min Yang이 arXiv에 게시한 &#x27;DoPE: Denoising Rotary Position Embedding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-17 00:00:00+0900+0900">2025년 11월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training">[논문리뷰] Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training">suayptalha이 arXiv에 게시한 &#x27;Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-14 00:00:00+0900+0900">2025년 11월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis">[논문리뷰] CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis">Jian Wu이 arXiv에 게시한 &#x27;CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-14 00:00:00+0900+0900">2025년 11월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents">[논문리뷰] Agentic Refactoring: An Empirical Study of AI Coding Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents">Hajimu Iida이 arXiv에 게시한 &#x27;Agentic Refactoring: An Empirical Study of AI Coding Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-13 00:00:00+0900+0900">2025년 11월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora">[논문리뷰] Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora">Mohamed Motasim Hamed이 arXiv에 게시한 &#x27;Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective">[논문리뷰] Walking the Tightrope of LLMs for Software Development: A Practitioners&#x27; Perspective</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective">Christoph Treude이 arXiv에 게시한 &#x27;Walking the Tightrope of LLMs for Software Development: A Practitioners&#x27; Perspective&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals">[논문리뷰] The Path Not Taken: RLVR Provably Learns Off the Principals</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals">arXiv에 게시된 &#x27;The Path Not Taken: RLVR Provably Learns Off the Principals&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration">[논문리뷰] Optimizing Diversity and Quality through Base-Aligned Model Collaboration</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration">Jonathan May이 arXiv에 게시한 &#x27;Optimizing Diversity and Quality through Base-Aligned Model Collaboration&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces">[논문리뷰] DynaAct: Large Language Model Reasoning with Dynamic Action Spaces</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces">Lingpeng Kong이 arXiv에 게시한 &#x27;DynaAct: Large Language Model Reasoning with Dynamic Action Spaces&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs">[논문리뷰] Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs">arXiv에 게시된 &#x27;Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems">[논문리뷰] Adaptive Multi-Agent Response Refinement in Conversational Systems</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems">arXiv에 게시된 &#x27;Adaptive Multi-Agent Response Refinement in Conversational Systems&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-12 00:00:00+0900+0900">2025년 11월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models">[논문리뷰] VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models">arXiv에 게시된 &#x27;VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-11 00:00:00+0900+0900">2025년 11월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery">[논문리뷰] The Station: An Open-World Environment for AI-Driven Discovery</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery">wydu이 arXiv에 게시한 &#x27;The Station: An Open-World Environment for AI-Driven Discovery&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-11 00:00:00+0900+0900">2025년 11월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs">[논문리뷰] Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs">arXiv에 게시된 &#x27;Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-11 00:00:00+0900+0900">2025년 11월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models">[논문리뷰] Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models">arXiv에 게시된 &#x27;Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-11 00:00:00+0900+0900">2025년 11월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling">[논문리뷰] NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling">arXiv에 게시된 &#x27;NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-11 00:00:00+0900+0900">2025년 11월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning">[논문리뷰] Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning">arXiv에 게시된 &#x27;Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-11 00:00:00+0900+0900">2025년 11월 11일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks">[논문리뷰] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks">arXiv에 게시된 &#x27;VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-10 00:00:00+0900+0900">2025년 11월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent">[논문리뷰] HAFixAgent: History-Aware Automated Program Repair Agent</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent">Ahmed E. Hassan이 arXiv에 게시한 &#x27;HAFixAgent: History-Aware Automated Program Repair Agent&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-10 00:00:00+0900+0900">2025년 11월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-10-Dense-Motion-Captioning">[논문리뷰] Dense Motion Captioning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-10-Dense-Motion-Captioning">Paolo Rota이 arXiv에 게시한 &#x27;Dense Motion Captioning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-10 00:00:00+0900+0900">2025년 11월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask">[논문리뷰] Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask">arXiv에 게시된 &#x27;Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 21:54:30+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs">[논문리뷰] Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs">Bo Bai이 arXiv에 게시한 &#x27;Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:35:02+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner">[논문리뷰] OpenSIR: Open-Ended Self-Improving Reasoner</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner">arXiv에 게시된 &#x27;OpenSIR: Open-Ended Self-Improving Reasoner&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:22:42+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation">[논문리뷰] Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation">arXiv에 게시된 &#x27;Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:22:42+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning">[논문리뷰] Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning">arXiv에 게시된 &#x27;Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-11-09 19:01:31+0900">2025년 11월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis">[논문리뷰] EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis">arXiv에 게시된 &#x27;EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games">[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games">Justin Cui이 arXiv에 게시한 &#x27;Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-31 18:37:31+0900">2025년 10월 31일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling">[논문리뷰] TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling">Zheng Zhang이 arXiv에 게시한 &#x27;TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining">[논문리뷰] Reasoning-Aware GRPO using Process Mining</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining">arXiv에 게시된 &#x27;Reasoning-Aware GRPO using Process Mining&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization">[논문리뷰] ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization">Ruihua Song이 arXiv에 게시한 &#x27;ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling">[논문리뷰] Parallel Loop Transformer for Efficient Test-Time Computation Scaling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling">arXiv에 게시된 &#x27;Parallel Loop Transformer for Efficient Test-Time Computation Scaling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence">[논문리뷰] JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence">arXiv에 게시된 &#x27;JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning">[논문리뷰] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning">Xin Liu이 arXiv에 게시한 &#x27;FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-30 13:06:06+0900">2025년 10월 30일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents">[논문리뷰] VisCoder2: Building Multi-Language Visualization Coding Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents">arXiv에 게시된 &#x27;VisCoder2: Building Multi-Language Visualization Coding Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-29 13:11:02+0900">2025년 10월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers">[논문리뷰] ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers">Ian L. V. Roque이 arXiv에 게시한 &#x27;ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-29 13:11:02+0900">2025년 10월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation">[논문리뷰] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation">arXiv에 게시된 &#x27;The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS">[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS">arXiv에 게시된 &#x27;Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking">[논문리뷰] LimRank: Less is More for Reasoning-Intensive Information Reranking</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking">Arman Cohan이 arXiv에 게시한 &#x27;LimRank: Less is More for Reasoning-Intensive Information Reranking&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-Knocking-Heads-Attention">[논문리뷰] Knocking-Heads Attention</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-Knocking-Heads-Attention">Jianguo Li이 arXiv에 게시한 &#x27;Knocking-Heads Attention&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback">[논문리뷰] Code Aesthetics with Agentic Reward Feedback</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback">Yupan Huang이 arXiv에 게시한 &#x27;Code Aesthetics with Agentic Reward Feedback&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-28 13:07:54+0900">2025년 10월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense">[논문리뷰] Soft Instruction De-escalation Defense</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense">arXiv에 게시된 &#x27;Soft Instruction De-escalation Defense&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-27 13:07:36+0900">2025년 10월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory">[논문리뷰] Document Understanding, Measurement, and Manipulation Using Category Theory</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory">arXiv에 게시된 &#x27;Document Understanding, Measurement, and Manipulation Using Category Theory&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-27 13:07:36+0900">2025년 10월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets">[논문리뷰] DeepAgent: A General Reasoning Agent with Scalable Toolsets</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets">Jiajie Jin이 arXiv에 게시한 &#x27;DeepAgent: A General Reasoning Agent with Scalable Toolsets&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-27 13:07:36+0900">2025년 10월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models">[논문리뷰] ARC-Encoder: learning compressed text representations for large language models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models">arXiv에 게시된 &#x27;ARC-Encoder: learning compressed text representations for large language models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-27 13:07:36+0900">2025년 10월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks">[논문리뷰] Machine Text Detectors are Membership Inference Attacks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks">Naoaki Okazaki이 arXiv에 게시한 &#x27;Machine Text Detectors are Membership Inference Attacks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-23 13:08:59+0900">2025년 10월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts">[논문리뷰] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts">arXiv에 게시된 &#x27;LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-23 13:08:59+0900">2025년 10월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping">[논문리뷰] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping">Junrui Shen이 arXiv에 게시한 &#x27;BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-23 13:08:59+0900">2025년 10월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation">[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation">Yujie Zhou이 arXiv에 게시한 &#x27;UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold">[논문리뷰] PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold">arXiv에 게시된 &#x27;PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-Extracting-alignment-data-in-open-models">[논문리뷰] Extracting alignment data in open models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-Extracting-alignment-data-in-open-models">arXiv에 게시된 &#x27;Extracting alignment data in open models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning">[논문리뷰] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning">Qipeng Guo이 arXiv에 게시한 &#x27;EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist">[논문리뷰] Chem-R: Learning to Reason as a Chemist</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist">arXiv에 게시된 &#x27;Chem-R: Learning to Reason as a Chemist&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-22 13:07:20+0900">2025년 10월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive">[논문리뷰] Paper2Web: Let&#x27;s Make Your Paper Alive!</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive">Yao Wan이 arXiv에 게시한 &#x27;Paper2Web: Let&#x27;s Make Your Paper Alive!&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-20 13:04:24+0900">2025년 10월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-20-Language-Models-Model-Language">[논문리뷰] Language Models Model Language</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-20-Language-Models-Model-Language">arXiv에 게시된 &#x27;Language Models Model Language&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-20 13:04:24+0900">2025년 10월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning">[논문리뷰] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning">arXiv에 게시된 &#x27;DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-20 13:04:24+0900">2025년 10월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification">[논문리뷰] VLA-0: Building State-of-the-Art VLAs with Zero Modification</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification">arXiv에 게시된 &#x27;VLA-0: Building State-of-the-Art VLAs with Zero Modification&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-17 13:09:57+0900">2025년 10월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models">[논문리뷰] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models">arXiv에 게시된 &#x27;The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-17 13:09:57+0900">2025년 10월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems">[논문리뷰] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems">arXiv에 게시된 &#x27;RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-17 13:09:57+0900">2025년 10월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval">[논문리뷰] LLM-guided Hierarchical Retrieval</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval">arXiv에 게시된 &#x27;LLM-guided Hierarchical Retrieval&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-17 13:09:57+0900">2025년 10월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization">[논문리뷰] Agentic Entropy-Balanced Policy Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization">arXiv에 게시된 &#x27;Agentic Entropy-Balanced Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-17 13:09:57+0900">2025년 10월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning">[논문리뷰] Revisiting Model Interpolation for Efficient Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning">arXiv에 게시된 &#x27;Revisiting Model Interpolation for Efficient Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training">[논문리뷰] MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training">arXiv에 게시된 &#x27;MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain">[논문리뷰] Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain">Lingxi Lu이 arXiv에 게시한 &#x27;Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-16 13:09:51+0900">2025년 10월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks">[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks">Xueyuan Lin이 arXiv에 게시한 &#x27;Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-15 13:01:40+0900">2025년 10월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation">[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation">arXiv에 게시된 &#x27;Information-Preserving Reformulation of Reasoning Traces for Antidistillation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-15 13:01:40+0900">2025년 10월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models">[논문리뷰] A Survey of Vibe Coding with Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models">arXiv에 게시된 &#x27;A Survey of Vibe Coding with Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-15 13:01:40+0900">2025년 10월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review">[논문리뷰] ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review">Christopher Pal이 arXiv에 게시한 &#x27;ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare">[논문리뷰] GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare">arXiv에 게시된 &#x27;GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting">[논문리뷰] Don&#x27;t Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting">Julia Kempe이 arXiv에 게시한 &#x27;Don&#x27;t Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation">[논문리뷰] DISCO: Diversifying Sample Condensation for Efficient Model Evaluation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation">arXiv에 게시된 &#x27;DISCO: Diversifying Sample Condensation for Efficient Model Evaluation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution">[논문리뷰] BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution">Hange Liu이 arXiv에 게시한 &#x27;BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion">[논문리뷰] AutoPR: Let&#x27;s Automate Your Academic Promotion!</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion">Yixin Yuan이 arXiv에 게시한 &#x27;AutoPR: Let&#x27;s Automate Your Academic Promotion!&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-13 13:44:18+0900">2025년 10월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG">[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG">arXiv에 게시된 &#x27;UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models">[논문리뷰] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models">James Cheng이 arXiv에 게시한 &#x27;Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training">[논문리뷰] Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training">Peng Cheng이 arXiv에 게시한 &#x27;Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens">[논문리뷰] Memory Retrieval and Consolidation in Large Language Models through Function Tokens</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens">arXiv에 게시된 &#x27;Memory Retrieval and Consolidation in Large Language Models through Function Tokens&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning">[논문리뷰] From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning">Feiwei Qin이 arXiv에 게시한 &#x27;From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints">[논문리뷰] Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints">Huazhe Xu이 arXiv에 게시한 &#x27;Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning">[논문리뷰] A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning">arXiv에 게시된 &#x27;A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference">[논문리뷰] Vibe Checker: Aligning Code Evaluation with Human Preference</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference">arXiv에 게시된 &#x27;Vibe Checker: Aligning Code Evaluation with Human Preference&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-09 13:45:06+0900">2025년 10월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-9-The-Markovian-Thinker">[논문리뷰] The Markovian Thinker</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-9-The-Markovian-Thinker">arXiv에 게시된 &#x27;The Markovian Thinker&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-09 13:45:06+0900">2025년 10월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents">[논문리뷰] DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents">arXiv에 게시된 &#x27;DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-09 13:45:06+0900">2025년 10월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models">[논문리뷰] Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models">arXiv에 게시된 &#x27;Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-09 13:45:06+0900">2025년 10월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness">[논문리뷰] Training Dynamics Impact Post-Training Quantization Robustness</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness">Jonas Geiping이 arXiv에 게시한 &#x27;Training Dynamics Impact Post-Training Quantization Robustness&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-08 13:48:12+0900">2025년 10월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning">[논문리뷰] TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning">arXiv에 게시된 &#x27;TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-08 13:48:12+0900">2025년 10월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization">[논문리뷰] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization">sirano1004이 arXiv에 게시한 &#x27;Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-08 13:48:12+0900">2025년 10월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation">[논문리뷰] CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation">arXiv에 게시된 &#x27;CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-08 13:48:12+0900">2025년 10월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization">[논문리뷰] ASPO: Asymmetric Importance Sampling Policy Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization">Xiu Li이 arXiv에 게시한 &#x27;ASPO: Asymmetric Importance Sampling Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-08 13:48:12+0900">2025년 10월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos">[논문리뷰] Watch and Learn: Learning to Use Computers from Online Videos</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos">Oriana Riva이 arXiv에 게시한 &#x27;Watch and Learn: Learning to Use Computers from Online Videos&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time">[논문리뷰] Self-Reflective Generation at Test Time</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time">Shuang Qiu이 arXiv에 게시한 &#x27;Self-Reflective Generation at Test Time&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm">[논문리뷰] Optimal Scaling Needs Optimal Norm</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm">Stefan Kesselheim이 arXiv에 게시한 &#x27;Optimal Scaling Needs Optimal Norm&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition">[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition">arXiv에 게시된 &#x27;MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning">[논문리뷰] Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning">arXiv에 게시된 &#x27;Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions">[논문리뷰] Judging with Confidence: Calibrating Autoraters to Preference Distributions</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions">arXiv에 게시된 &#x27;Judging with Confidence: Calibrating Autoraters to Preference Distributions&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models">[논문리뷰] Imperceptible Jailbreaking against Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models">arXiv에 게시된 &#x27;Imperceptible Jailbreaking against Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data">[논문리뷰] Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data">arXiv에 게시된 &#x27;Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty">[논문리뷰] EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty">Xuanwu Wang이 arXiv에 게시한 &#x27;EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models">[논문리뷰] Epistemic Diversity and Knowledge Collapse in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models">arXiv에 게시된 &#x27;Epistemic Diversity and Knowledge Collapse in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-07 13:36:57+0900">2025년 10월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents">[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents">Neil Zhenqiang Gong이 arXiv에 게시한 &#x27;WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-06 13:29:11+0900">2025년 10월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models">[논문리뷰] On Predictability of Reinforcement Learning Dynamics for Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models">Yuqing Huang이 arXiv에 게시한 &#x27;On Predictability of Reinforcement Learning Dynamics for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-02 13:30:22+0900">2025년 10월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents">[논문리뷰] Infusing Theory of Mind into Socially Intelligent LLM Agents</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents">arXiv에 게시된 &#x27;Infusing Theory of Mind into Socially Intelligent LLM Agents&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-02 13:30:22+0900">2025년 10월 2일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain">[논문리뷰] The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain">arXiv에 게시된 &#x27;The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs">[논문리뷰] Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs">Yao Shu이 arXiv에 게시한 &#x27;Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs">[논문리뷰] Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs">normanpaulsen이 arXiv에 게시한 &#x27;Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective">[논문리뷰] Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective">arXiv에 게시된 &#x27;Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models">[논문리뷰] Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models">arXiv에 게시된 &#x27;Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-10-01 14:04:08+0900">2025년 10월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning">[논문리뷰] PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning">Lingpeng Kong이 arXiv에 게시한 &#x27;PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-29 13:47:46+0900">2025년 9월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning">[논문리뷰] MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning">Weipeng Zhong이 arXiv에 게시한 &#x27;MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-29 13:47:46+0900">2025년 9월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards">[논문리뷰] Language Models Can Learn from Verbal Feedback Without Scalar Rewards</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards">arXiv에 게시된 &#x27;Language Models Can Learn from Verbal Feedback Without Scalar Rewards&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-29 13:47:46+0900">2025년 9월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing">[논문리뷰] Fine-tuning Done Right in Model Editing</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing">Du Su이 arXiv에 게시한 &#x27;Fine-tuning Done Right in Model Editing&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-29 13:47:46+0900">2025년 9월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models">[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models">Yuewei Zhang이 arXiv에 게시한 &#x27;VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them">[논문리뷰] TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them">Zhuohao Yu이 arXiv에 게시한 &#x27;TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models">[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models">Javad Lavaei이 arXiv에 게시한 &#x27;StyleBench: Evaluating thinking styles in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands">[논문리뷰] Interactive Recommendation Agent with Active User Commands</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands">Xueyang Feng이 arXiv에 게시한 &#x27;Interactive Recommendation Agent with Active User Commands&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning">[논문리뷰] CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning">Wenping Hu이 arXiv에 게시한 &#x27;CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information">[논문리뷰] Behind RoPE: How Does Causal Mask Encode Positional Information?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information">Yeyun Gong이 arXiv에 게시한 &#x27;Behind RoPE: How Does Causal Mask Encode Positional Information?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub">[논문리뷰] On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub">Hajimu Iida이 arXiv에 게시한 &#x27;On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-25 13:08:16+0900">2025년 9월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines">[논문리뷰] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines">Yanfang이 arXiv에 게시한 &#x27;LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-25 13:08:16+0900">2025년 9월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data">[논문리뷰] Reinforcement Learning on Pre-Training Data</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data">Evander Yang이 arXiv에 게시한 &#x27;Reinforcement Learning on Pre-Training Data&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-24 13:14:19+0900">2025년 9월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects">[논문리뷰] Large Language Models Discriminate Against Speakers of German Dialects</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects">Katharina von der Wense이 arXiv에 게시한 &#x27;Large Language Models Discriminate Against Speakers of German Dialects&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-24 13:14:19+0900">2025년 9월 24일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications">[논문리뷰] Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications">Fatma Betül Terzioğlu이 arXiv에 게시한 &#x27;Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning">[논문리뷰] SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning">Zhaopeng Tu이 arXiv에 게시한 &#x27;SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency">[논문리뷰] LIMI: Less is More for Agency</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency">happyZYM이 arXiv에 게시한 &#x27;LIMI: Less is More for Agency&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context">[논문리뷰] DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context">Maunendra Sankar Desarkar이 arXiv에 게시한 &#x27;DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing">[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing">Jaeho Lee이 arXiv에 게시한 &#x27;AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-23 13:36:03+0900">2025년 9월 23일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning">[논문리뷰] FlowRL: Matching Reward Distributions for LLM Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning">Hengli Li이 arXiv에 게시한 &#x27;FlowRL: Matching Reward Distributions for LLM Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-19 13:12:21+0900">2025년 9월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning">[논문리뷰] THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning">Yicheng Pan이 arXiv에 게시한 &#x27;THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-18 13:07:00+0900">2025년 9월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale">[논문리뷰] Hala Technical Report: Building Arabic-Centric Instruction &amp; Translation Models at Scale</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale">Bernard Ghanem이 arXiv에 게시한 &#x27;Hala Technical Report: Building Arabic-Centric Instruction &amp; Translation Models at Scale&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-18 13:07:00+0900">2025년 9월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling">[논문리뷰] Towards General Agentic Intelligence via Environment Scaling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling">Guangyu Li이 arXiv에 게시한 &#x27;Towards General Agentic Intelligence via Environment Scaling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-17 13:16:01+0900">2025년 9월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge">[논문리뷰] Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge">Wentao Zhang이 arXiv에 게시한 &#x27;Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-17 13:16:01+0900">2025년 9월 17일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning">[논문리뷰] UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning">Yongliang Shen이 arXiv에 게시한 &#x27;UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-16 13:16:41+0900">2025년 9월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI">[논문리뷰] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI">UVSKKR이 arXiv에 게시한 &#x27;EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-16 13:16:41+0900">2025년 9월 16일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs">[논문리뷰] The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs">Jonas Geiping이 arXiv에 게시한 &#x27;The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-15 13:12:08+0900">2025년 9월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading">[논문리뷰] QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading">Chenyu You이 arXiv에 게시한 &#x27;QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-15 13:12:08+0900">2025년 9월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge">[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge">Dipanjan Das이 arXiv에 게시한 &#x27;SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-10 13:11:01+0900">2025년 9월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning">[논문리뷰] Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning">Xinyu Yang이 arXiv에 게시한 &#x27;Parallel-R1: Towards Parallel Thinking via Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-10 13:11:01+0900">2025년 9월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training">[논문리뷰] Language Self-Play For Data-Free Training</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training">Vijai Mohan이 arXiv에 게시한 &#x27;Language Self-Play For Data-Free Training&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-10 13:11:01+0900">2025년 9월 10일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet">[논문리뷰] Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet">See-Kiong Ng이 arXiv에 게시한 &#x27;Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-09 13:19:09+0900">2025년 9월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models">[논문리뷰] Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models">Ke Shen이 arXiv에 게시한 &#x27;Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-09 13:19:09+0900">2025년 9월 9일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models">[논문리뷰] Symbolic Graphics Programming with Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models">Kaipeng Zhang이 arXiv에 게시한 &#x27;Symbolic Graphics Programming with Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-08 13:10:18+0900">2025년 9월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models">[논문리뷰] Behavioral Fingerprinting of Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models">Xing Li이 arXiv에 게시한 &#x27;Behavioral Fingerprinting of Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-08 13:10:18+0900">2025년 9월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth">[논문리뷰] Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth">Chi-Li Chen이 arXiv에 게시한 &#x27;Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-05 13:07:20+0900">2025년 9월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research">[논문리뷰] Open Data Synthesis For Deep Research</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research">Zheng Liu이 arXiv에 게시한 &#x27;Open Data Synthesis For Deep Research&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-04 12:56:15+0900">2025년 9월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use">[논문리뷰] VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use">Zhiheng Lyu이 arXiv에 게시한 &#x27;VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey">[논문리뷰] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey">Hejia Geng이 arXiv에 게시한 &#x27;The Landscape of Agentic Reinforcement Learning for LLMs: A Survey&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning">[논문리뷰] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning">Qian Liu이 arXiv에 게시한 &#x27;SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction">[논문리뷰] SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction">bindsch이 arXiv에 게시한 &#x27;SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning">[논문리뷰] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning">Zirui Wang이 arXiv에 게시한 &#x27;OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR">[논문리뷰] Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR">Lu Wang이 arXiv에 게시한 &#x27;Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them">[논문리뷰] Fantastic Pretraining Optimizers and Where to Find Them</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them">Percy Liang이 arXiv에 게시한 &#x27;Fantastic Pretraining Optimizers and Where to Find Them&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models">[논문리뷰] AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models">Rahul Karthikeyan이 arXiv에 게시한 &#x27;AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-03 13:36:21+0900">2025년 9월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models">[논문리뷰] Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models">Yifan Lu이 arXiv에 게시한 &#x27;Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-09-01 13:14:34+0900">2025년 9월 1일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning">[논문리뷰] TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning">Simin Ma이 arXiv에 게시한 &#x27;TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-29 13:14:44+0900">2025년 8월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models">[논문리뷰] Provable Benefits of In-Tool Learning for Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models">Vivien Cabannes이 arXiv에 게시한 &#x27;Provable Benefits of In-Tool Learning for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-29 13:14:44+0900">2025년 8월 29일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling">[논문리뷰] Predicting the Order of Upcoming Tokens Improves Language Modeling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling">Alham Fikri Aji이 arXiv에 게시한 &#x27;Predicting the Order of Upcoming Tokens Improves Language Modeling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-28 13:10:39+0900">2025년 8월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models">[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models">Yixiao Ge이 arXiv에 게시한 &#x27;AudioStory: Generating Long-Form Narrative Audio with Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-28 13:10:39+0900">2025년 8월 28일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities">[논문리뷰] Unraveling the cognitive patterns of Large Language Models through module communities</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities">Jianxi Gao이 arXiv에 게시한 &#x27;Unraveling the cognitive patterns of Large Language Models through module communities&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling">[논문리뷰] TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling">Zhoufutu Wen이 arXiv에 게시한 &#x27;TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting">[논문리뷰] QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting">Manuela Veloso이 arXiv에 게시한 &#x27;QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks">[논문리뷰] Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks">Daisuke Nohara이 arXiv에 게시한 &#x27;Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning">[논문리뷰] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning">Arman Cohan이 arXiv에 게시한 &#x27;Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation">[논문리뷰] ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation">Kun Kuang이 arXiv에 게시한 &#x27;ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics">[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics">Dongchen Huang이 arXiv에 게시한 &#x27;CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-27 13:22:18+0900">2025년 8월 27일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning">[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning">Xin Zheng이 arXiv에 게시한 &#x27;Explain Before You Answer: A Survey on Compositional Visual Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-26 13:21:57+0900">2025년 8월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning">[논문리뷰] Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning">Jiale Zhao이 arXiv에 게시한 &#x27;Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-26 13:21:57+0900">2025년 8월 26일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning">[논문리뷰] End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning">Pengcheng Qiu이 arXiv에 게시한 &#x27;End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-25 13:13:07+0900">2025년 8월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR">[논문리뷰] Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR">Ying Nian Wu이 arXiv에 게시한 &#x27;Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-25 13:13:07+0900">2025년 8월 25일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation">[논문리뷰] Mobile-Agent-v3: Foundamental Agents for GUI Automation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation">Haowei Liu이 arXiv에 게시한 &#x27;Mobile-Agent-v3: Foundamental Agents for GUI Automation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-22 13:10:52+0900">2025년 8월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models">[논문리뷰] Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models">Lifan Guo이 arXiv에 게시한 &#x27;Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-22 13:10:52+0900">2025년 8월 22일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs">[논문리뷰] Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs">Haobo Xu이 arXiv에 게시한 &#x27;Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-21 13:15:28+0900">2025년 8월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting">[논문리뷰] On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting">Guoyin Wang이 arXiv에 게시한 &#x27;On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-21 13:15:28+0900">2025년 8월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers">[논문리뷰] MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers">Prathyusha Jwalapuram이 arXiv에 게시한 &#x27;MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-21 13:15:28+0900">2025년 8월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery">[논문리뷰] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery">zijieqiu이 arXiv에 게시한 &#x27;From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-21 13:15:28+0900">2025년 8월 21일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language">[논문리뷰] Prompt Orchestration Markup Language</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language">Yuqing Yang이 arXiv에 게시한 &#x27;Prompt Orchestration Markup Language&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-20 13:26:54+0900">2025년 8월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding">[논문리뷰] Beyond Human Judgment: A Bayesian Evaluation of LLMs&#x27; Moral Values Understanding</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding">Alina Landowska이 arXiv에 게시한 &#x27;Beyond Human Judgment: A Bayesian Evaluation of LLMs&#x27; Moral Values Understanding&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-20 13:26:54+0900">2025년 8월 20일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models">[논문리뷰] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models">Jusen Du이 arXiv에 게시한 &#x27;Speed Always Wins: A Survey on Efficient Architectures for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-19 13:15:01+0900">2025년 8월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors">[논문리뷰] Reinforcement Learning with Rubric Anchors</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors">Haokai Xu이 arXiv에 게시한 &#x27;Reinforcement Learning with Rubric Anchors&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-19 13:15:01+0900">2025년 8월 19일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning">[논문리뷰] SSRL: Self-Search Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning">Yanxu Chen이 arXiv에 게시한 &#x27;SSRL: Self-Search Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-18 13:14:38+0900">2025년 8월 18일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models">[논문리뷰] Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models">Qinghao Ye이 arXiv에 게시한 &#x27;Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-15 13:09:31+0900">2025년 8월 15일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery">[논문리뷰] Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery">Di Zhang이 arXiv에 게시한 &#x27;Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-14 13:19:02+0900">2025년 8월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models">[논문리뷰] Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models">Guiyang Hou이 arXiv에 게시한 &#x27;Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-14 13:19:02+0900">2025년 8월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study">[논문리뷰] Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study">Gjergji Kasneci이 arXiv에 게시한 &#x27;Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-14 13:19:02+0900">2025년 8월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance">[논문리뷰] AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance">Yong Li이 arXiv에 게시한 &#x27;AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-14 13:19:02+0900">2025년 8월 14일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning">[논문리뷰] Train Long, Think Short: Curriculum Learning for Efficient Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning">Marzyeh Ghassemi이 arXiv에 게시한 &#x27;Train Long, Think Short: Curriculum Learning for Efficient Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-13 13:29:23+0900">2025년 8월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy">[논문리뷰] Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy">Elizabeth Karpinski이 arXiv에 게시한 &#x27;Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-13 13:29:23+0900">2025년 8월 13일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability">[논문리뷰] ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability">Yuchen Li이 arXiv에 게시한 &#x27;ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-12 13:29:09+0900">2025년 8월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning">[논문리뷰] Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning">Jiaheng Liu이 arXiv에 게시한 &#x27;Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-12 13:29:09+0900">2025년 8월 12일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling">[논문리뷰] Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling">Ruolin Shen이 arXiv에 게시한 &#x27;Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data">[논문리뷰] R-Zero: Self-Evolving Reasoning LLM from Zero Data</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data">Zongxia Li이 arXiv에 게시한 &#x27;R-Zero: Self-Evolving Reasoning LLM from Zero Data&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction">[논문리뷰] PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction">Prajit Das이 arXiv에 게시한 &#x27;PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking">[논문리뷰] I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking">Chao Wang이 arXiv에 게시한 &#x27;I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis">[논문리뷰] Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis">Reshmi Ghosh이 arXiv에 게시한 &#x27;Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation">[논문리뷰] Evaluating, Synthesizing, and Enhancing for Customer Support Conversation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation">Feng Chen이 arXiv에 게시한 &#x27;Evaluating, Synthesizing, and Enhancing for Customer Support Conversation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts">[논문리뷰] Are Today&#x27;s LLMs Ready to Explain Well-Being Concepts?</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts">Huan Liu이 arXiv에 게시한 &#x27;Are Today&#x27;s LLMs Ready to Explain Well-Being Concepts?&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-08 13:32:22+0900">2025년 8월 8일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning">[논문리뷰] Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning">Maksim Nekrashevich이 arXiv에 게시한 &#x27;Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-07 13:38:21+0900">2025년 8월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence">[논문리뷰] Sotopia-RL: Reward Design for Social Intelligence</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence">Keyang Xuan이 arXiv에 게시한 &#x27;Sotopia-RL: Reward Design for Social Intelligence&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-07 13:38:21+0900">2025년 8월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management">[논문리뷰] Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management">Yunxin Liu이 arXiv에 게시한 &#x27;Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-07 13:38:21+0900">2025년 8월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks">[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks">Haozhe Zhang이 arXiv에 게시한 &#x27;Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-07 13:38:21+0900">2025년 8월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization">[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization">Kechi Zhang이 arXiv에 게시한 &#x27;RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-07 13:38:21+0900">2025년 8월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation">[논문리뷰] EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation">Dong Chen이 arXiv에 게시한 &#x27;EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-07 13:38:21+0900">2025년 8월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning">[논문리뷰] Agent Lightning: Train ANY AI Agents with Reinforcement Learning</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning">Zilong Wang이 arXiv에 게시한 &#x27;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-07 13:38:21+0900">2025년 8월 7일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search">[논문리뷰] CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search">Jiwei Li이 arXiv에 게시한 &#x27;CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-06 13:46:36+0900">2025년 8월 6일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following">[논문리뷰] Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models&#x27; Instruction Following</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following">Jiaqing Liang이 arXiv에 게시한 &#x27;Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models&#x27; Instruction Following&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-05 11:40:52+0900">2025년 8월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks">[논문리뷰] AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks">Zhiwei Zhang이 arXiv에 게시한 &#x27;AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-05 11:40:52+0900">2025년 8월 5일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution">[논문리뷰] SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution">Heng Lian이 arXiv에 게시한 &#x27;SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-04 12:17:01+0900">2025년 8월 4일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article><article class="archive__item"><h2 class="archive__item-title"><a href="/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving">[논문리뷰] Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</a></h2><div class="archive__item-excerpt"><a href="/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving">Zhicheng Jiang이 arXiv에 게시한 &#x27;Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving&#x27; 논문에 대한 자세한 리뷰입니다.</a></div><div class="archive__item-meta"><time dateTime="2025-08-03 07:35:17+0900">2025년 8월 3일</time><span class="text-gray-400 text-sm ml-3" role="status"><svg class="inline w-3 h-3 animate-spin" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" aria-hidden="true"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg><span class="sr-only">댓글 수 로딩 중</span></span></div></article></div></div></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[9038,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js\"],\"default\"]\n6:I[231,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js\"],\"\"]\n7:I[227,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"605\",\"static/chunks/app/tags/%5Btag%5D/page-a05565f8ea9cbb05.js\"],\"default\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nb:I[9157,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"default\"]\nc:I[4080,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"\"]\ne:I[6130,[],\"\"]\n9:[\"tag\",\"Large%20Language%20Models\",\"d\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"mJI0q5Z-SQWBtT83kG_N7\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/tags/Large%20Language%20Models\",\"initialTree\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Large%20Language%20Models\",\"d\"],{\"children\":[\"__PAGE__?{\\\"tag\\\":\\\"Large Language Models\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"tags\",{\"children\":[[\"tag\",\"Large%20Language%20Models\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"$L5\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"CollectionPage\\\",\\\"name\\\":\\\"#Large Language Models - secrett2633's blog\\\",\\\"description\\\":\\\"Large Language Models 태그가 포함된 포스트 목록\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud/tags/Large%20Language%20Models\\\",\\\"isPartOf\\\":{\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"inLanguage\\\":\\\"ko\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"#Large Language Models\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/tags/Large%20Language%20Models\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2741,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/tags/Large%20Language%20Models\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"#Large Language Models\"}]]}]]]}]}],[\"$\",\"h1\",null,{\"className\":\"page__title mb-6\",\"children\":[\"#\",\"Large Language Models\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-500 mb-6\",\"children\":[319,\"개의 포스트\"]}],[\"$\",\"div\",null,{\"className\":\"entries-list\",\"children\":[[\"$\",\"article\",\"2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens\",\"children\":\"[논문리뷰] STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens\",\"children\":\"Zhilong Zheng이 arXiv에 게시한 'STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-18 00:00:00+0900+0900\",\"children\":\"2026년 2월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-18-STAPO-Stabilizing-Reinforcement-Learning-for-LLMs-by-Silencing-Rare-Spurious-Tokens\"}]]}]]}],[\"$\",\"article\",\"2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model\",\"children\":\"[논문리뷰] Query as Anchor: Scenario-Adaptive User Representation via Large Language Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model\",\"children\":\"arXiv에 게시된 'Query as Anchor: Scenario-Adaptive User Representation via Large Language Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-17 00:00:00+0900+0900\",\"children\":\"2026년 2월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-17-Query-as-Anchor-Scenario-Adaptive-User-Representation-via-Large-Language-Model\"}]]}]]}],[\"$\",\"article\",\"2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks\",\"children\":\"[논문리뷰] Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks\",\"children\":\"arXiv에 게시된 'Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-17 00:00:00+0900+0900\",\"children\":\"2026년 2월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-17-Exposing-the-Systematic-Vulnerability-of-Open-Weight-Models-to-Prefill-Attacks\"}]]}]]}],[\"$\",\"article\",\"2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation\",\"children\":\"[논문리뷰] Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation\",\"children\":\"Ryan Rossi이 arXiv에 게시한 'Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-17 00:00:00+0900+0900\",\"children\":\"2026년 2월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-17-Benchmarking-Knowledge-Extraction-Attack-and-Defense-on-Retrieval-Augmented-Generation\"}]]}]]}],[\"$\",\"article\",\"2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub\",\"children\":\"[논문리뷰] AIDev: Studying AI Coding Agents on GitHub\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub\",\"children\":\"Ahmed E. Hassan이 arXiv에 게시한 'AIDev: Studying AI Coding Agents on GitHub' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-17 00:00:00+0900+0900\",\"children\":\"2026년 2월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-17-AIDev-Studying-AI-Coding-Agents-on-GitHub\"}]]}]]}],[\"$\",\"article\",\"2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models\",\"children\":\"[논문리뷰] BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models\",\"children\":\"arXiv에 게시된 'BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-16 00:00:00+0900+0900\",\"children\":\"2026년 2월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-16-BPDQ-Bit-Plane-Decomposition-Quantization-on-a-Variable-Grid-for-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning\",\"children\":\"[논문리뷰] Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-13 00:00:00+0900+0900\",\"children\":\"2026년 2월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-13-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments\",\"children\":\"[논문리뷰] LawThinker: A Deep Research Legal Agent in Dynamic Environments\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments\",\"children\":\"arXiv에 게시된 'LawThinker: A Deep Research Legal Agent in Dynamic Environments' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-13 00:00:00+0900+0900\",\"children\":\"2026년 2월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-13-LawThinker-A-Deep-Research-Legal-Agent-in-Dynamic-Environments\"}]]}]]}],[\"$\",\"article\",\"2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models\",\"children\":\"[논문리뷰] Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models\",\"children\":\"arXiv에 게시된 'Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-13 00:00:00+0900+0900\",\"children\":\"2026년 2월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-13-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-Towards-Autonomous-Mathematics-Research\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research\",\"children\":\"[논문리뷰] Towards Autonomous Mathematics Research\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research\",\"children\":\"arXiv에 게시된 'Towards Autonomous Mathematics Research' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-Towards-Autonomous-Mathematics-Research\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions\",\"children\":\"[논문리뷰] TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions\",\"children\":\"arXiv에 게시된 'TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-TimeChat-Captioner-Scripting-Multi-Scene-Videos-with-Time-Aware-and-Structural-Audio-Visual-Captions\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models\",\"children\":\"[논문리뷰] Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models\",\"children\":\"Zhen Fang이 arXiv에 게시한 'Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models\",\"children\":\"[논문리뷰] Free(): Learning to Forget in Malloc-Only Reasoning Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models\",\"children\":\"arXiv에 게시된 'Free(): Learning to Forget in Malloc-Only Reasoning Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-12 00:00:00+0900+0900\",\"children\":\"2026년 2월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-12-Free-Learning-to-Forget-in-Malloc-Only-Reasoning-Models\"}]]}]]}],[\"$\",\"article\",\"2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth\",\"children\":\"[논문리뷰] LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth\",\"children\":\"arXiv에 게시된 'LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-10 00:00:00+0900+0900\",\"children\":\"2026년 2월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-10-LOCA-bench-Benchmarking-Language-Agents-Under-Controllable-and-Extreme-Context-Growth\"}]]}]]}],[\"$\",\"article\",\"2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning\",\"children\":\"[논문리뷰] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-09 00:00:00+0900+0900\",\"children\":\"2026년 2월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-09-InftyThink-Effective-and-Efficient-Infinite-Horizon-Reasoning-via-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight\",\"children\":\"[논문리뷰] Steering LLMs via Scalable Interactive Oversight\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight\",\"children\":\"arXiv에 게시된 'Steering LLMs via Scalable Interactive Oversight' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-06-Steering-LLMs-via-Scalable-Interactive-Oversight\"}]]}]]}],[\"$\",\"article\",\"2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\",\"children\":\"[논문리뷰] Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\",\"children\":\"arXiv에 게시된 'Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-06-Retrieval-Infused-Reasoning-Sandbox-A-Benchmark-for-Decoupling-Retrieval-and-Reasoning-Capabilities\"}]]}]]}],[\"$\",\"article\",\"2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments\",\"children\":\"[논문리뷰] ProAct: Agentic Lookahead in Interactive Environments\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments\",\"children\":\"arXiv에 게시된 'ProAct: Agentic Lookahead in Interactive Environments' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-06-ProAct-Agentic-Lookahead-in-Interactive-Environments\"}]]}]]}],[\"$\",\"article\",\"2026-02-06-BABE-Biology-Arena-BEnchmark\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark\",\"children\":\"[논문리뷰] BABE: Biology Arena BEnchmark\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark\",\"children\":\"arXiv에 게시된 'BABE: Biology Arena BEnchmark' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-06-BABE-Biology-Arena-BEnchmark\"}]]}]]}],[\"$\",\"article\",\"2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning\",\"children\":\"[논문리뷰] WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-05 00:00:00+0900+0900\",\"children\":\"2026년 2월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-05-WideSeek-R1-Exploring-Width-Scaling-for-Broad-Information-Seeking-via-Multi-Agent-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning\",\"children\":\"[논문리뷰] Self-Hinting Language Models Enhance Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'Self-Hinting Language Models Enhance Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-05 00:00:00+0900+0900\",\"children\":\"2026년 2월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-05-Self-Hinting-Language-Models-Enhance-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR\",\"children\":\"[논문리뷰] PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR\",\"children\":\"Alejandro Lozano이 arXiv에 게시한 'PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-05 00:00:00+0900+0900\",\"children\":\"2026년 2월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-05-PaperSearchQA-Learning-to-Search-and-Reason-over-Scientific-Papers-with-RLVR\"}]]}]]}],[\"$\",\"article\",\"2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models\",\"children\":\"[논문리뷰] OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models\",\"children\":\"Yiyan Ji이 arXiv에 게시한 'OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-05 00:00:00+0900+0900\",\"children\":\"2026년 2월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-05-OmniSIFT-Modality-Asymmetric-Token-Compression-for-Efficient-Omni-modal-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy\",\"children\":\"[논문리뷰] SimpleGPT: Improving GPT via A Simple Normalization Strategy\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy\",\"children\":\"Rong Xiao이 arXiv에 게시한 'SimpleGPT: Improving GPT via A Simple Normalization Strategy' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-04 00:00:00+0900+0900\",\"children\":\"2026년 2월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-04-SimpleGPT-Improving-GPT-via-A-Simple-Normalization-Strategy\"}]]}]]}],[\"$\",\"article\",\"2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration\",\"children\":\"[논문리뷰] AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration\",\"children\":\"Zhaoyang Yu이 arXiv에 게시한 'AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-04 00:00:00+0900+0900\",\"children\":\"2026년 2월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-04-AOrchestra-Automating-Sub-Agent-Creation-for-Agentic-Orchestration\"}]]}]]}],[\"$\",\"article\",\"2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System\",\"children\":\"[논문리뷰] RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System\",\"children\":\"arXiv에 게시된 'RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-03 00:00:00+0900+0900\",\"children\":\"2026년 2월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-03-RLAnything-Forge-Environment-Policy-and-Reward-Model-in-Completely-Dynamic-RL-System\"}]]}]]}],[\"$\",\"article\",\"2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation\",\"children\":\"[논문리뷰] RM -RF: Reward Model for Run-Free Unit Test Evaluation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation\",\"children\":\"Vadim Alperovich이 arXiv에 게시한 'RM -RF: Reward Model for Run-Free Unit Test Evaluation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-02 00:00:00+0900+0900\",\"children\":\"2026년 2월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-02-RM-RF-Reward-Model-for-Run-Free-Unit-Test-Evaluation\"}]]}]]}],[\"$\",\"article\",\"2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning\",\"children\":\"[논문리뷰] MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning\",\"children\":\"Yuxin Chen이 arXiv에 게시한 'MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-02 00:00:00+0900+0900\",\"children\":\"2026년 2월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-02-MemOCR-Layout-Aware-Visual-Memory-for-Efficient-Long-Horizon-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience\",\"children\":\"[논문리뷰] Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience\",\"children\":\"arXiv에 게시된 'Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-02 00:00:00+0900+0900\",\"children\":\"2026년 2월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-02-02-Deep-Search-with-Hierarchical-Meta-Cognitive-Monitoring-Inspired-by-Cognitive-Neuroscience\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods\",\"children\":\"[논문리뷰] MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods\",\"children\":\"arXiv에 게시된 'MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-MMFineReason-Closing-the-Multimodal-Reasoning-Gap-via-Open-Data-Centric-Methods\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience\",\"children\":\"[논문리뷰] Language-based Trial and Error Falls Behind in the Era of Experience\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience\",\"children\":\"arXiv에 게시된 'Language-based Trial and Error Falls Behind in the Era of Experience' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-Language-based-Trial-and-Error-Falls-Behind-in-the-Era-of-Experience\"}]]}]]}],[\"$\",\"article\",\"2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories\",\"children\":\"[논문리뷰] Discovering Hidden Gems in Model Repositories\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories\",\"children\":\"Yedid Hoshen이 arXiv에 게시한 'Discovering Hidden Gems in Model Repositories' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-30 00:00:00+0900+0900\",\"children\":\"2026년 1월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-30-Discovering-Hidden-Gems-in-Model-Repositories\"}]]}]]}],[\"$\",\"article\",\"2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection\",\"children\":\"[논문리뷰] GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection\",\"children\":\"arXiv에 게시된 'GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-29 00:00:00+0900+0900\",\"children\":\"2026년 1월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-29-GDCNet-Generative-Discrepancy-Comparison-Network-for-Multimodal-Sarcasm-Detection\"}]]}]]}],[\"$\",\"article\",\"2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep\",\"children\":\"[논문리뷰] Post-LayerNorm Is Back: Stable, ExpressivE, and Deep\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep\",\"children\":\"arXiv에 게시된 'Post-LayerNorm Is Back: Stable, ExpressivE, and Deep' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-28 00:00:00+0900+0900\",\"children\":\"2026년 1월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep\"}]]}]]}],[\"$\",\"article\",\"2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering\",\"children\":\"[논문리뷰] daVinci-Dev: Agent-native Mid-training for Software Engineering\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering\",\"children\":\"arXiv에 게시된 'daVinci-Dev: Agent-native Mid-training for Software Engineering' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-27 00:00:00+0900+0900\",\"children\":\"2026년 1월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering\"}]]}]]}],[\"$\",\"article\",\"2026-01-27-VIBEVOICE-ASR-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report\",\"children\":\"[논문리뷰] VIBEVOICE-ASR Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report\",\"children\":\"arXiv에 게시된 'VIBEVOICE-ASR Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-27 00:00:00+0900+0900\",\"children\":\"2026년 1월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion\",\"children\":\"[논문리뷰] STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion\",\"children\":\"arXiv에 게시된 'STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-27 00:00:00+0900+0900\",\"children\":\"2026년 1월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-27-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion\"}]]}]]}],[\"$\",\"article\",\"2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences\",\"children\":\"[논문리뷰] MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences\",\"children\":\"Jianwen Sun이 arXiv에 게시한 'MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-26 00:00:00+0900+0900\",\"children\":\"2026년 1월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-26-MeepleLM-A-Virtual-Playtester-Simulating-Diverse-Subjective-Experiences\"}]]}]]}],[\"$\",\"article\",\"2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization\",\"children\":\"[논문리뷰] Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization\",\"children\":\"Gabriele Bavota이 arXiv에 게시한 'Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-26 00:00:00+0900+0900\",\"children\":\"2026년 1월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization\"}]]}]]}],[\"$\",\"article\",\"2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind\",\"children\":\"[논문리뷰] Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind\",\"children\":\"Yi R Fung이 arXiv에 게시한 'Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-26 00:00:00+0900+0900\",\"children\":\"2026년 1월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-26-Dancing-in-Chains-Strategic-Persuasion-in-Academic-Rebuttal-via-Theory-of-Mind\"}]]}]]}],[\"$\",\"article\",\"2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs\",\"children\":\"[논문리뷰] Towards Automated Kernel Generation in the Era of LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs\",\"children\":\"Yixin Shen이 arXiv에 게시한 'Towards Automated Kernel Generation in the Era of LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-23 00:00:00+0900+0900\",\"children\":\"2026년 1월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-23-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs\"}]]}]]}],[\"$\",\"article\",\"2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model\",\"children\":\"[논문리뷰] Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model\",\"children\":\"arXiv에 게시된 'Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-23 00:00:00+0900+0900\",\"children\":\"2026년 1월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-23-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model\"}]]}]]}],[\"$\",\"article\",\"2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models\",\"children\":\"[논문리뷰] Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models\",\"children\":\"arXiv에 게시된 'Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-22 00:00:00+0900+0900\",\"children\":\"2026년 1월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-22-Lost-in-the-Prompt-Order-Revealing-the-Limitations-of-Causal-Attention-in-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\",\"children\":\"[논문리뷰] ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\",\"children\":\"arXiv에 게시된 'ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-21 00:00:00+0900+0900\",\"children\":\"2026년 1월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-21-ToolPRMBench-Evaluating-and-Advancing-Process-Reward-Models-for-Tool-using-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing\",\"children\":\"[논문리뷰] On the Evidentiary Limits of Membership Inference for Copyright Auditing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing\",\"children\":\"Marten van Dijk이 arXiv에 게시한 'On the Evidentiary Limits of Membership Inference for Copyright Auditing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-21 00:00:00+0900+0900\",\"children\":\"2026년 1월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-21-On-the-Evidentiary-Limits-of-Membership-Inference-for-Copyright-Auditing\"}]]}]]}],[\"$\",\"article\",\"2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search\",\"children\":\"[논문리뷰] Agentic-R: Learning to Retrieve for Agentic Search\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search\",\"children\":\"Daiting Shi이 arXiv에 게시한 'Agentic-R: Learning to Retrieve for Agentic Search' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-21 00:00:00+0900+0900\",\"children\":\"2026년 1월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-21-Agentic-R-Learning-to-Retrieve-for-Agentic-Search\"}]]}]]}],[\"$\",\"article\",\"2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge\",\"children\":\"[논문리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge\",\"children\":\"arXiv에 게시된 'Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-20 00:00:00+0900+0900\",\"children\":\"2026년 1월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-20-Multiplex-Thinking-Reasoning-via-Token-wise-Branch-and-Merge\"}]]}]]}],[\"$\",\"article\",\"2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models\",\"children\":\"[논문리뷰] Language of Thought Shapes Output Diversity in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models\",\"children\":\"arXiv에 게시된 'Language of Thought Shapes Output Diversity in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-19 00:00:00+0900+0900\",\"children\":\"2026년 1월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-19-Language-of-Thought-Shapes-Output-Diversity-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning\",\"children\":\"[논문리뷰] Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning\",\"children\":\"arXiv에 게시된 'Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-16 00:00:00+0900+0900\",\"children\":\"2026년 1월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-16-Collaborative-Multi-Agent-Test-Time-Reinforcement-Learning-for-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning\",\"children\":\"[논문리뷰] Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning\",\"children\":\"arXiv에 게시된 'Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-16 00:00:00+0900+0900\",\"children\":\"2026년 1월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-16-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5\",\"children\":\"[논문리뷰] A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5\",\"children\":\"Yutao Wu이 arXiv에 게시한 'A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-16 00:00:00+0900+0900\",\"children\":\"2026년 1월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-16-A-Safety-Report-on-GPT-5-2-Gemini-3-Pro-Qwen3-VL-Doubao-1-8-Grok-4-1-Fast-Nano-Banana-Pro-and-Seedream-4-5\"}]]}]]}],[\"$\",\"article\",\"2026-01-15-TranslateGemma-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-TranslateGemma-Technical-Report\",\"children\":\"[논문리뷰] TranslateGemma Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-TranslateGemma-Technical-Report\",\"children\":\"arXiv에 게시된 'TranslateGemma Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-15 00:00:00+0900+0900\",\"children\":\"2026년 1월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-15-TranslateGemma-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization\",\"children\":\"[논문리뷰] Controlled Self-Evolution for Algorithmic Code Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization\",\"children\":\"arXiv에 게시된 'Controlled Self-Evolution for Algorithmic Code Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-15 00:00:00+0900+0900\",\"children\":\"2026년 1월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-15-Controlled-Self-Evolution-for-Algorithmic-Code-Optimization\"}]]}]]}],[\"$\",\"article\",\"2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity\",\"children\":\"[논문리뷰] Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity\",\"children\":\"Chi Zhang이 arXiv에 게시한 'Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-15 00:00:00+0900+0900\",\"children\":\"2026년 1월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-15-Are-LLMs-Vulnerable-to-Preference-Undermining-Attacks-PUA-A-Factorial-Analysis-Methodology-for-Diagnosing-the-Trade-off-between-Preference-Alignment-and-Real-World-Validity\"}]]}]]}],[\"$\",\"article\",\"2026-01-14-Solar-Open-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-14-Solar-Open-Technical-Report\",\"children\":\"[논문리뷰] Solar Open Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-14-Solar-Open-Technical-Report\",\"children\":\"arXiv에 게시된 'Solar Open Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-14 00:00:00+0900+0900\",\"children\":\"2026년 1월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-14-Solar-Open-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2026-01-14-Ministral-3\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-14-Ministral-3\",\"children\":\"[논문리뷰] Ministral 3\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-14-Ministral-3\",\"children\":\"arXiv에 게시된 'Ministral 3' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-14 00:00:00+0900+0900\",\"children\":\"2026년 1월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-14-Ministral-3\"}]]}]]}],[\"$\",\"article\",\"2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents\",\"children\":\"[논문리뷰] SmartSearch: Process Reward-Guided Query Refinement for Search Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents\",\"children\":\"Guanting Dong이 arXiv에 게시한 'SmartSearch: Process Reward-Guided Query Refinement for Search Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-12 00:00:00+0900+0900\",\"children\":\"2026년 1월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-12-SmartSearch-Process-Reward-Guided-Query-Refinement-for-Search-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers\",\"children\":\"[논문리뷰] Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers\",\"children\":\"arXiv에 게시된 'Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-09 00:00:00+0900+0900\",\"children\":\"2026년 1월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-09-Learnable-Multipliers-Freeing-the-Scale-of-Language-Model-Matrix-Layers\"}]]}]]}],[\"$\",\"article\",\"2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs\",\"children\":\"[논문리뷰] DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs\",\"children\":\"Jing Ma이 arXiv에 게시한 'DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-09 00:00:00+0900+0900\",\"children\":\"2026년 1월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-09-DiffCoT-Diffusion-styled-Chain-of-Thought-Reasoning-in-LLMs\"}]]}]]}],[\"$\",\"article\",\"2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics\",\"children\":\"[논문리뷰] MDAgent2: Large Language Model for Code Generation and Knowledge Q\u0026A in Molecular Dynamics\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics\",\"children\":\"arXiv에 게시된 'MDAgent2: Large Language Model for Code Generation and Knowledge Q\u0026A in Molecular Dynamics' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-08 00:00:00+0900+0900\",\"children\":\"2026년 1월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-08-MDAgent2-Large-Language-Model-for-Code-Generation-and-Knowledge-QA-in-Molecular-Dynamics\"}]]}]]}],[\"$\",\"article\",\"2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents\",\"children\":\"[논문리뷰] MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents\",\"children\":\"Bingzhe Li이 arXiv에 게시한 'MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-08 00:00:00+0900+0900\",\"children\":\"2026년 1월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-08-MAGMA-A-Multi-Graph-based-Agentic-Memory-Architecture-for-AI-Agents\"}]]}]]}],[\"$\",\"article\",\"2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\",\"children\":\"[논문리뷰] EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\",\"children\":\"Guanchen Wu이 arXiv에 게시한 'EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-08 00:00:00+0900+0900\",\"children\":\"2026년 1월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-08-EpiQAL-Benchmarking-Large-Language-Models-in-Epidemiological-Question-Answering-for-Enhanced-Alignment-and-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2026-01-06-Recursive-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-Recursive-Language-Models\",\"children\":\"[논문리뷰] Recursive Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-06-Recursive-Language-Models\",\"children\":\"arXiv에 게시된 'Recursive Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-06 00:00:00+0900+0900\",\"children\":\"2026년 1월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-06-Recursive-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction\",\"children\":\"[논문리뷰] Diversity or Precision? A Deep Dive into Next Token Prediction\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction\",\"children\":\"arXiv에 게시된 'Diversity or Precision? A Deep Dive into Next Token Prediction' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-05 00:00:00+0900+0900\",\"children\":\"2026년 1월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-05-Diversity-or-Precision-A-Deep-Dive-into-Next-Token-Prediction\"}]]}]]}],[\"$\",\"article\",\"2026-01-01-mHC-Manifold-Constrained-Hyper-Connections\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections\",\"children\":\"[논문리뷰] mHC: Manifold-Constrained Hyper-Connections\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections\",\"children\":\"arXiv에 게시된 'mHC: Manifold-Constrained Hyper-Connections' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-01 00:00:00+0900+0900\",\"children\":\"2026년 1월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-01-mHC-Manifold-Constrained-Hyper-Connections\"}]]}]]}],[\"$\",\"article\",\"2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem\",\"children\":\"[논문리뷰] Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem\",\"children\":\"Wei Gao이 arXiv에 게시한 'Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-01-01 00:00:00+0900+0900\",\"children\":\"2026년 1월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2026-01-01-Let-It-Flow-Agentic-Crafting-on-Rock-and-Roll-Building-the-ROME-Model-within-an-Open-Agentic-Learning-Ecosystem\"}]]}]]}],[\"$\",\"article\",\"2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization\",\"children\":\"[논문리뷰] GraphLocator: Graph-guided Causal Reasoning for Issue Localization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization\",\"children\":\"Wei Zhang이 arXiv에 게시한 'GraphLocator: Graph-guided Causal Reasoning for Issue Localization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-31 00:00:00+0900+0900\",\"children\":\"2025년 12월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-31-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization\"}]]}]]}],[\"$\",\"article\",\"2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\",\"children\":\"[논문리뷰] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\",\"children\":\"arXiv에 게시된 'SlideTailor: Personalized Presentation Slide Generation for Scientific Papers' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-29 00:00:00+0900+0900\",\"children\":\"2025년 12월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-29-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers\"}]]}]]}],[\"$\",\"article\",\"2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation\",\"children\":\"[논문리뷰] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation\",\"children\":\"arXiv에 게시된 'Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-24 00:00:00+0900+0900\",\"children\":\"2025년 12월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-24-Multi-LLM-Thematic-Analysis-with-Dual-Reliability-Metrics-Combining-Cohens-Kappa-and-Semantic-Similarity-for-Qualitative-Research-Validation\"}]]}]]}],[\"$\",\"article\",\"2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos\",\"children\":\"[논문리뷰] LongVideoAgent: Multi-Agent Reasoning with Long Videos\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos\",\"children\":\"Renjie Pi이 arXiv에 게시한 'LongVideoAgent: Multi-Agent Reasoning with Long Videos' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-24 00:00:00+0900+0900\",\"children\":\"2025년 12월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-24-LongVideoAgent-Multi-Agent-Reasoning-with-Long-Videos\"}]]}]]}],[\"$\",\"article\",\"2025-12-24-INTELLECT-3-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-INTELLECT-3-Technical-Report\",\"children\":\"[논문리뷰] INTELLECT-3: Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-INTELLECT-3-Technical-Report\",\"children\":\"arXiv에 게시된 'INTELLECT-3: Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-24 00:00:00+0900+0900\",\"children\":\"2025년 12월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-24-INTELLECT-3-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies\",\"children\":\"[논문리뷰] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies\",\"children\":\"arXiv에 게시된 'Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-24 00:00:00+0900+0900\",\"children\":\"2025년 12월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-24-Bottom-up-Policy-Optimization-Your-Language-Model-Policy-Secretly-Contains-Internal-Policies\"}]]}]]}],[\"$\",\"article\",\"2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction\",\"children\":\"[논문리뷰] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction\",\"children\":\"Hong Jiao이 arXiv에 게시한 'Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-23 00:00:00+0900+0900\",\"children\":\"2025년 12월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-23-Can-LLMs-Estimate-Student-Struggles-Human-AI-Difficulty-Alignment-with-Proficiency-Simulation-for-Item-Difficulty-Prediction\"}]]}]]}],[\"$\",\"article\",\"2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience\",\"children\":\"[논문리뷰] Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience\",\"children\":\"arXiv에 게시된 'Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-22 00:00:00+0900+0900\",\"children\":\"2025년 12월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-22-Seed-Prover-1-5-Mastering-Undergraduate-Level-Theorem-Proving-via-Learning-from-Experience\"}]]}]]}],[\"$\",\"article\",\"2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward\",\"children\":\"[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward\",\"children\":\"arXiv에 게시된 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-19 00:00:00+0900+0900\",\"children\":\"2025년 12월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward\"}]]}]]}],[\"$\",\"article\",\"2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning\",\"children\":\"[논문리뷰] SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-18 00:00:00+0900+0900\",\"children\":\"2025년 12월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-18-SAGE-Training-Smart-Any-Horizon-Agents-for-Long-Video-Reasoning-with-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning\",\"children\":\"[논문리뷰] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning\",\"children\":\"arXiv에 게시된 'Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-18 00:00:00+0900+0900\",\"children\":\"2025년 12월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-18-Can-LLMs-Guide-Their-Own-Exploration-Gradient-Guided-Reinforcement-Learning-for-LLM-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-12-17-RecGPT-V2-Technical-Report\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-17-RecGPT-V2-Technical-Report\",\"children\":\"[논문리뷰] RecGPT-V2 Technical Report\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-17-RecGPT-V2-Technical-Report\",\"children\":\"Dian Chen이 arXiv에 게시한 'RecGPT-V2 Technical Report' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-17 00:00:00+0900+0900\",\"children\":\"2025년 12월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-17-RecGPT-V2-Technical-Report\"}]]}]]}],[\"$\",\"article\",\"2025-12-17-Olmo-3\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-17-Olmo-3\",\"children\":\"[논문리뷰] Olmo 3\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-17-Olmo-3\",\"children\":\"arXiv에 게시된 'Olmo 3' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-17 00:00:00+0900+0900\",\"children\":\"2025년 12월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-17-Olmo-3\"}]]}]]}],[\"$\",\"article\",\"2025-12-15-Sliding-Window-Attention-Adaptation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation\",\"children\":\"[논문리뷰] Sliding Window Attention Adaptation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation\",\"children\":\"arXiv에 게시된 'Sliding Window Attention Adaptation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-15 00:00:00+0900+0900\",\"children\":\"2025년 12월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-15-Sliding-Window-Attention-Adaptation\"}]]}]]}],[\"$\",\"article\",\"2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing\",\"children\":\"[논문리뷰] EtCon: Edit-then-Consolidate for Reliable Knowledge Editing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing\",\"children\":\"Chenglin Li이 arXiv에 게시한 'EtCon: Edit-then-Consolidate for Reliable Knowledge Editing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-11 00:00:00+0900+0900\",\"children\":\"2025년 12월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-11-EtCon-Edit-then-Consolidate-for-Reliable-Knowledge-Editing\"}]]}]]}],[\"$\",\"article\",\"2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning\",\"children\":\"[논문리뷰] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning\",\"children\":\"Jiacheng Chen이 arXiv에 게시한 'Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-09 00:00:00+0900+0900\",\"children\":\"2025년 12월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-09-Beyond-Token-level-Supervision-Unlocking-the-Potential-of-Decoding-based-Regression-via-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows\",\"children\":\"[논문리뷰] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows\",\"children\":\"arXiv에 게시된 'TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-08 00:00:00+0900+0900\",\"children\":\"2025년 12월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows\"}]]}]]}],[\"$\",\"article\",\"2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\",\"children\":\"[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\",\"children\":\"Yang Li이 arXiv에 게시한 'From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-08 00:00:00+0900+0900\",\"children\":\"2025년 12월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning\",\"children\":\"[논문리뷰] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning\",\"children\":\"Zijia Lin이 arXiv에 게시한 'Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-08 00:00:00+0900+0900\",\"children\":\"2025년 12월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-12-04-PretrainZero-Reinforcement-Active-Pretraining\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining\",\"children\":\"[논문리뷰] PretrainZero: Reinforcement Active Pretraining\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining\",\"children\":\"Guoqi Li이 arXiv에 게시한 'PretrainZero: Reinforcement Active Pretraining' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-04 00:00:00+0900+0900\",\"children\":\"2025년 12월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-04-PretrainZero-Reinforcement-Active-Pretraining\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models\",\"children\":\"[논문리뷰] The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models\",\"children\":\"arXiv에 게시된 'The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-The-Curious-Case-of-Analogies-Investigating-Analogical-Reasoning-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models\",\"children\":\"[논문리뷰] DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models\",\"children\":\"arXiv에 게시된 'DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-DeepSeek-V3-2-Pushing-the-Frontier-of-Open-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models\",\"children\":\"[논문리뷰] C^2DLM: Causal Concept-Guided Diffusion Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models\",\"children\":\"Xinpeng Dong이 arXiv에 게시한 'C^2DLM: Causal Concept-Guided Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-03 00:00:00+0900+0900\",\"children\":\"2025년 12월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-03-C2DLM-Causal-Concept-Guided-Diffusion-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models\",\"children\":\"[논문리뷰] Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models\",\"children\":\"Mikhail Burtsev이 arXiv에 게시한 'Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-Wikontic-Constructing-Wikidata-Aligned-Ontology-Aware-Knowledge-Graphs-with-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models\",\"children\":\"[논문리뷰] PromptBridge: Cross-Model Prompt Transfer for Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models\",\"children\":\"Wei Wei이 arXiv에 게시한 'PromptBridge: Cross-Model Prompt Transfer for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-PromptBridge-Cross-Model-Prompt-Transfer-for-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion\",\"children\":\"[논문리뷰] OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion\",\"children\":\"arXiv에 게시된 'OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-OmniFusion-Simultaneous-Multilingual-Multimodal-Translations-via-Modular-Fusion\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks\",\"children\":\"[논문리뷰] Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks\",\"children\":\"arXiv에 게시된 'Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-Generalist-Large-Language-Models-Outperform-Clinical-Tools-on-Medical-Benchmarks\"}]]}]]}],[\"$\",\"article\",\"2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution\",\"children\":\"[논문리뷰] Agentic Policy Optimization via Instruction-Policy Co-Evolution\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution\",\"children\":\"arXiv에 게시된 'Agentic Policy Optimization via Instruction-Policy Co-Evolution' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-02 00:00:00+0900+0900\",\"children\":\"2025년 12월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-02-Agentic-Policy-Optimization-via-Instruction-Policy-Co-Evolution\"}]]}]]}],[\"$\",\"article\",\"2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models\",\"children\":\"[논문리뷰] Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models\",\"children\":\"Wei Wu이 arXiv에 게시한 'Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-12-01 00:00:00+0900+0900\",\"children\":\"2025년 12월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-12-01-Every-Token-Counts-Generalizing-16M-Ultra-Long-Context-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-28-What-does-it-mean-to-understand-language\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-28-What-does-it-mean-to-understand-language\",\"children\":\"[논문리뷰] What does it mean to understand language?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-28-What-does-it-mean-to-understand-language\",\"children\":\"arXiv에 게시된 'What does it mean to understand language?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-28 00:00:00+0900+0900\",\"children\":\"2025년 11월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-28-What-does-it-mean-to-understand-language\"}]]}]]}],[\"$\",\"article\",\"2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems\",\"children\":\"[논문리뷰] Latent Collaboration in Multi-Agent Systems\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems\",\"children\":\"arXiv에 게시된 'Latent Collaboration in Multi-Agent Systems' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-27 00:00:00+0900+0900\",\"children\":\"2025년 11월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-27-Latent-Collaboration-in-Multi-Agent-Systems\"}]]}]]}],[\"$\",\"article\",\"2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion\",\"children\":\"[논문리뷰] Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion\",\"children\":\"Zhifei Yang이 arXiv에 게시한 'Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-26 00:00:00+0900+0900\",\"children\":\"2025년 11월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion\"}]]}]]}],[\"$\",\"article\",\"2025-11-26-Soft-Adaptive-Policy-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization\",\"children\":\"[논문리뷰] Soft Adaptive Policy Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization\",\"children\":\"arXiv에 게시된 'Soft Adaptive Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-26 00:00:00+0900+0900\",\"children\":\"2025년 11월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization\"}]]}]]}],[\"$\",\"article\",\"2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System\",\"children\":\"[논문리뷰] SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System\",\"children\":\"arXiv에 게시된 'SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-26 00:00:00+0900+0900\",\"children\":\"2025년 11월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System\"}]]}]]}],[\"$\",\"article\",\"2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking\",\"children\":\"[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking\",\"children\":\"Elias Stengel-Eskin이 arXiv에 게시한 'PRInTS: Reward Modeling for Long-Horizon Information Seeking' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-25 00:00:00+0900+0900\",\"children\":\"2025년 11월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking\"}]]}]]}],[\"$\",\"article\",\"2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser\",\"children\":\"[논문리뷰] AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser\",\"children\":\"arXiv에 게시된 'AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-25 00:00:00+0900+0900\",\"children\":\"2025년 11월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-25-AICC-Parse-HTML-Finer-Make-Models-Better-A-7-3T-AI-Ready-Corpus-Built-by-a-Model-Based-HTML-Parser\"}]]}]]}],[\"$\",\"article\",\"2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries\",\"children\":\"[논문리뷰] ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries\",\"children\":\"arXiv에 게시된 'ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-20 00:00:00+0900+0900\",\"children\":\"2025년 11월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-20-ARC-Chapter-Structuring-Hour-Long-Videos-into-Navigable-Chapters-and-Hierarchical-Summaries\"}]]}]]}],[\"$\",\"article\",\"2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models\",\"children\":\"[논문리뷰] OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models\",\"children\":\"Jian liu이 arXiv에 게시한 'OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-19 00:00:00+0900+0900\",\"children\":\"2025년 11월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-19-OmniZip-Audio-Guided-Dynamic-Token-Compression-for-Fast-Omnimodal-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models\",\"children\":\"[논문리뷰] Mitigating Label Length Bias in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models\",\"children\":\"Katharina von der Wense이 arXiv에 게시한 'Mitigating Label Length Bias in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-19 00:00:00+0900+0900\",\"children\":\"2025년 11월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-19-Mitigating-Label-Length-Bias-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost\",\"children\":\"[논문리뷰] LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost\",\"children\":\"Kengo Tajiri이 arXiv에 게시한 'LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-19 00:00:00+0900+0900\",\"children\":\"2025년 11월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-19-LLM-Powered-Fully-Automated-Chaos-Engineering-Towards-Enabling-Anyone-to-Build-Resilient-Software-Systems-at-Low-Cost\"}]]}]]}],[\"$\",\"article\",\"2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance\",\"children\":\"[논문리뷰] Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance\",\"children\":\"arXiv에 게시된 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-18 00:00:00+0900+0900\",\"children\":\"2025년 11월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-18-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance\"}]]}]]}],[\"$\",\"article\",\"2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning\",\"children\":\"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning\",\"children\":\"Haiyuan Wan이 arXiv에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-18 00:00:00+0900+0900\",\"children\":\"2025년 11월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling\",\"children\":\"[논문리뷰] MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling\",\"children\":\"cyyang822이 arXiv에 게시한 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-18 00:00:00+0900+0900\",\"children\":\"2025년 11월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-18-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling\"}]]}]]}],[\"$\",\"article\",\"2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing\",\"children\":\"[논문리뷰] AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing\",\"children\":\"Hongyu Lin이 arXiv에 게시한 'AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-18 00:00:00+0900+0900\",\"children\":\"2025년 11월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-18-AI-Salesman-Towards-Reliable-Large-Language-Model-Driven-Telemarketing\"}]]}]]}],[\"$\",\"article\",\"2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain\",\"children\":\"[논문리뷰] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain\",\"children\":\"Meng Jiang이 arXiv에 게시한 'A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-18 00:00:00+0900+0900\",\"children\":\"2025년 11월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-18-A-Decentralized-Retrieval-Augmented-Generation-System-with-Source-Reliabilities-Secured-on-Blockchain\"}]]}]]}],[\"$\",\"article\",\"2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward\",\"children\":\"[논문리뷰] miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward\",\"children\":\"Farzan Farnia이 arXiv에 게시한 'miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-17 00:00:00+0900+0900\",\"children\":\"2025년 11월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-17-miniF2F-Lean-Revisited-Reviewing-Limitations-and-Charting-a-Path-Forward\"}]]}]]}],[\"$\",\"article\",\"2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey\",\"children\":\"[논문리뷰] Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey\",\"children\":\"Mohammad Hossein Rohban이 arXiv에 게시한 'Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-17 00:00:00+0900+0900\",\"children\":\"2025년 11월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-17-Large-Language-Models-for-Scientific-Idea-Generation-A-Creativity-Centered-Survey\"}]]}]]}],[\"$\",\"article\",\"2025-11-17-DoPE-Denoising-Rotary-Position-Embedding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding\",\"children\":\"[논문리뷰] DoPE: Denoising Rotary Position Embedding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding\",\"children\":\"Min Yang이 arXiv에 게시한 'DoPE: Denoising Rotary Position Embedding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-17 00:00:00+0900+0900\",\"children\":\"2025년 11월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-17-DoPE-Denoising-Rotary-Position-Embedding\"}]]}]]}],[\"$\",\"article\",\"2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training\",\"children\":\"[논문리뷰] Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training\",\"children\":\"suayptalha이 arXiv에 게시한 'Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-14 00:00:00+0900+0900\",\"children\":\"2025년 11월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-14-Superpositional-Gradient-Descent-Harnessing-Quantum-Principles-for-Model-Training\"}]]}]]}],[\"$\",\"article\",\"2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis\",\"children\":\"[논문리뷰] CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis\",\"children\":\"Jian Wu이 arXiv에 게시한 'CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-14 00:00:00+0900+0900\",\"children\":\"2025년 11월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-14-CC30k-A-Citation-Contexts-Dataset-for-Reproducibility-Oriented-Sentiment-Analysis\"}]]}]]}],[\"$\",\"article\",\"2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents\",\"children\":\"[논문리뷰] Agentic Refactoring: An Empirical Study of AI Coding Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents\",\"children\":\"Hajimu Iida이 arXiv에 게시한 'Agentic Refactoring: An Empirical Study of AI Coding Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-13 00:00:00+0900+0900\",\"children\":\"2025년 11월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-13-Agentic-Refactoring-An-Empirical-Study-of-AI-Coding-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora\",\"children\":\"[논문리뷰] Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora\",\"children\":\"Mohamed Motasim Hamed이 arXiv에 게시한 'Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-Wasm-A-Pipeline-for-Constructing-Structured-Arabic-Interleaved-Multimodal-Corpora\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective\",\"children\":\"[논문리뷰] Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective\",\"children\":\"Christoph Treude이 arXiv에 게시한 'Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals\",\"children\":\"[논문리뷰] The Path Not Taken: RLVR Provably Learns Off the Principals\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals\",\"children\":\"arXiv에 게시된 'The Path Not Taken: RLVR Provably Learns Off the Principals' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-The-Path-Not-Taken-RLVR-Provably-Learns-Off-the-Principals\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration\",\"children\":\"[논문리뷰] Optimizing Diversity and Quality through Base-Aligned Model Collaboration\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration\",\"children\":\"Jonathan May이 arXiv에 게시한 'Optimizing Diversity and Quality through Base-Aligned Model Collaboration' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-Optimizing-Diversity-and-Quality-through-Base-Aligned-Model-Collaboration\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces\",\"children\":\"[논문리뷰] DynaAct: Large Language Model Reasoning with Dynamic Action Spaces\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces\",\"children\":\"Lingpeng Kong이 arXiv에 게시한 'DynaAct: Large Language Model Reasoning with Dynamic Action Spaces' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-DynaAct-Large-Language-Model-Reasoning-with-Dynamic-Action-Spaces\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs\",\"children\":\"[논문리뷰] Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs\",\"children\":\"arXiv에 게시된 'Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-Beyond-English-Toward-Inclusive-and-Scalable-Multilingual-Machine-Translation-with-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems\",\"children\":\"[논문리뷰] Adaptive Multi-Agent Response Refinement in Conversational Systems\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems\",\"children\":\"arXiv에 게시된 'Adaptive Multi-Agent Response Refinement in Conversational Systems' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-12 00:00:00+0900+0900\",\"children\":\"2025년 11월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-12-Adaptive-Multi-Agent-Response-Refinement-in-Conversational-Systems\"}]]}]]}],[\"$\",\"article\",\"2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models\",\"children\":\"[논문리뷰] VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models\",\"children\":\"arXiv에 게시된 'VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-11 00:00:00+0900+0900\",\"children\":\"2025년 11월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-11-VADER-Towards-Causal-Video-Anomaly-Understanding-with-Relation-Aware-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery\",\"children\":\"[논문리뷰] The Station: An Open-World Environment for AI-Driven Discovery\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery\",\"children\":\"wydu이 arXiv에 게시한 'The Station: An Open-World Environment for AI-Driven Discovery' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-11 00:00:00+0900+0900\",\"children\":\"2025년 11월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-11-The-Station-An-Open-World-Environment-for-AI-Driven-Discovery\"}]]}]]}],[\"$\",\"article\",\"2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs\",\"children\":\"[논문리뷰] Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs\",\"children\":\"arXiv에 게시된 'Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-11 00:00:00+0900+0900\",\"children\":\"2025년 11월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-11-Reinforcement-Learning-Improves-Traversal-of-Hierarchical-Knowledge-in-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models\",\"children\":\"[논문리뷰] Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models\",\"children\":\"arXiv에 게시된 'Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-11 00:00:00+0900+0900\",\"children\":\"2025년 11월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-11-Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling\",\"children\":\"[논문리뷰] NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling\",\"children\":\"arXiv에 게시된 'NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-11 00:00:00+0900+0900\",\"children\":\"2025년 11월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling\"}]]}]]}],[\"$\",\"article\",\"2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning\",\"children\":\"[논문리뷰] Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning\",\"children\":\"arXiv에 게시된 'Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-11 00:00:00+0900+0900\",\"children\":\"2025년 11월 11일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-11-Do-LLMs-Feel-Teaching-Emotion-Recognition-with-Prompts-Retrieval-and-Curriculum-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks\",\"children\":\"[논문리뷰] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks\",\"children\":\"arXiv에 게시된 'VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-10 00:00:00+0900+0900\",\"children\":\"2025년 11월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-10-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks\"}]]}]]}],[\"$\",\"article\",\"2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent\",\"children\":\"[논문리뷰] HAFixAgent: History-Aware Automated Program Repair Agent\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent\",\"children\":\"Ahmed E. Hassan이 arXiv에 게시한 'HAFixAgent: History-Aware Automated Program Repair Agent' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-10 00:00:00+0900+0900\",\"children\":\"2025년 11월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-10-HAFixAgent-History-Aware-Automated-Program-Repair-Agent\"}]]}]]}],[\"$\",\"article\",\"2025-11-10-Dense-Motion-Captioning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-Dense-Motion-Captioning\",\"children\":\"[논문리뷰] Dense Motion Captioning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-10-Dense-Motion-Captioning\",\"children\":\"Paolo Rota이 arXiv에 게시한 'Dense Motion Captioning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-10 00:00:00+0900+0900\",\"children\":\"2025년 11월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-10-Dense-Motion-Captioning\"}]]}]]}],[\"$\",\"article\",\"2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask\",\"children\":\"[논문리뷰] Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask\",\"children\":\"arXiv에 게시된 'Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 21:54:30+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-6-Grounded-Misunderstandings-in-Asymmetric-Dialogue-A-Perspectivist-Annotation-Scheme-for-MapTask\"}]]}]]}],[\"$\",\"article\",\"2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs\",\"children\":\"[논문리뷰] Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs\",\"children\":\"Bo Bai이 arXiv에 게시한 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:35:02+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-5-Forget-BIT-It-is-All-about-TOKEN-Towards-Semantic-Information-Theory-for-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner\",\"children\":\"[논문리뷰] OpenSIR: Open-Ended Self-Improving Reasoner\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner\",\"children\":\"arXiv에 게시된 'OpenSIR: Open-Ended Self-Improving Reasoner' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:22:42+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-4-OpenSIR-Open-Ended-Self-Improving-Reasoner\"}]]}]]}],[\"$\",\"article\",\"2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation\",\"children\":\"[논문리뷰] Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation\",\"children\":\"arXiv에 게시된 'Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:22:42+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-4-Every-Activation-Boosted-Scaling-General-Reasoner-to-1-Trillion-Open-Language-Foundation\"}]]}]]}],[\"$\",\"article\",\"2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning\",\"children\":\"[논문리뷰] Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-11-09 19:01:31+0900\",\"children\":\"2025년 11월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-11-3-Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis\",\"children\":\"[논문리뷰] EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis\",\"children\":\"arXiv에 게시된 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-EHR-R1-A-Reasoning-Enhanced-Foundational-Language-Model-for-Electronic-Health-Record-Analysis\"}]]}]]}],[\"$\",\"article\",\"2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\",\"children\":\"[논문리뷰] Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\",\"children\":\"Justin Cui이 arXiv에 게시한 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-31 18:37:31+0900\",\"children\":\"2025년 10월 31일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-31-Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling\",\"children\":\"[논문리뷰] TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling\",\"children\":\"Zheng Zhang이 arXiv에 게시한 'TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-TheraMind-A-Strategic-and-Adaptive-Agent-for-Longitudinal-Psychological-Counseling\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining\",\"children\":\"[논문리뷰] Reasoning-Aware GRPO using Process Mining\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining\",\"children\":\"arXiv에 게시된 'Reasoning-Aware GRPO using Process Mining' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-Reasoning-Aware-GRPO-using-Process-Mining\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization\",\"children\":\"[논문리뷰] ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization\",\"children\":\"Ruihua Song이 arXiv에 게시한 'ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-ReForm-Reflective-Autoformalization-with-Prospective-Bounded-Sequence-Optimization\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling\",\"children\":\"[논문리뷰] Parallel Loop Transformer for Efficient Test-Time Computation Scaling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling\",\"children\":\"arXiv에 게시된 'Parallel Loop Transformer for Efficient Test-Time Computation Scaling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-Parallel-Loop-Transformer-for-Efficient-Test-Time-Computation-Scaling\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence\",\"children\":\"[논문리뷰] JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence\",\"children\":\"arXiv에 게시된 'JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-JanusCoder-Towards-a-Foundational-Visual-Programmatic-Interface-for-Code-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning\",\"children\":\"[논문리뷰] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning\",\"children\":\"Xin Liu이 arXiv에 게시한 'FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-30 13:06:06+0900\",\"children\":\"2025년 10월 30일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-30-FAPO-Flawed-Aware-Policy-Optimization-for-Efficient-and-Reliable-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents\",\"children\":\"[논문리뷰] VisCoder2: Building Multi-Language Visualization Coding Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents\",\"children\":\"arXiv에 게시된 'VisCoder2: Building Multi-Language Visualization Coding Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-29 13:11:02+0900\",\"children\":\"2025년 10월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-29-VisCoder2-Building-Multi-Language-Visualization-Coding-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers\",\"children\":\"[논문리뷰] ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers\",\"children\":\"Ian L. V. Roque이 arXiv에 게시한 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-29 13:11:02+0900\",\"children\":\"2025년 10월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-29-ReplicationBench-Can-AI-Agents-Replicate-Astrophysics-Research-Papers\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation\",\"children\":\"[논문리뷰] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation\",\"children\":\"arXiv에 게시된 'The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-The-Best-of-N-Worlds-Aligning-Reinforcement-Learning-with-Best-of-N-Sampling-via-maxk-Optimisation\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\",\"children\":\"[논문리뷰] Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\",\"children\":\"arXiv에 게시된 'Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-Mitigating-Attention-Sinks-and-Massive-Activations-in-Audio-Visual-Speech-Recognition-with-LLMS\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking\",\"children\":\"[논문리뷰] LimRank: Less is More for Reasoning-Intensive Information Reranking\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking\",\"children\":\"Arman Cohan이 arXiv에 게시한 'LimRank: Less is More for Reasoning-Intensive Information Reranking' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-LimRank-Less-is-More-for-Reasoning-Intensive-Information-Reranking\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-Knocking-Heads-Attention\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Knocking-Heads-Attention\",\"children\":\"[논문리뷰] Knocking-Heads Attention\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Knocking-Heads-Attention\",\"children\":\"Jianguo Li이 arXiv에 게시한 'Knocking-Heads Attention' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-Knocking-Heads-Attention\"}]]}]]}],[\"$\",\"article\",\"2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback\",\"children\":\"[논문리뷰] Code Aesthetics with Agentic Reward Feedback\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback\",\"children\":\"Yupan Huang이 arXiv에 게시한 'Code Aesthetics with Agentic Reward Feedback' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-28 13:07:54+0900\",\"children\":\"2025년 10월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-28-Code-Aesthetics-with-Agentic-Reward-Feedback\"}]]}]]}],[\"$\",\"article\",\"2025-10-27-Soft-Instruction-De-escalation-Defense\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense\",\"children\":\"[논문리뷰] Soft Instruction De-escalation Defense\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense\",\"children\":\"arXiv에 게시된 'Soft Instruction De-escalation Defense' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-27 13:07:36+0900\",\"children\":\"2025년 10월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-27-Soft-Instruction-De-escalation-Defense\"}]]}]]}],[\"$\",\"article\",\"2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory\",\"children\":\"[논문리뷰] Document Understanding, Measurement, and Manipulation Using Category Theory\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory\",\"children\":\"arXiv에 게시된 'Document Understanding, Measurement, and Manipulation Using Category Theory' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-27 13:07:36+0900\",\"children\":\"2025년 10월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-27-Document-Understanding-Measurement-and-Manipulation-Using-Category-Theory\"}]]}]]}],[\"$\",\"article\",\"2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets\",\"children\":\"[논문리뷰] DeepAgent: A General Reasoning Agent with Scalable Toolsets\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets\",\"children\":\"Jiajie Jin이 arXiv에 게시한 'DeepAgent: A General Reasoning Agent with Scalable Toolsets' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-27 13:07:36+0900\",\"children\":\"2025년 10월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-27-DeepAgent-A-General-Reasoning-Agent-with-Scalable-Toolsets\"}]]}]]}],[\"$\",\"article\",\"2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models\",\"children\":\"[논문리뷰] ARC-Encoder: learning compressed text representations for large language models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models\",\"children\":\"arXiv에 게시된 'ARC-Encoder: learning compressed text representations for large language models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-27 13:07:36+0900\",\"children\":\"2025년 10월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-27-ARC-Encoder-learning-compressed-text-representations-for-large-language-models\"}]]}]]}],[\"$\",\"article\",\"2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks\",\"children\":\"[논문리뷰] Machine Text Detectors are Membership Inference Attacks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks\",\"children\":\"Naoaki Okazaki이 arXiv에 게시한 'Machine Text Detectors are Membership Inference Attacks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-23 13:08:59+0900\",\"children\":\"2025년 10월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-23-Machine-Text-Detectors-are-Membership-Inference-Attacks\"}]]}]]}],[\"$\",\"article\",\"2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts\",\"children\":\"[논문리뷰] LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts\",\"children\":\"arXiv에 게시된 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-23 13:08:59+0900\",\"children\":\"2025년 10월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-23-LoongRLReinforcement-Learning-for-Advanced-Reasoning-over-Long-Contexts\"}]]}]]}],[\"$\",\"article\",\"2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping\",\"children\":\"[논문리뷰] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping\",\"children\":\"Junrui Shen이 arXiv에 게시한 'BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-23 13:08:59+0900\",\"children\":\"2025년 10월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-23-BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\",\"children\":\"[논문리뷰] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\",\"children\":\"Yujie Zhou이 arXiv에 게시한 'UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-UniGenBench-A-Unified-Semantic-Evaluation-Benchmark-for-Text-to-Image-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold\",\"children\":\"[논문리뷰] PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold\",\"children\":\"arXiv에 게시된 'PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-Extracting-alignment-data-in-open-models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models\",\"children\":\"[논문리뷰] Extracting alignment data in open models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models\",\"children\":\"arXiv에 게시된 'Extracting alignment data in open models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-Extracting-alignment-data-in-open-models\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning\",\"children\":\"[논문리뷰] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning\",\"children\":\"Qipeng Guo이 arXiv에 게시한 'EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-EvoSyn-Generalizable-Evolutionary-Data-Synthesis-for-Verifiable-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist\",\"children\":\"[논문리뷰] Chem-R: Learning to Reason as a Chemist\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist\",\"children\":\"arXiv에 게시된 'Chem-R: Learning to Reason as a Chemist' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-22 13:07:20+0900\",\"children\":\"2025년 10월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-22-Chem-R-Learning-to-Reason-as-a-Chemist\"}]]}]]}],[\"$\",\"article\",\"2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive\",\"children\":\"[논문리뷰] Paper2Web: Let's Make Your Paper Alive!\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive\",\"children\":\"Yao Wan이 arXiv에 게시한 'Paper2Web: Let's Make Your Paper Alive!' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-20 13:04:24+0900\",\"children\":\"2025년 10월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-20-Paper2Web-Lets-Make-Your-Paper-Alive\"}]]}]]}],[\"$\",\"article\",\"2025-10-20-Language-Models-Model-Language\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-Language-Models-Model-Language\",\"children\":\"[논문리뷰] Language Models Model Language\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-Language-Models-Model-Language\",\"children\":\"arXiv에 게시된 'Language Models Model Language' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-20 13:04:24+0900\",\"children\":\"2025년 10월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-20-Language-Models-Model-Language\"}]]}]]}],[\"$\",\"article\",\"2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning\",\"children\":\"[논문리뷰] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-20 13:04:24+0900\",\"children\":\"2025년 10월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-20-DLER-Doing-Length-pEnalty-Right-Incentivizing-More-Intelligence-per-Token-via-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification\",\"children\":\"[논문리뷰] VLA-0: Building State-of-the-Art VLAs with Zero Modification\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification\",\"children\":\"arXiv에 게시된 'VLA-0: Building State-of-the-Art VLAs with Zero Modification' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-17 13:09:57+0900\",\"children\":\"2025년 10월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-17-VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification\"}]]}]]}],[\"$\",\"article\",\"2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models\",\"children\":\"[논문리뷰] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models\",\"children\":\"arXiv에 게시된 'The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-17 13:09:57+0900\",\"children\":\"2025년 10월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-17-The-German-Commons-154-Billion-Tokens-of-Openly-Licensed-Text-for-German-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems\",\"children\":\"[논문리뷰] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems\",\"children\":\"arXiv에 게시된 'RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-17 13:09:57+0900\",\"children\":\"2025년 10월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-17-RAGCap-Bench-Benchmarking-Capabilities-of-LLMs-in-Agentic-Retrieval-Augmented-Generation-Systems\"}]]}]]}],[\"$\",\"article\",\"2025-10-17-LLM-guided-Hierarchical-Retrieval\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval\",\"children\":\"[논문리뷰] LLM-guided Hierarchical Retrieval\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval\",\"children\":\"arXiv에 게시된 'LLM-guided Hierarchical Retrieval' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-17 13:09:57+0900\",\"children\":\"2025년 10월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-17-LLM-guided-Hierarchical-Retrieval\"}]]}]]}],[\"$\",\"article\",\"2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization\",\"children\":\"[논문리뷰] Agentic Entropy-Balanced Policy Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization\",\"children\":\"arXiv에 게시된 'Agentic Entropy-Balanced Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-17 13:09:57+0900\",\"children\":\"2025년 10월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-17-Agentic-Entropy-Balanced-Policy-Optimization\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning\",\"children\":\"[논문리뷰] Revisiting Model Interpolation for Efficient Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning\",\"children\":\"arXiv에 게시된 'Revisiting Model Interpolation for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-Revisiting-Model-Interpolation-for-Efficient-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training\",\"children\":\"[논문리뷰] MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training\",\"children\":\"arXiv에 게시된 'MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-MTSQL-R1-Towards-Long-Horizon-Multi-Turn-Text-to-SQL-via-Agentic-Training\"}]]}]]}],[\"$\",\"article\",\"2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain\",\"children\":\"[논문리뷰] Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain\",\"children\":\"Lingxi Lu이 arXiv에 게시한 'Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-16 13:09:51+0900\",\"children\":\"2025년 10월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-16-Hierarchical-Frequency-Tagging-Probe-HFTP-A-Unified-Approach-to-Investigate-Syntactic-Structure-Representations-in-Large-Language-Models-and-the-Human-Brain\"}]]}]]}],[\"$\",\"article\",\"2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks\",\"children\":\"[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks\",\"children\":\"Xueyuan Lin이 arXiv에 게시한 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-15 13:01:40+0900\",\"children\":\"2025년 10월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-15-Memory-as-Action-Autonomous-Context-Curation-for-Long-Horizon-Agentic-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation\",\"children\":\"[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation\",\"children\":\"arXiv에 게시된 'Information-Preserving Reformulation of Reasoning Traces for Antidistillation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-15 13:01:40+0900\",\"children\":\"2025년 10월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-15-Information-Preserving-Reformulation-of-Reasoning-Traces-for-Antidistillation\"}]]}]]}],[\"$\",\"article\",\"2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models\",\"children\":\"[논문리뷰] A Survey of Vibe Coding with Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models\",\"children\":\"arXiv에 게시된 'A Survey of Vibe Coding with Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-15 13:01:40+0900\",\"children\":\"2025년 10월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-15-A-Survey-of-Vibe-Coding-with-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review\",\"children\":\"[논문리뷰] ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review\",\"children\":\"Christopher Pal이 arXiv에 게시한 'ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-ReviewerToo-Should-AI-Join-The-Program-Committee-A-Look-At-The-Future-of-Peer-Review\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare\",\"children\":\"[논문리뷰] GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare\",\"children\":\"arXiv에 게시된 'GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting\",\"children\":\"[논문리뷰] Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting\",\"children\":\"Julia Kempe이 arXiv에 게시한 'Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-Dont-Waste-Mistakes-Leveraging-Negative-RL-Groups-via-Confidence-Reweighting\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation\",\"children\":\"[논문리뷰] DISCO: Diversifying Sample Condensation for Efficient Model Evaluation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation\",\"children\":\"arXiv에 게시된 'DISCO: Diversifying Sample Condensation for Efficient Model Evaluation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-DISCO-Diversifying-Sample-Condensation-for-Efficient-Model-Evaluation\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution\",\"children\":\"[논문리뷰] BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution\",\"children\":\"Hange Liu이 arXiv에 게시한 'BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-BigCodeArena-Unveiling-More-Reliable-Human-Preferences-in-Code-Generation-via-Execution\"}]]}]]}],[\"$\",\"article\",\"2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\",\"children\":\"[논문리뷰] AutoPR: Let's Automate Your Academic Promotion!\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\",\"children\":\"Yixin Yuan이 arXiv에 게시한 'AutoPR: Let's Automate Your Academic Promotion!' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-13 13:44:18+0900\",\"children\":\"2025년 10월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-13-AutoPR-Lets-Automate-Your-Academic-Promotion\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\",\"children\":\"[논문리뷰] UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\",\"children\":\"arXiv에 게시된 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-UNIDOC-BENCH-A-Unified-Benchmark-for-Document-Centric-Multimodal-RAG\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models\",\"children\":\"[논문리뷰] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models\",\"children\":\"James Cheng이 arXiv에 게시한 'Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training\",\"children\":\"[논문리뷰] Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training\",\"children\":\"Peng Cheng이 arXiv에 게시한 'Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-Recycling-Pretrained-Checkpoints-Orthogonal-Growth-of-Mixture-of-Experts-for-Efficient-Large-Language-Model-Pre-Training\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens\",\"children\":\"[논문리뷰] Memory Retrieval and Consolidation in Large Language Models through Function Tokens\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens\",\"children\":\"arXiv에 게시된 'Memory Retrieval and Consolidation in Large Language Models through Function Tokens' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-Memory-Retrieval-and-Consolidation-in-Large-Language-Models-through-Function-Tokens\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning\",\"children\":\"[논문리뷰] From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning\",\"children\":\"Feiwei Qin이 arXiv에 게시한 'From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-From-What-to-Why-A-Multi-Agent-System-for-Evidence-based-Chemical-Reaction-Condition-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints\",\"children\":\"[논문리뷰] Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints\",\"children\":\"Huazhe Xu이 arXiv에 게시한 'Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-Entropy-Regularizing-Activation-Boosting-Continuous-Control-Large-Language-Models-and-Image-Classification-with-Activation-as-Entropy-Constraints\"}]]}]]}],[\"$\",\"article\",\"2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning\",\"children\":\"[논문리뷰] A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'A^2Search: Ambiguity-Aware Question Answering with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-10-A2Search-Ambiguity-Aware-Question-Answering-with-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference\",\"children\":\"[논문리뷰] Vibe Checker: Aligning Code Evaluation with Human Preference\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference\",\"children\":\"arXiv에 게시된 'Vibe Checker: Aligning Code Evaluation with Human Preference' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-09 13:45:06+0900\",\"children\":\"2025년 10월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-9-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference\"}]]}]]}],[\"$\",\"article\",\"2025-10-9-The-Markovian-Thinker\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-The-Markovian-Thinker\",\"children\":\"[논문리뷰] The Markovian Thinker\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-The-Markovian-Thinker\",\"children\":\"arXiv에 게시된 'The Markovian Thinker' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-09 13:45:06+0900\",\"children\":\"2025년 10월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-9-The-Markovian-Thinker\"}]]}]]}],[\"$\",\"article\",\"2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents\",\"children\":\"[논문리뷰] DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents\",\"children\":\"arXiv에 게시된 'DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-09 13:45:06+0900\",\"children\":\"2025년 10월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-9-DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models\",\"children\":\"[논문리뷰] Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models\",\"children\":\"arXiv에 게시된 'Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-09 13:45:06+0900\",\"children\":\"2025년 10월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-9-Beyond-Monolingual-Assumptions-A-Survey-of-Code-Switched-NLP-in-the-Era-of-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness\",\"children\":\"[논문리뷰] Training Dynamics Impact Post-Training Quantization Robustness\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness\",\"children\":\"Jonas Geiping이 arXiv에 게시한 'Training Dynamics Impact Post-Training Quantization Robustness' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-08 13:48:12+0900\",\"children\":\"2025년 10월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-8-Training-Dynamics-Impact-Post-Training-Quantization-Robustness\"}]]}]]}],[\"$\",\"article\",\"2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning\",\"children\":\"[논문리뷰] TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning\",\"children\":\"arXiv에 게시된 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-08 13:48:12+0900\",\"children\":\"2025년 10월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-8-TaTToo-Tool-Grounded-Thinking-PRM-for-Test-Time-Scaling-in-Tabular-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization\",\"children\":\"[논문리뷰] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization\",\"children\":\"sirano1004이 arXiv에 게시한 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-08 13:48:12+0900\",\"children\":\"2025년 10월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-8-Margin-Adaptive-DPO-Leveraging-Reward-Model-for-Granular-Control-in-Preference-Optimization\"}]]}]]}],[\"$\",\"article\",\"2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation\",\"children\":\"[논문리뷰] CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation\",\"children\":\"arXiv에 게시된 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-08 13:48:12+0900\",\"children\":\"2025년 10월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-8-CARE-Cognitive-reasoning-Augmented-Reinforcement-for-Emotional-Support-Conversation\"}]]}]]}],[\"$\",\"article\",\"2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization\",\"children\":\"[논문리뷰] ASPO: Asymmetric Importance Sampling Policy Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization\",\"children\":\"Xiu Li이 arXiv에 게시한 'ASPO: Asymmetric Importance Sampling Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-08 13:48:12+0900\",\"children\":\"2025년 10월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-8-ASPO-Asymmetric-Importance-Sampling-Policy-Optimization\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos\",\"children\":\"[논문리뷰] Watch and Learn: Learning to Use Computers from Online Videos\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos\",\"children\":\"Oriana Riva이 arXiv에 게시한 'Watch and Learn: Learning to Use Computers from Online Videos' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Watch-and-Learn-Learning-to-Use-Computers-from-Online-Videos\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Self-Reflective-Generation-at-Test-Time\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time\",\"children\":\"[논문리뷰] Self-Reflective Generation at Test Time\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time\",\"children\":\"Shuang Qiu이 arXiv에 게시한 'Self-Reflective Generation at Test Time' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Self-Reflective-Generation-at-Test-Time\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Optimal-Scaling-Needs-Optimal-Norm\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm\",\"children\":\"[논문리뷰] Optimal Scaling Needs Optimal Norm\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm\",\"children\":\"Stefan Kesselheim이 arXiv에 게시한 'Optimal Scaling Needs Optimal Norm' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Optimal-Scaling-Needs-Optimal-Norm\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\",\"children\":\"[논문리뷰] MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\",\"children\":\"arXiv에 게시된 'MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-MoME-Mixture-of-Matryoshka-Experts-for-Audio-Visual-Speech-Recognition\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning\",\"children\":\"[논문리뷰] Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning\",\"children\":\"arXiv에 게시된 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Learning-on-the-Job-Test-Time-Curricula-for-Targeted-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions\",\"children\":\"[논문리뷰] Judging with Confidence: Calibrating Autoraters to Preference Distributions\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions\",\"children\":\"arXiv에 게시된 'Judging with Confidence: Calibrating Autoraters to Preference Distributions' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Judging-with-Confidence-Calibrating-Autoraters-to-Preference-Distributions\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models\",\"children\":\"[논문리뷰] Imperceptible Jailbreaking against Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models\",\"children\":\"arXiv에 게시된 'Imperceptible Jailbreaking against Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Imperceptible-Jailbreaking-against-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data\",\"children\":\"[논문리뷰] Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data\",\"children\":\"arXiv에 게시된 'Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Front-Loading-Reasoning-The-Synergy-between-Pretraining-and-Post-Training-Data\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty\",\"children\":\"[논문리뷰] EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty\",\"children\":\"Xuanwu Wang이 arXiv에 게시한 'EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-EvolProver-Advancing-Automated-Theorem-Proving-by-Evolving-Formalized-Problems-via-Symmetry-and-Difficulty\"}]]}]]}],[\"$\",\"article\",\"2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models\",\"children\":\"[논문리뷰] Epistemic Diversity and Knowledge Collapse in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models\",\"children\":\"arXiv에 게시된 'Epistemic Diversity and Knowledge Collapse in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-07 13:36:57+0900\",\"children\":\"2025년 10월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-7-Epistemic-Diversity-and-Knowledge-Collapse-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\",\"children\":\"[논문리뷰] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\",\"children\":\"Neil Zhenqiang Gong이 arXiv에 게시한 'WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-06 13:29:11+0900\",\"children\":\"2025년 10월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-6-WAInjectBench-Benchmarking-Prompt-Injection-Detections-for-Web-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models\",\"children\":\"[논문리뷰] On Predictability of Reinforcement Learning Dynamics for Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models\",\"children\":\"Yuqing Huang이 arXiv에 게시한 'On Predictability of Reinforcement Learning Dynamics for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-02 13:30:22+0900\",\"children\":\"2025년 10월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-2-On-Predictability-of-Reinforcement-Learning-Dynamics-for-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents\",\"children\":\"[논문리뷰] Infusing Theory of Mind into Socially Intelligent LLM Agents\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents\",\"children\":\"arXiv에 게시된 'Infusing Theory of Mind into Socially Intelligent LLM Agents' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-02 13:30:22+0900\",\"children\":\"2025년 10월 2일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-2-Infusing-Theory-of-Mind-into-Socially-Intelligent-LLM-Agents\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain\",\"children\":\"[논문리뷰] The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain\",\"children\":\"arXiv에 게시된 'The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs\",\"children\":\"[논문리뷰] Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs\",\"children\":\"Yao Shu이 arXiv에 게시한 'Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-Test-Time-Policy-Adaptation-for-Enhanced-Multi-Turn-Interactions-with-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs\",\"children\":\"[논문리뷰] Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs\",\"children\":\"normanpaulsen이 arXiv에 게시한 'Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-Context-Is-What-You-Need-The-Maximum-Effective-Context-Window-for-Real-World-Limits-of-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective\",\"children\":\"[논문리뷰] Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective\",\"children\":\"arXiv에 게시된 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-Benefits-and-Pitfalls-of-Reinforcement-Learning-for-Language-Model-Planning-A-Theoretical-Perspective\"}]]}]]}],[\"$\",\"article\",\"2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models\",\"children\":\"[논문리뷰] Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models\",\"children\":\"arXiv에 게시된 'Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-01 14:04:08+0900\",\"children\":\"2025년 10월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-10-1-Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning\",\"children\":\"[논문리뷰] PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning\",\"children\":\"Lingpeng Kong이 arXiv에 게시한 'PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-29 13:47:46+0900\",\"children\":\"2025년 9월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-29-PromptCoT-2-0-Scaling-Prompt-Synthesis-for-Large-Language-Model-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning\",\"children\":\"[논문리뷰] MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning\",\"children\":\"Weipeng Zhong이 arXiv에 게시한 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-29 13:47:46+0900\",\"children\":\"2025년 9월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-29-MesaTask-Towards-Task-Driven-Tabletop-Scene-Generation-via-3D-Spatial-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards\",\"children\":\"[논문리뷰] Language Models Can Learn from Verbal Feedback Without Scalar Rewards\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards\",\"children\":\"arXiv에 게시된 'Language Models Can Learn from Verbal Feedback Without Scalar Rewards' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-29 13:47:46+0900\",\"children\":\"2025년 9월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-29-Language-Models-Can-Learn-from-Verbal-Feedback-Without-Scalar-Rewards\"}]]}]]}],[\"$\",\"article\",\"2025-9-29-Fine-tuning-Done-Right-in-Model-Editing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing\",\"children\":\"[논문리뷰] Fine-tuning Done Right in Model Editing\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing\",\"children\":\"Du Su이 arXiv에 게시한 'Fine-tuning Done Right in Model Editing' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-29 13:47:46+0900\",\"children\":\"2025년 9월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-29-Fine-tuning-Done-Right-in-Model-Editing\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"children\":\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"children\":\"Yuewei Zhang이 arXiv에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them\",\"children\":\"[논문리뷰] TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them\",\"children\":\"Zhuohao Yu이 arXiv에 게시한 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\",\"children\":\"[논문리뷰] StyleBench: Evaluating thinking styles in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\",\"children\":\"Javad Lavaei이 arXiv에 게시한 'StyleBench: Evaluating thinking styles in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-StyleBench-Evaluating-thinking-styles-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands\",\"children\":\"[논문리뷰] Interactive Recommendation Agent with Active User Commands\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands\",\"children\":\"Xueyang Feng이 arXiv에 게시한 'Interactive Recommendation Agent with Active User Commands' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-Interactive-Recommendation-Agent-with-Active-User-Commands\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning\",\"children\":\"[논문리뷰] CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning\",\"children\":\"Wenping Hu이 arXiv에 게시한 'CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-CE-GPPO-Controlling-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information\",\"children\":\"[논문리뷰] Behind RoPE: How Does Causal Mask Encode Positional Information?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information\",\"children\":\"Yeyun Gong이 arXiv에 게시한 'Behind RoPE: How Does Causal Mask Encode Positional Information?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-26-Behind-RoPE-How-Does-Causal-Mask-Encode-Positional-Information\"}]]}]]}],[\"$\",\"article\",\"2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub\",\"children\":\"[논문리뷰] On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub\",\"children\":\"Hajimu Iida이 arXiv에 게시한 'On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-25 13:08:16+0900\",\"children\":\"2025년 9월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-25-On-the-Use-of-Agentic-Coding-An-Empirical-Study-of-Pull-Requests-on-GitHub\"}]]}]]}],[\"$\",\"article\",\"2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines\",\"children\":\"[논문리뷰] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines\",\"children\":\"Yanfang이 arXiv에 게시한 'LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-25 13:08:16+0900\",\"children\":\"2025년 9월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-25-LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines\"}]]}]]}],[\"$\",\"article\",\"2025-9-24-Reinforcement-Learning-on-Pre-Training-Data\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data\",\"children\":\"[논문리뷰] Reinforcement Learning on Pre-Training Data\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data\",\"children\":\"Evander Yang이 arXiv에 게시한 'Reinforcement Learning on Pre-Training Data' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-24 13:14:19+0900\",\"children\":\"2025년 9월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-24-Reinforcement-Learning-on-Pre-Training-Data\"}]]}]]}],[\"$\",\"article\",\"2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects\",\"children\":\"[논문리뷰] Large Language Models Discriminate Against Speakers of German Dialects\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects\",\"children\":\"Katharina von der Wense이 arXiv에 게시한 'Large Language Models Discriminate Against Speakers of German Dialects' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-24 13:14:19+0900\",\"children\":\"2025년 9월 24일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-24-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications\",\"children\":\"[논문리뷰] Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications\",\"children\":\"Fatma Betül Terzioğlu이 arXiv에 게시한 'Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-Turk-LettuceDetect-A-Hallucination-Detection-Models-for-Turkish-RAG-Applications\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning\",\"children\":\"[논문리뷰] SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning\",\"children\":\"Zhaopeng Tu이 arXiv에 게시한 'SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-SCAN-Self-Denoising-Monte-Carlo-Annotation-for-Robust-Process-Reward-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-LIMI-Less-is-More-for-Agency\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency\",\"children\":\"[논문리뷰] LIMI: Less is More for Agency\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency\",\"children\":\"happyZYM이 arXiv에 게시한 'LIMI: Less is More for Agency' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-LIMI-Less-is-More-for-Agency\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context\",\"children\":\"[논문리뷰] DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context\",\"children\":\"Maunendra Sankar Desarkar이 arXiv에 게시한 'DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-DIWALI-Diversity-and-Inclusivity-aWare-cuLture-specific-Items-for-India-Dataset-and-Assessment-of-LLMs-for-Cultural-Text-Adaptation-in-Indian-Context\"}]]}]]}],[\"$\",\"article\",\"2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\",\"children\":\"[논문리뷰] AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\",\"children\":\"Jaeho Lee이 arXiv에 게시한 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-23 13:36:03+0900\",\"children\":\"2025년 9월 23일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-23-AuditoryBench-Can-Language-Models-Understand-Auditory-Knowledge-without-Hearing\"}]]}]]}],[\"$\",\"article\",\"2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning\",\"children\":\"[논문리뷰] FlowRL: Matching Reward Distributions for LLM Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning\",\"children\":\"Hengli Li이 arXiv에 게시한 'FlowRL: Matching Reward Distributions for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-19 13:12:21+0900\",\"children\":\"2025년 9월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-19-FlowRL-Matching-Reward-Distributions-for-LLM-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning\",\"children\":\"[논문리뷰] THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning\",\"children\":\"Yicheng Pan이 arXiv에 게시한 'THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-18 13:07:00+0900\",\"children\":\"2025년 9월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-18-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale\",\"children\":\"[논문리뷰] Hala Technical Report: Building Arabic-Centric Instruction \u0026 Translation Models at Scale\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale\",\"children\":\"Bernard Ghanem이 arXiv에 게시한 'Hala Technical Report: Building Arabic-Centric Instruction \u0026 Translation Models at Scale' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-18 13:07:00+0900\",\"children\":\"2025년 9월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-18-Hala-Technical-Report-Building-Arabic-Centric-Instruction-Translation-Models-at-Scale\"}]]}]]}],[\"$\",\"article\",\"2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling\",\"children\":\"[논문리뷰] Towards General Agentic Intelligence via Environment Scaling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling\",\"children\":\"Guangyu Li이 arXiv에 게시한 'Towards General Agentic Intelligence via Environment Scaling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-17 13:16:01+0900\",\"children\":\"2025년 9월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling\"}]]}]]}],[\"$\",\"article\",\"2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge\",\"children\":\"[논문리뷰] Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge\",\"children\":\"Wentao Zhang이 arXiv에 게시한 'Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-17 13:16:01+0900\",\"children\":\"2025년 9월 17일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-17-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge\"}]]}]]}],[\"$\",\"article\",\"2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning\",\"children\":\"[논문리뷰] UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning\",\"children\":\"Yongliang Shen이 arXiv에 게시한 'UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-16 13:16:41+0900\",\"children\":\"2025년 9월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-16-UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI\",\"children\":\"[논문리뷰] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI\",\"children\":\"UVSKKR이 arXiv에 게시한 'EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-16 13:16:41+0900\",\"children\":\"2025년 9월 16일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-16-EthicsMH-A-Pilot-Benchmark-for-Ethical-Reasoning-in-Mental-Health-AI\"}]]}]]}],[\"$\",\"article\",\"2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs\",\"children\":\"[논문리뷰] The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs\",\"children\":\"Jonas Geiping이 arXiv에 게시한 'The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-15 13:12:08+0900\",\"children\":\"2025년 9월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-15-The-Illusion-of-Diminishing-Returns-Measuring-Long-Horizon-Execution-in-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading\",\"children\":\"[논문리뷰] QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading\",\"children\":\"Chenyu You이 arXiv에 게시한 'QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-15 13:12:08+0900\",\"children\":\"2025년 9월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-15-QuantAgent-Price-Driven-Multi-Agent-LLMs-for-High-Frequency-Trading\"}]]}]]}],[\"$\",\"article\",\"2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\",\"children\":\"[논문리뷰] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\",\"children\":\"Dipanjan Das이 arXiv에 게시한 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-10 13:11:01+0900\",\"children\":\"2025년 9월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-10-SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge\"}]]}]]}],[\"$\",\"article\",\"2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning\",\"children\":\"[논문리뷰] Parallel-R1: Towards Parallel Thinking via Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning\",\"children\":\"Xinyu Yang이 arXiv에 게시한 'Parallel-R1: Towards Parallel Thinking via Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-10 13:11:01+0900\",\"children\":\"2025년 9월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-10-Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-9-10-Language-Self-Play-For-Data-Free-Training\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training\",\"children\":\"[논문리뷰] Language Self-Play For Data-Free Training\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training\",\"children\":\"Vijai Mohan이 arXiv에 게시한 'Language Self-Play For Data-Free Training' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-10 13:11:01+0900\",\"children\":\"2025년 9월 10일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-10-Language-Self-Play-For-Data-Free-Training\"}]]}]]}],[\"$\",\"article\",\"2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet\",\"children\":\"[논문리뷰] Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet\",\"children\":\"See-Kiong Ng이 arXiv에 게시한 'Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-09 13:19:09+0900\",\"children\":\"2025년 9월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-9-Test-Time-Scaling-in-Reasoning-Models-Is-Not-Effective-for-Knowledge-Intensive-Tasks-Yet\"}]]}]]}],[\"$\",\"article\",\"2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models\",\"children\":\"[논문리뷰] Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models\",\"children\":\"Ke Shen이 arXiv에 게시한 'Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-09 13:19:09+0900\",\"children\":\"2025년 9월 9일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-9-Revolutionizing-Reinforcement-Learning-Framework-for-Diffusion-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models\",\"children\":\"[논문리뷰] Symbolic Graphics Programming with Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models\",\"children\":\"Kaipeng Zhang이 arXiv에 게시한 'Symbolic Graphics Programming with Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-08 13:10:18+0900\",\"children\":\"2025년 9월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-8-Symbolic-Graphics-Programming-with-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models\",\"children\":\"[논문리뷰] Behavioral Fingerprinting of Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models\",\"children\":\"Xing Li이 arXiv에 게시한 'Behavioral Fingerprinting of Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-08 13:10:18+0900\",\"children\":\"2025년 9월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-8-Behavioral-Fingerprinting-of-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth\",\"children\":\"[논문리뷰] Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth\",\"children\":\"Chi-Li Chen이 arXiv에 게시한 'Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-05 13:07:20+0900\",\"children\":\"2025년 9월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-5-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth\"}]]}]]}],[\"$\",\"article\",\"2025-9-4-Open-Data-Synthesis-For-Deep-Research\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research\",\"children\":\"[논문리뷰] Open Data Synthesis For Deep Research\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research\",\"children\":\"Zheng Liu이 arXiv에 게시한 'Open Data Synthesis For Deep Research' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-04 12:56:15+0900\",\"children\":\"2025년 9월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-4-Open-Data-Synthesis-For-Deep-Research\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use\",\"children\":\"[논문리뷰] VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use\",\"children\":\"Zhiheng Lyu이 arXiv에 게시한 'VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-VerlTool-Towards-Holistic-Agentic-Reinforcement-Learning-with-Tool-Use\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey\",\"children\":\"[논문리뷰] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey\",\"children\":\"Hejia Geng이 arXiv에 게시한 'The Landscape of Agentic Reinforcement Learning for LLMs: A Survey' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-The-Landscape-of-Agentic-Reinforcement-Learning-for-LLMs-A-Survey\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning\",\"children\":\"[논문리뷰] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning\",\"children\":\"Qian Liu이 arXiv에 게시한 'SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction\",\"children\":\"[논문리뷰] SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction\",\"children\":\"bindsch이 arXiv에 게시한 'SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-SQL-of-Thought-Multi-agentic-Text-to-SQL-with-Guided-Error-Correction\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning\",\"children\":\"[논문리뷰] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning\",\"children\":\"Zirui Wang이 arXiv에 게시한 'OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-OpenVision-2-A-Family-of-Generative-Pretrained-Visual-Encoders-for-Multimodal-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR\",\"children\":\"[논문리뷰] Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR\",\"children\":\"Lu Wang이 arXiv에 게시한 'Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them\",\"children\":\"[논문리뷰] Fantastic Pretraining Optimizers and Where to Find Them\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them\",\"children\":\"Percy Liang이 arXiv에 게시한 'Fantastic Pretraining Optimizers and Where to Find Them' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-Fantastic-Pretraining-Optimizers-and-Where-to-Find-Them\"}]]}]]}],[\"$\",\"article\",\"2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models\",\"children\":\"[논문리뷰] AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models\",\"children\":\"Rahul Karthikeyan이 arXiv에 게시한 'AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-03 13:36:21+0900\",\"children\":\"2025년 9월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-3-AMBEDKAR-A-Multi-level-Bias-Elimination-through-a-Decoding-Approach-with-Knowledge-Augmentation-for-Robust-Constitutional-Alignment-of-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models\",\"children\":\"[논문리뷰] Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models\",\"children\":\"Yifan Lu이 arXiv에 게시한 'Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-01 13:14:34+0900\",\"children\":\"2025년 9월 1일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-9-1-Think-in-Games-Learning-to-Reason-in-Games-via-Reinforcement-Learning-with-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning\",\"children\":\"[논문리뷰] TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning\",\"children\":\"Simin Ma이 arXiv에 게시한 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-29 13:14:44+0900\",\"children\":\"2025년 8월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-29-TCIA-A-Task-Centric-Instruction-Augmentation-Method-for-Instruction-Finetuning\"}]]}]]}],[\"$\",\"article\",\"2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models\",\"children\":\"[논문리뷰] Provable Benefits of In-Tool Learning for Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models\",\"children\":\"Vivien Cabannes이 arXiv에 게시한 'Provable Benefits of In-Tool Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-29 13:14:44+0900\",\"children\":\"2025년 8월 29일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-29-Provable-Benefits-of-In-Tool-Learning-for-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling\",\"children\":\"[논문리뷰] Predicting the Order of Upcoming Tokens Improves Language Modeling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling\",\"children\":\"Alham Fikri Aji이 arXiv에 게시한 'Predicting the Order of Upcoming Tokens Improves Language Modeling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-28 13:10:39+0900\",\"children\":\"2025년 8월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-28-Predicting-the-Order-of-Upcoming-Tokens-Improves-Language-Modeling\"}]]}]]}],[\"$\",\"article\",\"2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\",\"children\":\"[논문리뷰] AudioStory: Generating Long-Form Narrative Audio with Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\",\"children\":\"Yixiao Ge이 arXiv에 게시한 'AudioStory: Generating Long-Form Narrative Audio with Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-28 13:10:39+0900\",\"children\":\"2025년 8월 28일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-28-AudioStory-Generating-Long-Form-Narrative-Audio-with-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities\",\"children\":\"[논문리뷰] Unraveling the cognitive patterns of Large Language Models through module communities\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities\",\"children\":\"Jianxi Gao이 arXiv에 게시한 'Unraveling the cognitive patterns of Large Language Models through module communities' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-Unraveling-the-cognitive-patterns-of-Large-Language-Models-through-module-communities\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling\",\"children\":\"[논문리뷰] TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling\",\"children\":\"Zhoufutu Wen이 arXiv에 게시한 'TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modeling\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting\",\"children\":\"[논문리뷰] QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting\",\"children\":\"Manuela Veloso이 arXiv에 게시한 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-QueryBandits-for-Hallucination-Mitigation-Exploiting-Semantic-Features-for-No-Regret-Rewriting\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks\",\"children\":\"[논문리뷰] Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks\",\"children\":\"Daisuke Nohara이 arXiv에 게시한 'Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-Optimal-Sparsity-of-Mixture-of-Experts-Language-Models-for-Reasoning-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning\",\"children\":\"[논문리뷰] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning\",\"children\":\"Arman Cohan이 arXiv에 게시한 'Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-Demystifying-Scientific-Problem-Solving-in-LLMs-by-Probing-Knowledge-and-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation\",\"children\":\"[논문리뷰] ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation\",\"children\":\"Kun Kuang이 arXiv에 게시한 'ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generation\"}]]}]]}],[\"$\",\"article\",\"2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\",\"children\":\"[논문리뷰] CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\",\"children\":\"Dongchen Huang이 arXiv에 게시한 'CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-27 13:22:18+0900\",\"children\":\"2025년 8월 27일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-27-CMPhysBench-A-Benchmark-for-Evaluating-Large-Language-Models-in-Condensed-Matter-Physics\"}]]}]]}],[\"$\",\"article\",\"2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\",\"children\":\"[논문리뷰] Explain Before You Answer: A Survey on Compositional Visual Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\",\"children\":\"Xin Zheng이 arXiv에 게시한 'Explain Before You Answer: A Survey on Compositional Visual Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-26 13:21:57+0900\",\"children\":\"2025년 8월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-26-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning\",\"children\":\"[논문리뷰] Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning\",\"children\":\"Jiale Zhao이 arXiv에 게시한 'Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-26 13:21:57+0900\",\"children\":\"2025년 8월 26일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-26-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning\",\"children\":\"[논문리뷰] End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning\",\"children\":\"Pengcheng Qiu이 arXiv에 게시한 'End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-25 13:13:07+0900\",\"children\":\"2025년 8월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-25-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR\",\"children\":\"[논문리뷰] Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR\",\"children\":\"Ying Nian Wu이 arXiv에 게시한 'Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-25 13:13:07+0900\",\"children\":\"2025년 8월 25일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-25-Beyond-Pass1-Self-Play-with-Variational-Problem-Synthesis-Sustains-RLVR\"}]]}]]}],[\"$\",\"article\",\"2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation\",\"children\":\"[논문리뷰] Mobile-Agent-v3: Foundamental Agents for GUI Automation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation\",\"children\":\"Haowei Liu이 arXiv에 게시한 'Mobile-Agent-v3: Foundamental Agents for GUI Automation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-22 13:10:52+0900\",\"children\":\"2025년 8월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-22-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation\"}]]}]]}],[\"$\",\"article\",\"2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models\",\"children\":\"[논문리뷰] Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models\",\"children\":\"Lifan Guo이 arXiv에 게시한 'Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-22 13:10:52+0900\",\"children\":\"2025년 8월 22일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-22-Fin-PRM-A-Domain-Specialized-Process-Reward-Model-for-Financial-Reasoning-in-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs\",\"children\":\"[논문리뷰] Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs\",\"children\":\"Haobo Xu이 arXiv에 게시한 'Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-21 13:15:28+0900\",\"children\":\"2025년 8월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-21-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs\"}]]}]]}],[\"$\",\"article\",\"2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting\",\"children\":\"[논문리뷰] On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting\",\"children\":\"Guoyin Wang이 arXiv에 게시한 'On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-21 13:15:28+0900\",\"children\":\"2025년 8월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-21-On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting\"}]]}]]}],[\"$\",\"article\",\"2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers\",\"children\":\"[논문리뷰] MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers\",\"children\":\"Prathyusha Jwalapuram이 arXiv에 게시한 'MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-21 13:15:28+0900\",\"children\":\"2025년 8월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-21-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers\"}]]}]]}],[\"$\",\"article\",\"2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery\",\"children\":\"[논문리뷰] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery\",\"children\":\"zijieqiu이 arXiv에 게시한 'From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-21 13:15:28+0900\",\"children\":\"2025년 8월 21일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-21-From-AI-for-Science-to-Agentic-Science-A-Survey-on-Autonomous-Scientific-Discovery\"}]]}]]}],[\"$\",\"article\",\"2025-8-20-Prompt-Orchestration-Markup-Language\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language\",\"children\":\"[논문리뷰] Prompt Orchestration Markup Language\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language\",\"children\":\"Yuqing Yang이 arXiv에 게시한 'Prompt Orchestration Markup Language' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-20 13:26:54+0900\",\"children\":\"2025년 8월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-20-Prompt-Orchestration-Markup-Language\"}]]}]]}],[\"$\",\"article\",\"2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding\",\"children\":\"[논문리뷰] Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding\",\"children\":\"Alina Landowska이 arXiv에 게시한 'Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-20 13:26:54+0900\",\"children\":\"2025년 8월 20일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-20-Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMs-Moral-Values-Understanding\"}]]}]]}],[\"$\",\"article\",\"2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models\",\"children\":\"[논문리뷰] Speed Always Wins: A Survey on Efficient Architectures for Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models\",\"children\":\"Jusen Du이 arXiv에 게시한 'Speed Always Wins: A Survey on Efficient Architectures for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-19 13:15:01+0900\",\"children\":\"2025년 8월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-19-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-19-Reinforcement-Learning-with-Rubric-Anchors\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors\",\"children\":\"[논문리뷰] Reinforcement Learning with Rubric Anchors\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors\",\"children\":\"Haokai Xu이 arXiv에 게시한 'Reinforcement Learning with Rubric Anchors' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-19 13:15:01+0900\",\"children\":\"2025년 8월 19일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-19-Reinforcement-Learning-with-Rubric-Anchors\"}]]}]]}],[\"$\",\"article\",\"2025-8-18-SSRL-Self-Search-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning\",\"children\":\"[논문리뷰] SSRL: Self-Search Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning\",\"children\":\"Yanxu Chen이 arXiv에 게시한 'SSRL: Self-Search Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-18 13:14:38+0900\",\"children\":\"2025년 8월 18일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-18-SSRL-Self-Search-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models\",\"children\":\"[논문리뷰] Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models\",\"children\":\"Qinghao Ye이 arXiv에 게시한 'Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-15 13:09:31+0900\",\"children\":\"2025년 8월 15일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-15-Passk-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery\",\"children\":\"[논문리뷰] Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery\",\"children\":\"Di Zhang이 arXiv에 게시한 'Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-14 13:19:02+0900\",\"children\":\"2025년 8월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-14-Mol-R1-Towards-Explicit-Long-CoT-Reasoning-in-Molecule-Discovery\"}]]}]]}],[\"$\",\"article\",\"2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models\",\"children\":\"[논문리뷰] Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models\",\"children\":\"Guiyang Hou이 arXiv에 게시한 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-14 13:19:02+0900\",\"children\":\"2025년 8월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-14-Cooper-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models\"}]]}]]}],[\"$\",\"article\",\"2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study\",\"children\":\"[논문리뷰] Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study\",\"children\":\"Gjergji Kasneci이 arXiv에 게시한 'Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-14 13:19:02+0900\",\"children\":\"2025년 8월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-14-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study\"}]]}]]}],[\"$\",\"article\",\"2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance\",\"children\":\"[논문리뷰] AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance\",\"children\":\"Yong Li이 arXiv에 게시한 'AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-14 13:19:02+0900\",\"children\":\"2025년 8월 14일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-14-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance\"}]]}]]}],[\"$\",\"article\",\"2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning\",\"children\":\"[논문리뷰] Train Long, Think Short: Curriculum Learning for Efficient Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning\",\"children\":\"Marzyeh Ghassemi이 arXiv에 게시한 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-13 13:29:23+0900\",\"children\":\"2025년 8월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-13-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy\",\"children\":\"[논문리뷰] Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy\",\"children\":\"Elizabeth Karpinski이 arXiv에 게시한 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-13 13:29:23+0900\",\"children\":\"2025년 8월 13일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-13-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy\"}]]}]]}],[\"$\",\"article\",\"2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability\",\"children\":\"[논문리뷰] ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability\",\"children\":\"Yuchen Li이 arXiv에 게시한 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-12 13:29:09+0900\",\"children\":\"2025년 8월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-12-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability\"}]]}]]}],[\"$\",\"article\",\"2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning\",\"children\":\"[논문리뷰] Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning\",\"children\":\"Jiaheng Liu이 arXiv에 게시한 'Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-12 13:29:09+0900\",\"children\":\"2025년 8월 12일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-12-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling\",\"children\":\"[논문리뷰] Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling\",\"children\":\"Ruolin Shen이 arXiv에 게시한 'Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-Visual-Document-Understanding-and-Question-Answering-A-Multi-Agent-Collaboration-Framework-with-Test-Time-Scaling\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data\",\"children\":\"[논문리뷰] R-Zero: Self-Evolving Reasoning LLM from Zero Data\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data\",\"children\":\"Zongxia Li이 arXiv에 게시한 'R-Zero: Self-Evolving Reasoning LLM from Zero Data' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction\",\"children\":\"[논문리뷰] PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction\",\"children\":\"Prajit Das이 arXiv에 게시한 'PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-PRvL-Quantifying-the-Capabilities-and-Risks-of-Large-Language-Models-for-PII-Redaction\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking\",\"children\":\"[논문리뷰] I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking\",\"children\":\"Chao Wang이 arXiv에 게시한 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-I2CR-Intra-and-Inter-modal-Collaborative-Reflections-for-Multimodal-Entity-Linking\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis\",\"children\":\"[논문리뷰] Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis\",\"children\":\"Reshmi Ghosh이 arXiv에 게시한 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-Hop-Skip-and-Overthink-Diagnosing-Why-Reasoning-Models-Fumble-during-Multi-Hop-Analysis\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation\",\"children\":\"[논문리뷰] Evaluating, Synthesizing, and Enhancing for Customer Support Conversation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation\",\"children\":\"Feng Chen이 arXiv에 게시한 'Evaluating, Synthesizing, and Enhancing for Customer Support Conversation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-Evaluating-Synthesizing-and-Enhancing-for-Customer-Support-Conversation\"}]]}]]}],[\"$\",\"article\",\"2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts\",\"children\":\"[논문리뷰] Are Today's LLMs Ready to Explain Well-Being Concepts?\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts\",\"children\":\"Huan Liu이 arXiv에 게시한 'Are Today's LLMs Ready to Explain Well-Being Concepts?' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-08 13:32:22+0900\",\"children\":\"2025년 8월 8일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-8-Are-Todays-LLMs-Ready-to-Explain-Well-Being-Concepts\"}]]}]]}],[\"$\",\"article\",\"2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning\",\"children\":\"[논문리뷰] Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning\",\"children\":\"Maksim Nekrashevich이 arXiv에 게시한 'Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-07 13:38:21+0900\",\"children\":\"2025년 8월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-7-Training-Long-Context-Multi-Turn-Software-Engineering-Agents-with-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence\",\"children\":\"[논문리뷰] Sotopia-RL: Reward Design for Social Intelligence\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence\",\"children\":\"Keyang Xuan이 arXiv에 게시한 'Sotopia-RL: Reward Design for Social Intelligence' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-07 13:38:21+0900\",\"children\":\"2025년 8월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-7-Sotopia-RL-Reward-Design-for-Social-Intelligence\"}]]}]]}],[\"$\",\"article\",\"2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management\",\"children\":\"[논문리뷰] Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management\",\"children\":\"Yunxin Liu이 arXiv에 게시한 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-07 13:38:21+0900\",\"children\":\"2025년 8월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management\"}]]}]]}],[\"$\",\"article\",\"2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks\",\"children\":\"[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks\",\"children\":\"Haozhe Zhang이 arXiv에 게시한 'Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-07 13:38:21+0900\",\"children\":\"2025년 8월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks\"}]]}]]}],[\"$\",\"article\",\"2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization\",\"children\":\"[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization\",\"children\":\"Kechi Zhang이 arXiv에 게시한 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-07 13:38:21+0900\",\"children\":\"2025년 8월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization\"}]]}]]}],[\"$\",\"article\",\"2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation\",\"children\":\"[논문리뷰] EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation\",\"children\":\"Dong Chen이 arXiv에 게시한 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-07 13:38:21+0900\",\"children\":\"2025년 8월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-7-EVOC2RUST-A-Skeleton-guided-Framework-for-Project-Level-C-to-Rust-Translation\"}]]}]]}],[\"$\",\"article\",\"2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning\",\"children\":\"[논문리뷰] Agent Lightning: Train ANY AI Agents with Reinforcement Learning\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning\",\"children\":\"Zilong Wang이 arXiv에 게시한 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-07 13:38:21+0900\",\"children\":\"2025년 8월 7일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-7-Agent-Lightning-Train-ANY-AI-Agents-with-Reinforcement-Learning\"}]]}]]}],[\"$\",\"article\",\"2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search\",\"children\":\"[논문리뷰] CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search\",\"children\":\"Jiwei Li이 arXiv에 게시한 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-06 13:46:36+0900\",\"children\":\"2025년 8월 6일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-6-CRINN-Contrastive-Reinforcement-Learning-for-Approximate-Nearest-Neighbor-Search\"}]]}]]}],[\"$\",\"article\",\"2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following\",\"children\":\"[논문리뷰] Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following\",\"children\":\"Jiaqing Liang이 arXiv에 게시한 'Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-05 11:40:52+0900\",\"children\":\"2025년 8월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-5-Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Models-Instruction-Following\"}]]}]]}],[\"$\",\"article\",\"2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks\",\"children\":\"[논문리뷰] AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks\",\"children\":\"Zhiwei Zhang이 arXiv에 게시한 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-05 11:40:52+0900\",\"children\":\"2025년 8월 5일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-5-AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks\"}]]}]]}],[\"$\",\"article\",\"2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution\",\"children\":\"[논문리뷰] SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution\",\"children\":\"Heng Lian이 arXiv에 게시한 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-04 12:17:01+0900\",\"children\":\"2025년 8월 4일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-4-SWE-Debate-Competitive-Multi-Agent-Debate-for-Software-Issue-Resolution\"}]]}]]}],[\"$\",\"article\",\"2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving\",{\"className\":\"archive__item\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"archive__item-title\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving\",\"children\":\"[논문리뷰] Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-excerpt\",\"children\":[\"$\",\"$L6\",null,{\"href\":\"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving\",\"children\":\"Zhicheng Jiang이 arXiv에 게시한 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving' 논문에 대한 자세한 리뷰입니다.\"}]}],[\"$\",\"div\",null,{\"className\":\"archive__item-meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-03 07:35:17+0900\",\"children\":\"2025년 8월 3일\"}],[\"$\",\"$L7\",null,{\"postPermalink\":\"/ai/review/2025-8-3-Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving\"}]]}]]}]]}]]}]]}]}]]}]],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"tags\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$L6\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lc\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lc\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"#Large Language Models - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Large Language Models 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/tags/Large%20Language%20Models\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"#Large Language Models - secrett2633's blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"Large Language Models 태그가 포함된 포스트 목록\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/tags/Large%20Language%20Models\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"#Large Language Models - secrett2633's blog\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"Large Language Models 태그가 포함된 포스트 목록\"}],[\"$\",\"link\",\"20\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"21\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>