3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-c27e618569e042bc.js","157","static/chunks/157-66db9680a8c2215d.js","185","static/chunks/app/layout-b06e577e11976c7d.js"],"default"]
7:I[231,["231","static/chunks/231-c27e618569e042bc.js","931","static/chunks/app/page-d94f9df6fcdd5a21.js"],""]
4:["slug","ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","c"]
0:["nzohOLWTNWtwOpdfo2_Ao",[[["",{"children":[["slug","ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-11-11-MVU-Eval-Towards-Multi-Video-Understanding-Evaluation-for-Multimodal-LLMs","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edf391eeca43d999.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
9:Tc8e,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2511.07250">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Gavin Chang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Steven Huang, Zhaoxiang Zhang, Jiaheng Liu</p>
<h2>핵심 연구 목표</h2>
<p>본 연구는 기존 <strong>MLLM</strong> 평가 벤치마크가 단일 비디오 이해에만 초점을 맞추어 실세계의 다중 비디오 시나리오(예: 스포츠 분석, 자율 주행)의 중요성을 간과하는 한계를 해결하고자 합니다. 이를 위해 <strong>MLLM</strong> 의 다중 비디오 이해 능력을 종합적으로 평가할 수 있는 최초의 벤치마크인 <strong>MVU-Eval</strong> 을 제안합니다.</p>
<h2>핵심 방법론</h2>
<p><strong>MVU-Eval</strong> 은 <strong>4,959개</strong> 의 비디오에서 파생된 <strong>1,824개</strong> 의 질문-답변 쌍으로 구성되며, <strong>8가지 핵심 역량</strong> 을 평가합니다. 핵심 역량은 <strong>객체 인식(OR), 공간 이해(SU), 카운팅, 비교</strong> 와 같은 기본 인지 작업과 <strong>지식 집약적 추론(KIR), In-Context Learning (ICL), 검색 증강 생성(RAG), 시간 추론(TR)</strong> 과 같은 고차원 추론 작업으로 분류됩니다. 데이터는 자동 생성과 철저한 <strong>인간 검증</strong> 을 통해 수집되었으며, <strong>제로샷(zero-shot) 설정</strong> 에서 <strong>정확도</strong> 를 평가 척도로 사용합니다.</p>
<h2>주요 결과</h2>
<p><strong>MLLM</strong> 의 다중 비디오 이해 능력은 여전히 개선의 여지가 많으며, 최고 성능의 <strong>Gemini 2.5 Pro</strong> 도 <strong>MVU-Eval</strong> 에서 <strong>56.6%</strong> 의 정확도에 그쳤습니다. 모델별로 하위 작업 성능의 불균형이 나타나, 예를 들어 <strong>Qwen2.5-VL-72B</strong> 는 <strong>OR, SU</strong> 에서 높은 성능을 보였고 <strong>Gemini 1.5 Pro</strong> 는 <strong>카운팅</strong> 에서 우수했습니다. 모델 규모 확장(예: <strong>Qwen2.5-VL</strong> 및 <strong>InternVL 시리즈</strong> )은 일반적으로 성능 향상으로 이어지며, <strong>VideoLLaMA3-7B</strong> 와 같이 더 많은 프레임과 높은 해상도를 지원할수록 성능이 크게 향상되었습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><strong>MVU-Eval</strong> 은 <strong>MLLM</strong> 이 복잡한 다중 비디오 환경에서 아직 많은 한계를 가지고 있음을 보여줍니다. <strong>AI 실무자</strong> 는 이 벤치마크를 활용하여 <strong>MLLM</strong> 의 <strong>다중 비디오 처리 및 추론 능력</strong> 을 개선하기 위한 아키텍처 및 훈련 전략 개발에 집중할 수 있습니다. 특히, <strong>다중 비디오 시각적 정렬, 비동기적 시간 추론, 확장 가능한 멀티모달 퓨전 전략</strong> 등은 향후 연구 및 개발에서 중요한 방향이 될 것입니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2009,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-11-11 00:00:00+0900+0900","children":"2025년 11월 11일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 11월 11일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Multimodal Large Language Models (MLLMs)",{"className":"page__taxonomy-item","children":["#","Multimodal Large Language Models (MLLMs)"]}],["$","span","Multi-Video Understanding",{"className":"page__taxonomy-item","children":["#","Multi-Video Understanding"]}],["$","span","Evaluation Benchmark",{"className":"page__taxonomy-item","children":["#","Evaluation Benchmark"]}],["$","span","Video Perception",{"className":"page__taxonomy-item","children":["#","Video Perception"]}],["$","span","Video Reasoning",{"className":"page__taxonomy-item","children":["#","Video Reasoning"]}],["$","span","Sports Analytics",{"className":"page__taxonomy-item","children":["#","Sports Analytics"]}],["$","span","Autonomous Driving",{"className":"page__taxonomy-item","children":["#","Autonomous Driving"]}]]]}]}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-11-11-MPJudge-Towards-Perceptual-Assessment-of-Music-Induced-Paintings/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] MPJudge: Towards Perceptual Assessment of Music-Induced Paintings"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-11-11-NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS   Modeling"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.site/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.site/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","22",{"name":"next-size-adjust"}]]
1:null
