<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/edb8d4ad4fe2f3b0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-4b0d66bdf1ba1813.js" async=""></script><script src="/_next/static/chunks/23-41c976638cd1a58c.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-ee5764c1002761f9.js" async=""></script><script src="/_next/static/chunks/132-273e49420772df1e.js" async=""></script><script src="/_next/static/chunks/app/layout-d443cbc354279241.js" async=""></script><script src="/_next/static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><title>[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset - secrett2633&#x27;s blog</title><meta name="description" content="arXiv에 게시된 &#x27;Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset&#x27; 논문에 대한 자세한 리뷰입니다."/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset"/><meta property="og:description" content="arXiv에 게시된 &#x27;Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset&#x27; 논문에 대한 자세한 리뷰입니다."/><meta property="og:url" content="https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-10-21T04:08:30.000Z"/><meta property="article:modified_time" content="2025-10-21T04:08:30.000Z"/><meta property="article:author" content="secrett2633"/><meta property="article:section" content="Review"/><meta property="article:tag" content="Review"/><meta property="article:tag" content="3D Motion Dataset"/><meta property="article:tag" content="Multimodal Data"/><meta property="article:tag" content="Human Behavior"/><meta property="article:tag" content="Pose Tracking"/><meta property="article:tag" content="Hand Tracking"/><meta property="article:tag" content="Audio-Visual Data"/><meta property="article:tag" content="Large-scale Dataset"/><meta property="article:tag" content="SMPL-X"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@secrett2633"/><meta name="twitter:title" content="[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset"/><meta name="twitter:description" content="arXiv에 게시된 &#x27;Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset&#x27; 논문에 대한 자세한 리뷰입니다."/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset","description":"arXiv에 게시된 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset","datePublished":"2025-10-21T04:08:30.000Z","dateModified":"2025-10-21T04:08:30.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":277,"articleSection":"Review","keywords":"Review, 3D Motion Dataset, Multimodal Data, Human Behavior, Pose Tracking, Hand Tracking, Audio-Visual Data, Large-scale Dataset, SMPL-X"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"Review","item":"https://blog.secrett2633.cloud/ai/review"},{"@type":"ListItem","position":3,"name":"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset","item":"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset"}]}</script><div class="space-y-6"><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2741<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><a class="hover:text-gray-700" href="/ai/review">Review</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset</span></li></ol></nav><article class="page"><header class="mb-8"><h1 class="page__title">[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset</h1><div class="page__meta"><time dateTime="2025-10-21 13:08:30+0900">2025년 10월 21일</time><time class="ml-4" dateTime="2025-10-21T04:08:30.000Z">수정: <!-- -->2025년 10월 21일</time></div></header><div class="page__content"><div><blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2510.16258" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Julia Buffalini, Neham Jain, Steven Krenn, Moneish Kumar, Dejan Markovic, Evonne Ng, Fabian Prada, Andrew Saba, Siwei Zhang, Vasu Agrawal, Tim Godisart, Alexander Richard, Michael Zollhoefer</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>기존 2D 및 3D 모션 데이터셋이 가진 스케일, 품질, 완전성, 도메인 특화 문제점을 해결하는 것을 목표로 합니다. 특히, 사람의 행동 및 상호작용에 대한 포괄적인 이해와 합성을 가능하게 하는 대규모 고품질 멀티모달 3D 모션 데이터셋을 구축하고자 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p><strong>80대의 24-메가픽셀 카메라</strong> 를 사용한 다중 시점 시스템으로 데이터를 수집했습니다. 데이터에는 <strong>SMPL-X</strong> 형식의 3D 전신 및 손 트래킹, 신체 형태, <strong>640채널 마이크 어레이</strong> 로 분리된 오디오, 그리고 세밀한 텍스트 주석이 포함됩니다. <strong>Sapiens-1B 키포인트 검출 모델</strong> 과 <strong>헝가리안 알고리즘</strong> 기반의 키포인트 매칭, <strong>RANSAC</strong> 기반의 키포인트 삼각 측량을 포함하는 데이터 처리 파이프라인을 활용했으며, 오디오는 <strong>빔포밍</strong> 으로 각 참가자별로 분리했습니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>439명의 참가자</strong> 로부터 <strong>500 개별 시간</strong> 의 3D 모션 데이터를 확보했으며, 이는 <strong>5,400만 프레임 이상</strong> 의 트래킹된 3D 모션에 해당합니다. 데이터셋은 단일 개인 동작부터 다중 인원 대화, 협업 활동, 거주 시나리오에 이르는 광범위한 모션 유형을 포함합니다. 수집된 모든 세그먼트는 <strong>2.5점(5점 만점) 이상의 품질 점수</strong> 를 달성하도록 인간 평가를 거쳤습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>이 데이터셋은 견고한 인간 모션 이해 및 합성 시스템 개발을 위한 핵심 자원으로 활용될 수 있습니다. 특히 가상 인간 및 체화된 에이전트 개발, 복잡한 다중 인원 상호작용 모델링, 멀티모달 AI 연구에 중요한 진전을 가져올 것입니다. AI/ML 엔지니어는 <strong>고품질 3D 모션, 손 트래킹, 오디오 및 텍스트 주석</strong> 이 결합된 이 데이터셋을 활용하여 기존 모델의 성능 한계를 극복하고 보다 현실적인 AI 애플리케이션을 구축할 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
</div></div><footer class="page__meta mt-8"><div class="page__taxonomy"><span class="text-sm font-medium text-gray-900 mb-2 block">태그</span><a class="page__taxonomy-item" href="/tags/Review">#<!-- -->Review</a><a class="page__taxonomy-item" href="/tags/3D%20Motion%20Dataset">#<!-- -->3D Motion Dataset</a><a class="page__taxonomy-item" href="/tags/Multimodal%20Data">#<!-- -->Multimodal Data</a><a class="page__taxonomy-item" href="/tags/Human%20Behavior">#<!-- -->Human Behavior</a><a class="page__taxonomy-item" href="/tags/Pose%20Tracking">#<!-- -->Pose Tracking</a><a class="page__taxonomy-item" href="/tags/Hand%20Tracking">#<!-- -->Hand Tracking</a><a class="page__taxonomy-item" href="/tags/Audio-Visual%20Data">#<!-- -->Audio-Visual Data</a><a class="page__taxonomy-item" href="/tags/Large-scale%20Dataset">#<!-- -->Large-scale Dataset</a><a class="page__taxonomy-item" href="/tags/SMPL-X">#<!-- -->SMPL-X</a></div></footer><section class="comments-section mt-12 pt-8 border-t border-gray-200"></section><section class="mt-12 border-t border-gray-200 pt-8"><h3 class="text-base font-semibold text-gray-900 mb-4">Review<!-- --> 의 다른글</h3><ul class="space-y-2 text-sm"><li class="text-gray-500">이전글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense">[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense</a></li><li class="text-gray-900 font-semibold">현재글 : <!-- -->[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset</li><li class="text-gray-500">다음글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics">[논문리뷰] Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics</a></li></ul></section></article></div></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[9157,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"default\"]\na:I[231,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js\"],\"\"]\nb:I[4080,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"\"]\nd:I[6130,[],\"\"]\n7:[\"slug\",\"ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\",\"c\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"mJI0q5Z-SQWBtT83kG_N7\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"review\\\",\\\"2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\"],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lb\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lb\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"11:I[646,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js\"],\"default\"]\nf:T46e,{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset\",\"description\":\"arXiv에 게시된 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.\",\"url\":\"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\",\"datePublished\":\"2025-10-21T04:08:30.000Z\",\"dateModified\":\"2025-10-21T04:08:30.000Z\",\"author\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"},\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"},\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\"},\"image\":\"https://blog.secrett2633.cloud/og-default.png\",\"isAccessibleForFree\":true,\"inLanguage\":\"ko\",\"wordCount\":277,\"articleSection\":\"Review\",\"keywords\":\"Review, 3D Motion Dataset, Multimodal Data, Human Behavior, Pose Tracking, Hand Tracking, Audio-Visual Data, Large-scale Dataset, SMPL-X\"}10:Tca7,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e링크:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/2510.16258\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e논문 PDF로 바로 열기\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e저자:\u003c/strong\u003e Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Julia Buffalini, Neham Jain, Steven Krenn, Moneish Kumar, Dejan Markovic, Evonne Ng, Fabian Prada, Andrew Saba, Siwei Zhang, Vasu Agrawal, Tim Godisart, Alexander Richard, Michael Zollhoefer\u003c/p\u003e\n\u003ch2 id=\"핵심-연구-목표\"\u003e\u003ca href=\"#핵심-연구-목표\"\u003e핵심 연구 목표\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e기존 2D 및 3D 모션 데이터셋이 가진 스케일, 품질, 완전성, 도메인 특화 문제점을 해결하는 것을 목표로 합니다. 특히, 사람의 행동 및 상호작용에 대한 포괄적인 이해와 합성을 가능하게 하는 대규모 고품질 멀티모달 3D 모션 데이터셋을 구축하고자 합니다.\u003c/p\u003e\n\u003ch2 id=\"핵심-방법론\"\u003e\u003ca href=\"#핵심-방법론\"\u003e핵심 방법론\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e80대의 24-메가픽셀 카메라\u003c/strong\u003e 를 사용한 다중 시점 시스템으로 데이터를 수집했습니다. 데이터에는 \u003cstrong\u003eSMPL-X\u003c/strong\u003e 형식의 3D 전신 및 손 트래킹, 신체 형태, \u003cstrong\u003e640채널 마이크 어레이\u003c/strong\u003e 로 분리된 오디오, 그리고 세밀한 텍스트 주석이 포함됩니다. \u003cstrong\u003eSapiens-1B 키포인트 검출 모델\u003c/strong\u003e 과 \u003cstrong\u003e헝가리안 알고리즘\u003c/strong\u003e 기반의 키포인트 매칭, \u003cstrong\u003eRANSAC\u003c/strong\u003e 기반의 키포인트 삼각 측량을 포함하는 데이터 처리 파이프라인을 활용했으며, 오디오는 \u003cstrong\u003e빔포밍\u003c/strong\u003e 으로 각 참가자별로 분리했습니다.\u003c/p\u003e\n\u003ch2 id=\"주요-결과\"\u003e\u003ca href=\"#주요-결과\"\u003e주요 결과\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e439명의 참가자\u003c/strong\u003e 로부터 \u003cstrong\u003e500 개별 시간\u003c/strong\u003e 의 3D 모션 데이터를 확보했으며, 이는 \u003cstrong\u003e5,400만 프레임 이상\u003c/strong\u003e 의 트래킹된 3D 모션에 해당합니다. 데이터셋은 단일 개인 동작부터 다중 인원 대화, 협업 활동, 거주 시나리오에 이르는 광범위한 모션 유형을 포함합니다. 수집된 모든 세그먼트는 \u003cstrong\u003e2.5점(5점 만점) 이상의 품질 점수\u003c/strong\u003e 를 달성하도록 인간 평가를 거쳤습니다.\u003c/p\u003e\n\u003ch2 id=\"ai-실무자를-위한-시사점\"\u003e\u003ca href=\"#ai-실무자를-위한-시사점\"\u003eAI 실무자를 위한 시사점\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e이 데이터셋은 견고한 인간 모션 이해 및 합성 시스템 개발을 위한 핵심 자원으로 활용될 수 있습니다. 특히 가상 인간 및 체화된 에이전트 개발, 복잡한 다중 인원 상호작용 모델링, 멀티모달 AI 연구에 중요한 진전을 가져올 것입니다. AI/ML 엔지니어는 \u003cstrong\u003e고품질 3D 모션, 손 트래킹, 오디오 및 텍스트 주석\u003c/strong\u003e 이 결합된 이 데이터셋을 활용하여 기존 모델의 성능 한계를 극복하고 보다 현실적인 AI 애플리케이션을 구축할 수 있습니다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e⚠️ \u003cstrong\u003e알림:\u003c/strong\u003e 이 리뷰는 AI로 작성되었습니다.\u003c/p\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"$f\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"Review\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":3,\\\"name\\\":\\\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2741,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/ai/review\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"hover:text-gray-700\",\"children\":\"Review\"}]]}],[\"$\",\"li\",\"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset\"}]]}]]]}]}],[\"$\",\"article\",null,{\"className\":\"page\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"page__title\",\"children\":\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset\"}],[\"$\",\"div\",null,{\"className\":\"page__meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-21 13:08:30+0900\",\"children\":\"2025년 10월 21일\"}],[\"$\",\"time\",null,{\"className\":\"ml-4\",\"dateTime\":\"2025-10-21T04:08:30.000Z\",\"children\":[\"수정: \",\"2025년 10월 21일\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"page__content\",\"children\":[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$10\"}}]}],[\"$\",\"footer\",null,{\"className\":\"page__meta mt-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"page__taxonomy\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-gray-900 mb-2 block\",\"children\":\"태그\"}],[[\"$\",\"$La\",\"Review\",{\"href\":\"/tags/Review\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Review\"]}],[\"$\",\"$La\",\"3D Motion Dataset\",{\"href\":\"/tags/3D%20Motion%20Dataset\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"3D Motion Dataset\"]}],[\"$\",\"$La\",\"Multimodal Data\",{\"href\":\"/tags/Multimodal%20Data\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Multimodal Data\"]}],[\"$\",\"$La\",\"Human Behavior\",{\"href\":\"/tags/Human%20Behavior\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Human Behavior\"]}],[\"$\",\"$La\",\"Pose Tracking\",{\"href\":\"/tags/Pose%20Tracking\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Pose Tracking\"]}],[\"$\",\"$La\",\"Hand Tracking\",{\"href\":\"/tags/Hand%20Tracking\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Hand Tracking\"]}],[\"$\",\"$La\",\"Audio-Visual Data\",{\"href\":\"/tags/Audio-Visual%20Data\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Audio-Visual Data\"]}],[\"$\",\"$La\",\"Large-scale Dataset\",{\"href\":\"/tags/Large-scale%20Dataset\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Large-scale Dataset\"]}],[\"$\",\"$La\",\"SMPL-X\",{\"href\":\"/tags/SMPL-X\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"SMPL-X\"]}]]]}]}],[\"$\",\"$L11\",null,{\"postPermalink\":\"/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\",\"postId\":\"2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\"}],[\"$\",\"section\",null,{\"className\":\"mt-12 border-t border-gray-200 pt-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 mb-4\",\"children\":[\"Review\",\" 의 다른글\"]}],[\"$\",\"ul\",null,{\"className\":\"space-y-2 text-sm\",\"children\":[[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"이전글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-10-21-Distractor-Injection-Attacks-on-Large-Reasoning-Models-Characterization-and-Defense\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense\"}]]}],[\"$\",\"li\",null,{\"className\":\"text-gray-900 font-semibold\",\"children\":[\"현재글 : \",\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset\"]}],[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"다음글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-10-21-Enterprise-Deep-Research-Steerable-Multi-Agent-Deep-Research-for-Enterprise-Analytics\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics\"}]]}]]}]]}]]}]]}]]}]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"arXiv에 게시된 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"arXiv에 게시된 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/ai/review/2025-10-21-Embody-3D-A-Large-scale-Multimodal-Motion-and-Behavior-Dataset\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:published_time\",\"content\":\"2025-10-21T04:08:30.000Z\"}],[\"$\",\"meta\",\"18\",{\"property\":\"article:modified_time\",\"content\":\"2025-10-21T04:08:30.000Z\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:author\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:section\",\"content\":\"Review\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"Review\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"3D Motion Dataset\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"Multimodal Data\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"Human Behavior\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"Pose Tracking\"}],[\"$\",\"meta\",\"26\",{\"property\":\"article:tag\",\"content\":\"Hand Tracking\"}],[\"$\",\"meta\",\"27\",{\"property\":\"article:tag\",\"content\":\"Audio-Visual Data\"}],[\"$\",\"meta\",\"28\",{\"property\":\"article:tag\",\"content\":\"Large-scale Dataset\"}],[\"$\",\"meta\",\"29\",{\"property\":\"article:tag\",\"content\":\"SMPL-X\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:creator\",\"content\":\"@secrett2633\"}],[\"$\",\"meta\",\"32\",{\"name\":\"twitter:title\",\"content\":\"[논문리뷰] Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset\"}],[\"$\",\"meta\",\"33\",{\"name\":\"twitter:description\",\"content\":\"arXiv에 게시된 'Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"link\",\"34\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"35\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script></body></html>