3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b44b076173de406f.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-dfe24a3bbd58f9b9.js"],""]
4:["slug","ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs","c"]
0:["sXH0w-Ur4uCLT-31_N3TA",[[["",{"children":[["slug","ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d6cea809dcbae606.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
a:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-dfe24a3bbd58f9b9.js"],"default"]
9:Te0c,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2602.03048">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Zhiyuan Yao, Yi-Kai Zhang, Yuxin Chen, Yueqing Sun, Zishan Xu, Yu Yang, Tianhao Hu, Qi Gu, Hui Su, Xunliang Cai</p>
<h2>핵심 연구 목표</h2>
<p>논문은 LLM 추론을 강화하는 <strong>RLVR(Reinforcement Learning with Verifiable Rewards)</strong> 프레임워크에서 <strong>GRPO(Group Relative Policy Optimization)</strong> 와 같은 기존 방법론의 비효율적인 균일 롤아웃 예산 할당 문제를 해결하고자 합니다. 기존 적응형 방법들이 모델의 동적인 학습 상태를 포착하지 못하는 인스턴스 수준 지표에 의존하는 한계를 극복하여, 모델의 진화하는 능력에 기반한 적응형 롤아웃 예산 할당 알고리즘을 제안하는 것이 목표입니다.</p>
<h2>핵심 방법론</h2>
<p>제안된 <strong>CoBA-RL</strong> 은 모델의 진화하는 능력에 따라 롤아웃 예산을 동적으로 할당합니다. 특히, <strong>Capability-Oriented Value function</strong> 을 <strong>Beta 분포</strong> 로 모델링하여 개별 태스크의 잠재적 훈련 가치를 매핑하고, 전역 실패율을 지속적으로 모니터링하여 가치 함수의 형태를 조정합니다. 이를 통해 탐색과 활용의 균형을 자율적으로 조절하며, 계산 리소스를 높은 훈련 가치를 가진 샘플에 효율적으로 할당하기 위해 <strong>힙(heap) 기반의 그리디 전략</strong> 을 사용합니다.</p>
<h2>주요 결과</h2>
<p><strong>CoBA-RL</strong> 은 여러 난이도 높은 수학 벤치마크에서 <strong>GRPO</strong> 및 <strong>Knapsack-RL</strong> 과 같은 강력한 기준선 대비 뛰어난 성능을 보였습니다. 예를 들어, <strong>Qwen2.5-7B-Instruct</strong> 모델에서 평균 <strong>46.78%</strong> 의 정확도를 달성하여 <strong>GRPO</strong> 의 <strong>42.24%</strong> 보다 <strong>4.54%</strong> 높았고, <strong>Knapsack-RL</strong> 의 <strong>45.39%</strong> 보다 <strong>1.39%</strong> 높았습니다. 특히, 제한된 예산( <strong>B_total = 2048</strong> )에서도 <strong>45.52%</strong> 정확도를 기록하여 두 배의 예산( <strong>B_total = 4096</strong> )으로 훈련된 <strong>GRPO</strong> 의 <strong>42.78%</strong> 를 능가하는 우수한 데이터 효율성을 입증했습니다. 또한, <strong>힙 기반 그리디 전략</strong> 은 동적 프로그래밍 대비 <strong>약 928배</strong> 빠른 할당 시간으로 뛰어난 런타임 효율성을 제공합니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><strong>CoBA-RL</strong> 은 LLM의 후처리 훈련에서 <strong>계산 리소스의 효율성</strong> 을 극대화할 수 있는 실용적인 방법론을 제공합니다. 특히, 모델의 <strong>실시간 학습 능력</strong> 에 따라 롤아웃 예산 할당 전략을 동적으로 조정함으로써, 제한된 컴퓨팅 자원으로도 <strong>더 나은 일반화 성능</strong> 을 달성할 수 있음을 보여줍니다. 이는 <strong>capability-oriented value function</strong> 과 <strong>힙 기반 그리디 전략</strong> 이 대규모 LLM 훈련에 효과적으로 적용될 수 있는 메커니즘을 제시하여, 기존의 정적 예산 할당 방식의 한계를 극복하고 LLM의 <strong>탐색-활용 균형</strong> 을 최적화하는 데 기여합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2648,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2026-02-04 00:00:00+0900+0900","children":"2026년 2월 4일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2026년 2월 4일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","span","LLMs",{"className":"page__taxonomy-item","children":["#","LLMs"]}],["$","span","Budget Allocation",{"className":"page__taxonomy-item","children":["#","Budget Allocation"]}],["$","span","Adaptive Learning",{"className":"page__taxonomy-item","children":["#","Adaptive Learning"]}],["$","span","Capability-Oriented Value Function",{"className":"page__taxonomy-item","children":["#","Capability-Oriented Value Function"]}],["$","span","Exploration-Exploitation",{"className":"page__taxonomy-item","children":["#","Exploration-Exploitation"]}],["$","span","Resource Efficiency",{"className":"page__taxonomy-item","children":["#","Resource Efficiency"]}]]]}]}],["$","$La",null,{"postPermalink":"/ai/review/2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs/","postId":"2026-02-04-CoBA-RL-Capability-Oriented-Budget-Allocation-for-Reinforcement-Learning-in-LLMs"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-04-Balancing-Understanding-and-Generation-in-Discrete-Diffusion-Models/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Balancing Understanding and Generation in Discrete Diffusion Models"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-04-CodeOCR-On-the-Effectiveness-of-Vision-Language-Models-in-Code-Understanding/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.cloud/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.cloud/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","22",{"name":"next-size-adjust"}]]
1:null
