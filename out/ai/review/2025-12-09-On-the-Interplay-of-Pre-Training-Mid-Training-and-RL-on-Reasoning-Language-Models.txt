3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],""]
8:I[4080,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],""]
4:["slug","ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","c"]
0:["WcxaIiCPz9cbpnkGvOjOK",[[["",{"children":[["slug","ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/ddc331716d5e47a2.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],"default"]
a:T4ce,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models","description":"이 [arXiv]에 게시한 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","datePublished":"2025-12-08T15:00:00.000Z","dateModified":"2025-12-08T15:00:00.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":273,"articleSection":"Review","keywords":"Review, Reinforcement Learning (RL), Pre-training, Mid-training, Reasoning LMs, Generalization, Synthetic Reasoning Tasks, Process-level Supervision"}b:Tdd5,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2512.07783" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Charlie Zhang, Graham Neubig, Xiang Yue</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 대규모 언어 모델(LLMs)에서 사전 훈련(pre-training), 중간 훈련(mid-training), 강화 학습(RL) 기반 후처리 훈련(post-training)이 추론 능력의 일반화에 미치는 상호작용과 인과적 영향을 명확히 규명하는 것을 목표로 합니다. 특히, RL이 모델의 추론 능력을 기저 모델의 역량을 넘어서 확장하는지, 그리고 어떤 조건에서 그러한 확장이 일어나는지에 대한 상충하는 견해를 해소하고자 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>연구팀은 <strong>합성 추론 태스크(synthetic reasoning tasks)</strong> 를 활용하는 완전 제어 실험 프레임워크를 개발했습니다. 이 프레임워크는 <strong>DAG(Directed Acyclic Graph) 기반의 명시적 원자 연산</strong> 과 <strong>구문 분석 가능한 단계별 추론 과정</strong> 을 특징으로 합니다. 훈련 데이터 분포를 체계적으로 조작하여 각 훈련 단계의 기여도를 분리하고, <strong>외삽적 일반화(extrapolative generalization)</strong> 와 <strong>문맥적 일반화(contextual generalization)</strong> 두 가지 차원에서 모델을 평가합니다. 또한, <strong>프로세스 수준 검증(process-level verification)</strong> 을 보상 함수에 통합하여 보상 해킹을 완화합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>RL</strong> 은 사전 훈련으로 충분한 여유가 남아있고, RL 데이터가 모델의 "역량 경계(edge of competence)"에 맞춰질 때만 진정한 능력 향상(예: <strong>pass@128</strong> )을 가져왔습니다. <strong>중간 훈련(mid-training)</strong> 단계는 고정된 컴퓨팅 예산에서 OOD(Out-of-Distribution) 추론 성능을 크게 향상시키며, <strong>RL 단독 대비 OOD-hard 태스크에서 +10.8%</strong> 더 나은 성능을 보였습니다. 또한, <strong>프로세스 수준 보상</strong> 은 외삽적 설정에서 <strong>pass@1</strong> 을 <strong>4-5%</strong> 개선하여 보상 해킹을 완화하고 추론 충실도를 높였습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>AI 실무자들은 <strong>RL 데이터셋을 모델의 "역량 경계" 주변 태스크에 집중</strong> 하여 설계해야 합니다. 또한, <strong>사전 훈련 단계에서 기본적인 추론 원시 요소(atomic reasoning primitives)를 (최소 1%의 밀도로라도) 포함</strong> 시켜 문맥적 일반화에 필요한 "씨앗"을 제공하는 것이 중요합니다. <strong>중간 훈련</strong> 은 훈련 파이프라인의 강력한 레버이므로, 목표(예: 안정성 vs. 탐색)에 따라 <strong>컴퓨팅 예산을 중간 훈련과 RL 사이에 전략적으로 분배</strong> 해야 하며, <strong>보상 함수에 프로세스 수준 검증을 통합</strong> 하여 모델의 견고성과 추론 충실도를 높일 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models\"}]}"}}],["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2728,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-12-09 00:00:00+0900+0900","children":"2025년 12월 9일"}],["$","time",null,{"className":"ml-4","dateTime":"2025-12-08T15:00:00.000Z","children":["수정: ","2025년 12월 9일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Reinforcement Learning (RL)",{"href":"/tags/Reinforcement%20Learning%20(RL)","className":"page__taxonomy-item","children":["#","Reinforcement Learning (RL)"]}],["$","$L7","Pre-training",{"href":"/tags/Pre-training","className":"page__taxonomy-item","children":["#","Pre-training"]}],["$","$L7","Mid-training",{"href":"/tags/Mid-training","className":"page__taxonomy-item","children":["#","Mid-training"]}],["$","$L7","Reasoning LMs",{"href":"/tags/Reasoning%20LMs","className":"page__taxonomy-item","children":["#","Reasoning LMs"]}],["$","$L7","Generalization",{"href":"/tags/Generalization","className":"page__taxonomy-item","children":["#","Generalization"]}],["$","$L7","Synthetic Reasoning Tasks",{"href":"/tags/Synthetic%20Reasoning%20Tasks","className":"page__taxonomy-item","children":["#","Synthetic Reasoning Tasks"]}],["$","$L7","Process-level Supervision",{"href":"/tags/Process-level%20Supervision","className":"page__taxonomy-item","children":["#","Process-level Supervision"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models","postId":"2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-12-09-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-12-09-ReCamDriving-LiDAR-Free-Camera-Controlled-Novel-Trajectory-Video-Generation","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation"}]]}]]}]]}]]}]]}]]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"이 [arXiv]에 게시한 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models"}],["$","meta","14",{"property":"og:description","content":"이 [arXiv]에 게시한 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2025-12-09-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2025-12-08T15:00:00.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2025-12-08T15:00:00.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Reinforcement Learning (RL)"}],["$","meta","23",{"property":"article:tag","content":"Pre-training"}],["$","meta","24",{"property":"article:tag","content":"Mid-training"}],["$","meta","25",{"property":"article:tag","content":"Reasoning LMs"}],["$","meta","26",{"property":"article:tag","content":"Generalization"}],["$","meta","27",{"property":"article:tag","content":"Synthetic Reasoning Tasks"}],["$","meta","28",{"property":"article:tag","content":"Process-level Supervision"}],["$","meta","29",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","30",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","31",{"name":"twitter:title","content":"[논문리뷰] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models"}],["$","meta","32",{"name":"twitter:description","content":"이 [arXiv]에 게시한 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models' 논문에 대한 자세한 리뷰입니다."}],["$","link","33",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","34",{"name":"next-size-adjust"}]]
1:null
