3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-135ad156a55a8fe5.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-a0fe0cc578429896.js"],""]
4:["slug","ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots","c"]
0:["Z3C0dl-eqDisnYDXzkst9",[[["",{"children":[["slug","ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d6cea809dcbae606.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
a:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-a0fe0cc578429896.js"],"default"]
9:Tc38,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2602.00919">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Sber Robotics Center</p>
<h2>핵심 연구 목표</h2>
<p>본 논문은 로봇 학습의 고질적인 문제인 데이터의 이질성, 낮은 품질, 그리고 행동 모방 (Behavior Cloning, BC)의 장기 태스크 한계를 해결하고자 합니다. 이를 위해 다양한 로봇 플랫폼과 환경에 걸쳐 일반화되고 견고하며 효율적인 로봇 정책을 개발하는 <strong>단계적 (staged) Vision-Language-Action (VLA) 프레임워크</strong> 를 제안합니다.</p>
<h2>핵심 방법론</h2>
<p>Green-VLA는 <strong>L0부터 R2까지 5단계 학습 커리큘럼</strong> 을 따르며, <strong>기초 VLM (L0)</strong> , <strong>멀티모달 웹 사전 학습 (L1)</strong> , <strong>멀티-embodiment 로봇 사전 학습 (R0)</strong> , <strong>embodiment-특화 적응 (R1)</strong> , 그리고 <strong>RL 기반 정책 정렬 (R2)</strong> 을 포함합니다. 특히, <strong>DATAQA 파이프라인</strong> 으로 데이터 품질을 관리하고, <strong>통합된 액션 공간 (Au)</strong> 과 <strong>광학 흐름 기반 액션 정렬</strong> 을 통해 데이터 이질성을 완화합니다. 또한, <strong>JPM (Joint Prediction Module)</strong> 을 통한 정밀 타겟팅 가이드와 <strong>OOD (Out-of-Distribution) 감지기</strong> 를 사용하여 안전성과 안정성을 높입니다.</p>
<h2>주요 결과</h2>
<p><strong>R0 사전 학습 단계</strong> 에서 Green-VLA는 테이블 정리 태스크에서 <strong>"Tape" 작업 성공률 83.1%</strong> 와 평균 <strong>1분 35초</strong> 의 완료 시간을 기록하며 기존 VLA 모델들을 능가했습니다. <strong>R1 미세 조정 단계</strong> 에서는 JPM 가이던스를 통해 이커머스 선반 피킹 태스크의 <strong>"ID Coarse" 성공률을 62.3%에서 95.4%</strong> 로 크게 향상시켰습니다. <strong>R2 RL 정렬 단계</strong> 에서는 WidowX 태스크에서 성공률을 <strong>절대적인 24%</strong> 증가시켰고, CALVIN 벤치마크에서 <strong>평균 체인 길이 (ACL) 4.63</strong> 을 달성하며 장기적인 일관성과 오류 복구 능력을 입증했습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p>이 연구는 로봇 학습에서 <strong>데이터 품질 정렬</strong> , <strong>통일된 액션 공간 설계</strong> , 그리고 <strong>다단계 학습 전략</strong> 이 실질적인 성능 향상에 필수적임을 보여줍니다. 특히, <strong>강화 학습 (RL) 기반의 정책 정렬</strong> 은 행동 모방의 한계를 극복하고 로봇의 장기 태스크 성공률과 견고성을 획기적으로 개선하는 데 중요한 역할을 합니다. 이러한 접근 방식은 <strong>제너럴리스트 로봇</strong> 을 위한 실제 배포 가능한 <strong>고품질 VLA 모델</strong> 개발에 핵심적인 통찰을 제공합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2672,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] Green-VLA: Staged Vision-Language-Action Model for Generalist Robots"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2026-02-03 00:00:00+0900+0900","children":"2026년 2월 3일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2026년 2월 3일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Vision-Language-Action",{"className":"page__taxonomy-item","children":["#","Vision-Language-Action"]}],["$","span","Generalist Robots",{"className":"page__taxonomy-item","children":["#","Generalist Robots"]}],["$","span","Staged Training",{"className":"page__taxonomy-item","children":["#","Staged Training"]}],["$","span","Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","span","Multi-embodiment",{"className":"page__taxonomy-item","children":["#","Multi-embodiment"]}],["$","span","Data Quality",{"className":"page__taxonomy-item","children":["#","Data Quality"]}],["$","span","Humanoid Robotics",{"className":"page__taxonomy-item","children":["#","Humanoid Robotics"]}],["$","span","Flow Matching",{"className":"page__taxonomy-item","children":["#","Flow Matching"]}]]]}]}],["$","$La",null,{"postPermalink":"/ai/review/2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots/","postId":"2026-02-03-Green-VLA-Staged-Vision-Language-Action-Model-for-Generalist-Robots"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-03-FSVideo-Fast-Speed-Video-Diffusion-Model-in-a-Highly-Compressed-Latent-Space/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] Green-VLA: Staged Vision-Language-Action Model for Generalist Robots"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-03-How-Well-Do-Models-Follow-Visual-Instructions-VIBE-A-Systematic-Benchmark-for-Visual-Instruction-Driven-Image-Editing/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.cloud/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.cloud/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}]]
1:null
