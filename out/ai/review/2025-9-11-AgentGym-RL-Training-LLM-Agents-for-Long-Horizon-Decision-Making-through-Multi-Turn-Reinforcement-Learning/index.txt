3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-c27e618569e042bc.js","157","static/chunks/157-90aef3381d42edea.js","185","static/chunks/app/layout-b06e577e11976c7d.js"],"default"]
7:I[231,["231","static/chunks/231-c27e618569e042bc.js","877","static/chunks/app/%5B...slug%5D/page-9d772c571b4668c1.js"],""]
4:["slug","ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","c"]
0:["vXhrfv3Vb9rIvRlX83B9d",[[["",{"children":[["slug","ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-9-11-AgentGym-RL-Training-LLM-Agents-for-Long-Horizon-Decision-Making-through-Multi-Turn-Reinforcement-Learning","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edf391eeca43d999.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
9:Te49,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2509.08755">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang</p>
<h2>핵심 연구 목표</h2>
<p>본 연구는 복잡하고 실제와 같은 장기적 의사결정 태스크를 해결하기 위해 LLM 에이전트를 훈련시키는 통일된 <strong>대화형 강화 학습(RL) 프레임워크</strong> 의 부재를 해결하는 것을 목표로 합니다. 기존 방식의 <strong>SFT(Supervised Fine-Tuning)</strong> 의존성, 제한된 태스크 복잡성, 환경 다양성, 최적화 안정성 및 효율성 문제를 극복하여 처음부터 <strong>다양하고 사실적인 환경</strong> 에서 에이전트를 효과적으로 훈련하고자 합니다.</p>
<h2>핵심 방법론</h2>
<p>새로운 <strong>AgentGym-RL 프레임워크</strong> 는 모듈화되고 분리된 아키텍처를 특징으로 하며, <strong>환경, 에이전트, 훈련 모듈</strong> 을 통해 높은 유연성과 확장성을 제공합니다. <strong>PPO, GRPO, REINFORCE++</strong> 와 같은 주류 RL 알고리즘을 지원하며, <strong>ScalingInter-RL</strong> 이라는 점진적 상호작용 스케일링 방법론을 도입했습니다. 이 방법은 초기 단계에서는 상호작용 횟수를 제한하여 <strong>활용(exploitation)</strong> 에 중점을 두고, 점차적으로 상호작용 범위를 넓혀 <strong>탐색(exploration)</strong> 을 촉진함으로써 에이전트의 안정적인 최적화와 행동 다양성을 이끌어냅니다.</p>
<h2>주요 결과</h2>
<p><strong>AgentGym-RL 프레임워크</strong> 와 <strong>ScalingInter-RL</strong> 접근 방식은 5가지 시나리오의 27개 태스크에서 상업용 모델과 동등하거나 이를 능가하는 성능을 입증했습니다. 특히, <strong>7B 파라미터</strong> 의 <strong>ScalingInter-RL 모델</strong> 은 <strong>WebArena</strong> 에서 <strong>GPT-40</strong> 를 <strong>10% 이상</strong> 능가하는 <strong>26.00%</strong> 정확도를 달성하고, <strong>BabyAI</strong> 벤치마크에서는 <strong>OpenAI 03</strong> 및 <strong>GPT-40</strong> 를 뛰어넘는 <strong>96.67%</strong> 의 최고 정확도를 기록했습니다. 또한, <strong>Deep Search</strong> 및 <strong>SciWorld</strong> 와 같은 복잡한 환경에서도 뛰어난 성능 향상을 보여주며, <strong>모델 크기 증가보다 후처리 및 추론 시 연산 투자가 더 효과적</strong> 임을 시사합니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p>본 연구는 오픈소스 <strong>AgentGym-RL 프레임워크</strong> 와 <strong>ScalingInter-RL</strong> 방법론을 통해 LLM 에이전트 훈련의 효율성과 안정성을 크게 향상시켰습니다. 이는 <strong>대규모 언어 모델 기반 에이전트의 장기적, 다중 턴 의사결정 능력 개발</strong> 에 중요한 기여를 하며, 특히 <strong>명확한 피드백 환경</strong> 에서 RL의 효과가 두드러짐을 보여줍니다. 공개된 프레임워크와 데이터셋은 미래 AI 에이전트 연구에 실용적인 기반을 제공하며, <strong>모델 규모보다 전략적인 훈련 접근 방식</strong> 의 중요성을 강조합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",1702,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-09-11 13:02:36+0900","children":"2025년 9월 11일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 9월 11일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","LLM Agents",{"className":"page__taxonomy-item","children":["#","LLM Agents"]}],["$","span","Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","span","Multi-Turn Interaction",{"className":"page__taxonomy-item","children":["#","Multi-Turn Interaction"]}],["$","span","Long-Horizon Decision Making",{"className":"page__taxonomy-item","children":["#","Long-Horizon Decision Making"]}],["$","span","Agent Framework",{"className":"page__taxonomy-item","children":["#","Agent Framework"]}],["$","span","Exploration-Exploitation",{"className":"page__taxonomy-item","children":["#","Exploration-Exploitation"]}],["$","span","Progressive Scaling",{"className":"page__taxonomy-item","children":["#","Progressive Scaling"]}]]]}]}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-9-11-A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] A Survey of Reinforcement Learning for Large Reasoning Models"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-9-11-EnvX-Agentize-Everything-with-Agentic-AI/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] EnvX: Agentize Everything with Agentic AI"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.site/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.site/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","22",{"name":"next-size-adjust"}]]
1:null
