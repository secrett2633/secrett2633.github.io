<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/edb8d4ad4fe2f3b0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-4b0d66bdf1ba1813.js" async=""></script><script src="/_next/static/chunks/23-41c976638cd1a58c.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-ee5764c1002761f9.js" async=""></script><script src="/_next/static/chunks/132-273e49420772df1e.js" async=""></script><script src="/_next/static/chunks/app/layout-d443cbc354279241.js" async=""></script><script src="/_next/static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><title>[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense - secrett2633&#x27;s blog</title><meta name="description" content="arXiv에 게시된 &#x27;Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense&#x27; 논문에 대한 자세한 리뷰입니다."/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense"/><meta property="og:description" content="arXiv에 게시된 &#x27;Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense&#x27; 논문에 대한 자세한 리뷰입니다."/><meta property="og:url" content="https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-10-10T04:53:45.000Z"/><meta property="article:modified_time" content="2025-10-10T04:53:45.000Z"/><meta property="article:author" content="secrett2633"/><meta property="article:section" content="Review"/><meta property="article:tag" content="Review"/><meta property="article:tag" content="Reinforcement Learning"/><meta property="article:tag" content="Reward Modeling"/><meta property="article:tag" content="Large Language Models (LLMs)"/><meta property="article:tag" content="Mathematical Reasoning"/><meta property="article:tag" content="Sparse Rewards"/><meta property="article:tag" content="Dense Rewards"/><meta property="article:tag" content="Hybrid Reinforcement"/><meta property="article:tag" content="Verifier-based Rewards"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@secrett2633"/><meta name="twitter:title" content="[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense"/><meta name="twitter:description" content="arXiv에 게시된 &#x27;Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense&#x27; 논문에 대한 자세한 리뷰입니다."/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense","description":"arXiv에 게시된 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense","datePublished":"2025-10-10T04:53:45.000Z","dateModified":"2025-10-10T04:53:45.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":266,"articleSection":"Review","keywords":"Review, Reinforcement Learning, Reward Modeling, Large Language Models (LLMs), Mathematical Reasoning, Sparse Rewards, Dense Rewards, Hybrid Reinforcement, Verifier-based Rewards"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"Review","item":"https://blog.secrett2633.cloud/ai/review"},{"@type":"ListItem","position":3,"name":"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense","item":"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense"}]}</script><div class="space-y-6"><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2741<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><a class="hover:text-gray-700" href="/ai/review">Review</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense</span></li></ol></nav><article class="page"><header class="mb-8"><h1 class="page__title">[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense</h1><div class="page__meta"><time dateTime="2025-10-10 13:53:45+0900">2025년 10월 10일</time><time class="ml-4" dateTime="2025-10-10T04:53:45.000Z">수정: <!-- -->2025년 10월 10일</time></div></header><div class="page__content"><div><blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2510.07242" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu, Yixuan Li, Jason E Weston, Ping Yu</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 대규모 언어 모델(LLM) 추론 훈련에서 <strong>결정론적 검증기(deterministic checkers)</strong> 의 이진(0-1) 보상 신호가 야기하는 한계(학습 비효율성, 오분류)를 해결하는 것을 목표로 합니다. 특히, 부분적으로 정확하거나 형식에 민감한 응답에 대해 검증기가 보상을 제대로 주지 못하는 문제를 개선하기 위해, 검증기의 신뢰성은 유지하면서 <strong>보상 모델(Reward Model, RM)</strong> 의 풍부하고 연속적인 피드백을 활용하는 효과적인 하이브리드 프레임워크를 개발하고자 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>본 연구는 <strong>HERO(Hybrid Ensemble Reward optimization)</strong> 라는 강화 학습 프레임워크를 제안합니다. HERO는 첫째, <strong>계층적 정규화(stratified normalization)</strong> 를 통해 보상 모델 점수를 검증기가 정의한 정확성 그룹 내로 제한하여 모델 정확성을 보존하면서도 미묘한 품질 차이를 포착합니다. 둘째, <strong>분산-인식 가중치(variance-aware weighting)</strong> 메커니즘을 도입하여 훈련 중 정보량이 많은 어려운 프롬프트에 더 높은 가중치를 부여하고, 쉬운 프롬프트는 낮춰 학습 효율성을 최적화합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>Qwen3-4B-Base</strong> 모델 기반의 다양한 수학 추론 벤치마크에서, HERO는 <strong>RM-only</strong> 및 <strong>verifier-only</strong> 베이스라인을 일관되게 능가했습니다. 특히 검증하기 어려운 태스크에서 <strong>66.3%</strong> 의 성능을 달성하여 <strong>RM-only 훈련(54.6%)</strong> 보다 <strong>+11.7%p</strong> , <strong>verifier-only 훈련(57.1%)</strong> 보다 <strong>+9.2%p</strong> 높은 큰 폭의 개선을 보였습니다. 이는 하이브리드 보상 설계가 검증자의 안정성을 유지하면서 보상 모델의 섬세함을 활용하여 추론 능력을 발전시킴을 보여줍니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>AI 실무자들은 LLM 기반 추론 모델 훈련 시, 단순한 <strong>이진 검증 보상</strong> 의 한계를 인식하고 <strong>HERO와 같은 하이브리드 보상 접근 방식</strong> 을 적극적으로 고려해야 합니다. 이 방법론은 특히 부분적으로 정확하거나 형식에 민감한 응답이 많은 복잡한 추론 태스크에서 모델의 학습 안정성과 일반화 성능을 크게 향상시킬 수 있습니다. 또한, <strong>컴팩트한 보상 모델</strong> 로도 강력한 성능을 달성할 수 있어 효율적인 AI 시스템 개발 및 배포에 기여할 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
</div></div><footer class="page__meta mt-8"><div class="page__taxonomy"><span class="text-sm font-medium text-gray-900 mb-2 block">태그</span><a class="page__taxonomy-item" href="/tags/Review">#<!-- -->Review</a><a class="page__taxonomy-item" href="/tags/Reinforcement%20Learning">#<!-- -->Reinforcement Learning</a><a class="page__taxonomy-item" href="/tags/Reward%20Modeling">#<!-- -->Reward Modeling</a><a class="page__taxonomy-item" href="/tags/Large%20Language%20Models%20(LLMs)">#<!-- -->Large Language Models (LLMs)</a><a class="page__taxonomy-item" href="/tags/Mathematical%20Reasoning">#<!-- -->Mathematical Reasoning</a><a class="page__taxonomy-item" href="/tags/Sparse%20Rewards">#<!-- -->Sparse Rewards</a><a class="page__taxonomy-item" href="/tags/Dense%20Rewards">#<!-- -->Dense Rewards</a><a class="page__taxonomy-item" href="/tags/Hybrid%20Reinforcement">#<!-- -->Hybrid Reinforcement</a><a class="page__taxonomy-item" href="/tags/Verifier-based%20Rewards">#<!-- -->Verifier-based Rewards</a></div></footer><section class="comments-section mt-12 pt-8 border-t border-gray-200"></section><section class="mt-12 border-t border-gray-200 pt-8"><h3 class="text-base font-semibold text-gray-900 mb-4">Review<!-- --> 의 다른글</h3><ul class="space-y-2 text-sm"><li class="text-gray-500">이전글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-10-10-GCPO-When-Contrast-Fails-Go-Gold">[논문리뷰] GCPO: When Contrast Fails, Go Gold</a></li><li class="text-gray-900 font-semibold">현재글 : <!-- -->[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It&#x27;s Better to Be Dense</li><li class="text-gray-500">다음글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance">[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance</a></li></ul></section></article></div></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[9157,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"default\"]\na:I[231,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js\"],\"\"]\nb:I[4080,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"132\",\"static/chunks/132-273e49420772df1e.js\",\"185\",\"static/chunks/app/layout-d443cbc354279241.js\"],\"\"]\nd:I[6130,[],\"\"]\n7:[\"slug\",\"ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\",\"c\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/edb8d4ad4fe2f3b0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"mJI0q5Z-SQWBtT83kG_N7\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"review\\\",\\\"2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\"],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lb\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lb\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"11:I[646,[\"231\",\"static/chunks/231-ee5764c1002761f9.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js\"],\"default\"]\nf:T4a8,{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense\",\"description\":\"arXiv에 게시된 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.\",\"url\":\"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\",\"datePublished\":\"2025-10-10T04:53:45.000Z\",\"dateModified\":\"2025-10-10T04:53:45.000Z\",\"author\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"},\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"},\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\"},\"image\":\"https://blog.secrett2633.cloud/og-default.png\",\"isAccessibleForFree\":true,\"inLanguage\":\"ko\",\"wordCount\":266,\"articleSection\":\"Review\",\"keywords\":\"Review, Reinforcement Learning, Reward Modeling, Large Language Models (LLMs), Mathematical Reasoning, Sparse Rewards, Dense Rewards, Hybrid Reinforcement, Verifier-based Rewards\"}10:Td5b,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e링크:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/2510.07242\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e논문 PDF로 바로 열기\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e저자:\u003c/strong\u003e Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu, Yixuan Li, Jason E Weston, Ping Yu\u003c/p\u003e\n\u003ch2 id=\"핵심-연구-목표\"\u003e\u003ca href=\"#핵심-연구-목표\"\u003e핵심 연구 목표\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e본 논문은 대규모 언어 모델(LLM) 추론 훈련에서 \u003cstrong\u003e결정론적 검증기(deterministic checkers)\u003c/strong\u003e 의 이진(0-1) 보상 신호가 야기하는 한계(학습 비효율성, 오분류)를 해결하는 것을 목표로 합니다. 특히, 부분적으로 정확하거나 형식에 민감한 응답에 대해 검증기가 보상을 제대로 주지 못하는 문제를 개선하기 위해, 검증기의 신뢰성은 유지하면서 \u003cstrong\u003e보상 모델(Reward Model, RM)\u003c/strong\u003e 의 풍부하고 연속적인 피드백을 활용하는 효과적인 하이브리드 프레임워크를 개발하고자 합니다.\u003c/p\u003e\n\u003ch2 id=\"핵심-방법론\"\u003e\u003ca href=\"#핵심-방법론\"\u003e핵심 방법론\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e본 연구는 \u003cstrong\u003eHERO(Hybrid Ensemble Reward optimization)\u003c/strong\u003e 라는 강화 학습 프레임워크를 제안합니다. HERO는 첫째, \u003cstrong\u003e계층적 정규화(stratified normalization)\u003c/strong\u003e 를 통해 보상 모델 점수를 검증기가 정의한 정확성 그룹 내로 제한하여 모델 정확성을 보존하면서도 미묘한 품질 차이를 포착합니다. 둘째, \u003cstrong\u003e분산-인식 가중치(variance-aware weighting)\u003c/strong\u003e 메커니즘을 도입하여 훈련 중 정보량이 많은 어려운 프롬프트에 더 높은 가중치를 부여하고, 쉬운 프롬프트는 낮춰 학습 효율성을 최적화합니다.\u003c/p\u003e\n\u003ch2 id=\"주요-결과\"\u003e\u003ca href=\"#주요-결과\"\u003e주요 결과\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eQwen3-4B-Base\u003c/strong\u003e 모델 기반의 다양한 수학 추론 벤치마크에서, HERO는 \u003cstrong\u003eRM-only\u003c/strong\u003e 및 \u003cstrong\u003everifier-only\u003c/strong\u003e 베이스라인을 일관되게 능가했습니다. 특히 검증하기 어려운 태스크에서 \u003cstrong\u003e66.3%\u003c/strong\u003e 의 성능을 달성하여 \u003cstrong\u003eRM-only 훈련(54.6%)\u003c/strong\u003e 보다 \u003cstrong\u003e+11.7%p\u003c/strong\u003e , \u003cstrong\u003everifier-only 훈련(57.1%)\u003c/strong\u003e 보다 \u003cstrong\u003e+9.2%p\u003c/strong\u003e 높은 큰 폭의 개선을 보였습니다. 이는 하이브리드 보상 설계가 검증자의 안정성을 유지하면서 보상 모델의 섬세함을 활용하여 추론 능력을 발전시킴을 보여줍니다.\u003c/p\u003e\n\u003ch2 id=\"ai-실무자를-위한-시사점\"\u003e\u003ca href=\"#ai-실무자를-위한-시사점\"\u003eAI 실무자를 위한 시사점\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAI 실무자들은 LLM 기반 추론 모델 훈련 시, 단순한 \u003cstrong\u003e이진 검증 보상\u003c/strong\u003e 의 한계를 인식하고 \u003cstrong\u003eHERO와 같은 하이브리드 보상 접근 방식\u003c/strong\u003e 을 적극적으로 고려해야 합니다. 이 방법론은 특히 부분적으로 정확하거나 형식에 민감한 응답이 많은 복잡한 추론 태스크에서 모델의 학습 안정성과 일반화 성능을 크게 향상시킬 수 있습니다. 또한, \u003cstrong\u003e컴팩트한 보상 모델\u003c/strong\u003e 로도 강력한 성능을 달성할 수 있어 효율적인 AI 시스템 개발 및 배포에 기여할 수 있습니다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e⚠️ \u003cstrong\u003e알림:\u003c/strong\u003e 이 리뷰는 AI로 작성되었습니다.\u003c/p\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"$f\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"Review\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":3,\\\"name\\\":\\\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2741,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/ai/review\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"hover:text-gray-700\",\"children\":\"Review\"}]]}],[\"$\",\"li\",\"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense\"}]]}]]]}]}],[\"$\",\"article\",null,{\"className\":\"page\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"page__title\",\"children\":\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense\"}],[\"$\",\"div\",null,{\"className\":\"page__meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-10-10 13:53:45+0900\",\"children\":\"2025년 10월 10일\"}],[\"$\",\"time\",null,{\"className\":\"ml-4\",\"dateTime\":\"2025-10-10T04:53:45.000Z\",\"children\":[\"수정: \",\"2025년 10월 10일\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"page__content\",\"children\":[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$10\"}}]}],[\"$\",\"footer\",null,{\"className\":\"page__meta mt-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"page__taxonomy\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-gray-900 mb-2 block\",\"children\":\"태그\"}],[[\"$\",\"$La\",\"Review\",{\"href\":\"/tags/Review\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Review\"]}],[\"$\",\"$La\",\"Reinforcement Learning\",{\"href\":\"/tags/Reinforcement%20Learning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Reinforcement Learning\"]}],[\"$\",\"$La\",\"Reward Modeling\",{\"href\":\"/tags/Reward%20Modeling\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Reward Modeling\"]}],[\"$\",\"$La\",\"Large Language Models (LLMs)\",{\"href\":\"/tags/Large%20Language%20Models%20(LLMs)\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Large Language Models (LLMs)\"]}],[\"$\",\"$La\",\"Mathematical Reasoning\",{\"href\":\"/tags/Mathematical%20Reasoning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Mathematical Reasoning\"]}],[\"$\",\"$La\",\"Sparse Rewards\",{\"href\":\"/tags/Sparse%20Rewards\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Sparse Rewards\"]}],[\"$\",\"$La\",\"Dense Rewards\",{\"href\":\"/tags/Dense%20Rewards\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Dense Rewards\"]}],[\"$\",\"$La\",\"Hybrid Reinforcement\",{\"href\":\"/tags/Hybrid%20Reinforcement\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Hybrid Reinforcement\"]}],[\"$\",\"$La\",\"Verifier-based Rewards\",{\"href\":\"/tags/Verifier-based%20Rewards\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Verifier-based Rewards\"]}]]]}]}],[\"$\",\"$L11\",null,{\"postPermalink\":\"/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\",\"postId\":\"2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\"}],[\"$\",\"section\",null,{\"className\":\"mt-12 border-t border-gray-200 pt-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 mb-4\",\"children\":[\"Review\",\" 의 다른글\"]}],[\"$\",\"ul\",null,{\"className\":\"space-y-2 text-sm\",\"children\":[[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"이전글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-10-10-GCPO-When-Contrast-Fails-Go-Gold\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] GCPO: When Contrast Fails, Go Gold\"}]]}],[\"$\",\"li\",null,{\"className\":\"text-gray-900 font-semibold\",\"children\":[\"현재글 : \",\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense\"]}],[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"다음글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-10-10-InstructX-Towards-Unified-Visual-Editing-with-MLLM-Guidance\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] InstructX: Towards Unified Visual Editing with MLLM Guidance\"}]]}]]}]]}]]}]]}]]}]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"arXiv에 게시된 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"arXiv에 게시된 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/ai/review/2025-10-10-Hybrid-Reinforcement-When-Reward-Is-Sparse-Its-Better-to-Be-Dense\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:published_time\",\"content\":\"2025-10-10T04:53:45.000Z\"}],[\"$\",\"meta\",\"18\",{\"property\":\"article:modified_time\",\"content\":\"2025-10-10T04:53:45.000Z\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:author\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:section\",\"content\":\"Review\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"Review\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"Reinforcement Learning\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"Reward Modeling\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"Large Language Models (LLMs)\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"Mathematical Reasoning\"}],[\"$\",\"meta\",\"26\",{\"property\":\"article:tag\",\"content\":\"Sparse Rewards\"}],[\"$\",\"meta\",\"27\",{\"property\":\"article:tag\",\"content\":\"Dense Rewards\"}],[\"$\",\"meta\",\"28\",{\"property\":\"article:tag\",\"content\":\"Hybrid Reinforcement\"}],[\"$\",\"meta\",\"29\",{\"property\":\"article:tag\",\"content\":\"Verifier-based Rewards\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:creator\",\"content\":\"@secrett2633\"}],[\"$\",\"meta\",\"32\",{\"name\":\"twitter:title\",\"content\":\"[논문리뷰] Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense\"}],[\"$\",\"meta\",\"33\",{\"name\":\"twitter:description\",\"content\":\"arXiv에 게시된 'Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"link\",\"34\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"35\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script></body></html>