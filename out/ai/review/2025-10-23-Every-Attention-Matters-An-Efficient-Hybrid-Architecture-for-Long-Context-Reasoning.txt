3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
7:I[231,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],""]
8:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
4:["slug","ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","c"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":[["slug","ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],"default"]
a:T4e0,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning","description":"arXiv에 게시된 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","datePublished":"2025-10-23T04:08:59.000Z","dateModified":"2025-10-23T04:08:59.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":308,"articleSection":"Review","keywords":"Review, Long-Context LLM, Hybrid Attention, Linear Attention, Mixture-of-Experts, FP8 Training, GPU Optimization, Training-Inference Alignment, Reinforcement Learning"}b:Te9b,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2510.19338" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 기존의 <strong>Softmax Attention</strong> 이 긴 시퀀스 길이에서 겪는 <strong>계산 및 I/O 오버헤드 문제</strong> 를 해결하고, 순수 <strong>Linear Attention</strong> 모델의 성능 한계를 극복하기 위해 효율적인 하이브리드 아키텍처를 제안합니다. 이를 통해 <strong>장문 컨텍스트 추론</strong> 시 효율성을 극대화하면서도 <strong>SOTA 성능</strong> 을 유지하는 거대 언어 모델을 개발하는 것이 목표입니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>저자들은 <strong>Linear Attention</strong> 과 <strong>Softmax Attention</strong> 을 효과적으로 통합한 <strong>Ring-linear 모델 시리즈</strong> 를 제안하며, 특히 <strong>Ring-mini-linear-2.0</strong> (16B 파라미터)과 <strong>Ring-flash-linear-2.0</strong> (104B 파라미터) 모델을 공개했습니다. 아키텍처는 효율성을 위해 <strong>고도로 희소한 MoE (Mixture-of-Experts)</strong> 와 <strong>Lightning Attention</strong> 을 활용하고, <strong>self-developed FP8 operator library (linghe)</strong> 를 통해 훈련 효율을 <strong>50%</strong> 개선했습니다. 또한, RL 훈련의 안정성을 위해 <strong>KV Cache 정밀도</strong> , <strong>RMSNorm</strong> , <strong>RoPE</strong> 등에서 <strong>훈련-추론 불일치</strong> 를 체계적으로 해결했습니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>Ring-flash-linear-2.0</strong> 모델은 <strong>AIME'24에서 90.73%</strong> , <strong>CodeForces(Elo)에서 90.24%</strong> 의 높은 정확도를 달성하며 여러 추론 벤치마크에서 <strong>SOTA 성능</strong> 을 기록했습니다. 320억 파라미터의 Dense 모델 대비 추론 비용을 <strong>1/10</strong> 로 줄였고, 이전 Ring 시리즈 대비 <strong>50% 이상</strong> 의 비용 절감을 이루었습니다. 특히 128K 이상의 컨텍스트 길이에서 기존 모델 대비 <strong>8배 이상 빠른 Prefill 처리량</strong> 과 <strong>10배 이상 빠른 Decode 처리량</strong> 을 달성하며 뛰어난 장문 컨텍스트 효율성을 입증했습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>이 연구는 <strong>장문 컨텍스트 LLM</strong> 의 실용적 배포를 위한 효율적인 아키텍처 솔루션을 제공하며, 특히 <strong>FP8 혼합 정밀도 훈련</strong> 과 <strong>GPU 커널 최적화</strong> 를 통해 훈련 및 추론 비용을 획기적으로 절감할 수 있음을 보여줍니다. 또한, <strong>MoE 모델</strong> 과 <strong>RL 훈련</strong> 에서 발생하는 <strong>훈련-추론 불일치</strong> 를 체계적으로 해결하는 중요성을 강조하여, 고성능 AI 시스템을 구축하는 엔지니어들에게 실용적인 가이드라인을 제시합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","time",null,{"className":"ml-4","dateTime":"2025-10-23T04:08:59.000Z","children":["수정: ","2025년 10월 23일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Long-Context LLM",{"href":"/tags/Long-Context%20LLM","className":"page__taxonomy-item","children":["#","Long-Context LLM"]}],["$","$L7","Hybrid Attention",{"href":"/tags/Hybrid%20Attention","className":"page__taxonomy-item","children":["#","Hybrid Attention"]}],["$","$L7","Linear Attention",{"href":"/tags/Linear%20Attention","className":"page__taxonomy-item","children":["#","Linear Attention"]}],["$","$L7","Mixture-of-Experts",{"href":"/tags/Mixture-of-Experts","className":"page__taxonomy-item","children":["#","Mixture-of-Experts"]}],["$","$L7","FP8 Training",{"href":"/tags/FP8%20Training","className":"page__taxonomy-item","children":["#","FP8 Training"]}],["$","$L7","GPU Optimization",{"href":"/tags/GPU%20Optimization","className":"page__taxonomy-item","children":["#","GPU Optimization"]}],["$","$L7","Training-Inference Alignment",{"href":"/tags/Training-Inference%20Alignment","className":"page__taxonomy-item","children":["#","Training-Inference Alignment"]}],["$","$L7","Reinforcement Learning",{"href":"/tags/Reinforcement%20Learning","className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning","postId":"2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-10-23-Directional-Reasoning-Injection-for-Fine-Tuning-MLLMs","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Directional Reasoning Injection for Fine-Tuning MLLMs"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-10-23-FinSight-Towards-Real-World-Financial-Deep-Research","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] FinSight: Towards Real-World Financial Deep Research"}]]}]]}]]}]]}]]}]]}]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"arXiv에 게시된 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning"}],["$","meta","14",{"property":"og:description","content":"arXiv에 게시된 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2025-10-23-Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2025-10-23T04:08:59.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2025-10-23T04:08:59.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Long-Context LLM"}],["$","meta","23",{"property":"article:tag","content":"Hybrid Attention"}],["$","meta","24",{"property":"article:tag","content":"Linear Attention"}],["$","meta","25",{"property":"article:tag","content":"Mixture-of-Experts"}],["$","meta","26",{"property":"article:tag","content":"FP8 Training"}],["$","meta","27",{"property":"article:tag","content":"GPU Optimization"}],["$","meta","28",{"property":"article:tag","content":"Training-Inference Alignment"}],["$","meta","29",{"property":"article:tag","content":"Reinforcement Learning"}],["$","meta","30",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","31",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","32",{"name":"twitter:title","content":"[논문리뷰] Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning"}],["$","meta","33",{"name":"twitter:description","content":"arXiv에 게시된 'Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning' 논문에 대한 자세한 리뷰입니다."}],["$","link","34",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","35",{"name":"next-size-adjust"}]]
1:null
