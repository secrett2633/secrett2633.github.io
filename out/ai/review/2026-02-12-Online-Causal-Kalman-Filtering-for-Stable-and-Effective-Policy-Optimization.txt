3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],""]
8:I[4080,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],""]
4:["slug","ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","c"]
0:["WcxaIiCPz9cbpnkGvOjOK",[[["",{"children":[["slug","ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/ddc331716d5e47a2.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],"default"]
a:T4c7,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization","description":"이 [arXiv]에 게시한 'Online Causal Kalman Filtering for Stable and Effective Policy Optimization' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","datePublished":"2026-02-11T15:00:00.000Z","dateModified":"2026-02-11T15:00:00.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":263,"articleSection":"Review","keywords":"Review, Reinforcement Learning (RL), Large Language Models (LLMs), Policy Optimization, Importance Sampling (IS) Ratio, Kalman Filter, Variance Reduction, Math Reasoning"}b:Td14,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2602.10609" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Shuo He, Lang Feng, Xin Cheng, Lei Feng, Bo An</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>대규모 언어 모델(LLM)의 강화 학습(RL)에서 토큰 수준 중요도 샘플링(IS) 비율의 높은 분산이 정책 최적화의 불안정성을 야기하는 문제를 해결하고자 합니다. 기존 방법론들이 토큰 간의 시간적 비정책적 편차를 간과하여 정책 경사 업데이트를 왜곡하고 학습 실패로 이어질 수 있음을 식별하고, 이를 극복하여 안정적이고 효과적인 LLM 정책 최적화를 달성하는 것이 목표입니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>이 논문은 <code>Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO)</code>을 제안합니다. <code>KPO</code>는 원하는 IS 비율을 토큰 간에 진화하는 잠재 상태로 모델링하고, <strong>Kalman 필터</strong> 를 적용하여 과거 및 현재 토큰의 정보만을 사용하여 상태를 온라인으로 자기회귀적으로 업데이트합니다. 특히, IS 비율의 <strong>로그 스페이스</strong> 에서 필터링을 수행하고, <strong>과정 노이즈(Q)</strong> 및 <strong>관측 노이즈(V)</strong> 파라미터를 통해 평활화 강도를 조절합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>KPO</strong> 는 <strong>Qwen3-4B</strong> 모델을 사용하여 6개의 수학 추론 벤치마크에서 기존 최신 방법론(GRPO, GMPO, GSPO) 대비 <strong>일관되게 우수한 성능</strong> 을 달성했습니다. 예를 들어, <strong>AIME'24</strong> 벤치마크에서 <strong>GSPO</strong> 의 <strong>avg@16 32.70%</strong> 대비 <strong>KPO-clipped</strong> 는 <strong>37.91%</strong> 를 기록하며 크게 향상되었습니다. 또한 <strong>Kalman 필터링</strong> 을 통해 토큰의 <code>switch frequency</code>가 <strong>0.43에서 0.01로</strong> 대폭 감소하여, IS 비율의 국소적 구조적 일관성이 크게 개선되고 훈련 안정성이 향상됨을 입증했습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p><strong>KPO</strong> 는 LLM의 RL 학습에서 IS 비율 분산으로 인한 <strong>훈련 불안정성 문제</strong> 를 효과적으로 해결하는 실용적인 접근 방식을 제공합니다. 특히 <strong>수학 추론과 같은 복잡한 멀티스텝 태스크</strong> 에서 LLM의 성능과 안정성을 향상시킬 수 있으며, <strong>Q/V 비율</strong> 과 같은 <strong>Kalman 필터 파라미터</strong> 를 튜닝하여 다양한 도메인과 모델 특성(예: <code>MoE 모델</code> 사용)에 맞게 평활화 강도를 조절할 수 있습니다. 이는 복잡한 RL 환경에서 LLM을 성공적으로 훈련시키려는 AI 엔지니어들에게 중요한 도구가 될 것입니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization\"}]}"}}],["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2728,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2026-02-12 00:00:00+0900+0900","children":"2026년 2월 12일"}],["$","time",null,{"className":"ml-4","dateTime":"2026-02-11T15:00:00.000Z","children":["수정: ","2026년 2월 12일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Reinforcement Learning (RL)",{"href":"/tags/Reinforcement%20Learning%20(RL)","className":"page__taxonomy-item","children":["#","Reinforcement Learning (RL)"]}],["$","$L7","Large Language Models (LLMs)",{"href":"/tags/Large%20Language%20Models%20(LLMs)","className":"page__taxonomy-item","children":["#","Large Language Models (LLMs)"]}],["$","$L7","Policy Optimization",{"href":"/tags/Policy%20Optimization","className":"page__taxonomy-item","children":["#","Policy Optimization"]}],["$","$L7","Importance Sampling (IS) Ratio",{"href":"/tags/Importance%20Sampling%20(IS)%20Ratio","className":"page__taxonomy-item","children":["#","Importance Sampling (IS) Ratio"]}],["$","$L7","Kalman Filter",{"href":"/tags/Kalman%20Filter","className":"page__taxonomy-item","children":["#","Kalman Filter"]}],["$","$L7","Variance Reduction",{"href":"/tags/Variance%20Reduction","className":"page__taxonomy-item","children":["#","Variance Reduction"]}],["$","$L7","Math Reasoning",{"href":"/tags/Math%20Reasoning","className":"page__taxonomy-item","children":["#","Math Reasoning"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization","postId":"2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-12-Internalizing-Meta-Experience-into-Memory-for-Guided-Reinforcement-Learning-in-Large-Language-Models","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-12-PhyCritic-Multimodal-Critic-Models-for-Physical-AI","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] PhyCritic: Multimodal Critic Models for Physical AI"}]]}]]}]]}]]}]]}]]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"이 [arXiv]에 게시한 'Online Causal Kalman Filtering for Stable and Effective Policy Optimization' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization"}],["$","meta","14",{"property":"og:description","content":"이 [arXiv]에 게시한 'Online Causal Kalman Filtering for Stable and Effective Policy Optimization' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2026-02-12-Online-Causal-Kalman-Filtering-for-Stable-and-Effective-Policy-Optimization"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2026-02-11T15:00:00.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2026-02-11T15:00:00.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Reinforcement Learning (RL)"}],["$","meta","23",{"property":"article:tag","content":"Large Language Models (LLMs)"}],["$","meta","24",{"property":"article:tag","content":"Policy Optimization"}],["$","meta","25",{"property":"article:tag","content":"Importance Sampling (IS) Ratio"}],["$","meta","26",{"property":"article:tag","content":"Kalman Filter"}],["$","meta","27",{"property":"article:tag","content":"Variance Reduction"}],["$","meta","28",{"property":"article:tag","content":"Math Reasoning"}],["$","meta","29",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","30",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","31",{"name":"twitter:title","content":"[논문리뷰] Online Causal Kalman Filtering for Stable and Effective Policy Optimization"}],["$","meta","32",{"name":"twitter:description","content":"이 [arXiv]에 게시한 'Online Causal Kalman Filtering for Stable and Effective Policy Optimization' 논문에 대한 자세한 리뷰입니다."}],["$","link","33",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","34",{"name":"next-size-adjust"}]]
1:null
