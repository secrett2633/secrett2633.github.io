3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-c27e618569e042bc.js","157","static/chunks/157-17ec11482f243379.js","185","static/chunks/app/layout-c3e2e457f12fb6f6.js"],"default"]
7:I[231,["231","static/chunks/231-c27e618569e042bc.js","877","static/chunks/app/%5B...slug%5D/page-ce60b0a6591d04ed.js"],""]
4:["slug","ai/review/2025-10-23-BAPO_Stabilizing_Off-Policy_Reinforcement_Learning_for_LLMs_via_Balanced_Policy_Optimization_with_Adaptive_Clipping","c"]
0:["qd33V0IpW4K1wJhro15P-",[[["",{"children":[["slug","ai/review/2025-10-23-BAPO_Stabilizing_Off-Policy_Reinforcement_Learning_for_LLMs_via_Balanced_Policy_Optimization_with_Adaptive_Clipping","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-10-23-BAPO_Stabilizing_Off-Policy_Reinforcement_Learning_for_LLMs_via_Balanced_Policy_Optimization_with_Adaptive_Clipping\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-10-23-BAPO_Stabilizing_Off-Policy_Reinforcement_Learning_for_LLMs_via_Balanced_Policy_Optimization_with_Adaptive_Clipping","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/e975486d410ad4e9.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
9:Tb61,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2510.18927">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Junrui Shen, Enyu Zhou, Yang Nan, Xin Guo, Zhiheng Xi</p>
<h2>핵심 연구 목표</h2>
<p>본 논문은 대규모 언어 모델(LLMs)을 위한 오프-폴리시(off-policy) 강화 학습(RL)의 불안정성 문제를 해결하고자 합니다. 오프-폴리시 RL은 정책 엔트로피 급감, 불안정한 최적화, 그리고 훈련 붕괴로 이어지는 경향이 있어, 샘플 효율성에도 불구하고 LLMs에 적용하기 어렵습니다. 저자들은 최적화 불균형과 고정된 클리핑 메커니즘이 엔트로피 증가를 방해하는 문제를 근본 원인으로 지목합니다.</p>
<h2>핵심 방법론</h2>
<p>저자들은 이론적, 경험적 분석을 통해 <strong>최적화 불균형</strong>과 <strong>Entropy-Clip Rule</strong>이라는 두 가지 핵심 통찰을 제시합니다. 이를 바탕으로 **Balanced Policy Optimization with Adaptive Clipping (BAPO)**이라는 새로운 방법을 제안합니다. BAPO는 각 업데이트 단계에서 포지티브 및 네거티브 기여도를 재조정하고, 정책 엔트로피를 보존하며, RL 최적화를 안정화하기 위해 <strong>클리핑 범위(<code>Clow</code> 및 <code>Chigh</code>)를 동적으로 조정</strong>합니다.</p>
<h2>주요 결과</h2>
<p><strong>BAPO</strong>는 다양한 오프-폴리시 시나리오에서 빠르고 안정적이며 데이터 효율적인 훈련을 달성했습니다. <strong>7B BAPO 모델</strong>은 <strong>AIME24에서 70.8점</strong>, <strong>AIME25에서 62.5점</strong>을 기록하여 <strong>SkyWork-OR1-7B</strong>와 같은 오픈소스 모델을 능가했습니다. 특히, <strong>32B BAPO 모델</strong>은 <strong>AIME24에서 87.1점</strong>, <strong>AIME25에서 80.0점</strong>을 달성하여 <strong>Qwen3-32B</strong>와 같은 동급 모델뿐만 아니라 <strong>03-mini-medium</strong> 및 <strong>Gemini-2.5-Flash-Thinking</strong>과 같은 선도적인 상용 시스템의 성능을 뛰어넘었습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><strong>BAPO</strong>는 오프-폴리시 RL을 LLMs에 적용할 때 발생하는 주요 안정성 문제를 해결하는 <strong>간단하지만 효과적인 방법</strong>을 제공합니다. 이는 복잡한 수동 하이퍼파라미터 튜닝 없이도 LLM의 학습 안정성과 성능을 크게 향상시킬 수 있습니다. 특히 부분 롤아웃과 같은 최신 AI 인프라 환경에서 LLM의 <strong>탐색 능력 유지</strong> 및 <strong>과도한 과잉 착취 방지</strong>에 기여하여, 장기적이고 복잡한 LLM 태스크 개발에 실용적인 이점을 제공합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",1450,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-10-23 13:08:59+0900","children":"2025년 10월 23일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 10월 23일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Off-Policy Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Off-Policy Reinforcement Learning"]}],["$","span","Large Language Models",{"className":"page__taxonomy-item","children":["#","Large Language Models"]}],["$","span","Adaptive Clipping",{"className":"page__taxonomy-item","children":["#","Adaptive Clipping"]}],["$","span","Policy Optimization",{"className":"page__taxonomy-item","children":["#","Policy Optimization"]}],["$","span","PPO",{"className":"page__taxonomy-item","children":["#","PPO"]}],["$","span","Entropy Preservation",{"className":"page__taxonomy-item","children":["#","Entropy Preservation"]}],["$","span","RL Stabilization",{"className":"page__taxonomy-item","children":["#","RL Stabilization"]}]]]}]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://secrett2633.github.io/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://secrett2633.github.io/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}]]
1:null
