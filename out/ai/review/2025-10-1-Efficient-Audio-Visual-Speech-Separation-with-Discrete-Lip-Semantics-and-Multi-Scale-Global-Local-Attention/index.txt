3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-c27e618569e042bc.js","157","static/chunks/157-f46787b1a988ffec.js","185","static/chunks/app/layout-b06e577e11976c7d.js"],"default"]
7:I[231,["231","static/chunks/231-c27e618569e042bc.js","877","static/chunks/app/%5B...slug%5D/page-9d772c571b4668c1.js"],""]
4:["slug","ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","c"]
0:["sMj_gMIVMXFZmiE52HQzr",[[["",{"children":[["slug","ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-10-1-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edf391eeca43d999.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
9:T9ea,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2509.23610">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Kai Li, Kejun Gao, Xiaolin Hu</p>
<h2>핵심 연구 목표</h2>
<p>오디오-비주얼 음성 분리(AVSS) 분야에서 기존 모델들의 높은 연산 비용과 파라미터 수로 인해 발생하는 실용적 배포의 한계를 해결하는 것을 목표로 합니다. 특히, 시각 인코더의 '경로 의존성' 문제와 반복적 분리기의 추론 지연 문제를 극복하면서 분리 성능 저하 없이 효율성을 극대화하는 <strong>경량 AVSS 모델</strong> 을 개발하고자 합니다.</p>
<h2>핵심 방법론</h2>
<p>본 연구는 효율적인 AVSS 모델인 <strong>Dolphin</strong> 을 제안합니다. 이를 위해, 립 모션을 이산적인 오디오 정렬 의미 토큰으로 변환하는 이중 경로 경량 비디오 인코더인 <strong>DP-LipCoder</strong> 를 개발했습니다. 음성 분리에는 각 레이어에 멀티 스케일 종속성을 효율적으로 캡처하는 <strong>Global-Local Attention (GLA) 블록</strong> 을 통합한 경량 인코더-디코더 분리기를 구축했습니다.</p>
<h2>주요 결과</h2>
<p><strong>Dolphin</strong> 은 LRS2, LRS3, VoxCeleb2 벤치마크 데이터셋에서 최신 <strong>SOTA IIANet 모델</strong> 보다 우수한 분리 품질을 달성했습니다. 특히, 효율성 측면에서 <strong>IIANet</strong> 대비 <strong>파라미터 50% 이상 감소</strong> , <strong>MACs 2.4배 이상 감소</strong> , <strong>GPU 추론 속도 6배 이상 향상</strong> 이라는 놀라운 성과를 보였습니다. <strong>DP-LipCoder</strong> 는 <strong>3D ResNet-18</strong> 대비 파라미터 93%, MACs 70%를 줄이면서도 유사한 SI-SNRi 성능을 유지했습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><strong>Dolphin</strong> 은 고성능 AVSS를 요구하면서도 리소스가 제한적인 실제 시나리오, 특히 에지 디바이스에서의 배포에 실용적인 해결책을 제시합니다. <strong>DP-LipCoder</strong> 의 이산 립 의미론 사용과 <strong>GLA 블록</strong> 을 통한 효율적인 멀티스케일 특징 학습은 경량 모델 설계에 대한 새로운 방향을 제시하여, 향후 <strong>온디바이스 AI 음성 처리 애플리케이션</strong> 개발에 기여할 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",1863,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-10-01 14:04:08+0900","children":"2025년 10월 1일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 10월 1일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Audio-Visual Speech Separation",{"className":"page__taxonomy-item","children":["#","Audio-Visual Speech Separation"]}],["$","span","Deep Learning",{"className":"page__taxonomy-item","children":["#","Deep Learning"]}],["$","span","Efficiency",{"className":"page__taxonomy-item","children":["#","Efficiency"]}],["$","span","Discrete Lip Semantics",{"className":"page__taxonomy-item","children":["#","Discrete Lip Semantics"]}],["$","span","Global-Local Attention",{"className":"page__taxonomy-item","children":["#","Global-Local Attention"]}],["$","span","Lightweight Models",{"className":"page__taxonomy-item","children":["#","Lightweight Models"]}],["$","span","VQ-VAE",{"className":"page__taxonomy-item","children":["#","VQ-VAE"]}]]]}]}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-10-1-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-10-1-EntroPE-Entropy-Guided-Dynamic-Patch-Encoder-for-Time-Series-Forecasting/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.site/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.site/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","22",{"name":"next-size-adjust"}]]
1:null
