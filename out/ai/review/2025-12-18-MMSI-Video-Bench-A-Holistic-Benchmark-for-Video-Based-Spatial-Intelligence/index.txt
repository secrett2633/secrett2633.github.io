3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-3497e95e26431168.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-a0fe0cc578429896.js"],""]
4:["slug","ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","c"]
0:["uXVO9cTH_KJHTsIHFmRuq",[[["",{"children":[["slug","ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d6cea809dcbae606.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
a:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-a0fe0cc578429896.js"],"default"]
9:Tc59,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2512.10863">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Jingli Lin, Runsen Xu, Shaohao Zhu, Sihan Yang, Peizhou Cao, Yunlong Ran, Miao Hu, Chenming Zhu, Yiman Xie, Yilin Long, Wenbo Hu, Dahua Lin, Tai Wang, Jiangmiao Pang</p>
<h2>핵심 연구 목표</h2>
<p>본 논문은 MLLM(Multi-modal Large Language Models)이 물리적 환경에서 일반적인 비서 역할을 수행하기 위해 필수적인 <strong>비디오 기반 공간 지능</strong> 을 평가할 수 있는 포괄적인 벤치마크의 부재를 해결하고자 합니다. 기존 벤치마크의 한계인 비디오 입력 부족, 질문 유형의 비포괄성, 자동화된 질문 생성의 편향성, 그리고 데이터 소스의 불충분함을 극복하여 모델의 공간 추론 능력을 종합적으로 측정하는 것을 목표로 합니다.</p>
<h2>핵심 방법론</h2>
<p>저자들은 <strong>MMSI-Video-Bench</strong> 를 제안하며, <strong>Perception, Planning, Prediction, Cross-Video Reasoning</strong> 의 네 가지 수준으로 구성된 다단계 프레임워크를 기반으로 합니다. 이 벤치마크는 <strong>25개 공개 데이터셋</strong> 과 자체 촬영 비디오를 포함한 <strong>1,278개 비디오 클립</strong> 에서 파생된 <strong>1,106개의 질문</strong> 으로 구성되며, <strong>11명의 3DV 전문가</strong> 가 수작업으로 질문, 정답, 오답, 상세 근거를 주석했습니다. 또한 <strong>Uniform-50</strong> 및 <strong>Sufficient-Coverage</strong> 두 가지 샘플링 전략을 사용하여 모델을 평가했습니다.</p>
<h2>주요 결과</h2>
<p><strong>Gemini 3 Pro</strong> 를 포함한 최첨단 MLLM들은 인간 성능(96.4%)에 비해 <strong>최대 약 60%에 가까운 현저한 성능 격차(38.0%)</strong> 를 보였습니다. 특히 <strong>Prediction</strong> 카테고리와 <strong>Camera-Instance Spatial Relation</strong> 서브타입이 가장 도전적인 것으로 나타났으며, 공간적으로 미세 조정된 모델조차 벤치마크에 효과적으로 일반화되지 못했습니다. 일반적인 프레임 샘플링 전략인 <strong>AKS (Adaptive Keyframe Sampling)</strong> 나 <strong>3D 공간 단서</strong> 및 <strong>Chain-of-Thought 프롬프트</strong> 는 유의미한 성능 향상을 가져오지 못했습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><strong>MMSI-Video-Bench</strong> 는 MLLM의 비디오 기반 공간 지능 개발에 있어 상당한 개선이 필요함을 명확히 보여줍니다. AI 실무자들은 현재 모델들이 <strong>기하학적 추론, 미세한 동작 이해, 장기적 예측, 비디오 간 대응 관계</strong> 등에서 체계적인 실패를 겪고 있음을 인지하고, 이를 해결하기 위한 보다 <strong>강력한 공간 추론 아키텍처, 효율적인 키프레임 샘플링 전략, 그리고 공간 단서의 효과적인 통합</strong> 에 집중해야 합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2626,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-12-18 00:00:00+0900+0900","children":"2025년 12월 18일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 12월 18일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Video-Based Spatial Intelligence",{"className":"page__taxonomy-item","children":["#","Video-Based Spatial Intelligence"]}],["$","span","MLLM Benchmark",{"className":"page__taxonomy-item","children":["#","MLLM Benchmark"]}],["$","span","Spatial Reasoning",{"className":"page__taxonomy-item","children":["#","Spatial Reasoning"]}],["$","span","Multi-Modal Learning",{"className":"page__taxonomy-item","children":["#","Multi-Modal Learning"]}],["$","span","Perception",{"className":"page__taxonomy-item","children":["#","Perception"]}],["$","span","Planning",{"className":"page__taxonomy-item","children":["#","Planning"]}],["$","span","Prediction",{"className":"page__taxonomy-item","children":["#","Prediction"]}],["$","span","Cross-Video Reasoning",{"className":"page__taxonomy-item","children":["#","Cross-Video Reasoning"]}],["$","span","Human-AI Gap",{"className":"page__taxonomy-item","children":["#","Human-AI Gap"]}]]]}]}],["$","$La",null,{"postPermalink":"/ai/review/2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence/","postId":"2025-12-18-MMSI-Video-Bench-A-Holistic-Benchmark-for-Video-Based-Spatial-Intelligence"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-12-18-In-Pursuit-of-Pixel-Supervision-for-Visual-Pre-training/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] In Pursuit of Pixel Supervision for Visual Pre-training"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-12-18-Qwen-Image-Layered-Towards-Inherent-Editability-via-Layer-Decomposition/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.cloud/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.cloud/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}]]
1:null
