<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ddc331716d5e47a2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-0395f68b8cc78a20.js" async=""></script><script src="/_next/static/chunks/23-7d3f7f0b78aa2fd3.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-467e37449c5a68fc.js" async=""></script><script src="/_next/static/chunks/app/layout-b0a450f8e4964582.js" async=""></script><script src="/_next/static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><title>[논문리뷰] Reinforced Attention Learning - secrett2633&#x27;s blog</title><meta name="description" content="이 [arXiv]에 게시한 &#x27;Reinforced Attention Learning&#x27; 논문에 대한 자세한 리뷰입니다."/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="[논문리뷰] Reinforced Attention Learning"/><meta property="og:description" content="이 [arXiv]에 게시한 &#x27;Reinforced Attention Learning&#x27; 논문에 대한 자세한 리뷰입니다."/><meta property="og:url" content="https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2026-02-05T15:00:00.000Z"/><meta property="article:modified_time" content="2026-02-05T15:00:00.000Z"/><meta property="article:author" content="secrett2633"/><meta property="article:section" content="Review"/><meta property="article:tag" content="Review"/><meta property="article:tag" content="Reinforcement Learning"/><meta property="article:tag" content="Multimodal LLMs"/><meta property="article:tag" content="Attention Mechanisms"/><meta property="article:tag" content="Policy Gradient"/><meta property="article:tag" content="Knowledge Distillation"/><meta property="article:tag" content="Visual Grounding"/><meta property="article:tag" content="Post-training"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@secrett2633"/><meta name="twitter:title" content="[논문리뷰] Reinforced Attention Learning"/><meta name="twitter:description" content="이 [arXiv]에 게시한 &#x27;Reinforced Attention Learning&#x27; 논문에 대한 자세한 리뷰입니다."/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] Reinforced Attention Learning","description":"이 [arXiv]에 게시한 'Reinforced Attention Learning' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning","datePublished":"2026-02-05T15:00:00.000Z","dateModified":"2026-02-05T15:00:00.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":324,"articleSection":"Review","keywords":"Review, Reinforcement Learning, Multimodal LLMs, Attention Mechanisms, Policy Gradient, Knowledge Distillation, Visual Grounding, Post-training"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"Review","item":"https://blog.secrett2633.cloud/ai/review"},{"@type":"ListItem","position":3,"name":"[논문리뷰] Reinforced Attention Learning","item":"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning"}]}</script><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2728<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><a class="hover:text-gray-700" href="/ai/review">Review</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">[논문리뷰] Reinforced Attention Learning</span></li></ol></nav><article class="page"><header class="mb-8"><h1 class="page__title">[논문리뷰] Reinforced Attention Learning</h1><div class="page__meta"><time dateTime="2026-02-06 00:00:00+0900+0900">2026년 2월 6일</time><time class="ml-4" dateTime="2026-02-05T15:00:00.000Z">수정: <!-- -->2026년 2월 6일</time></div></header><div class="page__content"><div><blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2602.04884" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen and Derek Zhiyuan Cheng</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 기존 RL 기반 LLM 후처리 방식이 MLLM에서 시각적 추론을 위한 "생성할 내용"에만 초점을 맞추어 제한적인 성능 향상을 보이거나 심지어 성능을 저하시키는 문제를 해결하고자 합니다. 대신 "어디에 집중할 것인가"로 최적화 목표를 전환하여 MLLM의 <strong>내부 어텐션 분포</strong> 를 직접 최적화함으로써 멀티모달 입력에 대한 효과적인 정보 할당과 <strong>시각적 접지(visual grounding)</strong> 를 향상시키는 새로운 패러다임을 제안합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>저자들은 내부 어텐션 분포를 직접 정책으로 간주하여 최적화하는 <strong>Reinforced Attention Learning (RAL)</strong> 프레임워크를 제안합니다. 이 방식은 성공적인 어텐션 패턴과의 <strong>다이버전스(divergence)</strong> 를 최소화하고, 비최적 패턴으로부터는 다이버전스를 증가시키는 방식으로 모델을 학습시킵니다. 최종 목표 함수는 기존 <strong>토큰 레벨 정책 기울기(LRL)</strong> 와 어텐션 레벨 정규화 항( <strong>λattnLAttnRL</strong> )을 결합하며, <strong>Jensen-Shannon Divergence (JSD)</strong> 를 사용하여 어텐션 정책의 다이버전스를 측정합니다. 또한, 교사 모델의 "어디에 집중하는지"를 학생 모델에 전달하는 <strong>On-Policy Attention Distillation</strong> 기법을 도입하여 교차 모달 정렬을 강화합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p>RAL은 다양한 이미지 및 비디오 벤치마크에서 <strong>GRPO</strong> 및 기본 <strong>Qwen-2.5-VL-7B</strong> 모델보다 일관되게 우수한 성능을 보였습니다. 이미지 VQA에서 *<em>V</em> (+5.8점) **, ** MME (+94.1점) **, ** ChartQA (+2.8점) ** 등의 상당한 개선을 달성했으며, 장기 비디오 벤치마크에서는 ** LongVideoBench (+2.2점) *<em>, ** NExTQA (+3.4점) ** 등 6/7개의 데이터셋에서 GRPO를 능가했습니다. 특히 ** On-Policy Attention Distillation <strong>은 표준 증류 방식보다 7/8개의 벤치마크에서 더 나은 결과를 보이며, 시각적 해상도 증가에 따라 RAL의 성능 이점은 더욱 커지는 경향을 보였습니다(</strong> V</em> 2048 토큰에서 +6.3점 **).</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>RAL은 MLLM 후처리 과정에서 ** 내부 어텐션 메커니즘 **을 직접 최적화하는 것이 토큰 기반 RL보다 ** 더욱 안정적이고 일반화 가능한 학습 신호 **를 제공함을 보여주었습니다. 이는 AI/ML 엔지니어들이 멀티모달 모델의 ** 시각적 접지 ** 및 ** 추론 능력 **을 향상시키기 위한 효과적인 새로운 방법론을 제공합니다. 특히, ** On-Policy Attention Distillation <strong>은 교사 모델의 고품질 어텐션 패턴을 학생 모델에 전이시켜 ** 크로스 모달 정렬</strong>을 강화하고, 복잡한 시각-언어 이해 및 장기 비디오 분석 태스크에서 모델의 성능을 향상시키는 데 실용적인 가치를 가집니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
</div></div><footer class="page__meta mt-8"><div class="page__taxonomy"><span class="text-sm font-medium text-gray-900 mb-2 block">태그</span><a class="page__taxonomy-item" href="/tags/Review">#<!-- -->Review</a><a class="page__taxonomy-item" href="/tags/Reinforcement%20Learning">#<!-- -->Reinforcement Learning</a><a class="page__taxonomy-item" href="/tags/Multimodal%20LLMs">#<!-- -->Multimodal LLMs</a><a class="page__taxonomy-item" href="/tags/Attention%20Mechanisms">#<!-- -->Attention Mechanisms</a><a class="page__taxonomy-item" href="/tags/Policy%20Gradient">#<!-- -->Policy Gradient</a><a class="page__taxonomy-item" href="/tags/Knowledge%20Distillation">#<!-- -->Knowledge Distillation</a><a class="page__taxonomy-item" href="/tags/Visual%20Grounding">#<!-- -->Visual Grounding</a><a class="page__taxonomy-item" href="/tags/Post-training">#<!-- -->Post-training</a></div></footer><section class="comments-section mt-12 pt-8 border-t border-gray-200"></section><section class="mt-12 border-t border-gray-200 pt-8"><h3 class="text-base font-semibold text-gray-900 mb-4">Review<!-- --> 의 다른글</h3><ul class="space-y-2 text-sm"><li class="text-gray-500">이전글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules">[논문리뷰] RISE-Video: Can Video Generators Decode Implicit World Rules?</a></li><li class="text-gray-900 font-semibold">현재글 : <!-- -->[논문리뷰] Reinforced Attention Learning</li><li class="text-gray-500">다음글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents">[논문리뷰] Reinforcement World Model Learning for LLM-based Agents</a></li></ul></section></article></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ddc331716d5e47a2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[9157,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"default\"]\na:I[231,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js\"],\"\"]\nb:I[4080,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"\"]\nd:I[6130,[],\"\"]\n7:[\"slug\",\"ai/review/2026-02-06-Reinforced-Attention-Learning\",\"c\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ddc331716d5e47a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"WcxaIiCPz9cbpnkGvOjOK\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/ai/review/2026-02-06-Reinforced-Attention-Learning\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"ai/review/2026-02-06-Reinforced-Attention-Learning\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"review\\\",\\\"2026-02-06-Reinforced-Attention-Learning\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"ai/review/2026-02-06-Reinforced-Attention-Learning\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\"],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lb\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lb\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"10:I[646,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js\"],\"default\"]\nf:Teef,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e링크:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/2602.04884\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e논문 PDF로 바로 열기\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e저자:\u003c/strong\u003e Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen and Derek Zhiyuan Cheng\u003c/p\u003e\n\u003ch2 id=\"핵심-연구-목표\"\u003e\u003ca href=\"#핵심-연구-목표\"\u003e핵심 연구 목표\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e본 논문은 기존 RL 기반 LLM 후처리 방식이 MLLM에서 시각적 추론을 위한 \"생성할 내용\"에만 초점을 맞추어 제한적인 성능 향상을 보이거나 심지어 성능을 저하시키는 문제를 해결하고자 합니다. 대신 \"어디에 집중할 것인가\"로 최적화 목표를 전환하여 MLLM의 \u003cstrong\u003e내부 어텐션 분포\u003c/strong\u003e 를 직접 최적화함으로써 멀티모달 입력에 대한 효과적인 정보 할당과 \u003cstrong\u003e시각적 접지(visual grounding)\u003c/strong\u003e 를 향상시키는 새로운 패러다임을 제안합니다.\u003c/p\u003e\n\u003ch2 id=\"핵심-방법론\"\u003e\u003ca href=\"#핵심-방법론\"\u003e핵심 방법론\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e저자들은 내부 어텐션 분포를 직접 정책으로 간주하여 최적화하는 \u003cstrong\u003eReinforced Attention Learning (RAL)\u003c/strong\u003e 프레임워크를 제안합니다. 이 방식은 성공적인 어텐션 패턴과의 \u003cstrong\u003e다이버전스(divergence)\u003c/strong\u003e 를 최소화하고, 비최적 패턴으로부터는 다이버전스를 증가시키는 방식으로 모델을 학습시킵니다. 최종 목표 함수는 기존 \u003cstrong\u003e토큰 레벨 정책 기울기(LRL)\u003c/strong\u003e 와 어텐션 레벨 정규화 항( \u003cstrong\u003eλattnLAttnRL\u003c/strong\u003e )을 결합하며, \u003cstrong\u003eJensen-Shannon Divergence (JSD)\u003c/strong\u003e 를 사용하여 어텐션 정책의 다이버전스를 측정합니다. 또한, 교사 모델의 \"어디에 집중하는지\"를 학생 모델에 전달하는 \u003cstrong\u003eOn-Policy Attention Distillation\u003c/strong\u003e 기법을 도입하여 교차 모달 정렬을 강화합니다.\u003c/p\u003e\n\u003ch2 id=\"주요-결과\"\u003e\u003ca href=\"#주요-결과\"\u003e주요 결과\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eRAL은 다양한 이미지 및 비디오 벤치마크에서 \u003cstrong\u003eGRPO\u003c/strong\u003e 및 기본 \u003cstrong\u003eQwen-2.5-VL-7B\u003c/strong\u003e 모델보다 일관되게 우수한 성능을 보였습니다. 이미지 VQA에서 *\u003cem\u003eV\u003c/em\u003e (+5.8점) **, ** MME (+94.1점) **, ** ChartQA (+2.8점) ** 등의 상당한 개선을 달성했으며, 장기 비디오 벤치마크에서는 ** LongVideoBench (+2.2점) *\u003cem\u003e, ** NExTQA (+3.4점) ** 등 6/7개의 데이터셋에서 GRPO를 능가했습니다. 특히 ** On-Policy Attention Distillation \u003cstrong\u003e은 표준 증류 방식보다 7/8개의 벤치마크에서 더 나은 결과를 보이며, 시각적 해상도 증가에 따라 RAL의 성능 이점은 더욱 커지는 경향을 보였습니다(\u003c/strong\u003e V\u003c/em\u003e 2048 토큰에서 +6.3점 **).\u003c/p\u003e\n\u003ch2 id=\"ai-실무자를-위한-시사점\"\u003e\u003ca href=\"#ai-실무자를-위한-시사점\"\u003eAI 실무자를 위한 시사점\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eRAL은 MLLM 후처리 과정에서 ** 내부 어텐션 메커니즘 **을 직접 최적화하는 것이 토큰 기반 RL보다 ** 더욱 안정적이고 일반화 가능한 학습 신호 **를 제공함을 보여주었습니다. 이는 AI/ML 엔지니어들이 멀티모달 모델의 ** 시각적 접지 ** 및 ** 추론 능력 **을 향상시키기 위한 효과적인 새로운 방법론을 제공합니다. 특히, ** On-Policy Attention Distillation \u003cstrong\u003e은 교사 모델의 고품질 어텐션 패턴을 학생 모델에 전이시켜 ** 크로스 모달 정렬\u003c/strong\u003e을 강화하고, 복잡한 시각-언어 이해 및 장기 비디오 분석 태스크에서 모델의 성능을 향상시키는 데 실용적인 가치를 가집니다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e⚠️ \u003cstrong\u003e알림:\u003c/strong\u003e 이 리뷰는 AI로 작성되었습니다.\u003c/p\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"[논문리뷰] Reinforced Attention Learning\\\",\\\"description\\\":\\\"이 [arXiv]에 게시한 'Reinforced Attention Learning' 논문에 대한 자세한 리뷰입니다.\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning\\\",\\\"datePublished\\\":\\\"2026-02-05T15:00:00.000Z\\\",\\\"dateModified\\\":\\\"2026-02-05T15:00:00.000Z\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning\\\"},\\\"image\\\":\\\"https://blog.secrett2633.cloud/og-default.png\\\",\\\"isAccessibleForFree\\\":true,\\\"inLanguage\\\":\\\"ko\\\",\\\"wordCount\\\":324,\\\"articleSection\\\":\\\"Review\\\",\\\"keywords\\\":\\\"Review, Reinforcement Learning, Multimodal LLMs, Attention Mechanisms, Policy Gradient, Knowledge Distillation, Visual Grounding, Post-training\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"Review\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":3,\\\"name\\\":\\\"[논문리뷰] Reinforced Attention Learning\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2728,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/ai/review\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"hover:text-gray-700\",\"children\":\"Review\"}]]}],[\"$\",\"li\",\"/ai/review/2026-02-06-Reinforced-Attention-Learning\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"[논문리뷰] Reinforced Attention Learning\"}]]}]]]}]}],[\"$\",\"article\",null,{\"className\":\"page\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"page__title\",\"children\":\"[논문리뷰] Reinforced Attention Learning\"}],[\"$\",\"div\",null,{\"className\":\"page__meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2026-02-06 00:00:00+0900+0900\",\"children\":\"2026년 2월 6일\"}],[\"$\",\"time\",null,{\"className\":\"ml-4\",\"dateTime\":\"2026-02-05T15:00:00.000Z\",\"children\":[\"수정: \",\"2026년 2월 6일\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"page__content\",\"children\":[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$f\"}}]}],[\"$\",\"footer\",null,{\"className\":\"page__meta mt-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"page__taxonomy\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-gray-900 mb-2 block\",\"children\":\"태그\"}],[[\"$\",\"$La\",\"Review\",{\"href\":\"/tags/Review\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Review\"]}],[\"$\",\"$La\",\"Reinforcement Learning\",{\"href\":\"/tags/Reinforcement%20Learning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Reinforcement Learning\"]}],[\"$\",\"$La\",\"Multimodal LLMs\",{\"href\":\"/tags/Multimodal%20LLMs\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Multimodal LLMs\"]}],[\"$\",\"$La\",\"Attention Mechanisms\",{\"href\":\"/tags/Attention%20Mechanisms\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Attention Mechanisms\"]}],[\"$\",\"$La\",\"Policy Gradient\",{\"href\":\"/tags/Policy%20Gradient\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Policy Gradient\"]}],[\"$\",\"$La\",\"Knowledge Distillation\",{\"href\":\"/tags/Knowledge%20Distillation\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Knowledge Distillation\"]}],[\"$\",\"$La\",\"Visual Grounding\",{\"href\":\"/tags/Visual%20Grounding\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Visual Grounding\"]}],[\"$\",\"$La\",\"Post-training\",{\"href\":\"/tags/Post-training\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Post-training\"]}]]]}]}],[\"$\",\"$L10\",null,{\"postPermalink\":\"/ai/review/2026-02-06-Reinforced-Attention-Learning\",\"postId\":\"2026-02-06-Reinforced-Attention-Learning\"}],[\"$\",\"section\",null,{\"className\":\"mt-12 border-t border-gray-200 pt-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 mb-4\",\"children\":[\"Review\",\" 의 다른글\"]}],[\"$\",\"ul\",null,{\"className\":\"space-y-2 text-sm\",\"children\":[[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"이전글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2026-02-06-RISE-Video-Can-Video-Generators-Decode-Implicit-World-Rules\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] RISE-Video: Can Video Generators Decode Implicit World Rules?\"}]]}],[\"$\",\"li\",null,{\"className\":\"text-gray-900 font-semibold\",\"children\":[\"현재글 : \",\"[논문리뷰] Reinforced Attention Learning\"]}],[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"다음글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2026-02-06-Reinforcement-World-Model-Learning-for-LLM-based-Agents\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] Reinforcement World Model Learning for LLM-based Agents\"}]]}]]}]]}]]}]]}]]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"[논문리뷰] Reinforced Attention Learning - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"이 [arXiv]에 게시한 'Reinforced Attention Learning' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"[논문리뷰] Reinforced Attention Learning\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"이 [arXiv]에 게시한 'Reinforced Attention Learning' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/ai/review/2026-02-06-Reinforced-Attention-Learning\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:published_time\",\"content\":\"2026-02-05T15:00:00.000Z\"}],[\"$\",\"meta\",\"18\",{\"property\":\"article:modified_time\",\"content\":\"2026-02-05T15:00:00.000Z\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:author\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:section\",\"content\":\"Review\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"Review\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"Reinforcement Learning\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"Multimodal LLMs\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"Attention Mechanisms\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"Policy Gradient\"}],[\"$\",\"meta\",\"26\",{\"property\":\"article:tag\",\"content\":\"Knowledge Distillation\"}],[\"$\",\"meta\",\"27\",{\"property\":\"article:tag\",\"content\":\"Visual Grounding\"}],[\"$\",\"meta\",\"28\",{\"property\":\"article:tag\",\"content\":\"Post-training\"}],[\"$\",\"meta\",\"29\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:creator\",\"content\":\"@secrett2633\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:title\",\"content\":\"[논문리뷰] Reinforced Attention Learning\"}],[\"$\",\"meta\",\"32\",{\"name\":\"twitter:description\",\"content\":\"이 [arXiv]에 게시한 'Reinforced Attention Learning' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"link\",\"33\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"34\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script></body></html>