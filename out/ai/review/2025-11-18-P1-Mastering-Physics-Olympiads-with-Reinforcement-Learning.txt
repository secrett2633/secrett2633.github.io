3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],""]
8:I[4080,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],""]
4:["slug","ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","c"]
0:["WcxaIiCPz9cbpnkGvOjOK",[[["",{"children":[["slug","ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/ddc331716d5e47a2.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],"default"]
a:T471,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning","description":"Haiyuan Wan이 [arXiv]에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","datePublished":"2025-11-17T15:00:00.000Z","dateModified":"2025-11-17T15:00:00.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":263,"articleSection":"Review","keywords":"Review, Reinforcement Learning, Large Language Models, Physics Reasoning, Agentic AI, Olympiad Problems, Post-Training, Knowledge Transfer"}b:Tcac,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2511.13612" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Jiacheng Chen, Qianjia Cheng, Fangchen Yu, Haiyuan Wan, Yuchen Zhang, Shenghe Zheng, Junchi Yao, et al.</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 대규모 언어 모델(LLM)이 퍼즐 풀이를 넘어 과학 수준의 추론 능력을 갖추도록 발전시키고, 특히 복잡한 물리학 올림피아드 문제를 해결하는 능력을 향상시키는 것을 목표로 합니다. 이를 통해 LLM이 물리적 현실과 자연 법칙의 엄격한 제약을 준수하는, 진정한 과학적 추론 능력을 입증하고자 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>P1 모델은 <strong>강화 학습(RL)</strong> 을 통한 사후 훈련으로 개발되었으며, <strong>Group Sequence Policy Optimization (GSPO)</strong> 을 기반으로 <strong>적응형 학습 가능성 조정(Adaptive Learnability Adjustment)</strong> 및 <strong>훈련 안정화 메커니즘(Training Stabilization Mechanism)</strong> 을 통합합니다. 추론 시에는 P1 모델을 <strong>PhysicsMinions 에이전트 프레임워크</strong> 와 결합하여 반복적인 수정 및 자가 검증 능력을 통해 문제 해결 깊이를 확장합니다. 훈련 데이터는 5,065개의 올림피아드 수준 물리 문제로 구성된 <strong>HiPhO 벤치마크</strong> 를 사용합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p>플래그십 모델인 <strong>P1-235B-A22B</strong> 는 IPhO 2025에서 <strong>21.2/30점</strong> 을 획득하여 최초로 오픈소스 모델 중 금메달 성능을 달성했으며, 전체 HiPhO 스위트에서 <strong>12개의 금메달과 1개의 은메달</strong> 을 획득했습니다. <strong>PhysicsMinions 에이전트 시스템</strong> 과 결합된 <strong>P1-235B-A22B+PhysicsMinions</strong> 는 IPhO 2025에서 <strong>23.2/30점</strong> 으로 전체 1위를 차지했습니다. 또한, 30B 모델은 7개의 수학, 코딩, 일반 추론 벤치마크에서 베이스 모델을 뛰어넘는 뛰어난 일반화 성능을 보였습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>P1 모델의 성공은 LLM이 복잡한 과학적 추론 태스크에서 인간 수준의 성능을 달성할 수 있음을 보여주며, 특히 <strong>강화 학습</strong> 과 <strong>에이전트 시스템</strong> 의 결합이 그 핵심임을 시사합니다. 이는 AI가 미래 물리학 연구를 지원하거나 선도하는 데 기여할 수 있는 중요한 단계를 나타냅니다. 또한, 도메인 특화된 사후 훈련이 타 도메인으로의 <strong>일반화 및 전이 학습 능력</strong> 을 향상시킬 수 있음을 입증하여, 특정 과학 분야 LLM 개발에 대한 새로운 방향을 제시합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning\"}]}"}}],["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2728,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-11-18 00:00:00+0900+0900","children":"2025년 11월 18일"}],["$","time",null,{"className":"ml-4","dateTime":"2025-11-17T15:00:00.000Z","children":["수정: ","2025년 11월 18일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Reinforcement Learning",{"href":"/tags/Reinforcement%20Learning","className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","$L7","Large Language Models",{"href":"/tags/Large%20Language%20Models","className":"page__taxonomy-item","children":["#","Large Language Models"]}],["$","$L7","Physics Reasoning",{"href":"/tags/Physics%20Reasoning","className":"page__taxonomy-item","children":["#","Physics Reasoning"]}],["$","$L7","Agentic AI",{"href":"/tags/Agentic%20AI","className":"page__taxonomy-item","children":["#","Agentic AI"]}],["$","$L7","Olympiad Problems",{"href":"/tags/Olympiad%20Problems","className":"page__taxonomy-item","children":["#","Olympiad Problems"]}],["$","$L7","Post-Training",{"href":"/tags/Post-Training","className":"page__taxonomy-item","children":["#","Post-Training"]}],["$","$L7","Knowledge Transfer",{"href":"/tags/Knowledge%20Transfer","className":"page__taxonomy-item","children":["#","Knowledge Transfer"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning","postId":"2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-11-18-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-11-18-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Part-X-MLLM: Part-aware 3D Multimodal Large Language Model"}]]}]]}]]}]]}]]}]]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"Haiyuan Wan이 [arXiv]에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning"}],["$","meta","14",{"property":"og:description","content":"Haiyuan Wan이 [arXiv]에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2025-11-18-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2025-11-17T15:00:00.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2025-11-17T15:00:00.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Reinforcement Learning"}],["$","meta","23",{"property":"article:tag","content":"Large Language Models"}],["$","meta","24",{"property":"article:tag","content":"Physics Reasoning"}],["$","meta","25",{"property":"article:tag","content":"Agentic AI"}],["$","meta","26",{"property":"article:tag","content":"Olympiad Problems"}],["$","meta","27",{"property":"article:tag","content":"Post-Training"}],["$","meta","28",{"property":"article:tag","content":"Knowledge Transfer"}],["$","meta","29",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","30",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","31",{"name":"twitter:title","content":"[논문리뷰] P1: Mastering Physics Olympiads with Reinforcement Learning"}],["$","meta","32",{"name":"twitter:description","content":"Haiyuan Wan이 [arXiv]에 게시한 'P1: Mastering Physics Olympiads with Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}],["$","link","33",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","34",{"name":"next-size-adjust"}]]
1:null
