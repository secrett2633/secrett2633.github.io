3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-c27e618569e042bc.js","157","static/chunks/157-90aef3381d42edea.js","185","static/chunks/app/layout-b06e577e11976c7d.js"],"default"]
7:I[231,["231","static/chunks/231-c27e618569e042bc.js","877","static/chunks/app/%5B...slug%5D/page-9d772c571b4668c1.js"],""]
4:["slug","ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","c"]
0:["vXhrfv3Vb9rIvRlX83B9d",[[["",{"children":[["slug","ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-8-20-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edf391eeca43d999.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
9:Tb08,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2508.09789">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Marco De Nadai, Andreas Damianou, Mounia Lalmas</p>
<h2>핵심 연구 목표</h2>
<p>기존 비디오 추천 시스템의 한계인 저수준 시각/음성 특징 및 메타데이터의 의미론적 깊이 부족 문제를 해결하는 것이 목표입니다. 사용자의 의도, 유머, 세계 지식과 같은 고수준의 의미를 포착하여 비디오 클립이 시청자에게 공감을 얻는 이유를 파악하고, 이를 통해 개인화된 추천의 질을 향상시키고자 합니다.</p>
<h2>핵심 방법론</h2>
<p>이 논문은 추천 시스템 모델에 구애받지 않는 <strong>제로-미세 조정(zero-finetuning) 프레임워크</strong> 를 제안합니다. 오프-더-셸프 <strong>Multimodal Large Language Model (MLLM)</strong> 인 <strong>Qwen-VL</strong> 을 활용하여 각 비디오 클립을 풍부한 자연어 설명으로 요약하고, 오디오를 위해 <strong>Whisper</strong> 와 <strong>Qwen-Audio</strong> 를 결합하여 텍스트 및 사운드 분류를 통합합니다. 생성된 텍스트 설명은 <strong>BGE-large 텍스트 인코더</strong> 를 통해 임베딩으로 변환된 후, <strong>Two-Towers</strong> 및 <strong>SASRec</strong> 과 같은 표준 협업 및 콘텐츠 기반 추천 모델에 입력됩니다.</p>
<h2>주요 결과</h2>
<p><strong>MicroLens-100K 데이터셋</strong> 에서 진행된 실험 결과, MLLM 기반 특징은 기존 시각, 오디오, 메타데이터 특징을 일관되게 능가했습니다. 특히 <strong>Two-Towers 모델</strong> 에서 MLLM 오디오 특징은 <strong>HR@10을 0.0253에서 0.0405로, nDCG@10을 0.0130에서 0.0214로 향상</strong> 시켜 약 <strong>60%의 상대적 성능 향상</strong> 을 보였습니다. MLLM 비디오 특징 또한 <strong>HR@10을 0.0393에서 0.0489로</strong> 향상시키는 등 긍정적인 결과를 보였으며, 이는 <strong>30초 이상의 긴 비디오에서 더욱 두드러졌습니다.</strong></p>
<h2>AI 실무자를 위한 시사점</h2>
<p>이 연구는 <strong>MLLM</strong> 이 비디오 추천 시스템의 성능을 혁신적으로 개선할 수 있음을 보여줍니다. <strong>제로-미세 조정 접근 방식</strong> 은 대규모 MLLM을 직접 학습시키기 어려운 환경에서 매우 실용적이며, 기존 추천 파이프라인에 쉽게 통합할 수 있습니다. AI 실무자들은 이 프레임워크를 활용하여 사용자의 내재된 선호와 콘텐츠의 미묘한 맥락을 더 잘 이해하는 추천 시스템을 구축할 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",1702,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-08-20 13:26:54+0900","children":"2025년 8월 20일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 8월 20일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Multimodal Large Language Models",{"className":"page__taxonomy-item","children":["#","Multimodal Large Language Models"]}],["$","span","Video Recommendation",{"className":"page__taxonomy-item","children":["#","Video Recommendation"]}],["$","span","Zero-Shot Learning",{"className":"page__taxonomy-item","children":["#","Zero-Shot Learning"]}],["$","span","Content-Based Filtering",{"className":"page__taxonomy-item","children":["#","Content-Based Filtering"]}],["$","span","Natural Language Processing",{"className":"page__taxonomy-item","children":["#","Natural Language Processing"]}],["$","span","Foundation Models",{"className":"page__taxonomy-item","children":["#","Foundation Models"]}]]]}]}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-8-20-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-8-20-Embodied-R1-Reinforced-Embodied-Reasoning-for-General-Robotic-Manipulation/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.site/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.site/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","22",{"name":"next-size-adjust"}]]
1:null
