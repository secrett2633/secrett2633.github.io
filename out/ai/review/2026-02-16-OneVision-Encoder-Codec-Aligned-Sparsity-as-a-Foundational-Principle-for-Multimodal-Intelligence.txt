3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
7:I[231,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],""]
8:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
4:["slug","ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","c"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":[["slug","ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],"default"]
a:T513,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence","description":"arXiv에 게시된 'OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","datePublished":"2026-02-15T15:00:00.000Z","dateModified":"2026-02-15T15:00:00.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":282,"articleSection":"Review","keywords":"Review, Multimodal AI, Video Understanding, Sparse Attention, Vision Transformer, Codec-Aligned Processing, Self-Supervised Learning, Predictive Coding, Efficient AI"}b:Td47,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2602.08683" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Feilong Tang, Xiang An, Yunyao Yan, Yin Xie, Bin Qin, Kaicheng Yang, Yifei Shen, Yuanhan Zhang, Chunyuan Li, Shikun Feng, Changrui Chen, Huajie Tan, Ming Hu, Manyuan Zhang, Bo Li, Ziyong Feng, Ziwei Liu, Zongyuan Ge, Jiankang Deng</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 현대 비전 아키텍처가 시각 신호의 본질적인 중복성과 변별 정보의 희소성을 효율적으로 다루지 못한다는 문제의식에서 출발합니다. 시각적 이해 문제를 해결하기 위해 <strong>정보 이론적 원칙(코덱)</strong> 에 아키텍처를 맞춰 계산 효율성을 높이고 성능을 개선하며, 범용 멀티모달 지능을 위한 확장 가능한 기반을 구축하는 것을 목표로 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p><strong>OneVision-Encoder</strong> 는 <strong>HEVC(High Efficiency Video Coding) 스타일 비전 트랜스포머</strong> 를 사용하여 예측 시각 구조를 의미론적 의미로 압축합니다. <strong>Codec Patchification</strong> 을 통해 신호 엔트로피가 풍부한 영역(전체 영역의 <strong>3.1%-25%</strong> )에만 선택적으로 초점을 맞추며, 불규칙한 토큰 레이아웃을 위해 <strong>공유 3D ROPE</strong> 를 사용합니다. <strong>백만 개 이상의 의미 개념</strong> 에 대한 <strong>클러스터 판별 목적 함수</strong> 로 학습되어 객체 영속성과 모션 역학을 동시에 포착합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>OneVision-Encoder</strong> 는 효율성과 정확도가 양의 상관관계를 가지며, 밀집 그리드와 희소 시맨틱 간의 격차를 해소하여 성능 한계를 재정의했습니다. <strong>Qwen3-ViT</strong> 및 <strong>SigLIP2</strong> 와 같은 강력한 비전 백본을 <strong>16개 이미지, 비디오, 문서 이해 벤치마크</strong> 에서 일관되게 능가하며, <strong>Qwen3-ViT 대비 비디오 태스크에서 평균 4.1% 성능 향상</strong> 을 달성했습니다. <strong>Diving-48</strong> 에서는 <strong>SigLIP2 및 DINOv3 대비 각각 17.1% 및 8.1% Top-1 정확도 향상</strong> 을 보여주었습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p><strong>코덱 정렬 희소성 원칙</strong> 은 비전 아키텍처 설계에 있어 중요한 패러다임 변화를 제시합니다. <strong>OneVision-Encoder</strong> 는 적은 시각 토큰과 적은 사전 훈련 데이터로 뛰어난 멀티모달 성능을 달성하여, <strong>리소스 효율적인 AI 모델 개발</strong> 에 기여합니다. 이는 비디오 코덱의 구조적 원리를 활용하여 대규모 멀티모달 모델에서 <strong>시공간적 이해와 추론 능력</strong> 을 향상시키는 실용적인 방법을 제공하며, 차세대 범용 시각 인코더의 기반이 될 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2026-02-16 00:00:00+0900+0900","children":"2026년 2월 16일"}],["$","time",null,{"className":"ml-4","dateTime":"2026-02-15T15:00:00.000Z","children":["수정: ","2026년 2월 16일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Multimodal AI",{"href":"/tags/Multimodal%20AI","className":"page__taxonomy-item","children":["#","Multimodal AI"]}],["$","$L7","Video Understanding",{"href":"/tags/Video%20Understanding","className":"page__taxonomy-item","children":["#","Video Understanding"]}],["$","$L7","Sparse Attention",{"href":"/tags/Sparse%20Attention","className":"page__taxonomy-item","children":["#","Sparse Attention"]}],["$","$L7","Vision Transformer",{"href":"/tags/Vision%20Transformer","className":"page__taxonomy-item","children":["#","Vision Transformer"]}],["$","$L7","Codec-Aligned Processing",{"href":"/tags/Codec-Aligned%20Processing","className":"page__taxonomy-item","children":["#","Codec-Aligned Processing"]}],["$","$L7","Self-Supervised Learning",{"href":"/tags/Self-Supervised%20Learning","className":"page__taxonomy-item","children":["#","Self-Supervised Learning"]}],["$","$L7","Predictive Coding",{"href":"/tags/Predictive%20Coding","className":"page__taxonomy-item","children":["#","Predictive Coding"]}],["$","$L7","Efficient AI",{"href":"/tags/Efficient%20AI","className":"page__taxonomy-item","children":["#","Efficient AI"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence","postId":"2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-16-On-Robustness-and-Chain-of-Thought-Consistency-of-RL-Finetuned-VLMs","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2026-02-16-RLinf-Co-Reinforcement-Learning-Based-Sim-Real-Co-Training-for-VLA-Models","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models"}]]}]]}]]}]]}]]}]]}]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"arXiv에 게시된 'OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence"}],["$","meta","14",{"property":"og:description","content":"arXiv에 게시된 'OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2026-02-16-OneVision-Encoder-Codec-Aligned-Sparsity-as-a-Foundational-Principle-for-Multimodal-Intelligence"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2026-02-15T15:00:00.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2026-02-15T15:00:00.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Multimodal AI"}],["$","meta","23",{"property":"article:tag","content":"Video Understanding"}],["$","meta","24",{"property":"article:tag","content":"Sparse Attention"}],["$","meta","25",{"property":"article:tag","content":"Vision Transformer"}],["$","meta","26",{"property":"article:tag","content":"Codec-Aligned Processing"}],["$","meta","27",{"property":"article:tag","content":"Self-Supervised Learning"}],["$","meta","28",{"property":"article:tag","content":"Predictive Coding"}],["$","meta","29",{"property":"article:tag","content":"Efficient AI"}],["$","meta","30",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","31",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","32",{"name":"twitter:title","content":"[논문리뷰] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence"}],["$","meta","33",{"name":"twitter:description","content":"arXiv에 게시된 'OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence' 논문에 대한 자세한 리뷰입니다."}],["$","link","34",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","35",{"name":"next-size-adjust"}]]
1:null
