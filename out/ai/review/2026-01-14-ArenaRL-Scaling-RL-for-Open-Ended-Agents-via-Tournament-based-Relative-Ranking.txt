3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-53ec9cbf619543e1.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-80e07cee06d17a0b.js"],""]
4:["slug","ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","c"]
0:["_3-KrKKQh7tcSoXiYwQMB",[[["",{"children":[["slug","ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d6cea809dcbae606.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
a:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-80e07cee06d17a0b.js"],"default"]
9:Te7e,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2601.06487">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang, Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang, Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun Zha</p>
<h2>핵심 연구 목표</h2>
<p>본 연구는 개방형 에이전트 태스크에서 <strong>LLM 에이전트</strong> 의 강화 학습(RL) 성능을 저해하는 <strong>"판별 붕괴(discriminative collapse)"</strong> 문제를 해결하고자 합니다. 기존의 개별 응답에 대한 <strong>점수 기반 보상 모델</strong> 이 미묘한 성능 차이를 구분하지 못하고 노이즈에 취약해지는 문제를 해결하여, 효율적이고 안정적인 정책 최적화를 가능하게 하는 새로운 <strong>보상 신호 패러다임</strong> 을 제시하는 것이 목표입니다.</p>
<h2>핵심 방법론</h2>
<p>제안된 <strong>ArenaRL</strong> 은 <strong>점수 기반의 스칼라 보상</strong> 대신 <strong>그룹 내 상대적 순위</strong> 를 사용하는 강화 학습 패러다임입니다. 이를 위해 <strong>다단계 루브릭</strong> 을 활용한 <strong>과정 인지 쌍대 평가(process-aware pairwise evaluation)</strong> 메커니즘을 도입하여 궤적에 대한 미세한 상대 점수를 할당하며, <strong>토너먼트 기반의 순위 결정 체계</strong> 를 고안하여 안정적인 이점 신호를 획득합니다. 특히, <code>Seeded Single-Elimination</code> 토폴로지가 <strong>O(N) 복잡도</strong> 로 <strong>O(N^2) 복잡도</strong> 의 완전한 쌍대 비교와 거의 동등한 정확도를 달성하며 효율성과 정확성의 최적 균형을 제공함을 검증했습니다.</p>
<h2>주요 결과</h2>
<p><code>ArenaRL</code>은 <strong>Open-Travel</strong> 및 <strong>Open-DeepResearch</strong> 벤치마크에서 표준 RL 베이스라인(예: <strong>SFT</strong> , <strong>GRPO</strong> , <strong>GSPO</strong> )을 크게 능가했습니다. <strong>Open-Travel</strong> 에서 <code>ArenaRL</code>은 평균 승률 <strong>41.8%</strong> 를 달성하여 <strong>GRPO(16.4%)</strong> 및 <strong>GSPO(17.2%)</strong> 를 크게 앞섰으며, <strong>Open-DeepResearch</strong> 에서는 승률 <strong>64.3%</strong> 와 유효 생성률 <strong>99%</strong> 를 기록했습니다. <code>Seeded Single-Elimination</code> 방식은 <strong>O(N) 복잡도</strong> 로 <strong>Round-Robin</strong> 의 <strong>O(N^2)</strong> 방식에 상응하는 <strong>32.5%</strong> 의 평균 승률을 보였습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><code>ArenaRL</code>은 <strong>개방형 태스크</strong> 와 같이 객관적인 정답이 불분명하고 보상 모델이 불안정한 환경에서 <strong>LLM 에이전트</strong> 의 성능을 개선할 수 있는 강력한 방법을 제공합니다. 특히, <strong>O(N) 복잡도</strong> 의 효율적인 <strong>토너먼트 기반 순위 결정</strong> 은 대규모 에이전트 훈련에 실용적이며, <strong>과정 인지 평가 메커니즘</strong> 은 에이전트의 추론 및 계획 능력에 대한 보다 세분화된 피드백을 가능하게 합니다. 새롭게 구축된 <strong>Open-Travel</strong> 및 <strong>Open-DeepResearch 벤치마크</strong> 는 실제 시나리오를 반영하여 차세대 <strong>개방형 에이전트</strong> 개발 및 평가를 위한 중요한 자원이 될 것입니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2728,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2026-01-14 00:00:00+0900+0900","children":"2026년 1월 14일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2026년 1월 14일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","span","LLM Agents",{"className":"page__taxonomy-item","children":["#","LLM Agents"]}],["$","span","Open-Ended Tasks",{"className":"page__taxonomy-item","children":["#","Open-Ended Tasks"]}],["$","span","Relative Ranking",{"className":"page__taxonomy-item","children":["#","Relative Ranking"]}],["$","span","Tournament-based Ranking",{"className":"page__taxonomy-item","children":["#","Tournament-based Ranking"]}],["$","span","Discriminative Collapse",{"className":"page__taxonomy-item","children":["#","Discriminative Collapse"]}],["$","span","Reward Modeling",{"className":"page__taxonomy-item","children":["#","Reward Modeling"]}],["$","span","Benchmarks",{"className":"page__taxonomy-item","children":["#","Benchmarks"]}]]]}]}],["$","$La",null,{"postPermalink":"/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking","postId":"2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2026-01-14-Aligning-Text-Code-and-Vision-A-Multi-Objective-Reinforcement-Learning-Framework-for-Text-to-Visualization","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2026-01-14-End-to-End-Video-Character-Replacement-without-Structural-Guidance","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] End-to-End Video Character Replacement without Structural Guidance"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"이 [arXiv]에 게시한 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking"}],["$","meta","13",{"property":"og:description","content":"이 [arXiv]에 게시한 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking' 논문에 대한 자세한 리뷰입니다."}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2026-01-14-ArenaRL-Scaling-RL-for-Open-Ended-Agents-via-Tournament-based-Relative-Ranking"}],["$","meta","15",{"property":"og:type","content":"article"}],["$","meta","16",{"name":"twitter:card","content":"summary"}],["$","meta","17",{"name":"twitter:title","content":"[논문리뷰] ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking"}],["$","meta","18",{"name":"twitter:description","content":"이 [arXiv]에 게시한 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking' 논문에 대한 자세한 리뷰입니다."}],["$","link","19",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}]]
1:null
