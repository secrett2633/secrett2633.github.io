3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-53ec9cbf619543e1.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-80e07cee06d17a0b.js"],""]
4:["slug","ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","c"]
0:["_3-KrKKQh7tcSoXiYwQMB",[[["",{"children":[["slug","ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d6cea809dcbae606.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
a:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-80e07cee06d17a0b.js"],"default"]
9:Tb82,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2511.06281">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, Yu Cheng</p>
<h2>핵심 연구 목표</h2>
<p>본 연구는 Multimodal Large Language Models (MLLMs)의 비디오 이해 능력을 향상시키기 위해, 기존 비디오 데이터셋의 높은 주석 비용, 복잡성 부족, 그리고 주석 과정에서의 편향성이라는 한계를 극복하는 것을 목표로 합니다. 비디오 내의 풍부한 내재적 정보를 활용하여 고품질의 검증 가능한 훈련 데이터를 자체적으로 생성하는 방법을 모색합니다.</p>
<h2>핵심 방법론</h2>
<p>저자들은 비디오 자체에서 질문-답변 쌍을 생성할 수 있는 세 가지 자체 지도(self-supervised) 프리텍스트 태스크인 <strong>Anomaly Grounding</strong> , <strong>Object Counting</strong> , <strong>Temporal Jigsaw</strong> 를 설계했습니다. 이 태스크들의 난이도를 검증하기 위해 <strong>VIUBench</strong> 를 구축하고, 이를 통해 생성된 <strong>VideoSSR-30K</strong> 데이터셋으로 <strong>GRPO</strong> 를 활용한 <strong>RLVR(Reinforcement Learning with Verifiable Reward)</strong> 훈련을 수행합니다. 효율적이고 안정적인 RLVR 훈련을 위해 각 태스크에 맞는 <strong>부드러운 보상 함수</strong> 도 고안되었습니다.</p>
<h2>주요 결과</h2>
<p><strong>VIUBench</strong> 평가 결과, <strong>GPT-5</strong> 와 같은 최첨단 모델조차 <strong>평균 58.7%</strong> 의 점수를 기록하며 내재적 비디오 이해에 어려움을 겪는 것으로 나타났습니다. 제안된 <strong>VideoSSR</strong> 프레임워크는 17개 벤치마크(General Video QA, Long Video QA, Temporal Grounding, Complex Reasoning 포함)에서 <strong>평균 5% 이상</strong> 의 성능 향상을 달성했습니다. 특히 <strong>Anomaly Grounding</strong> 태스크는 <strong>QVHighlights</strong> 에서 <strong>+15.9%</strong> , <strong>ActivityNet</strong> 에서 <strong>+5.6%</strong> 개선을 보였습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><strong>VideoSSR</strong> 는 수동 또는 다중 에이전트 주석 없이 확장 가능하고 고품질의 비디오 훈련 데이터를 생성하는 실용적인 방법을 제공하여 MLLM 개발의 주요 병목 현상을 해결합니다. 난이도를 매개변수적으로 조절할 수 있는 프리텍스트 태스크는 모델 평가 및 훈련에 유용하며, 다양한 태스크에 걸친 일반화 능력은 비디오 이해 MLLM을 위한 강력한 기반이 될 수 있습니다. 이는 단일 태스크 확장이 아닌 다양한 자체 지도 태스크의 중요성을 시사합니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2728,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-11-12 00:00:00+0900+0900","children":"2025년 11월 12일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 11월 12일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Video Understanding",{"className":"page__taxonomy-item","children":["#","Video Understanding"]}],["$","span","Self-Supervised Learning",{"className":"page__taxonomy-item","children":["#","Self-Supervised Learning"]}],["$","span","Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","span","MLLMs",{"className":"page__taxonomy-item","children":["#","MLLMs"]}],["$","span","Pretext Tasks",{"className":"page__taxonomy-item","children":["#","Pretext Tasks"]}],["$","span","Verifiable Rewards",{"className":"page__taxonomy-item","children":["#","Verifiable Rewards"]}],["$","span","Data Generation",{"className":"page__taxonomy-item","children":["#","Data Generation"]}],["$","span","Temporal Grounding",{"className":"page__taxonomy-item","children":["#","Temporal Grounding"]}]]]}]}],["$","$La",null,{"postPermalink":"/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning","postId":"2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-11-12-Tiny-Model-Big-Logic-Diversity-Driven-Optimization-Elicits-Large-Model-Reasoning-Ability-in-VibeThinker-1-5B","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model   Reasoning Ability in VibeThinker-1.5B"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-11-12-Walking-the-Tightrope-of-LLMs-for-Software-Development-A-Practitioners-Perspective","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"이 [arXiv]에 게시한 'VideoSSR: Video Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning"}],["$","meta","13",{"property":"og:description","content":"이 [arXiv]에 게시한 'VideoSSR: Video Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2025-11-12-VideoSSR-Video-Self-Supervised-Reinforcement-Learning"}],["$","meta","15",{"property":"og:type","content":"article"}],["$","meta","16",{"name":"twitter:card","content":"summary"}],["$","meta","17",{"name":"twitter:title","content":"[논문리뷰] VideoSSR: Video Self-Supervised Reinforcement Learning"}],["$","meta","18",{"name":"twitter:description","content":"이 [arXiv]에 게시한 'VideoSSR: Video Self-Supervised Reinforcement Learning' 논문에 대한 자세한 리뷰입니다."}],["$","link","19",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}]]
1:null
