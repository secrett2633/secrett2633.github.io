3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
7:I[231,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],""]
8:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
4:["slug","ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","c"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":[["slug","ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],"default"]
a:T4ba,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning","description":"Zheli Liu이 arXiv에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","datePublished":"2025-10-08T04:48:12.000Z","dateModified":"2025-10-08T04:48:12.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":278,"articleSection":"Review","keywords":"Review, Preference Learning, LLMs, User Feedback, Dissatisfaction Signals, DPO, Iterative Training, RLHF, Exploration"}b:Td79,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2510.02341" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>대규모 언어 모델(LLM) 배포 환경에서 희소한 명시적 만족(SAT) 피드백 대신, 풍부하게 발생하는 암묵적인 사용자 불만족(DSAT) 신호를 효과적으로 활용하여 모델 성능을 개선하는 확장 가능하고 효율적인 선호 학습 방법론을 개발하는 것이 목표입니다. 기존 선호 학습 방법들이 데이터 프로파일에 제대로 부합하지 않는 문제점을 해결하고자 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>본 논문은 <strong>DRIFT (Dissatisfaction-Refined Iterative preFerence Training)</strong> 를 제안합니다. 이 방법론은 실제 사용자 피드백에서 추출된 <strong>DSAT 신호</strong> 를 고품질의 <strong>부정적인 응답(rejected response)</strong> 으로 사용하고, 현재 발전 중인 정책에서 <strong>긍정적인 응답(chosen response)</strong> 을 동적으로 샘플링하여 <strong>DPO(Direct Preference Optimization) 손실</strong> 을 통해 모델을 반복적으로 학습시킵니다. 이 접근 방식은 <strong>WildFeedback</strong> 과 같은 실제 사용자 피드백 데이터셋에 기반하여, 기존 <strong>SPIN</strong> 이나 <strong>Iterative DPO</strong> 의 한계점인 그래디언트 붕괴(gradient collapse)를 방지하며 선호 마진을 유지합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>DRIFT</strong> 는 <strong>WildBench Task Score</strong> 에서 기준 모델 대비 <strong>7B 모델은 최대 +6.23%</strong> , <strong>14B 모델은 최대 +7.61%</strong> 의 성능 향상을 달성했습니다. <strong>AlpacaEval2 Win Rate</strong> 에서는 <strong>7B 모델은 최대 +8.95%</strong> , <strong>14B 모델은 최대 +12.29%</strong> 의 개선을 보이며, <strong>Iterative DPO</strong> 및 <strong>SPIN</strong> 과 같은 강력한 베이스라인을 능가했습니다. 특히 <strong>14B DRIFT 모델</strong> 은 <strong>GPT-4o-mini</strong> 의 성능을 <strong>WildBench</strong> 에서 앞섰으며, 탐색 능력 분석 결과 더 넓고 다양한 고보상 솔루션 공간을 탐색하는 것으로 나타났습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>AI 실무자들은 <strong>DRIFT</strong> 를 통해 비용이 많이 드는 수동 주석 없이도 실제 사용자 불만족 데이터를 활용하여 LLM의 정렬(alignment)을 효과적으로 개선할 수 있습니다. 이는 실제 서비스 환경에서의 모델 배포 및 지속적인 개선에 큰 이점을 제공합니다. 동적 샘플링과 반복 학습 설계는 모델이 협소한 해법에 수렴하지 않고 다양한 고품질 응답을 탐색하도록 도와, 모델의 일반화 및 견고성을 향상시키는 실용적인 방안이 될 것입니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-10-08 13:48:12+0900","children":"2025년 10월 8일"}],["$","time",null,{"className":"ml-4","dateTime":"2025-10-08T04:48:12.000Z","children":["수정: ","2025년 10월 8일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Preference Learning",{"href":"/tags/Preference%20Learning","className":"page__taxonomy-item","children":["#","Preference Learning"]}],["$","$L7","LLMs",{"href":"/tags/LLMs","className":"page__taxonomy-item","children":["#","LLMs"]}],["$","$L7","User Feedback",{"href":"/tags/User%20Feedback","className":"page__taxonomy-item","children":["#","User Feedback"]}],["$","$L7","Dissatisfaction Signals",{"href":"/tags/Dissatisfaction%20Signals","className":"page__taxonomy-item","children":["#","Dissatisfaction Signals"]}],["$","$L7","DPO",{"href":"/tags/DPO","className":"page__taxonomy-item","children":["#","DPO"]}],["$","$L7","Iterative Training",{"href":"/tags/Iterative%20Training","className":"page__taxonomy-item","children":["#","Iterative Training"]}],["$","$L7","RLHF",{"href":"/tags/RLHF","className":"page__taxonomy-item","children":["#","RLHF"]}],["$","$L7","Exploration",{"href":"/tags/Exploration","className":"page__taxonomy-item","children":["#","Exploration"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning","postId":"2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-10-8-CoDA-Coding-LM-via-Diffusion-Adaptation","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] CoDA: Coding LM via Diffusion Adaptation"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-10-8-Deforming-Videos-to-Masks-Flow-Matching-for-Referring-Video-Segmentation","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation"}]]}]]}]]}]]}]]}]]}]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"Zheli Liu이 arXiv에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning"}],["$","meta","14",{"property":"og:description","content":"Zheli Liu이 arXiv에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2025-10-8-DRIFT-Learning-from-Abundant-User-Dissatisfaction-in-Real-World-Preference-Learning"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2025-10-08T04:48:12.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2025-10-08T04:48:12.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Preference Learning"}],["$","meta","23",{"property":"article:tag","content":"LLMs"}],["$","meta","24",{"property":"article:tag","content":"User Feedback"}],["$","meta","25",{"property":"article:tag","content":"Dissatisfaction Signals"}],["$","meta","26",{"property":"article:tag","content":"DPO"}],["$","meta","27",{"property":"article:tag","content":"Iterative Training"}],["$","meta","28",{"property":"article:tag","content":"RLHF"}],["$","meta","29",{"property":"article:tag","content":"Exploration"}],["$","meta","30",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","31",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","32",{"name":"twitter:title","content":"[논문리뷰] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning"}],["$","meta","33",{"name":"twitter:description","content":"Zheli Liu이 arXiv에 게시한 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning' 논문에 대한 자세한 리뷰입니다."}],["$","link","34",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","35",{"name":"next-size-adjust"}]]
1:null
