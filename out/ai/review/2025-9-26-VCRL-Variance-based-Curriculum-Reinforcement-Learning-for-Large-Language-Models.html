<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ddc331716d5e47a2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-0395f68b8cc78a20.js" async=""></script><script src="/_next/static/chunks/23-7d3f7f0b78aa2fd3.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-467e37449c5a68fc.js" async=""></script><script src="/_next/static/chunks/app/layout-b0a450f8e4964582.js" async=""></script><script src="/_next/static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><title>[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models - secrett2633&#x27;s blog</title><meta name="description" content="Yuewei Zhang이 [arXiv]에 게시한 &#x27;VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다."/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models"/><meta property="og:description" content="Yuewei Zhang이 [arXiv]에 게시한 &#x27;VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다."/><meta property="og:url" content="https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-09-26T04:35:32.000Z"/><meta property="article:modified_time" content="2025-09-26T04:35:32.000Z"/><meta property="article:author" content="secrett2633"/><meta property="article:section" content="Review"/><meta property="article:tag" content="Review"/><meta property="article:tag" content="Reinforcement Learning"/><meta property="article:tag" content="Curriculum Learning"/><meta property="article:tag" content="Large Language Models"/><meta property="article:tag" content="Mathematical Reasoning"/><meta property="article:tag" content="Variance-based Sampling"/><meta property="article:tag" content="Replay Learning"/><meta property="article:tag" content="Policy Optimization"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@secrett2633"/><meta name="twitter:title" content="[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models"/><meta name="twitter:description" content="Yuewei Zhang이 [arXiv]에 게시한 &#x27;VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models&#x27; 논문에 대한 자세한 리뷰입니다."/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models","description":"Yuewei Zhang이 [arXiv]에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models","datePublished":"2025-09-26T04:35:32.000Z","dateModified":"2025-09-26T04:35:32.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":266,"articleSection":"Review","keywords":"Review, Reinforcement Learning, Curriculum Learning, Large Language Models, Mathematical Reasoning, Variance-based Sampling, Replay Learning, Policy Optimization"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"Review","item":"https://blog.secrett2633.cloud/ai/review"},{"@type":"ListItem","position":3,"name":"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models","item":"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models"}]}</script><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2728<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><a class="hover:text-gray-700" href="/ai/review">Review</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models</span></li></ol></nav><article class="page"><header class="mb-8"><h1 class="page__title">[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models</h1><div class="page__meta"><time dateTime="2025-09-26 13:35:32+0900">2025년 9월 26일</time><time class="ml-4" dateTime="2025-09-26T04:35:32.000Z">수정: <!-- -->2025년 9월 26일</time></div></header><div class="page__content"><div><blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2509.19803" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang*</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>기존 롤아웃 기반 강화 학습(RL) 방법론이 LLM의 동적인 학습 능력과 샘플 난이도를 효과적으로 매칭하지 못하는 문제를 해결하는 것이 목표입니다. 특히 수학적 추론 태스크에서 LLM의 효율적인 학습을 저해하는 고정된 난이도 샘플링과 불안정한 훈련을 개선하고자 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>본 논문은 <strong>VCRL(Variance-based Curriculum Reinforcement Learning)</strong> 프레임워크를 제안합니다. 이 방법은 롤아웃 그룹의 보상 분산을 통해 샘플 난이도를 동적으로 측정하는 <strong>Variance-based Dynamic Sampling</strong> 을 사용하여, 현재 모델에게 <em>너무 쉽거나 어려운</em> 샘플 대신 <em>중간 난이도</em>의 학습에 가장 가치 있는 샘플을 선별합니다. 여기에 <strong>Replay Learning</strong> 과 <em>메모리 뱅크</em>를 결합하여 높은 분산 값을 가진 샘플을 유지 및 재활용하며, GRPO(Group Relative Policy Optimization) 목표 함수를 수정하여 학습 안정성과 효율성을 높입니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>VCRL</strong> 은 5개의 수학 벤치마크(AIME-2024, AIME-2025, MATH500, OlympiadBench, AMC23)에서 <strong>Qwen3-4B-Base</strong> 및 <strong>Qwen3-8B-Base</strong> 모델을 사용하여 기존 RL 방법론 대비 뛰어난 성능을 입증했습니다. 특히 <strong>Qwen3-8B-Base</strong> 모델에서 <strong>VCRL</strong> 은 평균 점수 <strong>57.76%</strong> 를 달성하여 최강의 베이스라인인 <strong>GSPO(53.09%)</strong> 보다 <strong>4.67%p</strong> 높은 성능을 보였으며, <strong>Qwen3-4B-Base</strong> 에서는 평균 성능이 <strong>26.68%(Base Model)</strong> 에서 <strong>49.43%</strong> 로 크게 향상되었습니다. 훈련 중 <strong>더 높은 보상 점수</strong> 와 <strong>안정적인 기울기 노름</strong> 을 보여 훈련 효율성과 안정성이 개선됨을 확인했습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p><strong>VCRL</strong> 은 LLM이 복잡한 수학적 추론 능력을 습득하는 데 있어 효과적인 <strong>강화 학습 기반 미세 조정 전략</strong> 을 제공합니다. 샘플 난이도를 동적으로 조절하고 중요한 학습 샘플에 집중하는 능력은 <strong>LLM 훈련의 효율성과 안정성</strong> 을 크게 향상시킬 수 있어, 자원 제약이 있는 환경에서도 고성능 LLM을 개발하는 데 기여합니다. 특히, <strong>모델의 현재 능력에 맞춰 학습 데이터를 선별</strong> 하는 접근 방식은 범용적인 RL 문제에도 적용 가능성이 높습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
</div></div><footer class="page__meta mt-8"><div class="page__taxonomy"><span class="text-sm font-medium text-gray-900 mb-2 block">태그</span><a class="page__taxonomy-item" href="/tags/Review">#<!-- -->Review</a><a class="page__taxonomy-item" href="/tags/Reinforcement%20Learning">#<!-- -->Reinforcement Learning</a><a class="page__taxonomy-item" href="/tags/Curriculum%20Learning">#<!-- -->Curriculum Learning</a><a class="page__taxonomy-item" href="/tags/Large%20Language%20Models">#<!-- -->Large Language Models</a><a class="page__taxonomy-item" href="/tags/Mathematical%20Reasoning">#<!-- -->Mathematical Reasoning</a><a class="page__taxonomy-item" href="/tags/Variance-based%20Sampling">#<!-- -->Variance-based Sampling</a><a class="page__taxonomy-item" href="/tags/Replay%20Learning">#<!-- -->Replay Learning</a><a class="page__taxonomy-item" href="/tags/Policy%20Optimization">#<!-- -->Policy Optimization</a></div></footer><section class="comments-section mt-12 pt-8 border-t border-gray-200"></section><section class="mt-12 border-t border-gray-200 pt-8"><h3 class="text-base font-semibold text-gray-900 mb-4">Review<!-- --> 의 다른글</h3><ul class="space-y-2 text-sm"><li class="text-gray-500">이전글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models">[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models</a></li><li class="text-gray-900 font-semibold">현재글 : <!-- -->[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models</li><li class="text-gray-500">다음글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity">[논문리뷰] When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity</a></li></ul></section></article></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ddc331716d5e47a2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[9157,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"default\"]\na:I[231,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js\"],\"\"]\nb:I[4080,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"\"]\nd:I[6130,[],\"\"]\n7:[\"slug\",\"ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"c\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ddc331716d5e47a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"WcxaIiCPz9cbpnkGvOjOK\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"review\\\",\\\"2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\"],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lb\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lb\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"11:I[646,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js\"],\"default\"]\nf:T4db,{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\",\"description\":\"Yuewei Zhang이 [arXiv]에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.\",\"url\":\"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"datePublished\":\"2025-09-26T04:35:32.000Z\",\"dateModified\":\"2025-09-26T04:35:32.000Z\",\"author\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"},\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"},\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\"},\"image\":\"https://blog.secrett2633.cloud/og-default.png\",\"isAccessibleForFree\":true,\"inLanguage\":\"ko\",\"wordCount\":266,\"articleSection\":\"Review\",\"keywords\":\"Review, Reinforcement Learning, Curriculum Learning, Large Language Models, Mathematical Reasoning, Variance-based Sampling, Replay Learning, Policy Optimization\"}10:Td3f,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e링크:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/2509.19803\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e논문 PDF로 바로 열기\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e저자:\u003c/strong\u003e Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang*\u003c/p\u003e\n\u003ch2 id=\"핵심-연구-목표\"\u003e\u003ca href=\"#핵심-연구-목표\"\u003e핵심 연구 목표\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e기존 롤아웃 기반 강화 학습(RL) 방법론이 LLM의 동적인 학습 능력과 샘플 난이도를 효과적으로 매칭하지 못하는 문제를 해결하는 것이 목표입니다. 특히 수학적 추론 태스크에서 LLM의 효율적인 학습을 저해하는 고정된 난이도 샘플링과 불안정한 훈련을 개선하고자 합니다.\u003c/p\u003e\n\u003ch2 id=\"핵심-방법론\"\u003e\u003ca href=\"#핵심-방법론\"\u003e핵심 방법론\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e본 논문은 \u003cstrong\u003eVCRL(Variance-based Curriculum Reinforcement Learning)\u003c/strong\u003e 프레임워크를 제안합니다. 이 방법은 롤아웃 그룹의 보상 분산을 통해 샘플 난이도를 동적으로 측정하는 \u003cstrong\u003eVariance-based Dynamic Sampling\u003c/strong\u003e 을 사용하여, 현재 모델에게 \u003cem\u003e너무 쉽거나 어려운\u003c/em\u003e 샘플 대신 \u003cem\u003e중간 난이도\u003c/em\u003e의 학습에 가장 가치 있는 샘플을 선별합니다. 여기에 \u003cstrong\u003eReplay Learning\u003c/strong\u003e 과 \u003cem\u003e메모리 뱅크\u003c/em\u003e를 결합하여 높은 분산 값을 가진 샘플을 유지 및 재활용하며, GRPO(Group Relative Policy Optimization) 목표 함수를 수정하여 학습 안정성과 효율성을 높입니다.\u003c/p\u003e\n\u003ch2 id=\"주요-결과\"\u003e\u003ca href=\"#주요-결과\"\u003e주요 결과\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eVCRL\u003c/strong\u003e 은 5개의 수학 벤치마크(AIME-2024, AIME-2025, MATH500, OlympiadBench, AMC23)에서 \u003cstrong\u003eQwen3-4B-Base\u003c/strong\u003e 및 \u003cstrong\u003eQwen3-8B-Base\u003c/strong\u003e 모델을 사용하여 기존 RL 방법론 대비 뛰어난 성능을 입증했습니다. 특히 \u003cstrong\u003eQwen3-8B-Base\u003c/strong\u003e 모델에서 \u003cstrong\u003eVCRL\u003c/strong\u003e 은 평균 점수 \u003cstrong\u003e57.76%\u003c/strong\u003e 를 달성하여 최강의 베이스라인인 \u003cstrong\u003eGSPO(53.09%)\u003c/strong\u003e 보다 \u003cstrong\u003e4.67%p\u003c/strong\u003e 높은 성능을 보였으며, \u003cstrong\u003eQwen3-4B-Base\u003c/strong\u003e 에서는 평균 성능이 \u003cstrong\u003e26.68%(Base Model)\u003c/strong\u003e 에서 \u003cstrong\u003e49.43%\u003c/strong\u003e 로 크게 향상되었습니다. 훈련 중 \u003cstrong\u003e더 높은 보상 점수\u003c/strong\u003e 와 \u003cstrong\u003e안정적인 기울기 노름\u003c/strong\u003e 을 보여 훈련 효율성과 안정성이 개선됨을 확인했습니다.\u003c/p\u003e\n\u003ch2 id=\"ai-실무자를-위한-시사점\"\u003e\u003ca href=\"#ai-실무자를-위한-시사점\"\u003eAI 실무자를 위한 시사점\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eVCRL\u003c/strong\u003e 은 LLM이 복잡한 수학적 추론 능력을 습득하는 데 있어 효과적인 \u003cstrong\u003e강화 학습 기반 미세 조정 전략\u003c/strong\u003e 을 제공합니다. 샘플 난이도를 동적으로 조절하고 중요한 학습 샘플에 집중하는 능력은 \u003cstrong\u003eLLM 훈련의 효율성과 안정성\u003c/strong\u003e 을 크게 향상시킬 수 있어, 자원 제약이 있는 환경에서도 고성능 LLM을 개발하는 데 기여합니다. 특히, \u003cstrong\u003e모델의 현재 능력에 맞춰 학습 데이터를 선별\u003c/strong\u003e 하는 접근 방식은 범용적인 RL 문제에도 적용 가능성이 높습니다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e⚠️ \u003cstrong\u003e알림:\u003c/strong\u003e 이 리뷰는 AI로 작성되었습니다.\u003c/p\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"$f\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"Review\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":3,\\\"name\\\":\\\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2728,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/ai/review\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"hover:text-gray-700\",\"children\":\"Review\"}]]}],[\"$\",\"li\",\"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\"}]]}]]]}]}],[\"$\",\"article\",null,{\"className\":\"page\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"page__title\",\"children\":\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\"}],[\"$\",\"div\",null,{\"className\":\"page__meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-26 13:35:32+0900\",\"children\":\"2025년 9월 26일\"}],[\"$\",\"time\",null,{\"className\":\"ml-4\",\"dateTime\":\"2025-09-26T04:35:32.000Z\",\"children\":[\"수정: \",\"2025년 9월 26일\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"page__content\",\"children\":[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$10\"}}]}],[\"$\",\"footer\",null,{\"className\":\"page__meta mt-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"page__taxonomy\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-gray-900 mb-2 block\",\"children\":\"태그\"}],[[\"$\",\"$La\",\"Review\",{\"href\":\"/tags/Review\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Review\"]}],[\"$\",\"$La\",\"Reinforcement Learning\",{\"href\":\"/tags/Reinforcement%20Learning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Reinforcement Learning\"]}],[\"$\",\"$La\",\"Curriculum Learning\",{\"href\":\"/tags/Curriculum%20Learning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Curriculum Learning\"]}],[\"$\",\"$La\",\"Large Language Models\",{\"href\":\"/tags/Large%20Language%20Models\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Large Language Models\"]}],[\"$\",\"$La\",\"Mathematical Reasoning\",{\"href\":\"/tags/Mathematical%20Reasoning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Mathematical Reasoning\"]}],[\"$\",\"$La\",\"Variance-based Sampling\",{\"href\":\"/tags/Variance-based%20Sampling\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Variance-based Sampling\"]}],[\"$\",\"$La\",\"Replay Learning\",{\"href\":\"/tags/Replay%20Learning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Replay Learning\"]}],[\"$\",\"$La\",\"Policy Optimization\",{\"href\":\"/tags/Policy%20Optimization\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Policy Optimization\"]}]]]}]}],[\"$\",\"$L11\",null,{\"postPermalink\":\"/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\",\"postId\":\"2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\"}],[\"$\",\"section\",null,{\"className\":\"mt-12 border-t border-gray-200 pt-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 mb-4\",\"children\":[\"Review\",\" 의 다른글\"]}],[\"$\",\"ul\",null,{\"className\":\"space-y-2 text-sm\",\"children\":[[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"이전글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-9-26-V-GameGym-Visual-Game-Generation-for-Code-Large-Language-Models\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] V-GameGym: Visual Game Generation for Code Large Language Models\"}]]}],[\"$\",\"li\",null,{\"className\":\"text-gray-900 font-semibold\",\"children\":[\"현재글 : \",\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\"]}],[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"다음글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-9-26-When-Judgment-Becomes-Noise-How-Design-Failures-in-LLM-Judge-Benchmarks-Silently-Undermine-Validity\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity\"}]]}]]}]]}]]}]]}]]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Yuewei Zhang이 [arXiv]에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"Yuewei Zhang이 [arXiv]에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/ai/review/2025-9-26-VCRL-Variance-based-Curriculum-Reinforcement-Learning-for-Large-Language-Models\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:published_time\",\"content\":\"2025-09-26T04:35:32.000Z\"}],[\"$\",\"meta\",\"18\",{\"property\":\"article:modified_time\",\"content\":\"2025-09-26T04:35:32.000Z\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:author\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:section\",\"content\":\"Review\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"Review\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"Reinforcement Learning\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"Curriculum Learning\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"Large Language Models\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"Mathematical Reasoning\"}],[\"$\",\"meta\",\"26\",{\"property\":\"article:tag\",\"content\":\"Variance-based Sampling\"}],[\"$\",\"meta\",\"27\",{\"property\":\"article:tag\",\"content\":\"Replay Learning\"}],[\"$\",\"meta\",\"28\",{\"property\":\"article:tag\",\"content\":\"Policy Optimization\"}],[\"$\",\"meta\",\"29\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:creator\",\"content\":\"@secrett2633\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:title\",\"content\":\"[논문리뷰] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models\"}],[\"$\",\"meta\",\"32\",{\"name\":\"twitter:description\",\"content\":\"Yuewei Zhang이 [arXiv]에 게시한 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"link\",\"33\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"34\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script></body></html>