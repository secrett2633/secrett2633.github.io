3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],""]
8:I[4080,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-b0a450f8e4964582.js"],""]
4:["slug","ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","c"]
0:["WcxaIiCPz9cbpnkGvOjOK",[[["",{"children":[["slug","ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/ddc331716d5e47a2.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js"],"default"]
a:T4eb,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation","description":"이 [arXiv]에 게시한 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","datePublished":"2026-01-20T15:00:00.000Z","dateModified":"2026-01-20T15:00:00.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":304,"articleSection":"Review","keywords":"Review, Vision-Language Navigation, Chain-of-Thought Reasoning, Multimodal AI, Implicit Reasoning, Visual AutoRegressor, Embodied AI, Long-Horizon Planning"}b:Te0f,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2601.13976" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu, Yonggang Qi</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>Vision-and-Language Navigation (VLN)에서 기존 Chain-of-Thought (CoT) 추론 방식의 한계, 즉 공간 접지 부족, 희소한 주석에 대한 과적합, 또는 상상된 시각적 관찰 생성으로 인한 심각한 토큰 팽창 문제로 실시간 내비게이션이 비실용적인 문제를 해결하는 것을 목표로 합니다. 명시적인 토큰 오버헤드 없이 CoT 추론의 이점을 유지하면서 추론 인지(reasoning-aware) 및 실시간 내비게이션을 가능하게 하는 통합 암시적 추론 프레임워크를 개발하는 것이 주 목적입니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p><strong>FANTASYVLN</strong> 은 통합 암시적 추론 프레임워크를 제안합니다. 훈련 중 다중 모드 CoT 추론이 생성하는 <strong>상상된 시각 토큰</strong> 을 미리 훈련된 <strong>Visual AutoRegressor (VAR)</strong> 를 사용하여 압축된 잠재 공간으로 인코딩합니다. 또한, 텍스트 전용, 시각 전용, 텍스트-시각 결합 CoT 모드에서 동시에 학습하는 통합 <strong>멀티-CoT 훈련 전략</strong> 을 도입하며, 다양한 추론 모드의 액션 예측을 비-CoT 모드와 정렬하는 <strong>Cross-Mode Alignment Constraint</strong> 를 적용하여 견고한 잠재 표현을 학습합니다. 추론 시에는 명시적인 CoT 생성 없이 직접 명령어-액션 매핑을 수행합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p><strong>LH-VLN 벤치마크</strong> 에서 <strong>FANTASYVLN</strong> 은 모든 메트릭에서 우수한 내비게이션 정확도를 달성했습니다: <strong>SR 2.44</strong> , <strong>ISR 11.01</strong> , <strong>CSR 9.64</strong> , <strong>CGT 8.99</strong> . 이는 기존 기준선들을 크게 능가하는 결과입니다. 또한, 명시적 CoT 방법론인 <strong>CoT-VLA의 0.19 APS</strong> 와 비교하여 추론 지연 시간을 10배 이상 단축하여 <strong>1.03 APS</strong> 를 달성함으로써 추론 효율성을 크게 향상시켰습니다. 특히 <strong>교차 모드 정렬 제약(cross-mode alignment constraint)</strong> 이 필수적임을 입증했습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>이 연구는 명시적인 CoT 방식의 토큰 팽창 문제 없이 <strong>실시간 다중 모드 암시적 추론</strong> 이 복잡한 환경에서 실현 가능한 내비게이션의 실용적인 경로임을 보여줍니다. 미리 훈련된 <strong>VAR 모델</strong> 을 활용하여 시각적 상상을 효율적으로 압축하는 기법은 다른 embodied AI task에도 확장 적용될 수 있는 중요한 통찰을 제공합니다. 또한, <strong>통합 멀티-CoT 훈련 전략</strong> 과 <strong>교차 모드 정렬</strong> 은 추론 속도 저하 없이 다양한 추론 패턴을 통합하는 강력한 방법을 제공하여, 보다 해석 가능하고 효율적인 AI 에이전트 설계에 기여할 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation\"}]}"}}],["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2728,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2026-01-21 00:00:00+0900+0900","children":"2026년 1월 21일"}],["$","time",null,{"className":"ml-4","dateTime":"2026-01-20T15:00:00.000Z","children":["수정: ","2026년 1월 21일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Vision-Language Navigation",{"href":"/tags/Vision-Language%20Navigation","className":"page__taxonomy-item","children":["#","Vision-Language Navigation"]}],["$","$L7","Chain-of-Thought Reasoning",{"href":"/tags/Chain-of-Thought%20Reasoning","className":"page__taxonomy-item","children":["#","Chain-of-Thought Reasoning"]}],["$","$L7","Multimodal AI",{"href":"/tags/Multimodal%20AI","className":"page__taxonomy-item","children":["#","Multimodal AI"]}],["$","$L7","Implicit Reasoning",{"href":"/tags/Implicit%20Reasoning","className":"page__taxonomy-item","children":["#","Implicit Reasoning"]}],["$","$L7","Visual AutoRegressor",{"href":"/tags/Visual%20AutoRegressor","className":"page__taxonomy-item","children":["#","Visual AutoRegressor"]}],["$","$L7","Embodied AI",{"href":"/tags/Embodied%20AI","className":"page__taxonomy-item","children":["#","Embodied AI"]}],["$","$L7","Long-Horizon Planning",{"href":"/tags/Long-Horizon%20Planning","className":"page__taxonomy-item","children":["#","Long-Horizon Planning"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation","postId":"2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2026-01-21-Being-H0-5-Scaling-Human-Centric-Robot-Learning-for-Cross-Embodiment-Generalization","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2026-01-21-Fundamental-Limitations-of-Favorable-Privacy-Utility-Guarantees-for-DP-SGD","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD"}]]}]]}]]}]]}]]}]]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"이 [arXiv]에 게시한 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation"}],["$","meta","14",{"property":"og:description","content":"이 [arXiv]에 게시한 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2026-01-21-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2026-01-20T15:00:00.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2026-01-20T15:00:00.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Vision-Language Navigation"}],["$","meta","23",{"property":"article:tag","content":"Chain-of-Thought Reasoning"}],["$","meta","24",{"property":"article:tag","content":"Multimodal AI"}],["$","meta","25",{"property":"article:tag","content":"Implicit Reasoning"}],["$","meta","26",{"property":"article:tag","content":"Visual AutoRegressor"}],["$","meta","27",{"property":"article:tag","content":"Embodied AI"}],["$","meta","28",{"property":"article:tag","content":"Long-Horizon Planning"}],["$","meta","29",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","30",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","31",{"name":"twitter:title","content":"[논문리뷰] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation"}],["$","meta","32",{"name":"twitter:description","content":"이 [arXiv]에 게시한 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation' 논문에 대한 자세한 리뷰입니다."}],["$","link","33",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","34",{"name":"next-size-adjust"}]]
1:null
