3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],"default"]
7:I[231,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],""]
8:I[4080,["231","static/chunks/231-ee5764c1002761f9.js","132","static/chunks/132-273e49420772df1e.js","185","static/chunks/app/layout-d443cbc354279241.js"],""]
4:["slug","ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","c"]
0:["mJI0q5Z-SQWBtT83kG_N7",[[["",{"children":[["slug","ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com","crossOrigin":"anonymous"}],["$","link",null,{"rel":"dns-prefetch","href":"https://giscus.app"}],["$","link",null,{"rel":"preconnect","href":"https://giscus.app","crossOrigin":"anonymous"}],["$","meta",null,{"httpEquiv":"X-Content-Type-Options","content":"nosniff"}],["$","meta",null,{"name":"referrer","content":"strict-origin-when-cross-origin"}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"secrett2633's blog\",\"url\":\"https://blog.secrett2633.cloud\",\"description\":\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\",\"inLanguage\":\"ko\",\"publisher\":{\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\"}}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Person\",\"name\":\"secrett2633\",\"url\":\"https://blog.secrett2633.cloud\",\"sameAs\":[\"https://github.com/secrett2633\"]}"}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":[["$","a",null,{"href":"#main-content","className":"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600","children":"본문으로 건너뛰기"}],["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"id":"main-content","className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":["© ",2026," secrett2633. All rights reserved."]}]}]}]}]]}],["$","$L8",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY","strategy":"afterInteractive"}],["$","$L8",null,{"id":"gtag-init","strategy":"afterInteractive","children":"window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-NE2W3CFPNY');"}]]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","role":"status","aria-label":"로딩 중","children":[["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}],["$","span",null,{"className":"sr-only","children":"로딩 중..."}]]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/edb8d4ad4fe2f3b0.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L9"]]]]]
c:I[646,["231","static/chunks/231-ee5764c1002761f9.js","877","static/chunks/app/%5B...slug%5D/page-ef33f0f4c1a350bd.js"],"default"]
a:T4f8,{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges","description":"arXiv에 게시된 'An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","datePublished":"2025-12-21T15:00:00.000Z","dateModified":"2025-12-21T15:00:00.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":293,"articleSection":"Review","keywords":"Review, Vision-Language-Action Models, Embodied Intelligence, Robotics, Foundation Models, Multi-modal Learning, Reinforcement Learning, Sim-to-Real Transfer, Human-Robot Interaction"}b:Td5e,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2512.11362" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Chao Xu, Suyu Zhang, Yang Liu, Baigui Sun, Weihong Chen, Bo Xu, Qi Liu, Juncheng Wang, Shujun Wang, Shan Luo, Jan Peters, Athanasios V. Vasilakos, Stefanos Zafeiriou, Jiankang Deng</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 급변하는 <strong>Vision-Language-Action (VLA) 모델</strong> 분야에 대한 명확하고 구조화된 가이드를 제공하는 것을 목표로 합니다. 기존 서베이의 한계를 극복하기 위해 핵심 연구 도전 과제를 중심으로 문제를 체계적으로 분석하고, 미래 연구 방향을 제시하여 임베디드 AI 분야의 학습 가속화 및 새로운 아이디어 촉발을 목적으로 합니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>본 서베이는 VLA 모델을 <strong>기본 모듈(Perception, Brain, Action)</strong> , <strong>주요 발전 이정표</strong> , 그리고 <strong>핵심 도전 과제</strong> 의 세 가지 계층적 구조로 분석합니다. 특히, <strong>표현(Representation), 실행(Execution), 일반화(Generalization), 안전(Safety), 데이터셋 및 평가(Dataset and Evaluation)</strong> 라는 다섯 가지 주요 도전 과제를 깊이 있게 다루며, 각 과제에 대한 기존 접근 방식과 미래 기회를 상세히 검토합니다. 이러한 구조는 연구자가 자연스럽게 분야를 학습하는 경로를 따르도록 설계되었습니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p>이 서베이 논문은 새로운 정량적 실험 결과를 제시하지 않지만, <strong>VLA 모델</strong> 이 기존 모듈형 파이프라인을 능가하는 <strong>일반화 및 적응 능력</strong> 에서 전례 없는 발전을 이루었음을 강조합니다. 특히 <strong>RT-2, Diffusion Policy, Open X-Embodiment</strong> 와 같은 모델들이 종단간 VLA 프레임워크, 생성형 액션 모델링, 크로스-임바디먼트 데이터 스케일링 등에서 중요한 발전을 이끌었음을 조명합니다. 이를 통해 복잡하고 빠르게 성장하는 VLA 연구 분야의 종합적인 로드맵을 제공합니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p>본 논문은 <strong>VLA 모델</strong> 의 기술적 구성 요소, 발전 과정, 그리고 핵심 도전 과제에 대한 깊이 있는 분석을 통해 AI/ML 실무자들에게 실용적인 통찰력을 제공합니다. 특히 <strong>멀티모달 정렬, 로봇 행동의 안전성 및 신뢰성, 실시간 실행 효율성</strong> 등의 복잡한 문제를 해결하기 위한 <strong>대규모 언어 모델(LLM), 비전 트랜스포머(ViT), 확산 모델(Diffusion Model)</strong> 등의 활용 방안을 제시합니다. 이는 로봇 공학 및 임베디드 AI 애플리케이션 개발 시 <strong>모델 선택, 데이터 전략, 배포 시 고려사항</strong> 등을 결정하는 데 중요한 참고 자료가 될 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"$a"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"홈\",\"item\":\"https://blog.secrett2633.cloud/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Review\",\"item\":\"https://blog.secrett2633.cloud/ai/review\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges\",\"item\":\"https://blog.secrett2633.cloud/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges\"}]}"}}],["$","div",null,{"className":"space-y-6","children":["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","aria-label":"카테고리 네비게이션","children":[["$","div","Backend",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","$L7",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","$L7",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","$L7",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","$L7",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","$L7",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2741,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","$L7",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","$L7",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","$L7",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","$L7",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","$L7",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","$L7",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","p",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","$L7",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","$L7",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","div",null,{"className":"flex-1","children":[["$","nav",null,{"aria-label":"breadcrumb","className":"text-sm text-gray-500 mb-4","children":["$","ol",null,{"className":"flex flex-wrap items-center gap-1","children":[["$","li",null,{"children":["$","$L7",null,{"href":"/","className":"hover:text-gray-700","children":"홈"}]}],[["$","li","/ai/review",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","$L7",null,{"href":"/ai/review","className":"hover:text-gray-700","children":"Review"}]]}],["$","li","/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges",{"className":"flex items-center gap-1","children":[["$","span",null,{"aria-hidden":"true","children":"/"}],["$","span",null,{"className":"text-gray-900","aria-current":"page","children":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges"}]]}]]]}]}],["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-12-22 00:00:00+0900+0900","children":"2025년 12월 22일"}],["$","time",null,{"className":"ml-4","dateTime":"2025-12-21T15:00:00.000Z","children":["수정: ","2025년 12월 22일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$b"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","span",null,{"className":"text-sm font-medium text-gray-900 mb-2 block","children":"태그"}],[["$","$L7","Review",{"href":"/tags/Review","className":"page__taxonomy-item","children":["#","Review"]}],["$","$L7","Vision-Language-Action Models",{"href":"/tags/Vision-Language-Action%20Models","className":"page__taxonomy-item","children":["#","Vision-Language-Action Models"]}],["$","$L7","Embodied Intelligence",{"href":"/tags/Embodied%20Intelligence","className":"page__taxonomy-item","children":["#","Embodied Intelligence"]}],["$","$L7","Robotics",{"href":"/tags/Robotics","className":"page__taxonomy-item","children":["#","Robotics"]}],["$","$L7","Foundation Models",{"href":"/tags/Foundation%20Models","className":"page__taxonomy-item","children":["#","Foundation Models"]}],["$","$L7","Multi-modal Learning",{"href":"/tags/Multi-modal%20Learning","className":"page__taxonomy-item","children":["#","Multi-modal Learning"]}],["$","$L7","Reinforcement Learning",{"href":"/tags/Reinforcement%20Learning","className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","$L7","Sim-to-Real Transfer",{"href":"/tags/Sim-to-Real%20Transfer","className":"page__taxonomy-item","children":["#","Sim-to-Real Transfer"]}],["$","$L7","Human-Robot Interaction",{"href":"/tags/Human-Robot%20Interaction","className":"page__taxonomy-item","children":["#","Human-Robot Interaction"]}]]]}]}],["$","$Lc",null,{"postPermalink":"/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges","postId":"2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-12-22-4D-RGPT-Toward-Region-level-4D-Understanding-via-Perceptual-Distillation","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-12-22-Are-We-on-the-Right-Way-to-Assessing-LLM-as-a-Judge","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Are We on the Right Way to Assessing LLM-as-a-Judge?"}]]}]]}]]}]]}]]}]]}]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"arXiv에 게시된 'An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","link","5",{"rel":"manifest","href":"/manifest.json","crossOrigin":"use-credentials"}],["$","meta","6",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","7",{"name":"creator","content":"secrett2633"}],["$","meta","8",{"name":"publisher","content":"secrett2633"}],["$","meta","9",{"name":"robots","content":"index, follow"}],["$","meta","10",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","11",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges"}],["$","meta","12",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","13",{"property":"og:title","content":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges"}],["$","meta","14",{"property":"og:description","content":"arXiv에 게시된 'An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges' 논문에 대한 자세한 리뷰입니다."}],["$","meta","15",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2025-12-22-An-Anatomy-of-Vision-Language-Action-Models-From-Modules-to-Milestones-and-Challenges"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2025-12-21T15:00:00.000Z"}],["$","meta","18",{"property":"article:modified_time","content":"2025-12-21T15:00:00.000Z"}],["$","meta","19",{"property":"article:author","content":"secrett2633"}],["$","meta","20",{"property":"article:section","content":"Review"}],["$","meta","21",{"property":"article:tag","content":"Review"}],["$","meta","22",{"property":"article:tag","content":"Vision-Language-Action Models"}],["$","meta","23",{"property":"article:tag","content":"Embodied Intelligence"}],["$","meta","24",{"property":"article:tag","content":"Robotics"}],["$","meta","25",{"property":"article:tag","content":"Foundation Models"}],["$","meta","26",{"property":"article:tag","content":"Multi-modal Learning"}],["$","meta","27",{"property":"article:tag","content":"Reinforcement Learning"}],["$","meta","28",{"property":"article:tag","content":"Sim-to-Real Transfer"}],["$","meta","29",{"property":"article:tag","content":"Human-Robot Interaction"}],["$","meta","30",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","31",{"name":"twitter:creator","content":"@secrett2633"}],["$","meta","32",{"name":"twitter:title","content":"[논문리뷰] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges"}],["$","meta","33",{"name":"twitter:description","content":"arXiv에 게시된 'An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges' 논문에 대한 자세한 리뷰입니다."}],["$","link","34",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}],["$","meta","35",{"name":"next-size-adjust"}]]
1:null
