3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-53ec9cbf619543e1.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-80e07cee06d17a0b.js"],""]
4:["slug","ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","c"]
0:["_3-KrKKQh7tcSoXiYwQMB",[[["",{"children":[["slug","ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d6cea809dcbae606.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
a:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-80e07cee06d17a0b.js"],"default"]
9:Td44,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2601.16443">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Kanishk Gandhi, Shivam Garg, Noah D. Goodman, Dimitris Papailiopoulos</p>
<h2>핵심 연구 목표</h2>
<p>본 논문은 자체 개선 에이전트 훈련을 위한 환경이 부족하다는 문제점을 해결하고, 확장 가능한 RL 환경을 제공하는 것을 목표로 합니다. 특히 터미널 사용 태스크에 초점을 맞춰, 기존 벤치마크가 평가용으로 설계되어 훈련에 적합하지 않다는 점을 극복하고 인간의 개입 없이 <strong>터미널 사용 태스크를 자동으로 생성하는 파이프라인</strong> 을 구축하고자 합니다.</p>
<h2>핵심 방법론</h2>
<p><code>Endless Terminals</code> 파이프라인은 네 단계로 구성됩니다. 첫째, <strong>LLM</strong> 을 활용하여 카테고리, 복잡도, 시나리오를 샘플링해 다양한 태스크 설명과 권한이 부여된 정답 데이터를 생성합니다. 둘째, <strong>컨테이너화된 환경(Docker/Apptainer)</strong> 을 설정하고 <strong>반복적인 정제 루프</strong> 를 통해 초기 상태 유효성 검사 테스트를 통과시킵니다. 셋째, 태스크 완료 후 기대되는 최종 상태를 검증하는 완료 테스트를 생성합니다. 넷째, <strong>강력한 모델(03)</strong> 에서 <strong>16개의 솔루션</strong> 을 샘플링하여 태스크의 해결 가능성을 확인하고 필터링합니다. 에이전트 훈련에는 <strong>바닐라 PPO</strong> 와 최소한의 상호작용 루프(추론, 명령 실행, 출력 관찰)가 사용됩니다.</p>
<h2>주요 결과</h2>
<p>이 파이프라인을 통해 <strong>3255개의 검증된 태스크</strong> 를 성공적으로 생성했습니다. <code>Endless Terminals</code>로 훈련된 모델들은 자체 개발 dev 세트에서 <strong>Llama-3.2-3B가 4.0%에서 18.2%로</strong> , <strong>Qwen2.5-7B가 10.7%에서 53.3%로</strong> , <strong>Qwen3-8B-openthinker-sft가 42.6%에서 59.0%로</strong> 크게 향상되었습니다. 이러한 개선 사항은 인간이 큐레이션한 <strong>TerminalBench 2.0 벤치마크</strong> 에서도 <strong>Llama-3.2-3B가 0.0%에서 2.2%로</strong> , <strong>Qwen2.5-7B가 2.2%에서 3.4%로</strong> , <strong>Qwen3-8B-openthinker-sft가 1.1%에서 6.7%로</strong> 성능이 향상되어, 다른 복잡한 에이전틱 스캐폴드를 사용한 접근 방식보다 우수한 결과를 보였습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p>이 연구는 <strong>환경의 규모 확장</strong> 이 복잡한 대화형 태스크에서 <strong>단순한 RL(PPO) 설정으로도 상당한 성능 향상</strong> 을 가져올 수 있음을 입증합니다. <strong>절차적 생성 파이프라인</strong> 은 인간의 개입 없이 다양하고 검증 가능한 훈련 데이터를 효과적으로 생성하는 방법을 제시하여, <strong>터미널 에이전트 개발 비용과 시간을 절감</strong> 할 수 있는 실용적인 해결책을 제공합니다. 이는 <strong>LLM 기반 에이전트</strong> 의 훈련 데이터를 자동화하고 확장하는 데 중요한 이정표가 될 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2728,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] Endless Terminals: Scaling RL Environments for Terminal Agents"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2026-01-26 00:00:00+0900+0900","children":"2026년 1월 26일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2026년 1월 26일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","span","Procedural Generation",{"className":"page__taxonomy-item","children":["#","Procedural Generation"]}],["$","span","Terminal Agents",{"className":"page__taxonomy-item","children":["#","Terminal Agents"]}],["$","span","Environment Scaling",{"className":"page__taxonomy-item","children":["#","Environment Scaling"]}],["$","span","Language Models (LLMs)",{"className":"page__taxonomy-item","children":["#","Language Models (LLMs)"]}],["$","span","PPO",{"className":"page__taxonomy-item","children":["#","PPO"]}],["$","span","Task Generation",{"className":"page__taxonomy-item","children":["#","Task Generation"]}],["$","span","Automated Verification",{"className":"page__taxonomy-item","children":["#","Automated Verification"]}]]]}]}],["$","$La",null,{"postPermalink":"/ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents","postId":"2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2026-01-26-DSGym-A-Holistic-Framework-for-Evaluating-and-Training-Data-Science-Agents","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] DSGym: A Holistic Framework for Evaluating and Training Data Science Agents"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] Endless Terminals: Scaling RL Environments for Terminal Agents"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2026-01-26-Guidelines-to-Prompt-Large-Language-Models-for-Code-Generation-An-Empirical-Characterization","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"[논문리뷰] Endless Terminals: Scaling RL Environments for Terminal Agents - secrett2633's blog"}],["$","meta","3",{"name":"description","content":"이 [arXiv]에 게시한 'Endless Terminals: Scaling RL Environments for Terminal Agents' 논문에 대한 자세한 리뷰입니다."}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.cloud/ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"[논문리뷰] Endless Terminals: Scaling RL Environments for Terminal Agents"}],["$","meta","13",{"property":"og:description","content":"이 [arXiv]에 게시한 'Endless Terminals: Scaling RL Environments for Terminal Agents' 논문에 대한 자세한 리뷰입니다."}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.cloud/ai/review/2026-01-26-Endless-Terminals-Scaling-RL-Environments-for-Terminal-Agents"}],["$","meta","15",{"property":"og:type","content":"article"}],["$","meta","16",{"name":"twitter:card","content":"summary"}],["$","meta","17",{"name":"twitter:title","content":"[논문리뷰] Endless Terminals: Scaling RL Environments for Terminal Agents"}],["$","meta","18",{"name":"twitter:description","content":"이 [arXiv]에 게시한 'Endless Terminals: Scaling RL Environments for Terminal Agents' 논문에 대한 자세한 리뷰입니다."}],["$","link","19",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}]]
1:null
