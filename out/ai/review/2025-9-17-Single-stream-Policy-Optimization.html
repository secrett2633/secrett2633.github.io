<!DOCTYPE html><html lang="ko" class="no-js"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/ddc331716d5e47a2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-90b03762f46d1ba4.js"/><script src="/_next/static/chunks/fd9d1056-0395f68b8cc78a20.js" async=""></script><script src="/_next/static/chunks/23-7d3f7f0b78aa2fd3.js" async=""></script><script src="/_next/static/chunks/main-app-6087bc228fd56b83.js" async=""></script><script src="/_next/static/chunks/231-467e37449c5a68fc.js" async=""></script><script src="/_next/static/chunks/app/layout-b0a450f8e4964582.js" async=""></script><script src="/_next/static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY" as="script"/><title>[논문리뷰] Single-stream Policy Optimization - secrett2633&#x27;s blog</title><meta name="description" content="Zihan Ding이 [arXiv]에 게시한 &#x27;Single-stream Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다."/><meta name="author" content="secrett2633"/><link rel="manifest" href="/manifest.json" crossorigin="use-credentials"/><meta name="keywords" content="Django, Python, DevOps, AI, ML, 블로그, 기술"/><meta name="creator" content="secrett2633"/><meta name="publisher" content="secrett2633"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="[논문리뷰] Single-stream Policy Optimization"/><meta property="og:description" content="Zihan Ding이 [arXiv]에 게시한 &#x27;Single-stream Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다."/><meta property="og:url" content="https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-09-17T04:16:01.000Z"/><meta property="article:modified_time" content="2025-09-17T04:16:01.000Z"/><meta property="article:author" content="secrett2633"/><meta property="article:section" content="Review"/><meta property="article:tag" content="Review"/><meta property="article:tag" content="Reinforcement Learning"/><meta property="article:tag" content="LLM Optimization"/><meta property="article:tag" content="Policy Gradient"/><meta property="article:tag" content="Variance Reduction"/><meta property="article:tag" content="Adaptive Sampling"/><meta property="article:tag" content="Scalability"/><meta property="article:tag" content="Agentic Systems"/><meta property="article:tag" content="RLVR"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@secrett2633"/><meta name="twitter:title" content="[논문리뷰] Single-stream Policy Optimization"/><meta name="twitter:description" content="Zihan Ding이 [arXiv]에 게시한 &#x27;Single-stream Policy Optimization&#x27; 논문에 대한 자세한 리뷰입니다."/><link rel="icon" href="/icon.ico?6d9f34d4948640b8" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><meta name="msapplication-TileColor" content="#ffc40d"/><meta name="theme-color" content="#ffffff"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="dns-prefetch" href="https://giscus.app"/><link rel="preconnect" href="https://giscus.app" crossorigin="anonymous"/><meta http-equiv="X-Content-Type-Options" content="nosniff"/><meta name="referrer" content="strict-origin-when-cross-origin"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"secrett2633's blog","url":"https://blog.secrett2633.cloud","description":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트","inLanguage":"ko","publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud","sameAs":["https://github.com/secrett2633"]}</script><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f367f3 layout--default"><a href="#main-content" class="sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600">본문으로 건너뛰기</a><div class="min-h-screen bg-gray-50"><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav" aria-label="메인 네비게이션"><a class="site-title" href="/">secrett2633&#x27;s blog</a><div class="flex items-center space-x-4"><ul class="visible-links"><li class="masthead__menu-item"><a href="https://github.com/secrett2633" target="_blank" rel="noopener noreferrer">GitHub</a></li></ul><button class="search__toggle" type="button" aria-label="검색"><svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16"><path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path></svg></button></div></nav></div></div></div><main id="main-content" class="initial-content"><!--$--><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문리뷰] Single-stream Policy Optimization","description":"Zihan Ding이 [arXiv]에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.","url":"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization","datePublished":"2025-09-17T04:16:01.000Z","dateModified":"2025-09-17T04:16:01.000Z","author":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"publisher":{"@type":"Person","name":"secrett2633","url":"https://blog.secrett2633.cloud"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization"},"image":"https://blog.secrett2633.cloud/og-default.png","isAccessibleForFree":true,"inLanguage":"ko","wordCount":246,"articleSection":"Review","keywords":"Review, Reinforcement Learning, LLM Optimization, Policy Gradient, Variance Reduction, Adaptive Sampling, Scalability, Agentic Systems, RLVR"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"홈","item":"https://blog.secrett2633.cloud/"},{"@type":"ListItem","position":2,"name":"Review","item":"https://blog.secrett2633.cloud/ai/review"},{"@type":"ListItem","position":3,"name":"[논문리뷰] Single-stream Policy Optimization","item":"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization"}]}</script><div class="flex flex-col lg:flex-row gap-8"><aside class="lg:w-64 xl:w-72 order-1 lg:order-none"><div class="sidebar sticky"><nav class="space-y-4" aria-label="카테고리 네비게이션"><div><p class="font-medium text-gray-900 mb-2">Backend</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/django">Django<!-- --> (<!-- -->6<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/backend/logging">Logging<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">Python</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/python/pep">PEP<!-- --> (<!-- -->650<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">AI/ML</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/llm">LLM<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/ai/review">Review<!-- --> (<!-- -->2728<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">DevOps</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/nginx">Nginx<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/docker">Docker<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/safeline">SafeLine<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/jenkins">Jenkins<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/github-actions">GitHub Actions<!-- --> (<!-- -->1<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/devops/aws">AWS<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div><div><p class="font-medium text-gray-900 mb-2">etc</p><ul class="space-y-1 ml-4"><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/me">Me<!-- --> (<!-- -->3<!-- -->)</a></li><li><a class="text-sm text-gray-600 hover:text-primary-600 block py-1" href="/etc/chrome-extension">Chrome Extension<!-- --> (<!-- -->1<!-- -->)</a></li></ul></div></nav></div></aside><div class="flex-1"><nav aria-label="breadcrumb" class="text-sm text-gray-500 mb-4"><ol class="flex flex-wrap items-center gap-1"><li><a class="hover:text-gray-700" href="/">홈</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><a class="hover:text-gray-700" href="/ai/review">Review</a></li><li class="flex items-center gap-1"><span aria-hidden="true">/</span><span class="text-gray-900" aria-current="page">[논문리뷰] Single-stream Policy Optimization</span></li></ol></nav><article class="page"><header class="mb-8"><h1 class="page__title">[논문리뷰] Single-stream Policy Optimization</h1><div class="page__meta"><time dateTime="2025-09-17 13:16:01+0900">2025년 9월 17일</time><time class="ml-4" dateTime="2025-09-17T04:16:01.000Z">수정: <!-- -->2025년 9월 17일</time></div></header><div class="page__content"><div><blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2509.13232" target="_blank" rel="noopener noreferrer">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Zhongwen Xu, Zihan Ding</p>
<h2 id="핵심-연구-목표"><a href="#핵심-연구-목표">핵심 연구 목표</a></h2>
<p>본 논문은 LLM을 위한 기존 그룹 기반 정책 최적화 방식( <strong>GRPO</strong> 등)이 겪는 비효율성(퇴화 그룹으로 인한 학습 신호 손실)과 동기화 장벽으로 인한 확장성 문제를 해결하고자 합니다. 연구 목표는 이러한 한계를 극복하고 LLM 추론을 위한 안정적이고 효율적이며 확장 가능한 학습 신호를 제공하는 단일 스트림 최적화 방법론을 제시하는 것입니다.</p>
<h2 id="핵심-방법론"><a href="#핵심-방법론">핵심 방법론</a></h2>
<p>제안된 <strong>Single-stream Policy Optimization (SPO)</strong> 은 세 가지 핵심 구성 요소를 통합합니다. 첫째, 각 프롬프트에 대한 성공 확률의 지속적인 베이지안 추정치를 유지하는 <strong>KL-적응형 가치 추적기</strong> 를 사용하여 저분산 기준선(baseline)을 제공합니다. 둘째, 배치 전체에 걸쳐 이점(advantage)을 전역적으로 정규화하여 그룹별 통계의 불안정성을 방지합니다. 마지막으로, 우선순위 샘플링을 통해 학습 잠재력이 높은 프롬프트에 자원을 집중하는 <strong>적응형 커리큘럼</strong> 을 구현합니다.</p>
<h2 id="주요-결과"><a href="#주요-결과">주요 결과</a></h2>
<p>실험 결과, <strong>SPO</strong> 는 <strong>Qwen3-8B</strong> 모델을 사용하여 5개의 난이도 높은 수학 벤치마크에서 <strong>GRPO</strong> 보다 일관되게 우수한 성능을 보였습니다. 평균 <strong>maj@32</strong> 점수를 <strong>GRPO</strong> 대비 <strong>+3.4%p</strong> 향상시켰으며, 특히 <strong>BRUMO 25 (+7.3%p)</strong> , <strong>AIME 25 (+4.4%p)</strong> , <strong>HMMT 25 (+3.3%p)</strong> 에서 상당한 절대 점수 상승을 달성했습니다. 또한, 그룹 동기화 병목 현상을 제거하여 에이전트 환경 시뮬레이션에서 <strong>4.35배</strong> 의 학습 처리량 향상을 입증했습니다.</p>
<h2 id="ai-실무자를-위한-시사점"><a href="#ai-실무자를-위한-시사점">AI 실무자를 위한 시사점</a></h2>
<p><strong>SPO</strong> 는 LLM 최적화를 위한 보다 강력하고 확장 가능하며 효율적인 기반을 제공하여, 특히 상호작용 시간이 가변적인 복잡한 추론 및 에이전트 태스크에 유리합니다. 이는 RL 알고리즘에 불필요한 복잡성을 추가하는 경향에 도전하며, 기초적인 RL 원칙이 LLM 추론 발전의 다음 단계를 주도할 수 있음을 시사합니다. 따라서, 복잡한 에이전트 시스템이나 장기적인 추론 작업에서 LLM의 효율적인 훈련 및 배포에 중요한 영향을 미칠 것으로 기대됩니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
</div></div><footer class="page__meta mt-8"><div class="page__taxonomy"><span class="text-sm font-medium text-gray-900 mb-2 block">태그</span><a class="page__taxonomy-item" href="/tags/Review">#<!-- -->Review</a><a class="page__taxonomy-item" href="/tags/Reinforcement%20Learning">#<!-- -->Reinforcement Learning</a><a class="page__taxonomy-item" href="/tags/LLM%20Optimization">#<!-- -->LLM Optimization</a><a class="page__taxonomy-item" href="/tags/Policy%20Gradient">#<!-- -->Policy Gradient</a><a class="page__taxonomy-item" href="/tags/Variance%20Reduction">#<!-- -->Variance Reduction</a><a class="page__taxonomy-item" href="/tags/Adaptive%20Sampling">#<!-- -->Adaptive Sampling</a><a class="page__taxonomy-item" href="/tags/Scalability">#<!-- -->Scalability</a><a class="page__taxonomy-item" href="/tags/Agentic%20Systems">#<!-- -->Agentic Systems</a><a class="page__taxonomy-item" href="/tags/RLVR">#<!-- -->RLVR</a></div></footer><section class="comments-section mt-12 pt-8 border-t border-gray-200"></section><section class="mt-12 border-t border-gray-200 pt-8"><h3 class="text-base font-semibold text-gray-900 mb-4">Review<!-- --> 의 다른글</h3><ul class="space-y-2 text-sm"><li class="text-gray-500">이전글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-9-17-Scaling-Agents-via-Continual-Pre-training">[논문리뷰] Scaling Agents via Continual Pre-training</a></li><li class="text-gray-900 font-semibold">현재글 : <!-- -->[논문리뷰] Single-stream Policy Optimization</li><li class="text-gray-500">다음글<!-- --> <a class="text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4" href="/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling">[논문리뷰] Towards General Agentic Intelligence via Environment Scaling</a></li></ul></section></article></div></div><!--/$--></main><div id="footer" class="page__footer"><footer class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="text-center text-gray-500 text-sm"><p>© <!-- -->2026<!-- --> secrett2633. All rights reserved.</p></div></footer></div></div><script src="/_next/static/chunks/webpack-90b03762f46d1ba4.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/ddc331716d5e47a2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[9157,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"default\"]\na:I[231,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js\"],\"\"]\nb:I[4080,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"185\",\"static/chunks/app/layout-b0a450f8e4964582.js\"],\"\"]\nd:I[6130,[],\"\"]\n7:[\"slug\",\"ai/review/2025-9-17-Single-stream-Policy-Optimization\",\"c\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/ddc331716d5e47a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"WcxaIiCPz9cbpnkGvOjOK\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/ai/review/2025-9-17-Single-stream-Policy-Optimization\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-9-17-Single-stream-Policy-Optimization\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ai\\\",\\\"review\\\",\\\"2025-9-17-Single-stream-Policy-Optimization\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"ai/review/2025-9-17-Single-stream-Policy-Optimization\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\"],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"no-js\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#ffc40d\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#ffffff\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://www.googletagmanager.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://www.googletagmanager.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://giscus.app\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://giscus.app\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"meta\",null,{\"httpEquiv\":\"X-Content-Type-Options\",\"content\":\"nosniff\"}],[\"$\",\"meta\",null,{\"name\":\"referrer\",\"content\":\"strict-origin-when-cross-origin\"}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"secrett2633's blog\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"description\\\":\\\"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트\\\",\\\"inLanguage\\\":\\\"ko\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\",\\\"sameAs\\\":[\\\"https://github.com/secrett2633\\\"]}\"}}]]}],[\"$\",\"body\",null,{\"className\":\"__className_f367f3 layout--default\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"sr-only focus:not-sr-only focus:absolute focus:z-50 focus:p-4 focus:bg-white focus:text-blue-600\",\"children\":\"본문으로 건너뛰기\"}],[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-gray-50\",\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"initial-content\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center bg-gray-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-6xl font-bold text-primary-600 mb-4\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-semibold text-gray-900 mb-4\",\"children\":\"페이지를 찾을 수 없습니다\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8\",\"children\":\"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다.\"}],[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors\",\"children\":\"홈으로 돌아가기\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"div\",null,{\"id\":\"footer\",\"className\":\"page__footer\",\"children\":[\"$\",\"footer\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center text-gray-500 text-sm\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2026,\" secrett2633. All rights reserved.\"]}]}]}]}]]}],[\"$\",\"$Lb\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$Lb\",null,{\"id\":\"gtag-init\",\"strategy\":\"afterInteractive\",\"children\":\"window.dataLayer = window.dataLayer || [];\\n            function gtag(){dataLayer.push(arguments);}\\n            gtag('js', new Date());\\n            gtag('config', 'G-NE2W3CFPNY');\"}]]}]]}],null],[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center min-h-screen\",\"role\":\"status\",\"aria-label\":\"로딩 중\",\"children\":[[\"$\",\"div\",null,{\"className\":\"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600\"}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"로딩 중...\"}]]}],[],[]]],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"10:I[646,[\"231\",\"static/chunks/231-467e37449c5a68fc.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-1f60377561abdb46.js\"],\"default\"]\nf:Tc50,"])</script><script>self.__next_f.push([1,"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e링크:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/2509.13232\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e논문 PDF로 바로 열기\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e저자:\u003c/strong\u003e Zhongwen Xu, Zihan Ding\u003c/p\u003e\n\u003ch2 id=\"핵심-연구-목표\"\u003e\u003ca href=\"#핵심-연구-목표\"\u003e핵심 연구 목표\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e본 논문은 LLM을 위한 기존 그룹 기반 정책 최적화 방식( \u003cstrong\u003eGRPO\u003c/strong\u003e 등)이 겪는 비효율성(퇴화 그룹으로 인한 학습 신호 손실)과 동기화 장벽으로 인한 확장성 문제를 해결하고자 합니다. 연구 목표는 이러한 한계를 극복하고 LLM 추론을 위한 안정적이고 효율적이며 확장 가능한 학습 신호를 제공하는 단일 스트림 최적화 방법론을 제시하는 것입니다.\u003c/p\u003e\n\u003ch2 id=\"핵심-방법론\"\u003e\u003ca href=\"#핵심-방법론\"\u003e핵심 방법론\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e제안된 \u003cstrong\u003eSingle-stream Policy Optimization (SPO)\u003c/strong\u003e 은 세 가지 핵심 구성 요소를 통합합니다. 첫째, 각 프롬프트에 대한 성공 확률의 지속적인 베이지안 추정치를 유지하는 \u003cstrong\u003eKL-적응형 가치 추적기\u003c/strong\u003e 를 사용하여 저분산 기준선(baseline)을 제공합니다. 둘째, 배치 전체에 걸쳐 이점(advantage)을 전역적으로 정규화하여 그룹별 통계의 불안정성을 방지합니다. 마지막으로, 우선순위 샘플링을 통해 학습 잠재력이 높은 프롬프트에 자원을 집중하는 \u003cstrong\u003e적응형 커리큘럼\u003c/strong\u003e 을 구현합니다.\u003c/p\u003e\n\u003ch2 id=\"주요-결과\"\u003e\u003ca href=\"#주요-결과\"\u003e주요 결과\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e실험 결과, \u003cstrong\u003eSPO\u003c/strong\u003e 는 \u003cstrong\u003eQwen3-8B\u003c/strong\u003e 모델을 사용하여 5개의 난이도 높은 수학 벤치마크에서 \u003cstrong\u003eGRPO\u003c/strong\u003e 보다 일관되게 우수한 성능을 보였습니다. 평균 \u003cstrong\u003emaj@32\u003c/strong\u003e 점수를 \u003cstrong\u003eGRPO\u003c/strong\u003e 대비 \u003cstrong\u003e+3.4%p\u003c/strong\u003e 향상시켰으며, 특히 \u003cstrong\u003eBRUMO 25 (+7.3%p)\u003c/strong\u003e , \u003cstrong\u003eAIME 25 (+4.4%p)\u003c/strong\u003e , \u003cstrong\u003eHMMT 25 (+3.3%p)\u003c/strong\u003e 에서 상당한 절대 점수 상승을 달성했습니다. 또한, 그룹 동기화 병목 현상을 제거하여 에이전트 환경 시뮬레이션에서 \u003cstrong\u003e4.35배\u003c/strong\u003e 의 학습 처리량 향상을 입증했습니다.\u003c/p\u003e\n\u003ch2 id=\"ai-실무자를-위한-시사점\"\u003e\u003ca href=\"#ai-실무자를-위한-시사점\"\u003eAI 실무자를 위한 시사점\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSPO\u003c/strong\u003e 는 LLM 최적화를 위한 보다 강력하고 확장 가능하며 효율적인 기반을 제공하여, 특히 상호작용 시간이 가변적인 복잡한 추론 및 에이전트 태스크에 유리합니다. 이는 RL 알고리즘에 불필요한 복잡성을 추가하는 경향에 도전하며, 기초적인 RL 원칙이 LLM 추론 발전의 다음 단계를 주도할 수 있음을 시사합니다. 따라서, 복잡한 에이전트 시스템이나 장기적인 추론 작업에서 LLM의 효율적인 훈련 및 배포에 중요한 영향을 미칠 것으로 기대됩니다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e⚠️ \u003cstrong\u003e알림:\u003c/strong\u003e 이 리뷰는 AI로 작성되었습니다.\u003c/p\u003e\n\u003c/blockquote\u003e\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"[논문리뷰] Single-stream Policy Optimization\\\",\\\"description\\\":\\\"Zihan Ding이 [arXiv]에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization\\\",\\\"datePublished\\\":\\\"2025-09-17T04:16:01.000Z\\\",\\\"dateModified\\\":\\\"2025-09-17T04:16:01.000Z\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"secrett2633\\\",\\\"url\\\":\\\"https://blog.secrett2633.cloud\\\"},\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization\\\"},\\\"image\\\":\\\"https://blog.secrett2633.cloud/og-default.png\\\",\\\"isAccessibleForFree\\\":true,\\\"inLanguage\\\":\\\"ko\\\",\\\"wordCount\\\":246,\\\"articleSection\\\":\\\"Review\\\",\\\"keywords\\\":\\\"Review, Reinforcement Learning, LLM Optimization, Policy Gradient, Variance Reduction, Adaptive Sampling, Scalability, Agentic Systems, RLVR\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"홈\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"Review\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":3,\\\"name\\\":\\\"[논문리뷰] Single-stream Policy Optimization\\\",\\\"item\\\":\\\"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization\\\"}]}\"}}],[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:flex-row gap-8\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"lg:w-64 xl:w-72 order-1 lg:order-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"sidebar sticky\",\"children\":[\"$\",\"nav\",null,{\"className\":\"space-y-4\",\"aria-label\":\"카테고리 네비게이션\",\"children\":[[\"$\",\"div\",\"Backend\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Backend\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Django\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/django\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Django\",\" (\",6,\")\"]}]}],[\"$\",\"li\",\"Logging\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/backend/logging\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Logging\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"Python\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"Python\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"PEP\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/python/pep\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"PEP\",\" (\",650,\")\"]}]}]]}]]}],[\"$\",\"div\",\"AI/ML\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"AI/ML\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"LLM\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/llm\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"LLM\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Review\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Review\",\" (\",2728,\")\"]}]}]]}]]}],[\"$\",\"div\",\"DevOps\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"DevOps\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Nginx\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/nginx\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Nginx\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Docker\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/docker\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Docker\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"SafeLine\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/safeline\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"SafeLine\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"Jenkins\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/jenkins\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Jenkins\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"GitHub Actions\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/github-actions\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"GitHub Actions\",\" (\",1,\")\"]}]}],[\"$\",\"li\",\"AWS\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/devops/aws\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"AWS\",\" (\",1,\")\"]}]}]]}]]}],[\"$\",\"div\",\"etc\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"font-medium text-gray-900 mb-2\",\"children\":\"etc\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1 ml-4\",\"children\":[[\"$\",\"li\",\"Me\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/me\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Me\",\" (\",3,\")\"]}]}],[\"$\",\"li\",\"Chrome Extension\",{\"children\":[\"$\",\"$La\",null,{\"href\":\"/etc/chrome-extension\",\"className\":\"text-sm text-gray-600 hover:text-primary-600 block py-1\",\"children\":[\"Chrome Extension\",\" (\",1,\")\"]}]}]]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"nav\",null,{\"aria-label\":\"breadcrumb\",\"className\":\"text-sm text-gray-500 mb-4\",\"children\":[\"$\",\"ol\",null,{\"className\":\"flex flex-wrap items-center gap-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$La\",null,{\"href\":\"/\",\"className\":\"hover:text-gray-700\",\"children\":\"홈\"}]}],[[\"$\",\"li\",\"/ai/review\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"$La\",null,{\"href\":\"/ai/review\",\"className\":\"hover:text-gray-700\",\"children\":\"Review\"}]]}],[\"$\",\"li\",\"/ai/review/2025-9-17-Single-stream-Policy-Optimization\",{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-900\",\"aria-current\":\"page\",\"children\":\"[논문리뷰] Single-stream Policy Optimization\"}]]}]]]}]}],[\"$\",\"article\",null,{\"className\":\"page\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"page__title\",\"children\":\"[논문리뷰] Single-stream Policy Optimization\"}],[\"$\",\"div\",null,{\"className\":\"page__meta\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-09-17 13:16:01+0900\",\"children\":\"2025년 9월 17일\"}],[\"$\",\"time\",null,{\"className\":\"ml-4\",\"dateTime\":\"2025-09-17T04:16:01.000Z\",\"children\":[\"수정: \",\"2025년 9월 17일\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"page__content\",\"children\":[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$f\"}}]}],[\"$\",\"footer\",null,{\"className\":\"page__meta mt-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"page__taxonomy\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm font-medium text-gray-900 mb-2 block\",\"children\":\"태그\"}],[[\"$\",\"$La\",\"Review\",{\"href\":\"/tags/Review\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Review\"]}],[\"$\",\"$La\",\"Reinforcement Learning\",{\"href\":\"/tags/Reinforcement%20Learning\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Reinforcement Learning\"]}],[\"$\",\"$La\",\"LLM Optimization\",{\"href\":\"/tags/LLM%20Optimization\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"LLM Optimization\"]}],[\"$\",\"$La\",\"Policy Gradient\",{\"href\":\"/tags/Policy%20Gradient\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Policy Gradient\"]}],[\"$\",\"$La\",\"Variance Reduction\",{\"href\":\"/tags/Variance%20Reduction\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Variance Reduction\"]}],[\"$\",\"$La\",\"Adaptive Sampling\",{\"href\":\"/tags/Adaptive%20Sampling\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Adaptive Sampling\"]}],[\"$\",\"$La\",\"Scalability\",{\"href\":\"/tags/Scalability\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Scalability\"]}],[\"$\",\"$La\",\"Agentic Systems\",{\"href\":\"/tags/Agentic%20Systems\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"Agentic Systems\"]}],[\"$\",\"$La\",\"RLVR\",{\"href\":\"/tags/RLVR\",\"className\":\"page__taxonomy-item\",\"children\":[\"#\",\"RLVR\"]}]]]}]}],[\"$\",\"$L10\",null,{\"postPermalink\":\"/ai/review/2025-9-17-Single-stream-Policy-Optimization\",\"postId\":\"2025-9-17-Single-stream-Policy-Optimization\"}],[\"$\",\"section\",null,{\"className\":\"mt-12 border-t border-gray-200 pt-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 mb-4\",\"children\":[\"Review\",\" 의 다른글\"]}],[\"$\",\"ul\",null,{\"className\":\"space-y-2 text-sm\",\"children\":[[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"이전글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-9-17-Scaling-Agents-via-Continual-Pre-training\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] Scaling Agents via Continual Pre-training\"}]]}],[\"$\",\"li\",null,{\"className\":\"text-gray-900 font-semibold\",\"children\":[\"현재글 : \",\"[논문리뷰] Single-stream Policy Optimization\"]}],[\"$\",\"li\",null,{\"className\":\"text-gray-500\",\"children\":[\"다음글\",\" \",[\"$\",\"$La\",null,{\"href\":\"/ai/review/2025-9-17-Towards-General-Agentic-Intelligence-via-Environment-Scaling\",\"className\":\"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4\",\"children\":\"[논문리뷰] Towards General Agentic Intelligence via Environment Scaling\"}]]}]]}]]}]]}]]}]]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"[논문리뷰] Single-stream Policy Optimization - secrett2633's blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Zihan Ding이 [arXiv]에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"secrett2633\"}],[\"$\",\"link\",\"5\",{\"rel\":\"manifest\",\"href\":\"/manifest.json\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"Django, Python, DevOps, AI, ML, 블로그, 기술\"}],[\"$\",\"meta\",\"7\",{\"name\":\"creator\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"8\",{\"name\":\"publisher\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"9\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"10\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"11\",{\"rel\":\"canonical\",\"href\":\"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization\"}],[\"$\",\"meta\",\"12\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:title\",\"content\":\"[논문리뷰] Single-stream Policy Optimization\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:description\",\"content\":\"Zihan Ding이 [arXiv]에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:url\",\"content\":\"https://blog.secrett2633.cloud/ai/review/2025-9-17-Single-stream-Policy-Optimization\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:published_time\",\"content\":\"2025-09-17T04:16:01.000Z\"}],[\"$\",\"meta\",\"18\",{\"property\":\"article:modified_time\",\"content\":\"2025-09-17T04:16:01.000Z\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:author\",\"content\":\"secrett2633\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:section\",\"content\":\"Review\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"Review\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"Reinforcement Learning\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"LLM Optimization\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"Policy Gradient\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"Variance Reduction\"}],[\"$\",\"meta\",\"26\",{\"property\":\"article:tag\",\"content\":\"Adaptive Sampling\"}],[\"$\",\"meta\",\"27\",{\"property\":\"article:tag\",\"content\":\"Scalability\"}],[\"$\",\"meta\",\"28\",{\"property\":\"article:tag\",\"content\":\"Agentic Systems\"}],[\"$\",\"meta\",\"29\",{\"property\":\"article:tag\",\"content\":\"RLVR\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:creator\",\"content\":\"@secrett2633\"}],[\"$\",\"meta\",\"32\",{\"name\":\"twitter:title\",\"content\":\"[논문리뷰] Single-stream Policy Optimization\"}],[\"$\",\"meta\",\"33\",{\"name\":\"twitter:description\",\"content\":\"Zihan Ding이 [arXiv]에 게시한 'Single-stream Policy Optimization' 논문에 대한 자세한 리뷰입니다.\"}],[\"$\",\"link\",\"34\",{\"rel\":\"icon\",\"href\":\"/icon.ico?6d9f34d4948640b8\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"35\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script></body></html>