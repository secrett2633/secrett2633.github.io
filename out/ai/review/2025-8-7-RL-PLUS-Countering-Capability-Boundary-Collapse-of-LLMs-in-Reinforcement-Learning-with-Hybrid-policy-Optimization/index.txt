3:I[9275,[],""]
5:I[1343,[],""]
6:I[9157,["231","static/chunks/231-467e37449c5a68fc.js","185","static/chunks/app/layout-3497e95e26431168.js"],"default"]
7:I[231,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-a0fe0cc578429896.js"],""]
4:["slug","ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","c"]
0:["uXVO9cTH_KJHTsIHFmRuq",[[["",{"children":[["slug","ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","c"],{"children":["__PAGE__?{\"slug\":[\"ai\",\"review\",\"2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"ko","className":"no-js","children":[["$","head",null,{"children":[["$","meta",null,{"name":"msapplication-TileColor","content":"#ffc40d"}],["$","meta",null,{"name":"theme-color","content":"#ffffff"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-NE2W3CFPNY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              window.dataLayer = window.dataLayer || [];\n              function gtag(){dataLayer.push(arguments);}\n              gtag('js', new Date());\n              gtag('config', 'G-NE2W3CFPNY');\n            "}}]]}],["$","body",null,{"className":"__className_f367f3 layout--default","children":["$","div",null,{"className":"min-h-screen bg-gray-50","children":[["$","$L6",null,{}],["$","main",null,{"className":"initial-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-h-screen flex items-center justify-center bg-gray-50","children":["$","div",null,{"className":"max-w-md w-full bg-white rounded-lg shadow-md p-8 text-center","children":[["$","h1",null,{"className":"text-6xl font-bold text-primary-600 mb-4","children":"404"}],["$","h2",null,{"className":"text-2xl font-semibold text-gray-900 mb-4","children":"페이지를 찾을 수 없습니다"}],["$","p",null,{"className":"text-gray-600 mb-8","children":"요청하신 페이지가 존재하지 않거나 이동되었을 수 있습니다."}],["$","$L7",null,{"href":"/","className":"inline-flex items-center px-4 py-2 bg-primary-600 text-white rounded-md hover:bg-primary-700 transition-colors","children":"홈으로 돌아가기"}]]}]}],"notFoundStyles":[],"styles":null}]}],["$","div",null,{"id":"footer","className":"page__footer","children":["$","footer",null,{"className":"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8","children":["$","div",null,{"className":"text-center text-gray-500 text-sm","children":["$","p",null,{"children":"© 2025 secrett2633. All rights reserved."}]}]}]}]]}]}]]}],null],[["$","div",null,{"className":"flex items-center justify-center min-h-screen","children":["$","div",null,{"className":"animate-spin rounded-full h-32 w-32 border-b-2 border-primary-600"}]}],[],[]]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d6cea809dcbae606.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$L8"]]]]]
a:I[646,["231","static/chunks/231-467e37449c5a68fc.js","877","static/chunks/app/%5B...slug%5D/page-a0fe0cc578429896.js"],"default"]
9:Tbc0,<blockquote>
<p><strong>링크:</strong> <a href="https://arxiv.org/abs/2508.00222">논문 PDF로 바로 열기</a></p>
</blockquote>
<p><strong>저자:</strong> Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, Zhi Jin, Fei Huang, Yongbin Li, Ge Li</p>
<h2>핵심 연구 목표</h2>
<p>본 논문은 <strong>LLM</strong> 의 강화 학습(RLVR) 과정에서 발생하는 '능력 경계 붕괴(capability boundary collapse)' 문제를 해결하는 것을 목표로 합니다. 기존 RLVR 방식이 LLM의 내재된 능력 범위를 넘어서는 새로운 추론 능력을 획득하지 못하고 문제 해결 범위를 축소시키는 한계를 극복하고자 합니다.</p>
<h2>핵심 방법론</h2>
<p>저자들은 내부 활용과 외부 데이터 학습을 시너지화하는 새로운 하이브리드 정책 최적화 접근법인 <strong>RL-PLUS</strong> 를 제안합니다. 주요 구성 요소는 외부 데이터의 분포 불일치를 완화하는 <strong>다중 중요도 샘플링(Multiple Importance Sampling)</strong> 과 고가치 및 미탐색 추론 경로를 유도하기 위한 <strong>탐색 기반 어드밴티지 함수(Exploration-Based Advantage Function)</strong> 입니다. 특히, 탐색 기반 어드밴티지 함수는 <strong>Ci,t = (1 - detach(πθ(ei,t|q, ei,&#x3C;t)))γ</strong> 형태로, 모델이 낮은 확률로 예측했지만 정답인 토큰에 대한 학습 신호를 증폭시킵니다.</p>
<h2>주요 결과</h2>
<p><strong>RL-PLUS</strong> 는 6가지 수학 추론 벤치마크에서 <strong>53.4</strong> 의 평균 점수를 달성하며 기존 RLVR 방식 대비 <strong>5.2점</strong> 향상된 SOTA 성능을 기록했습니다. 또한, 6가지 OOD(Out-of-Distribution) 추론 태스크에서 <strong>48.8</strong> 의 평균 점수로 우수한 일반화 성능을 보였습니다. 다양한 모델 패밀리(예: <strong>Qwen2.5-Math-7B</strong> , <strong>LLaMA-3.1-8B</strong> )에 걸쳐 일관된 성능 향상을 입증했으며, 특히 <strong>Pass@k</strong> 분석을 통해 기존 방식에서 나타나는 능력 경계 붕괴 문제를 효과적으로 해결함을 보였습니다.</p>
<h2>AI 실무자를 위한 시사점</h2>
<p><strong>RL-PLUS</strong> 는 LLM의 추론 능력을 확장하고 새로운 지식을 통합하는 견고한 프레임워크를 제공합니다. 이는 희소한 보상과 방대한 액션 공간으로 인해 외부 탐색이 어려웠던 기존 RLVR의 한계를 극복하고, 모델이 미리 숙달된 지식에 대한 업데이트는 줄이고 탐색이 어려웠던 고가치 액션에 집중하도록 학습을 유도합니다. 이 접근 방식은 다양한 LLM 아키텍처와 규모에 적용 가능하여 실제 AI 애플리케이션에서 LLM의 문제 해결 능력을 향상시키는 데 기여할 수 있습니다.</p>
<blockquote>
<p>⚠️ <strong>알림:</strong> 이 리뷰는 AI로 작성되었습니다.</p>
</blockquote>
2:["$","div",null,{"className":"flex flex-col lg:flex-row gap-8","children":[["$","aside",null,{"className":"lg:w-64 xl:w-72 order-1 lg:order-none","children":["$","div",null,{"className":"sidebar sticky","children":["$","nav",null,{"className":"space-y-4","children":[["$","div","Backend",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Backend"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Django",{"children":["$","a",null,{"href":"/backend/django/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Django"," (",6,")"]}]}],["$","li","Logging",{"children":["$","a",null,{"href":"/backend/logging/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Logging"," (",1,")"]}]}]]}]]}],["$","div","Python",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"Python"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","PEP",{"children":["$","a",null,{"href":"/python/pep/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["PEP"," (",650,")"]}]}]]}]]}],["$","div","AI/ML",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"AI/ML"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","LLM",{"children":["$","a",null,{"href":"/ai/llm/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["LLM"," (",1,")"]}]}],["$","li","Review",{"children":["$","a",null,{"href":"/ai/review/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Review"," (",2626,")"]}]}]]}]]}],["$","div","DevOps",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"DevOps"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Nginx",{"children":["$","a",null,{"href":"/devops/nginx/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Nginx"," (",1,")"]}]}],["$","li","Docker",{"children":["$","a",null,{"href":"/devops/docker/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Docker"," (",1,")"]}]}],["$","li","SafeLine",{"children":["$","a",null,{"href":"/devops/safeline/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["SafeLine"," (",1,")"]}]}],["$","li","Jenkins",{"children":["$","a",null,{"href":"/devops/jenkins/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Jenkins"," (",3,")"]}]}],["$","li","GitHub Actions",{"children":["$","a",null,{"href":"/devops/github-actions/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["GitHub Actions"," (",1,")"]}]}],["$","li","AWS",{"children":["$","a",null,{"href":"/devops/aws/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["AWS"," (",1,")"]}]}]]}]]}],["$","div","etc",{"children":[["$","h4",null,{"className":"font-medium text-gray-900 mb-2","children":"etc"}],["$","ul",null,{"className":"space-y-1 ml-4","children":[["$","li","Me",{"children":["$","a",null,{"href":"/etc/me/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Me"," (",3,")"]}]}],["$","li","Chrome Extension",{"children":["$","a",null,{"href":"/etc/chrome-extension/","className":"text-sm text-gray-600 hover:text-primary-600 block py-1","children":["Chrome Extension"," (",1,")"]}]}]]}]]}]]}]}]}],["$","main",null,{"className":"flex-1","children":["$","article",null,{"className":"page","children":[["$","header",null,{"className":"mb-8","children":[["$","h1",null,{"className":"page__title","children":"[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization"}],["$","div",null,{"className":"page__meta","children":[["$","time",null,{"dateTime":"2025-08-07 13:38:21+0900","children":"2025년 8월 7일"}],["$","span",null,{"className":"ml-4","children":["수정: ","2025년 8월 7일"]}]]}]]}],["$","div",null,{"className":"page__content","children":["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]}],["$","footer",null,{"className":"page__meta mt-8","children":["$","div",null,{"className":"page__taxonomy","children":[["$","h4",null,{"className":"text-sm font-medium text-gray-900 mb-2","children":"태그"}],[["$","span","Review",{"className":"page__taxonomy-item","children":["#","Review"]}],["$","span","Large Language Models",{"className":"page__taxonomy-item","children":["#","Large Language Models"]}],["$","span","Reinforcement Learning",{"className":"page__taxonomy-item","children":["#","Reinforcement Learning"]}],["$","span","Capability Collapse",{"className":"page__taxonomy-item","children":["#","Capability Collapse"]}],["$","span","Hybrid Policy Optimization",{"className":"page__taxonomy-item","children":["#","Hybrid Policy Optimization"]}],["$","span","Multiple Importance Sampling",{"className":"page__taxonomy-item","children":["#","Multiple Importance Sampling"]}],["$","span","Exploration",{"className":"page__taxonomy-item","children":["#","Exploration"]}],["$","span","Math Reasoning",{"className":"page__taxonomy-item","children":["#","Math Reasoning"]}],["$","span","Out-of-Distribution",{"className":"page__taxonomy-item","children":["#","Out-of-Distribution"]}]]]}]}],["$","$La",null,{"postPermalink":"/ai/review/2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization/","postId":"2025-8-7-RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization"}],["$","section",null,{"className":"mt-12 border-t border-gray-200 pt-8","children":[["$","h3",null,{"className":"text-base font-semibold text-gray-900 mb-4","children":["Review"," 의 다른글"]}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"className":"text-gray-500","children":["이전글"," ",["$","$L7",null,{"href":"/ai/review/2025-8-7-Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks"}]]}],["$","li",null,{"className":"text-gray-900 font-semibold","children":["현재글 : ","[논문리뷰] RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization"]}],["$","li",null,{"className":"text-gray-500","children":["다음글"," ",["$","$L7",null,{"href":"/ai/review/2025-8-7-Sculptor-Empowering-LLMs-with-Cognitive-Agency-via-Active-Context-Management/","className":"text-gray-500 hover:text-gray-700 underline decoration-dotted underline-offset-4","children":"[논문리뷰] Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management"}]]}]]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"secrett2633's blog"}],["$","meta","3",{"name":"description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","4",{"name":"author","content":"secrett2633"}],["$","meta","5",{"name":"keywords","content":"Django, Python, DevOps, AI, ML, 블로그, 기술"}],["$","meta","6",{"name":"creator","content":"secrett2633"}],["$","meta","7",{"name":"publisher","content":"secrett2633"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","10",{"rel":"canonical","href":"https://blog.secrett2633.cloud/"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"secrett2633's blog"}],["$","meta","13",{"property":"og:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","meta","14",{"property":"og:url","content":"https://blog.secrett2633.cloud/"}],["$","meta","15",{"property":"og:site_name","content":"secrett2633's blog"}],["$","meta","16",{"property":"og:locale","content":"ko_KR"}],["$","meta","17",{"property":"og:type","content":"website"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"secrett2633's blog"}],["$","meta","20",{"name":"twitter:description","content":"기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트"}],["$","link","21",{"rel":"icon","href":"/icon.ico?6d9f34d4948640b8","type":"image/x-icon","sizes":"16x16"}]]
1:null
