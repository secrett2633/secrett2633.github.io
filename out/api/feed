<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Wed, 15 Oct 2025 13:08:18 GMT</lastBuildDate>
    <pubDate>Wed, 15 Oct 2025 13:08:18 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] What If : Understanding Motion Through Sparse Interactions</title>
      <description>이 [arXiv]에 게시한 &#39;What If : Understanding Motion Through Sparse Interactions&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-What_If_Understanding_Motion_Through_Sparse_Interactions/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-What_If_Understanding_Motion_Through_Sparse_Interactions/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Motion Understanding</category><category>Sparse Interactions</category><category>Multimodal Prediction</category><category>Flow Poke Transformer</category><category>Physical Scene Dynamics</category><category>Uncertainty Quantification</category><category>Generative Models</category><category>Computer Vision</category>
    </item>
    <item>
      <title>[논문리뷰] ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</title>
      <description>이 [arXiv]에 게시한 &#39;ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-ViCO_A_Training_Strategy_towards_Semantic_Aware_Dynamic_High-Resolution/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-ViCO_A_Training_Strategy_towards_Semantic_Aware_Dynamic_High-Resolution/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models (MLLMs)</category><category>Dynamic Resolution</category><category>Token Compression</category><category>Semantic Awareness</category><category>Visual Consistency Learning (ViCO)</category><category>Visual Resolution Router (ViR)</category><category>Inference Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] UniFusion: Vision-Language Model as Unified Encoder in Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;UniFusion: Vision-Language Model as Unified Encoder in Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-UniFusion_Vision-Language_Model_as_Unified_Encoder_in_Image_Generation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-UniFusion_Vision-Language_Model_as_Unified_Encoder_in_Image_Generation/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Model</category><category>Unified Encoder</category><category>Image Generation</category><category>Diffusion Models</category><category>Multimodal Learning</category><category>Text-to-Image</category><category>Image Editing</category><category>Zero-shot Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Tensor Logic: The Language of AI</title>
      <description>Pedro Domingos이 [arXiv]에 게시한 &#39;Tensor Logic: The Language of AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Tensor_Logic_The_Language_of_AI/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Tensor_Logic_The_Language_of_AI/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Tensor Logic</category><category>Neurosymbolic AI</category><category>Logic Programming</category><category>Tensor Algebra</category><category>Deep Learning</category><category>Automated Reasoning</category><category>Embedding Space</category>
    </item>
    <item>
      <title>[논문리뷰] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models</title>
      <description>이 [arXiv]에 게시한 &#39;Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Temporal_Alignment_Guidance_On-Manifold_Sampling_in_Diffusion_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Temporal_Alignment_Guidance_On-Manifold_Sampling_in_Diffusion_Models/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Models</category><category>Generative Models</category><category>Guidance</category><category>On-Manifold Sampling</category><category>Temporal Alignment</category><category>Score Approximation Error</category><category>Training-Free Guidance</category>
    </item>
    <item>
      <title>[논문리뷰] SynthID-Image: Image watermarking at internet scale</title>
      <description>이 [arXiv]에 게시한 &#39;SynthID-Image: Image watermarking at internet scale&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-SynthID-Image_Image_watermarking_at_internet_scale/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-SynthID-Image_Image_watermarking_at_internet_scale/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Watermarking</category><category>AI-Generated Content</category><category>Provenance</category><category>Robustness</category><category>Security</category><category>Deep Learning</category><category>Internet Scale</category><category>Post-hoc</category>
    </item>
    <item>
      <title>[논문리뷰] SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models</title>
      <description>이 [arXiv]에 게시한 &#39;SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-SRUM_Fine-Grained_Self-Rewarding_for_Unified_Multimodal_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-SRUM_Fine-Grained_Self-Rewarding_for_Unified_Multimodal_Models/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Unified Multimodal Models</category><category>Self-Rewarding</category><category>Text-to-Image Generation</category><category>Image Understanding</category><category>Post-Training</category><category>Global-Local Reward</category><category>Compositional Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</title>
      <description>이 [arXiv]에 게시한 &#39;Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Spatial_Forcing_Implicit_Spatial_Representation_Alignment_for_Vision-language-action_Model/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Spatial_Forcing_Implicit_Spatial_Representation_Alignment_for_Vision-language-action_Model/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Models</category><category>Spatial Perception</category><category>Implicit Representation Alignment</category><category>3D Foundation Models</category><category>Robotics</category><category>Data Efficiency</category><category>Representation Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Language-Centric Omnimodal Representation Learning</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Language-Centric Omnimodal Representation Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Scaling_Language-Centric_Omnimodal_Representation_Learning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Scaling_Language-Centric_Omnimodal_Representation_Learning/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Embeddings</category><category>MLLMs</category><category>Contrastive Learning</category><category>Cross-modal Alignment</category><category>Generative Pretraining</category><category>Representation Learning</category><category>Scaling Laws</category>
    </item>
    <item>
      <title>[논문리뷰] SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</title>
      <description>이 [arXiv]에 게시한 &#39;SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-SAIL-Embedding_Technical_Report_Omni-modal_Embedding_Foundation_Model/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-SAIL-Embedding_Technical_Report_Omni-modal_Embedding_Foundation_Model/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Omni-modal Embedding</category><category>Multimodal Learning</category><category>Recommendation Systems</category><category>Hard Negative Mining</category><category>Contrastive Learning</category><category>Large Language Models (LLMs)</category><category>Data Balancing</category><category>Multitask Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Robot Learning: A Tutorial</title>
      <description>이 [arXiv]에 게시한 &#39;Robot Learning: A Tutorial&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Robot_Learning_A_Tutorial/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Robot_Learning_A_Tutorial/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Robot Learning</category><category>Reinforcement Learning</category><category>Imitation Learning</category><category>Behavioral Cloning</category><category>Vision-Language-Action Models</category><category>Diffusion Models</category><category>Transformers</category><category>LeRobot</category>
    </item>
    <item>
      <title>[논문리뷰] ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability</title>
      <description>Tsui-Wei Weng이 [arXiv]에 게시한 &#39;ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-ReFIne_A_Framework_for_Trustworthy_Large_Reasoning_Models_with_Reliability_Faithfulness_and_Interpretability/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-ReFIne_A_Framework_for_Trustworthy_Large_Reasoning_Models_with_Reliability_Faithfulness_and_Interpretability/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Trustworthy AI</category><category>Large Reasoning Models (LRMs)</category><category>Interpretability</category><category>Faithfulness</category><category>Reliability</category><category>Chain-of-Thought (CoT)</category><category>Supervised Fine-tuning (SFT)</category><category>GRPO</category>
    </item>
    <item>
      <title>[논문리뷰] One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration</title>
      <description>Mohit Bansal이 [arXiv]에 게시한 &#39;One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-One_Life_to_Learn_Inferring_Symbolic_World_Models_for_Stochastic_Environments_from_Unguided_Exploration/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-One_Life_to_Learn_Inferring_Symbolic_World_Models_for_Stochastic_Environments_from_Unguided_Exploration/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Symbolic World Models</category><category>Stochastic Environments</category><category>Unguided Exploration</category><category>Probabilistic Programming</category><category>Law Synthesis</category><category>Crafter-OO</category><category>Program Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces</title>
      <description>Sungchul Kim이 [arXiv]에 게시한 &#39;MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-MLLM_as_a_UI_Judge_Benchmarking_Multimodal_LLMs_for_Predicting_Human_Perception_of_User_Interfaces/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-MLLM_as_a_UI_Judge_Benchmarking_Multimodal_LLMs_for_Predicting_Human_Perception_of_User_Interfaces/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>UI Evaluation</category><category>Human Perception</category><category>Benchmarking</category><category>UX Research</category><category>MLLM-as-a-Judge</category><category>Cognitive Factors</category><category>Pairwise Comparison</category>
    </item>
    <item>
      <title>[논문리뷰] Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks</title>
      <description>Xueyuan Lin이 [arXiv]에 게시한 &#39;Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Memory_as_Action_Autonomous_Context_Curation_for_Long-Horizon_Agentic_Tasks/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Memory_as_Action_Autonomous_Context_Curation_for_Long-Horizon_Agentic_Tasks/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long-Horizon Tasks</category><category>Agentic AI</category><category>Context Curation</category><category>Working Memory</category><category>Reinforcement Learning</category><category>Policy Optimization</category><category>Large Language Models</category><category>Memory-as-Action</category>
    </item>
    <item>
      <title>[논문리뷰] LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens</title>
      <description>이 [arXiv]에 게시한 &#39;LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-LLM_Reasoning_for_Machine_Translation_Synthetic_Data_Generation_over_Thinking_Tokens/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-LLM_Reasoning_for_Machine_Translation_Synthetic_Data_Generation_over_Thinking_Tokens/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Language Models (LLMs)</category><category>Machine Translation (MT)</category><category>Chain-of-Thought (CoT)</category><category>Knowledge Distillation</category><category>Fine-tuning</category><category>Prompt Engineering</category><category>Synthetic Data</category>
    </item>
    <item>
      <title>[논문리뷰] Information-Preserving Reformulation of Reasoning Traces for Antidistillation</title>
      <description>이 [arXiv]에 게시한 &#39;Information-Preserving Reformulation of Reasoning Traces for Antidistillation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-Information-Preserving_Reformulation_of_Reasoning_Traces_for_Antidistillation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-Information-Preserving_Reformulation_of_Reasoning_Traces_for_Antidistillation/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Antidistillation</category><category>Reasoning Traces</category><category>Large Language Models</category><category>Knowledge Distillation</category><category>Information Preservation</category><category>Trace Reformulation</category><category>Supervised Fine-Tuning</category>
    </item>
    <item>
      <title>[논문리뷰] HoneyBee: Data Recipes for Vision-Language Reasoners</title>
      <description>이 [arXiv]에 게시한 &#39;HoneyBee: Data Recipes for Vision-Language Reasoners&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-HoneyBee_Data_Recipes_for_Vision-Language_Reasoners/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-HoneyBee_Data_Recipes_for_Vision-Language_Reasoners/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Data Curation</category><category>Chain-of-Thought</category><category>VL Reasoning</category><category>Dataset Scaling</category><category>Supervised Finetuning</category><category>HONEYBEE</category><category>Test-Time Scaling</category>
    </item>
    <item>
      <title>[논문리뷰] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</title>
      <description>Yihao Liu이 [arXiv]에 게시한 &#39;FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-FlashVSR_Towards_Real-Time_Diffusion-Based_Streaming_Video_Super-Resolution/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-FlashVSR_Towards_Real-Time_Diffusion-Based_Streaming_Video_Super-Resolution/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Super-Resolution (VSR)</category><category>Diffusion Models</category><category>Real-time VSR</category><category>Streaming VSR</category><category>Sparse Attention</category><category>Distillation</category><category>Conditional Decoder</category><category>High-resolution</category>
    </item>
    <item>
      <title>[논문리뷰] ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-10-15-ExpVid_A_Benchmark_for_Experiment_Video_Understanding_Reasoning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-10-15-ExpVid_A_Benchmark_for_Experiment_Video_Understanding_Reasoning/</guid>
      <pubDate>Wed, 15 Oct 2025 04:01:40 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Experiment Video Understanding</category><category>Multimodal Large Language Models (MLLMs)</category><category>Scientific Reasoning</category><category>Benchmark</category><category>Wet-Lab Experiments</category><category>Procedural Understanding</category><category>Fine-grained Perception</category><category>Video QA</category>
    </item>
    
  </channel>
</rss>