<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Sun, 09 Nov 2025 15:13:23 GMT</lastBuildDate>
    <pubDate>Sun, 09 Nov 2025 15:13:23 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] V-Thinker: Interactive Thinking with Images</title>
      <description>Peiqing Yang이 [arXiv]에 게시한 &#39;V-Thinker: Interactive Thinking with Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-V-Thinker_Interactive_Thinking_with_Images/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-V-Thinker_Interactive_Thinking_with_Images/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Multimodal Models</category><category>Interactive Reasoning</category><category>Vision-Centric Thinking</category><category>Reinforcement Learning</category><category>Data Synthesis</category><category>Visual Tools</category><category>Curriculum Learning</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</title>
      <description>이 [arXiv]에 게시한 &#39;Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Thinking_with_Video_Video_Generation_as_a_Promising_Multimodal_Reasoning_Paradigm/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Thinking_with_Video_Video_Generation_as_a_Promising_Multimodal_Reasoning_Paradigm/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Multimodal Reasoning</category><category>Temporal Understanding</category><category>Spatial Reasoning</category><category>Foundation Models</category><category>AI Benchmarking</category><category>In-Context Learning</category><category>Self-Consistency</category>
    </item>
    <item>
      <title>[논문리뷰] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms</title>
      <description>Susumu Takeuchi이 [arXiv]에 게시한 &#39;The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-The_Strong_Lottery_Ticket_Hypothesis_for_Multi-Head_Attention_Mechanisms/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-The_Strong_Lottery_Ticket_Hypothesis_for_Multi-Head_Attention_Mechanisms/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Strong Lottery Ticket Hypothesis</category><category>Multi-Head Attention</category><category>Transformers</category><category>Neural Network Pruning</category><category>Overparameterization</category><category>Weight Initialization</category><category>Model Compression</category>
    </item>
    <item>
      <title>[논문리뷰] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-SIMS-V_Simulated_Instruction-Tuning_for_Spatial_Video_Understanding/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-SIMS-V_Simulated_Instruction-Tuning_for_Spatial_Video_Understanding/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Reasoning</category><category>Video Understanding</category><category>Simulated Data</category><category>Instruction Tuning</category><category>Multimodal LLMs</category><category>Sim-to-Real Transfer</category><category>AI2-THOR</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Agent Learning via Experience Synthesis</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Agent Learning via Experience Synthesis&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Scaling_Agent_Learning_via_Experience_Synthesis/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Scaling_Agent_Learning_via_Experience_Synthesis/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>LLM Agents</category><category>Experience Synthesis</category><category>World Models</category><category>Curriculum Learning</category><category>Sim-to-Real Transfer</category><category>Web Agents</category>
    </item>
    <item>
      <title>[논문리뷰] SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning</title>
      <description>이 [arXiv]에 게시한 &#39;SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-SAIL-RL_Guiding_MLLMs_in_When_and_How_to_Think_via_Dual-Reward_RL_Tuning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-SAIL-RL_Guiding_MLLMs_in_When_and_How_to_Think_via_Dual-Reward_RL_Tuning/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models</category><category>Reinforcement Learning</category><category>Post-training</category><category>Reasoning</category><category>Dual-Reward System</category><category>Thinking Reward</category><category>Judging Reward</category><category>Hallucination Reduction</category>
    </item>
    <item>
      <title>[논문리뷰] RDMA Point-to-Point Communication for LLM Systems</title>
      <description>이 [arXiv]에 게시한 &#39;RDMA Point-to-Point Communication for LLM Systems&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-RDMA_Point-to-Point_Communication_for_LLM_Systems/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-RDMA_Point-to-Point_Communication_for_LLM_Systems/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RDMA</category><category>LLM</category><category>Point-to-Point Communication</category><category>Disaggregated Inference</category><category>MoE Routing</category><category>KvCache</category><category>AWS EFA</category><category>NVIDIA ConnectX</category>
    </item>
    <item>
      <title>[논문리뷰] NVIDIA Nemotron Nano V2 VL</title>
      <description>이 [arXiv]에 게시한 &#39;NVIDIA Nemotron Nano V2 VL&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-NVIDIA_Nemotron_Nano_V2_VL/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-NVIDIA_Nemotron_Nano_V2_VL/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Model</category><category>Hybrid Architecture</category><category>Mamba-Transformer</category><category>Long-Context Understanding</category><category>Quantization</category><category>Efficient Inference</category><category>Document AI</category><category>Video AI</category>
    </item>
    <item>
      <title>[논문리뷰] Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</title>
      <description>이 [arXiv]에 게시한 &#39;Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Learning_Vision-Driven_Reactive_Soccer_Skills_for_Humanoid_Robots/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Learning_Vision-Driven_Reactive_Soccer_Skills_for_Humanoid_Robots/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Humanoid Robot</category><category>Reinforcement Learning</category><category>RoboCup</category><category>Soccer Skills</category><category>Vision-Driven Control</category><category>Adversarial Motion Priors</category><category>Sim-to-Real</category><category>Perception-Action Coordination</category>
    </item>
    <item>
      <title>[논문리뷰] How to Evaluate Speech Translation with Source-Aware Neural MT Metrics</title>
      <description>Luisa Bentivogli이 [arXiv]에 게시한 &#39;How to Evaluate Speech Translation with Source-Aware Neural MT Metrics&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-How_to_Evaluate_Speech_Translation_with_Source-Aware_Neural_MT_Metrics/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-How_to_Evaluate_Speech_Translation_with_Source-Aware_Neural_MT_Metrics/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Speech Translation</category><category>Neural MT Metrics</category><category>Source-Aware Evaluation</category><category>Automatic Speech Recognition (ASR)</category><category>Back-Translation (BT)</category><category>Cross-lingual Re-segmentation</category><category>COMET</category><category>MetricX</category>
    </item>
    <item>
      <title>[논문리뷰] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</title>
      <description>이 [arXiv]에 게시한 &#39;GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-GUI-360_A_Comprehensive_Dataset_and_Benchmark_for_Computer-Using_Agents/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-GUI-360_A_Comprehensive_Dataset_and_Benchmark_for_Computer-Using_Agents/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Computer-Using Agents</category><category>GUI Grounding</category><category>Screen Parsing</category><category>Action Prediction</category><category>Desktop Automation</category><category>Dataset</category><category>Benchmark</category><category>Multimodal Learning</category><category>LLM-augmented Data</category>
    </item>
    <item>
      <title>[논문리뷰] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</title>
      <description>이 [arXiv]에 게시한 &#39;EVTAR: End-to-End Try on with Additional Unpaired Visual Reference&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-EVTAR_End-to-End_Try_on_with_Additional_Unpaired_Visual_Reference/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-EVTAR_End-to-End_Try_on_with_Additional_Unpaired_Visual_Reference/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Virtual Try-on</category><category>Diffusion Models</category><category>End-to-End Learning</category><category>Reference Images</category><category>Unpaired Data</category><category>Flow Matching</category><category>Transformer Architecture</category><category>Generative AI</category>
    </item>
    <item>
      <title>[논문리뷰] Contamination Detection for VLMs using Multi-Modal Semantic Perturbation</title>
      <description>이 [arXiv]에 게시한 &#39;Contamination Detection for VLMs using Multi-Modal Semantic Perturbation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Contamination_Detection_for_VLMs_using_Multi-Modal_Semantic_Perturbation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Contamination_Detection_for_VLMs_using_Multi-Modal_Semantic_Perturbation/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>VLM Contamination</category><category>Test-set Leakage</category><category>Multi-modal Perturbation</category><category>Generative Models</category><category>Generalization</category><category>Model Memorization</category><category>VLMs</category>
    </item>
    <item>
      <title>[논문리뷰] Cambrian-S: Towards Spatial Supersensing in Video</title>
      <description>Zihao Yang이 [arXiv]에 게시한 &#39;Cambrian-S: Towards Spatial Supersensing in Video&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Cambrian-S_Towards_Spatial_Supersensing_in_Video/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Cambrian-S_Towards_Spatial_Supersensing_in_Video/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Supersensing</category><category>Video Understanding</category><category>Multimodal LLMs</category><category>Predictive Sensing</category><category>Memory Management</category><category>Event Segmentation</category><category>VSI-SUPER</category><category>Instruction Tuning</category>
    </item>
    <item>
      <title>[논문리뷰] Benchmark Designers Should &#39;Train on the Test Set&#39; to Expose Exploitable Non-Visual Shortcuts</title>
      <description>이 [arXiv]에 게시한 &#39;Benchmark Designers Should &#39;Train on the Test Set&#39; to Expose Exploitable Non-Visual Shortcuts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-7-Benchmark_Designers_Should_Train_on_the_Test_Set_to_Expose_Exploitable_Non-Visual_Shortcuts/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-7-Benchmark_Designers_Should_Train_on_the_Test_Set_to_Expose_Exploitable_Non-Visual_Shortcuts/</guid>
      <pubDate>Sun, 09 Nov 2025 13:08:24 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>Benchmark Design</category><category>Non-Visual Shortcuts</category><category>Test-Set Stress-Test</category><category>Bias Mitigation</category><category>Model Evaluation</category><category>Benchmark Robustness</category>
    </item>
    <item>
      <title>[논문리뷰] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</title>
      <description>이 [arXiv]에 게시한 &#39;UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-UniAVGen_Unified_Audio_and_Video_Generation_with_Asymmetric_Cross-Modal_Interactions/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-UniAVGen_Unified_Audio_and_Video_Generation_with_Asymmetric_Cross-Modal_Interactions/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Joint Audio-Video Generation</category><category>Cross-Modal Interaction</category><category>Diffusion Transformer</category><category>Face-Aware Modulation</category><category>Classifier-Free Guidance</category><category>Multimodal AI</category><category>Generative Models</category>
    </item>
    <item>
      <title>[논문리뷰] The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute</title>
      <description>이 [arXiv]에 게시한 &#39;The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-The_Sequential_Edge_Inverse-Entropy_Voting_Beats_Parallel_Self-Consistency_at_Matched_Compute/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-The_Sequential_Edge_Inverse-Entropy_Voting_Beats_Parallel_Self-Consistency_at_Matched_Compute/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Sequential Reasoning</category><category>Parallel Self-Consistency</category><category>Inverse-Entropy Voting</category><category>LLM Reasoning</category><category>Test-Time Scaling</category><category>Inference Optimization</category><category>Iterative Refinement</category><category>Error Correction</category>
    </item>
    <item>
      <title>[논문리뷰] TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-TabTune_A_Unified_Library_for_Inference_and_Fine-Tuning_Tabular_Foundation_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-TabTune_A_Unified_Library_for_Inference_and_Fine-Tuning_Tabular_Foundation_Models/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Tabular Foundation Models</category><category>Fine-Tuning</category><category>PEFT</category><category>Meta-Learning</category><category>Calibration</category><category>Fairness</category><category>Unified Library</category><category>Benchmarking</category>
    </item>
    <item>
      <title>[논문리뷰] Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</title>
      <description>이 [arXiv]에 게시한 &#39;Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-Orion-MSP_Multi-Scale_Sparse_Attention_for_Tabular_In-Context_Learning/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-Orion-MSP_Multi-Scale_Sparse_Attention_for_Tabular_In-Context_Learning/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Tabular Data</category><category>In-Context Learning</category><category>Multi-Scale Attention</category><category>Sparse Attention</category><category>Foundation Models</category><category>Perceiver Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</title>
      <description>이 [arXiv]에 게시한 &#39;MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-6-MME-CC_A_Challenging_Multi-Modal_Evaluation_Benchmark_of_Cognitive_Capacity/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-6-MME-CC_A_Challenging_Multi-Modal_Evaluation_Benchmark_of_Cognitive_Capacity/</guid>
      <pubDate>Sun, 09 Nov 2025 12:54:30 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>Benchmark</category><category>Cognitive Capacity</category><category>Visual Reasoning</category><category>MLLM Evaluation</category><category>Error Analysis</category><category>Chain-of-Thought</category>
    </item>
    
  </channel>
</rss>