<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Thu, 27 Nov 2025 05:42:16 GMT</lastBuildDate>
    <pubDate>Thu, 27 Nov 2025 05:42:16 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-iMontage-Unified-Versatile-Highly-Dynamic-Many-to-many-Image-Generation/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Generation</category><category>Video Models</category><category>Diffusion Models</category><category>Many-to-many</category><category>Unified Framework</category><category>Temporal Consistency</category><category>Image Editing</category><category>Positional Embedding</category>
    </item>
    <item>
      <title>[논문리뷰] Yo&#39;City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion</title>
      <description>Zhifei Yang이 [arXiv]에 게시한 &#39;Yo&#39;City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-YoCity-Personalized-and-Boundless-3D-Realistic-City-Scene-Generation-via-Self-Critic-Expansion/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D City Generation</category><category>Generative AI</category><category>Large Language Models</category><category>Vision-Language Models</category><category>Multi-Agent Framework</category><category>Self-Critic Learning</category><category>Scene Graph</category><category>Text-to-3D</category>
    </item>
    <item>
      <title>[논문리뷰] VQ-VA World: Towards High-Quality Visual Question-Visual Answering</title>
      <description>Feng Li이 [arXiv]에 게시한 &#39;VQ-VA World: Towards High-Quality Visual Question-Visual Answering&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-VQ-VA-World-Towards-High-Quality-Visual-Question-Visual-Answering/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Question Answering (VQA)</category><category>Image Generation</category><category>Data-centric AI</category><category>Agentic Pipeline</category><category>Multimodal Models</category><category>Web-scale Data</category><category>Benchmark</category><category>LightFusion</category>
    </item>
    <item>
      <title>[논문리뷰] Unified all-atom molecule generation with neural fields</title>
      <description>이 [arXiv]에 게시한 &#39;Unified all-atom molecule generation with neural fields&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Unified-all-atom-molecule-generation-with-neural-fields/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Molecule Generation</category><category>Neural Fields</category><category>Score-based Generative Models</category><category>Drug Design</category><category>Modality-agnostic</category><category>Antibody Design</category><category>Macrocyclic Peptides</category><category>All-atom</category>
    </item>
    <item>
      <title>[논문리뷰] UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers</title>
      <description>이 [arXiv]에 게시한 &#39;UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-UltraViCo-Breaking-Extrapolation-Limits-in-Video-Diffusion-Transformers/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Diffusion Transformers</category><category>Length Extrapolation</category><category>Attention Mechanism</category><category>Attention Dispersion</category><category>Periodic Content Repetition</category><category>Quality Degradation</category><category>Training-free Method</category><category>Plug-and-play</category>
    </item>
    <item>
      <title>[논문리뷰] Soft Adaptive Policy Optimization</title>
      <description>이 [arXiv]에 게시한 &#39;Soft Adaptive Policy Optimization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Soft-Adaptive-Policy-Optimization/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Large Language Models</category><category>Policy Optimization</category><category>Importance Ratios</category><category>Soft Clipping</category><category>Trust Region</category><category>Mixture-of-Experts</category><category>Asymmetric Temperature</category>
    </item>
    <item>
      <title>[논문리뷰] SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System</title>
      <description>이 [arXiv]에 게시한 &#39;SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-SciEducator-Scientific-Video-Understanding-and-Educating-via-Deming-Cycle-Multi-Agent-System/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-Agent System</category><category>Video Understanding</category><category>Scientific Education</category><category>Deming Cycle</category><category>Large Language Models</category><category>Iterative Optimization</category><category>Knowledge Integration</category><category>Educational Content Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Scaling-Agentic-Reinforcement-Learning-for-Tool-Integrated-Reasoning-in-VLMs/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models (VLMs)</category><category>Reinforcement Learning (RL)</category><category>Tool-Integrated Reasoning (TIR)</category><category>Agentic AI</category><category>VQA</category><category>Training Environment</category><category>Behavioral Cloning</category><category>Policy Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space</title>
      <description>Yulan He이 [arXiv]에 게시한 &#39;SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-SSA-Sparse-Sparse-Attention-by-Aligning-Full-and-Sparse-Attention-Outputs-in-Feature-Space/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Sparse Attention</category><category>Full Attention</category><category>Large Language Models (LLMs)</category><category>Context Length</category><category>Attention Sparsity</category><category>Alignment Loss</category><category>Long-Context Extrapolation</category>
    </item>
    <item>
      <title>[논문리뷰] ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding</title>
      <description>이 [arXiv]에 게시한 &#39;ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-ReDirector-Creating-Any-Length-Video-Retakes-with-Rotary-Camera-Encoding/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Retake Generation</category><category>Camera Control</category><category>Rotary Position Embedding (RoPE)</category><category>Rotary Camera Encoding (RoCE)</category><category>Geometric Consistency</category><category>Video Generative Models</category><category>Transformer Architecture</category><category>Multi-view Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding</title>
      <description>Hongzhi Zhang이 [arXiv]에 게시한 &#39;PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-PhysChoreo-Physics-Controllable-Video-Generation-with-Part-Aware-Semantic-Grounding/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Physics Simulation</category><category>Controllable AI</category><category>Part-Aware</category><category>Semantic Grounding</category><category>Material Properties</category><category>Image-to-Video</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation</title>
      <description>이 [arXiv]에 게시한 &#39;OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-OmniAlpha-A-Sequence-to-Sequence-Framework-for-Unified-Multi-Task-RGBA-Generation/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RGBA Generation</category><category>Multi-Task Learning</category><category>Diffusion Transformers</category><category>Image Matting</category><category>Layer Decomposition</category><category>Object Removal</category><category>Alpha-aware VAE</category><category>MSROPE-BiL</category>
    </item>
    <item>
      <title>[논문리뷰] MedSAM3: Delving into Segment Anything with Medical Concepts</title>
      <description>Yi Lu이 [arXiv]에 게시한 &#39;MedSAM3: Delving into Segment Anything with Medical Concepts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-MedSAM3-Delving-into-Segment-Anything-with-Medical-Concepts/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Medical Image Segmentation</category><category>Segment Anything Model (SAM)</category><category>Promptable Concept Segmentation (PCS)</category><category>Multimodal Large Language Models (MLLMs)</category><category>Agentic AI</category><category>Domain Adaptation</category><category>Text-guided Segmentation</category>
    </item>
    <item>
      <title>[논문리뷰] MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</title>
      <description>이 [arXiv]에 게시한 &#39;MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-MajutsuCity-Language-driven-Aesthetic-adaptive-City-Generation-with-Controllable-3D-Assets-and-Layouts/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D City Generation</category><category>Natural Language Processing</category><category>Aesthetic Adaptation</category><category>Controllable Assets</category><category>Layout Generation</category><category>Interactive Editing</category><category>Diffusion Models</category><category>Multimodal Dataset</category>
    </item>
    <item>
      <title>[논문리뷰] HunyuanOCR Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;HunyuanOCR Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-HunyuanOCR-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-HunyuanOCR-Technical-Report/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Optical Character Recognition</category><category>Multimodal Large Language Model</category><category>End-to-End Learning</category><category>Reinforcement Learning</category><category>Document Parsing</category><category>Information Extraction</category><category>Text Spotting</category>
    </item>
    <item>
      <title>[논문리뷰] GigaWorld-0: World Models as Data Engine to Empower Embodied AI</title>
      <description>Chaojun Ni이 [arXiv]에 게시한 &#39;GigaWorld-0: World Models as Data Engine to Empower Embodied AI&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-GigaWorld-0-World-Models-as-Data-Engine-to-Empower-Embodied-AI/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Embodied AI</category><category>Data Generation</category><category>Video Generation</category><category>3D Scene Reconstruction</category><category>Robotics</category><category>Vision-Language-Action</category>
    </item>
    <item>
      <title>[논문리뷰] GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</title>
      <description>이 [arXiv]에 게시한 &#39;GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-GigaEvo-An-Open-Source-Optimization-Framework-Powered-By-LLMs-And-Evolution-Algorithms/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM-driven Evolutionary Computation</category><category>Quality-Diversity</category><category>MAP-Elites</category><category>Program Synthesis</category><category>Open-source Framework</category><category>Algorithmic Discovery</category><category>Genetic Algorithms</category>
    </item>
    <item>
      <title>[논문리뷰] Fara-7B: An Efficient Agentic Model for Computer Use</title>
      <description>이 [arXiv]에 게시한 &#39;Fara-7B: An Efficient Agentic Model for Computer Use&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Fara-7B-An-Efficient-Agentic-Model-for-Computer-Use/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Computer Use Agents</category><category>Synthetic Data Generation</category><category>Multi-modal LLM</category><category>On-device AI</category><category>Web Automation</category><category>Pixel-in Action-out</category><category>Fara-7B</category><category>WebTailBench</category>
    </item>
    <item>
      <title>[논문리뷰] Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward</title>
      <description>이 [arXiv]에 게시한 &#39;Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-Does-Understanding-Inform-Generation-in-Unified-Multimodal-Models-From-Analysis-to-Path-Forward/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Unified Multimodal Models</category><category>Understanding-Generation Gap</category><category>Reasoning</category><category>Knowledge Transfer</category><category>Chain-of-Thought</category><category>Self-Training</category><category>Synthetic Data</category><category>Evaluation Framework</category>
    </item>
    <item>
      <title>[논문리뷰] DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection</title>
      <description>Mike Zheng Shou이 [arXiv]에 게시한 &#39;DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-26-DiffSeg30k-A-Multi-Turn-Diffusion-Editing-Benchmark-for-Localized-AIGC-Detection/</guid>
      <pubDate>Tue, 25 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AIGC Detection</category><category>Diffusion Models</category><category>Image Editing</category><category>Semantic Segmentation</category><category>Localization</category><category>Model Attribution</category><category>Benchmark</category><category>Multi-turn Editing</category>
    </item>
    
  </channel>
</rss>