<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Tue, 09 Dec 2025 18:33:03 GMT</lastBuildDate>
    <pubDate>Tue, 09 Dec 2025 18:33:03 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] World Models That Know When They Don&#39;t Know: Controllable Video Generation with Calibrated Uncertainty</title>
      <description>Anirudha Majumdar이 [arXiv]에 게시한 &#39;World Models That Know When They Don&#39;t Know: Controllable Video Generation with Calibrated Uncertainty&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-World-Models-That-Know-When-They-Dont-Know-Controllable-Video-Generation-with-Calibrated-Uncertainty/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Controllable Video Generation</category><category>Uncertainty Quantification</category><category>Video Models</category><category>Calibration</category><category>Out-of-Distribution Detection</category><category>Proper Scoring Rules</category><category>Latent Space</category>
    </item>
    <item>
      <title>[논문리뷰] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</title>
      <description>이 [arXiv]에 게시한 &#39;TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-TwinFlow-Realizing-One-step-Generation-on-Large-Models-with-Self-adversarial-Flows/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative Models</category><category>One-step Generation</category><category>Self-Adversarial Learning</category><category>Flow Matching</category><category>Large Language Models</category><category>Text-to-Image</category><category>Efficient Inference</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation</title>
      <description>Salih Tileylioglu이 [arXiv]에 게시한 &#39;TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-TimesNet-Gen-Deep-Learning-based-Site-Specific-Strong-Motion-Generation/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Strong Motion Generation</category><category>Deep Learning</category><category>TimesNet</category><category>Conditional Generation</category><category>Site Effects</category><category>Seismology</category><category>HVSR</category><category>Time Series</category>
    </item>
    <item>
      <title>[논문리뷰] SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling</title>
      <description>Marc Pollefeys이 [arXiv]에 게시한 &#39;SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-SpaceControl-Introducing-Test-Time-Spatial-Control-to-3D-Generative-Modeling/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Generative Models</category><category>Spatial Control</category><category>Test-Time Guidance</category><category>Rectified Flow</category><category>Superquadrics</category><category>Training-Free</category><category>Trellis</category>
    </item>
    <item>
      <title>[논문리뷰] Self-Improving VLM Judges Without Human Annotations</title>
      <description>이 [arXiv]에 게시한 &#39;Self-Improving VLM Judges Without Human Annotations&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-Self-Improving-VLM-Judges-Without-Human-Annotations/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Self-Improvement</category><category>Judge Models</category><category>Synthetic Data Generation</category><category>Iterative Refinement</category><category>Reward Modeling</category><category>Human-free Alignment</category>
    </item>
    <item>
      <title>[논문리뷰] SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs</title>
      <description>Minghui Yu이 [arXiv]에 게시한 &#39;SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-SQ-format-A-Unified-Sparse-Quantized-Hardware-friendly-Data-Format-for-LLMs/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Quantization</category><category>Sparsification</category><category>Hardware Acceleration</category><category>Mixed-Precision</category><category>Post-Training Quantization</category><category>Data Format</category><category>GPU Optimization</category><category>AI Accelerator</category>
    </item>
    <item>
      <title>[논문리뷰] SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</title>
      <description>이 [arXiv]에 게시한 &#39;SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-SCAIL-Towards-Studio-Grade-Character-Animation-via-In-Context-Learning-of-3D-Consistent-Pose-Representations/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Character Animation</category><category>3D Pose Representation</category><category>In-Context Learning</category><category>Diffusion Transformer</category><category>Studio-Grade Animation</category><category>Spatio-Temporal Reasoning</category><category>Video Generation</category>
    </item>
    <item>
      <title>[논문리뷰] RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards</title>
      <description>Zilong Huang이 [arXiv]에 게시한 &#39;RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-RealGen-Photorealistic-Text-to-Image-Generation-via-Detector-Guided-Rewards/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image Generation</category><category>Photorealism</category><category>Reinforcement Learning</category><category>Diffusion Models</category><category>Adversarial Learning</category><category>Detector-Guided Rewards</category><category>LLM Prompt Optimization</category><category>Image Quality Assessment</category>
    </item>
    <item>
      <title>[논문리뷰] ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning</title>
      <description>Shengju Qian이 [arXiv]에 게시한 &#39;ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-ReVSeg-Incentivizing-the-Reasoning-Chain-for-Video-Segmentation-with-Reinforcement-Learning/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Object Segmentation</category><category>Reinforcement Learning</category><category>Vision-Language Models</category><category>Reasoning Chain</category><category>Explainable AI</category><category>Multi-step Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] ProPhy: Progressive Physical Alignment for Dynamic World Simulation</title>
      <description>Yuhao Cheng이 [arXiv]에 게시한 &#39;ProPhy: Progressive Physical Alignment for Dynamic World Simulation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-ProPhy-Progressive-Physical-Alignment-for-Dynamic-World-Simulation/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Physics-aware</category><category>World Simulation</category><category>Progressive Alignment</category><category>Mixture-of-Experts</category><category>Vision-Language Models</category><category>Token-level Routing</category>
    </item>
    <item>
      <title>[논문리뷰] Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image</title>
      <description>이 [arXiv]에 게시한 &#39;Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-Joint-3D-Geometry-Reconstruction-and-Motion-Generation-for-4D-Synthesis-from-a-Single-Image/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>4D Synthesis</category><category>3D Reconstruction</category><category>Motion Generation</category><category>Single Image</category><category>Diffusion Model</category><category>Point Cloud</category><category>Dataset Curation</category><category>View Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks</title>
      <description>Yang Li이 [arXiv]에 게시한 &#39;From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-From-Imitation-to-Discrimination-Toward-A-Generalized-Curriculum-Advantage-Mechanism-Enhancing-Cross-Domain-Reasoning-Tasks/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Large Language Models</category><category>Curriculum Learning</category><category>Advantage Function</category><category>Reasoning Tasks</category><category>Multimodal AI</category><category>Policy Optimization</category><category>Generalization</category>
    </item>
    <item>
      <title>[논문리뷰] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning</title>
      <description>Zijia Lin이 [arXiv]에 게시한 &#39;Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-Entropy-Ratio-Clipping-as-a-Soft-Global-Constraint-for-Stable-Reinforcement-Learning/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Policy Optimization</category><category>Trust Region</category><category>Entropy Clipping</category><category>Large Language Models</category><category>Training Stability</category><category>Distributional Shift</category>
    </item>
    <item>
      <title>[논문리뷰] EditThinker: Unlocking Iterative Reasoning for Any Image Editor</title>
      <description>Ziyu Guo이 [arXiv]에 게시한 &#39;EditThinker: Unlocking Iterative Reasoning for Any Image Editor&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-EditThinker-Unlocking-Iterative-Reasoning-for-Any-Image-Editor/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Editing</category><category>Iterative Reasoning</category><category>Multimodal Large Language Model (MLLM)</category><category>Reinforcement Learning (RL)</category><category>Instruction Following</category><category>Critique-Refine-Repeat Cycle</category><category>Think-while-Edit</category>
    </item>
    <item>
      <title>[논문리뷰] COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</title>
      <description>Jiawei Sheng이 [arXiv]에 게시한 &#39;COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-COOPER-A-Unified-Model-for-Cooperative-Perception-and-Reasoning-in-Spatial-Intelligence/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Large Language Models (MLLMs)</category><category>Spatial Reasoning</category><category>Perception Enhancement</category><category>Auxiliary Modalities</category><category>Adaptive Interleaved Reasoning</category><category>Reinforcement Learning</category><category>Chain-of-Thought</category>
    </item>
    <item>
      <title>[논문리뷰] AI &amp; Human Co-Improvement for Safer Co-Superintelligence</title>
      <description>이 [arXiv]에 게시한 &#39;AI &amp; Human Co-Improvement for Safer Co-Superintelligence&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-08-AI-Human-Co-Improvement-for-Safer-Co-Superintelligence/</guid>
      <pubDate>Mon, 08 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Safety</category><category>Superintelligence</category><category>Human-AI Collaboration</category><category>Self-Improving AI</category><category>Co-Improvement</category><category>Alignment</category><category>AI Research Agents</category>
    </item>
    <item>
      <title>[논문리뷰] UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers</title>
      <description>이 [arXiv]에 게시한 &#39;UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-05-UltraImage-Rethinking-Resolution-Extrapolation-in-Image-Diffusion-Transformers/</guid>
      <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Transformers</category><category>Resolution Extrapolation</category><category>Positional Encoding</category><category>Frequency Analysis</category><category>Adaptive Attention</category><category>High-Resolution Image Generation</category><category>Image Quality</category><category>Content Repetition</category>
    </item>
    <item>
      <title>[논문리뷰] TV2TV: A Unified Framework for Interleaved Language and Video Generation</title>
      <description>이 [arXiv]에 게시한 &#39;TV2TV: A Unified Framework for Interleaved Language and Video Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-05-TV2TV-A-Unified-Framework-for-Interleaved-Language-and-Video-Generation/</guid>
      <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Language Modeling</category><category>Multimodal AI</category><category>Interleaved Generation</category><category>Flow Matching</category><category>Transformer</category><category>Controllability</category><category>World Models</category>
    </item>
    <item>
      <title>[논문리뷰] Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</title>
      <description>Yu-Lun Liu이 [arXiv]에 게시한 &#39;Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-05-Splannequin-Freezing-Monocular-Mannequin-Challenge-Footage-with-Dual-Detection-Splatting/</guid>
      <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Monocular 3D Reconstruction</category><category>Mannequin Challenge</category><category>Dynamic Gaussian Splatting</category><category>Freeze-Time Video</category><category>Temporal Consistency</category><category>Artifact Suppression</category><category>Regularization</category>
    </item>
    <item>
      <title>[논문리뷰] SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-05-SignRoundV2-Closing-the-Performance-Gap-in-Extremely-Low-Bit-Post-Training-Quantization-for-LLMs/</guid>
      <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Post-Training Quantization (PTQ)</category><category>Large Language Models (LLMs)</category><category>Low-Bit Quantization</category><category>Mixed-Precision Quantization</category><category>Sensitivity Metric</category><category>Quantization Scale Initialization</category><category>Accuracy Preservation</category>
    </item>
    
  </channel>
</rss>