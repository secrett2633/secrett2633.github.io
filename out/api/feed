<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.cloud</link>
    <atom:link href="https://blog.secrett2633.cloud/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Thu, 29 Jan 2026 14:54:58 GMT</lastBuildDate>
    <pubDate>Thu, 29 Jan 2026 14:54:58 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] World Craft: Agentic Framework to Create Visualizable Worlds via Text</title>
      <description>이 [arXiv]에 게시한 &#39;World Craft: Agentic Framework to Create Visualizable Worlds via Text&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-World-Craft-Agentic-Framework-to-Create-Visualizable-Worlds-via-Text/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative Agents</category><category>AI Town</category><category>LLM</category><category>Environment Creation</category><category>Multi-agent System</category><category>Spatial Reasoning</category><category>Text-to-World</category><category>Reverse Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</title>
      <description>이 [arXiv]에 게시한 &#39;Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-Visual-Generation-Unlocks-Human-Like-Reasoning-through-Multimodal-World-Models/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal AI</category><category>World Models</category><category>Visual Generation</category><category>Chain-of-Thought (CoT)</category><category>Multimodal Reasoning</category><category>Unified Multimodal Models</category><category>Spatial-Physical Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment</title>
      <description>이 [arXiv]에 게시한 &#39;TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-TriPlay-RL-Tri-Role-Self-Play-Reinforcement-Learning-for-LLM-Safety-Alignment/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Safety Alignment</category><category>Reinforcement Learning</category><category>Self-Play</category><category>Red Teaming</category><category>Adversarial Training</category><category>Multi-Role Framework</category><category>Reward Hacking Mitigation</category>
    </item>
    <item>
      <title>[논문리뷰] Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection</title>
      <description>이 [arXiv]에 게시한 &#39;Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-Selective-Steering-Norm-Preserving-Control-Through-Discriminative-Layer-Selection/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Activation Steering</category><category>Large Language Models (LLMs)</category><category>Norm Preservation</category><category>Discriminative Layer Selection</category><category>Behavior Control</category><category>Inference-time Intervention</category><category>Angular Steering</category>
    </item>
    <item>
      <title>[논문리뷰] Revisiting Parameter Server in LLM Post-Training</title>
      <description>이 [arXiv]에 게시한 &#39;Revisiting Parameter Server in LLM Post-Training&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-Revisiting-Parameter-Server-in-LLM-Post-Training/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Post-Training</category><category>Parameter Server</category><category>Distributed Training</category><category>FSDP</category><category>On-Demand Communication</category><category>Workload Imbalance</category><category>Communication Optimization</category><category>Deep Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Post-LayerNorm Is Back: Stable, ExpressivE, and Deep</title>
      <description>이 [arXiv]에 게시한 &#39;Post-LayerNorm Is Back: Stable, ExpressivE, and Deep&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-Post-LayerNorm-Is-Back-Stable-ExpressivE-and-Deep/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Transformer Architecture</category><category>Layer Normalization</category><category>Depth Scaling</category><category>Training Stability</category><category>Large Language Models</category><category>Gradient Flow</category><category>Highway Networks</category><category>Post-LayerNorm</category>
    </item>
    <item>
      <title>[논문리뷰] HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences</title>
      <description>Taro Watanabe이 [arXiv]에 게시한 &#39;HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-HalluCitation-Matters-Revealing-the-Impact-of-Hallucinated-References-with-300-Hallucinated-Papers-in-ACL-Conferences/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Hallucinated Citations</category><category>NLP Conferences</category><category>Citation Detection</category><category>Academic Integrity</category><category>Peer Review</category><category>Large Language Models (LLMs)</category><category>Bibliometrics</category>
    </item>
    <item>
      <title>[논문리뷰] GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery</title>
      <description>이 [arXiv]에 게시한 &#39;GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-GPCR-Filter-a-deep-learning-framework-for-efficient-and-precise-GPCR-modulator-discovery/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>GPCR</category><category>Drug Discovery</category><category>Deep Learning</category><category>Protein Language Model</category><category>Graph Neural Network</category><category>Attention Mechanism</category><category>Drug Target Interaction</category><category>Virtual Screening</category>
    </item>
    <item>
      <title>[논문리뷰] FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-FABLE-Forest-Based-Adaptive-Bi-Path-LLM-Enhanced-Retrieval-for-Multi-Document-Reasoning/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>RAG</category><category>LLM-Enhanced Retrieval</category><category>Multi-Document Reasoning</category><category>Hierarchical Indexing</category><category>Bi-Path Retrieval</category><category>Adaptive Retrieval</category><category>Knowledge Organization</category><category>Context Window Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security</title>
      <description>이 [arXiv]에 게시한 &#39;AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-AgentDoG-A-Diagnostic-Guardrail-Framework-for-AI-Agent-Safety-and-Security/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Agents</category><category>Safety Guardrails</category><category>Explainable AI (XAI)</category><category>Risk Taxonomy</category><category>Benchmarking</category><category>LLM Safety</category><category>Tool Use</category><category>Agent Alignment</category>
    </item>
    <item>
      <title>[논문리뷰] AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning</title>
      <description>이 [arXiv]에 게시한 &#39;AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-AdaReasoner-Dynamic-Tool-Orchestration-for-Iterative-Visual-Reasoning/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>Tool Orchestration</category><category>Visual Reasoning</category><category>Reinforcement Learning</category><category>Adaptive Learning</category><category>Generalization</category><category>Tool Use</category>
    </item>
    <item>
      <title>[논문리뷰] AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs&#39; Contextual and Cultural Knowledge and Thinking</title>
      <description>이 [arXiv]에 게시한 &#39;AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs&#39; Contextual and Cultural Knowledge and Thinking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-AVMeme-Exam-A-Multimodal-Multilingual-Multicultural-Benchmark-for-LLMs-Contextual-and-Cultural-Knowledge-and-Thinking/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLMs</category><category>Benchmark</category><category>Cultural Understanding</category><category>Contextual Inference</category><category>Audio-Visual Memes</category><category>Multilingual</category><category>Q&amp;A Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] A Pragmatic VLA Foundation Model</title>
      <description>이 [arXiv]에 게시한 &#39;A Pragmatic VLA Foundation Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-28-A-Pragmatic-VLA-Foundation-Model/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-28-A-Pragmatic-VLA-Foundation-Model/</guid>
      <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language-Action Model</category><category>Robotics</category><category>Foundation Models</category><category>Multi-Embodiment Learning</category><category>Data Scaling</category><category>Computational Efficiency</category><category>Real-world Deployment</category>
    </item>
    <item>
      <title>[논문리뷰] iFSQ: Improving FSQ for Image Generation with 1 Line of Code</title>
      <description>이 [arXiv]에 게시한 &#39;iFSQ: Improving FSQ for Image Generation with 1 Line of Code&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-27-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code/</guid>
      <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Finite Scalar Quantization (FSQ)</category><category>Image Generation</category><category>Autoregressive Models</category><category>Diffusion Models</category><category>Quantization</category><category>Tokenization</category><category>Representation Alignment (REPA)</category><category>Latent Space</category>
    </item>
    <item>
      <title>[논문리뷰] daVinci-Dev: Agent-native Mid-training for Software Engineering</title>
      <description>이 [arXiv]에 게시한 &#39;daVinci-Dev: Agent-native Mid-training for Software Engineering&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-27-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering/</guid>
      <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Agentic Software Engineering</category><category>Mid-training</category><category>Large Language Models</category><category>Agent-native Data</category><category>Contextual Trajectories</category><category>Environmental Trajectories</category><category>SWE-Bench Verified</category><category>Code Generation</category>
    </item>
    <item>
      <title>[논문리뷰] VIBEVOICE-ASR Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;VIBEVOICE-ASR Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-27-VIBEVOICE-ASR-Technical-Report/</guid>
      <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Automatic Speech Recognition</category><category>Speaker Diarization</category><category>Long-form Audio</category><category>Large Language Models</category><category>End-to-end Speech Processing</category><category>Multilingual</category><category>Context-aware ASR</category>
    </item>
    <item>
      <title>[논문리뷰] The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</title>
      <description>이 [arXiv]에 게시한 &#39;The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-27-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation/</guid>
      <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Dialogue-to-Video Generation</category><category>Agentic AI</category><category>Cinematic Scripting</category><category>Long-Horizon Video Synthesis</category><category>Visual Coherence</category><category>Reinforcement Learning</category><category>Multimodal LLM</category>
    </item>
    <item>
      <title>[논문리뷰] Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</title>
      <description>이 [arXiv]에 게시한 &#39;Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-27-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability/</guid>
      <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Meta-RL</category><category>Curriculum Learning</category><category>Self-Play</category><category>LLM Reasoning</category><category>Sparse Rewards</category><category>Question Generation</category><category>Bilevel Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] SkyReels-V3 Technique Report</title>
      <description>이 [arXiv]에 게시한 &#39;SkyReels-V3 Technique Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-27-SkyReels-V3-Technique-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-27-SkyReels-V3-Technique-Report/</guid>
      <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Multimodal AI</category><category>Diffusion Models</category><category>Transformer Architecture</category><category>Reference-guided Generation</category><category>Video-to-Video</category><category>Audio-driven Animation</category><category>Temporal Consistency</category>
    </item>
    <item>
      <title>[논문리뷰] Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility</title>
      <description>이 [arXiv]에 게시한 &#39;Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.cloud/ai/review/2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility/</link>
      <guid isPermaLink="true">https://blog.secrett2633.cloud/ai/review/2026-01-27-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility/</guid>
      <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Scientific Image Synthesis</category><category>Multimodal Reasoning</category><category>Text-to-Image</category><category>Benchmarking</category><category>Programmatic Synthesis</category><category>Large Multimodal Models</category><category>Synthetic Data</category>
    </item>
    
  </channel>
</rss>