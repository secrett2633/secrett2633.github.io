<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Mon, 22 Dec 2025 18:36:18 GMT</lastBuildDate>
    <pubDate>Mon, 22 Dec 2025 18:36:18 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</title>
      <description>이 [arXiv]에 게시한 &#39;VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-VenusBench-GD-A-Comprehensive-Multi-Platform-GUI-Benchmark-for-Diverse-Grounding-Tasks/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>GUI Grounding</category><category>Multi-Platform</category><category>Benchmark</category><category>MLLM</category><category>Hierarchical Evaluation</category><category>Human-in-the-Loop Annotation</category><category>GUI Agents</category><category>Multilingual Dataset</category>
    </item>
    <item>
      <title>[논문리뷰] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</title>
      <description>이 [arXiv]에 게시한 &#39;The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-The-World-is-Your-Canvas-Painting-Promptable-Events-with-Reference-Images-Trajectories-and-Text/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Video Generation</category><category>Multimodal Control</category><category>Trajectory Guidance</category><category>Reference Images</category><category>Promptable Events</category><category>Cross-Attention</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</title>
      <description>이 [arXiv]에 게시한 &#39;StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-StereoPilot-Learning-Unified-and-Efficient-Stereo-Conversion-via-Generative-Priors/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Monocular-to-Stereo Conversion</category><category>Video Generation</category><category>Diffusion Models</category><category>Feed-Forward Architecture</category><category>Domain Switcher</category><category>Cycle Consistency</category><category>Unified Dataset</category><category>Depth Ambiguity</category>
    </item>
    <item>
      <title>[논문리뷰] Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
      <description>이 [arXiv]에 게시한 &#39;Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Seedance-1-5-pro-A-Native-Audio-Visual-Joint-Generation-Foundation-Model/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Audio-Visual Generation</category><category>Diffusion Transformer</category><category>Multimodal AI</category><category>Speech Synchronization</category><category>Video Generation</category><category>Reinforcement Learning from Human Feedback</category><category>Inference Acceleration</category>
    </item>
    <item>
      <title>[논문리뷰] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</title>
      <description>Yuqi Liu이 [arXiv]에 게시한 &#39;RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-RePlan-Reasoning-guided-Region-Planning-for-Complex-Instruction-based-Image-Editing/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Image Editing</category><category>Vision-Language Models</category><category>Diffusion Models</category><category>Region-aligned Guidance</category><category>Reinforcement Learning</category><category>Instruction-Visual Complexity</category><category>Attention Mechanism</category>
    </item>
    <item>
      <title>[논문리뷰] REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</title>
      <description>Giorgos Sfikas이 [arXiv]에 게시한 &#39;REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-REGLUE-Your-Latents-with-Global-and-Local-Semantics-for-Entangled-Diffusion/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Latent Diffusion Models</category><category>Vision Foundation Models</category><category>Semantic Compression</category><category>Global-Local Semantics</category><category>Image Generation</category><category>Representation Entanglement</category><category>Transformer Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] Next-Embedding Prediction Makes Strong Vision Learners</title>
      <description>이 [arXiv]에 게시한 &#39;Next-Embedding Prediction Makes Strong Vision Learners&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Next-Embedding-Prediction-Makes-Strong-Vision-Learners/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-supervised Learning</category><category>Generative Pretraining</category><category>Vision Transformer</category><category>Next-Embedding Prediction</category><category>Autoregressive Model</category><category>Image Classification</category><category>Semantic Segmentation</category><category>Causal Masking</category>
    </item>
    <item>
      <title>[논문리뷰] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-N3D-VLM-Native-3D-Grounding-Enables-Accurate-Spatial-Reasoning-in-Vision-Language-Models/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Grounding</category><category>Spatial Reasoning</category><category>Vision-Language Models</category><category>Depth Estimation</category><category>3D Object Detection</category><category>Chain-of-Thought</category><category>Data Generation</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title>
      <description>이 [arXiv]에 게시한 &#39;Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Multimodal-RewardBench-2-Evaluating-Omni-Reward-Models-for-Interleaved-Text-and-Image/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reward Models</category><category>Multimodal LLMs</category><category>Benchmark</category><category>Text-to-Image Generation</category><category>Image Editing</category><category>Interleaved Generation</category><category>Multimodal Reasoning</category><category>MLLM-as-a-judge</category>
    </item>
    <item>
      <title>[논문리뷰] Kling-Omni Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Kling-Omni Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Kling-Omni-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Kling-Omni-Technical-Report/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Multimodal Visual Language</category><category>Generative AI</category><category>Video Editing</category><category>Reasoning-enhanced Generation</category><category>Diffusion Transformer</category><category>Multi-modal World Simulators</category>
    </item>
    <item>
      <title>[논문리뷰] Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language</title>
      <description>이 [arXiv]에 게시한 &#39;Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Insight-Miner-A-Time-Series-Analysis-Dataset-for-Cross-Domain-Alignment-with-Natural-Language/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Time Series Analysis</category><category>Multimodal Language Models</category><category>Natural Language Generation</category><category>Dataset Creation</category><category>Instruction Tuning</category><category>GPT-4</category><category>LLaVA</category><category>Cross-Domain Alignment</category>
    </item>
    <item>
      <title>[논문리뷰] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</title>
      <description>Carlos Escolano이 [arXiv]에 게시한 &#39;Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Hearing-to-Translate-The-Effectiveness-of-Speech-Modality-Integration-into-LLMs/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Speech-to-Text Translation</category><category>Multimodal LLMs</category><category>Speech Foundation Models</category><category>Cascaded Systems</category><category>Benchmarking</category><category>Speech Modality Integration</category><category>Robustness</category><category>Evaluation Metrics</category>
    </item>
    <item>
      <title>[논문리뷰] Generative Refocusing: Flexible Defocus Control from a Single Image</title>
      <description>Yu-Lun Liu이 [arXiv]에 게시한 &#39;Generative Refocusing: Flexible Defocus Control from a Single Image&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Generative-Refocusing-Flexible-Defocus-Control-from-a-Single-Image/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Generative AI</category><category>Image Refocusing</category><category>Defocus Deblurring</category><category>Bokeh Synthesis</category><category>Depth of Field Control</category><category>Semi-Supervised Learning</category><category>Diffusion Models</category><category>Aperture Shape Control</category>
    </item>
    <item>
      <title>[논문리뷰] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering</title>
      <description>Hendrik P. A. Lensch이 [arXiv]에 게시한 &#39;FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-FrameDiffuser-G-Buffer-Conditioned-Diffusion-for-Neural-Forward-Frame-Rendering/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Neural Rendering</category><category>Diffusion Models</category><category>G-Buffer</category><category>Autoregressive Generation</category><category>Temporal Consistency</category><category>ControlNet</category><category>ControlLoRA</category><category>Interactive Applications</category>
    </item>
    <item>
      <title>[논문리뷰] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</title>
      <description>이 [arXiv]에 게시한 &#39;FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-FlashPortrait-6x-Faster-Infinite-Portrait-Animation-with-Adaptive-Latent-Prediction/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Portrait Animation</category><category>Diffusion Models</category><category>Inference Acceleration</category><category>Identity Preservation</category><category>Video Generation</category><category>Latent Prediction</category><category>Sliding Window</category>
    </item>
    <item>
      <title>[논문리뷰] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title>
      <description>이 [arXiv]에 게시한 &#39;Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Exploration-v-s-Exploitation-Rethinking-RLVR-through-Clipping-Entropy-and-Spurious-Reward/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Large Language Models</category><category>Exploration-Exploitation</category><category>Clipping</category><category>Policy Entropy</category><category>Spurious Rewards</category><category>Mathematical Reasoning</category><category>RLVR</category>
    </item>
    <item>
      <title>[논문리뷰] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title>
      <description>이 [arXiv]에 게시한 &#39;Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Differences-That-Matter-Auditing-Models-for-Capability-Gap-Discovery-and-Rectification/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>MLLM</category><category>Model Auditing</category><category>Capability Gaps</category><category>Failure Mode Discovery</category><category>Reinforcement Learning</category><category>Data Rectification</category><category>Counterfactual Generation</category><category>VQA</category>
    </item>
    <item>
      <title>[논문리뷰] Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</title>
      <description>Wenxuan Lu이 [arXiv]에 게시한 &#39;Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Depth-Any-Panoramas-A-Foundation-Model-for-Panoramic-Depth-Estimation/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Panoramic Depth Estimation</category><category>Foundation Model</category><category>Semi-Supervised Learning</category><category>Pseudo-Labeling</category><category>Data-in-the-Loop</category><category>DINOv3</category><category>Metric Depth</category><category>360-degree Vision</category>
    </item>
    <item>
      <title>[논문리뷰] DeContext as Defense: Safe Image Editing in Diffusion Transformers</title>
      <description>이 [arXiv]에 게시한 &#39;DeContext as Defense: Safe Image Editing in Diffusion Transformers&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-DeContext-as-Defense-Safe-Image-Editing-in-Diffusion-Transformers/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Diffusion Transformers</category><category>Image Editing</category><category>Privacy Protection</category><category>Adversarial Attack</category><category>Attention Mechanism</category><category>Identity Preservation</category><category>Deepfake Defense</category><category>In-context Learning</category>
    </item>
    <item>
      <title>[논문리뷰] Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</title>
      <description>Jiarong Ou이 [arXiv]에 게시한 &#39;Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-12-19-Alchemist-Unlocking-Efficiency-in-Text-to-Image-Model-Training-via-Meta-Gradient-Data-Selection/</guid>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image</category><category>Data Selection</category><category>Meta-Learning</category><category>Meta-Gradient</category><category>Data Efficiency</category><category>Generative Models</category><category>Coreset Selection</category><category>Data Pruning</category>
    </item>
    
  </channel>
</rss>