<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://blog.secrett2633.site</link>
    <atom:link href="https://blog.secrett2633.site/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Wed, 26 Nov 2025 09:26:46 GMT</lastBuildDate>
    <pubDate>Wed, 26 Nov 2025 09:26:46 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios</title>
      <description>이 [arXiv]에 게시한 &#39;UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-UltraFlux-Data-Model-Co-Design-for-High-quality-Native-4K-Text-to-Image-Generation-across-Diverse-Aspect-Ratios/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Text-to-Image Generation</category><category>Diffusion Transformers</category><category>4K Resolution</category><category>Aspect Ratio Extrapolation</category><category>Data-Model Co-Design</category><category>VAE Post-training</category><category>Positional Encoding</category><category>Diffusion Models</category>
    </item>
    <item>
      <title>[논문리뷰] Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</title>
      <description>Zhaowei Lu이 [arXiv]에 게시한 &#39;Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Target-Bench-Can-World-Models-Achieve-Mapless-Path-Planning-with-Semantic-Targets/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>World Models</category><category>Mapless Navigation</category><category>Semantic Path Planning</category><category>Robot Learning</category><category>Video Prediction</category><category>Benchmark</category><category>Trajectory Generation</category>
    </item>
    <item>
      <title>[논문리뷰] SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis</title>
      <description>Hongwen Zhang이 [arXiv]에 게시한 &#39;SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-SyncMV4D-Synchronized-Multi-view-Joint-Diffusion-of-Appearance-and-Motion-for-Hand-Object-Interaction-Synthesis/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Hand-Object Interaction</category><category>Multi-view Video Generation</category><category>4D Motion Synthesis</category><category>Diffusion Models</category><category>Spatio-temporal Consistency</category><category>Geometric Consistency</category><category>Appearance and Motion Joint Modeling</category>
    </item>
    <item>
      <title>[논문리뷰] Plan-X: Instruct Video Generation via Semantic Planning</title>
      <description>Chenxu Zhang이 [arXiv]에 게시한 &#39;Plan-X: Instruct Video Generation via Semantic Planning&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Plan-X-Instruct-Video-Generation-via-Semantic-Planning/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Semantic Planning</category><category>Multimodal LLM</category><category>Diffusion Transformer</category><category>Spatio-temporal Guidance</category><category>Visual Hallucination</category><category>Prompt Alignment</category><category>Instruction Following</category>
    </item>
    <item>
      <title>[논문리뷰] Pillar-0: A New Frontier for Radiology Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;Pillar-0: A New Frontier for Radiology Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Pillar-0-A-New-Frontier-for-Radiology-Foundation-Models/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Radiology Foundation Model</category><category>Volumetric Imaging</category><category>Multi-window Tokenization</category><category>Multi-scale Attention</category><category>Contrastive Learning</category><category>Clinical Evaluation</category><category>Data Efficiency</category><category>Medical Imaging</category>
    </item>
    <item>
      <title>[논문리뷰] PRInTS: Reward Modeling for Long-Horizon Information Seeking</title>
      <description>Elias Stengel-Eskin이 [arXiv]에 게시한 &#39;PRInTS: Reward Modeling for Long-Horizon Information Seeking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-PRInTS-Reward-Modeling-for-Long-Horizon-Information-Seeking/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reward Modeling</category><category>Long-Horizon Tasks</category><category>Information Seeking</category><category>Large Language Models</category><category>Trajectory Summarization</category><category>Reinforcement Learning</category><category>Tool Use</category><category>Process Reward Models</category>
    </item>
    <item>
      <title>[논문리뷰] Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO</title>
      <description>이 [arXiv]에 게시한 &#39;Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multi-Agent Systems</category><category>Reinforcement Learning</category><category>LLM Training</category><category>Hierarchical Credit Assignment</category><category>Trajectory Alignment</category><category>Group Relative Policy Optimization</category><category>Tool-Augmented Reasoning</category><category>Vertical Architecture</category>
    </item>
    <item>
      <title>[논문리뷰] MIST: Mutual Information Via Supervised Training</title>
      <description>Kyunghyun Cho이 [arXiv]에 게시한 &#39;MIST: Mutual Information Via Supervised Training&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-MIST-Mutual-Information-Via-Supervised-Training/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Mutual Information Estimation</category><category>Supervised Learning</category><category>Meta-Learning</category><category>Neural Networks</category><category>Uncertainty Quantification</category><category>SetTransformer</category><category>Quantile Regression</category>
    </item>
    <item>
      <title>[논문리뷰] MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models</title>
      <description>이 [arXiv]에 게시한 &#39;MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-MASS-Motion-Aware-Spatial-Temporal-Grounding-for-Physics-Reasoning-and-Comprehension-in-Vision-Language-Models/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Models</category><category>Physics Reasoning</category><category>Motion Tracking</category><category>Spatial-Temporal Grounding</category><category>Video QA</category><category>AIGC Analysis</category><category>Reinforcement Learning</category>
    </item>
    <item>
      <title>[논문리뷰] M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark</title>
      <description>Bangwei Guo이 [arXiv]에 게시한 &#39;M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-M3-Bench-Multi-Modal-Multi-Hop-Multi-Threaded-Tool-Using-MLLM-Agent-Benchmark/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal LLM</category><category>Tool Use</category><category>Agent Benchmark</category><category>Model Context Protocol</category><category>Multi-Hop Reasoning</category><category>Multi-Threaded Execution</category><category>Evaluation Metrics</category><category>Similarity Alignment</category>
    </item>
    <item>
      <title>[논문리뷰] In-Video Instructions: Visual Signals as Generative Control</title>
      <description>이 [arXiv]에 게시한 &#39;In-Video Instructions: Visual Signals as Generative Control&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-In-Video-Instructions-Visual-Signals-as-Generative-Control/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Controllable AI</category><category>Visual Instructions</category><category>Image-to-Video</category><category>Spatial Control</category><category>Zero-shot Learning</category><category>Generative Models</category>
    </item>
    <item>
      <title>[논문리뷰] HunyuanVideo 1.5 Technical Report</title>
      <description>Fang Yang이 [arXiv]에 게시한 &#39;HunyuanVideo 1.5 Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-HunyuanVideo-1-5-Technical-Report/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Diffusion Transformer</category><category>Sparse Attention</category><category>Super-Resolution</category><category>Open-Source</category><category>Multimodal Understanding</category><category>Training Optimization</category><category>Efficient Inference</category>
    </item>
    <item>
      <title>[논문리뷰] General Agentic Memory Via Deep Research</title>
      <description>이 [arXiv]에 게시한 &#39;General Agentic Memory Via Deep Research&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-General-Agentic-Memory-Via-Deep-Research/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Agents</category><category>Memory Systems</category><category>Large Language Models (LLMs)</category><category>Just-in-Time (JIT) Compilation</category><category>Memorizer</category><category>Researcher</category><category>Reinforcement Learning</category><category>Context Management</category>
    </item>
    <item>
      <title>[논문리뷰] Flow Map Distillation Without Data</title>
      <description>Tommi Jaakkola이 [arXiv]에 게시한 &#39;Flow Map Distillation Without Data&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Flow-Map-Distillation-Without-Data/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Flow Map Distillation</category><category>Data-Free Learning</category><category>Generative Models</category><category>Teacher-Student</category><category>Diffusion Acceleration</category><category>Teacher-Data Mismatch</category><category>One-Step Sampling</category>
    </item>
    <item>
      <title>[논문리뷰] Fidelity-Aware Recommendation Explanations via Stochastic Path Integration</title>
      <description>Oren Barkan이 [arXiv]에 게시한 &#39;Fidelity-Aware Recommendation Explanations via Stochastic Path Integration&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Fidelity-Aware-Recommendation-Explanations-via-Stochastic-Path-Integration/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Recommender Systems</category><category>Explainable AI (XAI)</category><category>Explanation Fidelity</category><category>Path Integration</category><category>Stochastic Sampling</category><category>Counterfactual Explanations</category><category>Model-Agnostic</category><category>Sparse Data</category>
    </item>
    <item>
      <title>[논문리뷰] Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems</title>
      <description>Oren Barkan이 [arXiv]에 게시한 &#39;Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Extracting-Interaction-Aware-Monosemantic-Concepts-in-Recommender-Systems/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Recommender Systems</category><category>Sparse Autoencoder (SAE)</category><category>Monosemantic Neurons</category><category>Interpretability</category><category>Prediction-Aware Loss</category><category>User-Item Interactions</category><category>Post-hoc Control</category>
    </item>
    <item>
      <title>[논문리뷰] DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-DeCo-Frequency-Decoupled-Pixel-Diffusion-for-End-to-End-Image-Generation/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Pixel Diffusion</category><category>Image Generation</category><category>Frequency Decoupling</category><category>Diffusion Transformer (DiT)</category><category>Flow Matching</category><category>AdaLN</category><category>Text-to-Image Synthesis</category>
    </item>
    <item>
      <title>[논문리뷰] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research</title>
      <description>이 [arXiv]에 게시한 &#39;DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-DR-Tulu-Reinforcement-Learning-with-Evolving-Rubrics-for-Deep-Research/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Evolving Rubrics</category><category>Deep Research</category><category>LLM Agents</category><category>Tool Use</category><category>Long-form QA</category><category>Open-source AI</category><category>Dynamic Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] Controllable Layer Decomposition for Reversible Multi-Layer Image Generation</title>
      <description>이 [arXiv]에 게시한 &#39;Controllable Layer Decomposition for Reversible Multi-Layer Image Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Controllable-Layer-Decomposition-for-Reversible-Multi-Layer-Image-Generation/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Controllable Layer Decomposition</category><category>Diffusion Models</category><category>Multi-Layer Image Generation</category><category>Layer Separation</category><category>Bounding Box Guidance</category><category>Generative AI</category><category>Image Editing</category>
    </item>
    <item>
      <title>[논문리뷰] Computer-Use Agents as Judges for Generative User Interface</title>
      <description>이 [arXiv]에 게시한 &#39;Computer-Use Agents as Judges for Generative User Interface&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://blog.secrett2633.site/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface/</link>
      <guid isPermaLink="true">https://blog.secrett2633.site/ai/review/2025-11-25-Computer-Use-Agents-as-Judges-for-Generative-User-Interface/</guid>
      <pubDate>Mon, 24 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Computer-Use Agents</category><category>Generative UI</category><category>AI-assisted Design</category><category>Human-Computer Interaction</category><category>LLM</category><category>AUI-Gym</category><category>Feedback Loop</category><category>Agent-centric Design</category>
    </item>
    
  </channel>
</rss>