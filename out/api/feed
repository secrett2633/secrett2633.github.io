<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secrett2633's blog</title>
    <description>기술 블로그 - Django, Python, DevOps, AI/ML 관련 포스트</description>
    <link>https://secrett2633.github.io</link>
    <atom:link href="https://secrett2633.github.io/api/feed" rel="self" type="application/rss+xml" />
    <language>ko-KR</language>
    <lastBuildDate>Sat, 22 Nov 2025 19:29:16 GMT</lastBuildDate>
    <pubDate>Sat, 22 Nov 2025 19:29:16 GMT</pubDate>
    <ttl>60</ttl>
    <generator>Next.js RSS Generator</generator>
    <managingEditor>secrett2633@example.com (secrett2633)</managingEditor>
    <webMaster>secrett2633@example.com (secrett2633)</webMaster>
    <category>Technology</category>
    <category>Programming</category>
    <category>Django</category>
    <category>Python</category>
    <category>DevOps</category>
    <category>AI/ML</category>
    
        <item>
      <title>[논문리뷰] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</title>
      <description>이 [arXiv]에 게시한 &#39;Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-Video-as-Answer_Predict_and_Generate_Next_Video_Event_with_Joint-GRPO/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-Video-as-Answer_Predict_and_Generate_Next_Video_Event_with_Joint-GRPO/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Next Event Prediction</category><category>Reinforcement Learning</category><category>Vision-Language Model</category><category>Video Diffusion Model</category><category>Joint Optimization</category><category>Multimodal AI</category><category>Procedural Learning</category>
    </item>
    <item>
      <title>[논문리뷰] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</title>
      <description>Baijiong Lin이 [arXiv]에 게시한 &#39;V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-V-ReasonBench_Toward_Unified_Reasoning_Benchmark_Suite_for_Video_Generation_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-V-ReasonBench_Toward_Unified_Reasoning_Benchmark_Suite_for_Video_Generation_Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Reasoning Benchmark</category><category>Chain-of-Frame</category><category>Evaluation</category><category>Multimodal AI</category><category>Physical Dynamics</category><category>Spatial Cognition</category><category>Pattern Inference</category>
    </item>
    <item>
      <title>[논문리뷰] TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval</title>
      <description>이 [arXiv]에 게시한 &#39;TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-TurkColBERT_A_Benchmark_of_Dense_and_Late-Interaction_Models_for_Turkish_Information_Retrieval/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-TurkColBERT_A_Benchmark_of_Dense_and_Late-Interaction_Models_for_Turkish_Information_Retrieval/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Information Retrieval</category><category>Turkish Language</category><category>Late-Interaction Models</category><category>ColBERT</category><category>Dense Retrieval</category><category>MUVERA</category><category>Benchmarking</category><category>Low-Resource NLP</category><category>Fine-tuning</category>
    </item>
    <item>
      <title>[논문리뷰] TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
      <description>이 [arXiv]에 게시한 &#39;TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-TimeViper_A_Hybrid_Mamba-Transformer_Vision-Language_Model_for_Efficient_Long_Video_Understanding/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-TimeViper_A_Hybrid_Mamba-Transformer_Vision-Language_Model_for_Efficient_Long_Video_Understanding/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Long Video Understanding</category><category>Hybrid Mamba-Transformer</category><category>Vision-Language Model</category><category>Token Compression</category><category>Vision-to-Text Aggregation</category><category>Efficient LLM</category><category>Multimodal AI</category>
    </item>
    <item>
      <title>[논문리뷰] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</title>
      <description>Xinyan Chen이 [arXiv]에 게시한 &#39;Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-Thinking-while-Generating_Interleaving_Textual_Reasoning_throughout_Visual_Generation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-Thinking-while-Generating_Interleaving_Textual_Reasoning_throughout_Visual_Generation/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Visual Generation</category><category>Textual Reasoning</category><category>Interleaving</category><category>Large Multimodal Models (LMMs)</category><category>Chain-of-Thought (CoT)</category><category>Zero-shot Learning</category><category>Supervised Fine-tuning (SFT)</category><category>Reinforcement Learning (RL)</category>
    </item>
    <item>
      <title>[논문리뷰] Step-Audio-R1 Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;Step-Audio-R1 Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-Step-Audio-R1_Technical_Report/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-Step-Audio-R1_Technical_Report/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Audio Reasoning</category><category>Multimodal LLMs</category><category>Modality-Grounded Reasoning Distillation (MGRD)</category><category>Chain-of-Thought</category><category>Reinforcement Learning</category><category>Audio Understanding</category><category>Self-Distillation</category>
    </item>
    <item>
      <title>[논문리뷰] SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
      <description>이 [arXiv]에 게시한 &#39;SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-SRPO_Self-Referential_Policy_Optimization_for_Vision-Language-Action_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-SRPO_Self-Referential_Policy_Optimization_for_Vision-Language-Action_Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Reinforcement Learning</category><category>Vision-Language-Action Models</category><category>Reward Shaping</category><category>World Models</category><category>Self-Referential Learning</category><category>Robotics</category><category>Trajectory Optimization</category>
    </item>
    <item>
      <title>[논문리뷰] Scaling Spatial Intelligence with Multimodal Foundation Models</title>
      <description>이 [arXiv]에 게시한 &#39;Scaling Spatial Intelligence with Multimodal Foundation Models&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-Scaling_Spatial_Intelligence_with_Multimodal_Foundation_Models/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-Scaling_Spatial_Intelligence_with_Multimodal_Foundation_Models/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Spatial Intelligence</category><category>Multimodal Foundation Models</category><category>Data Scaling</category><category>Perspective-taking</category><category>Visual Question Answering</category><category>Emergent Capabilities</category><category>Embodied AI</category><category>Benchmark Evaluation</category>
    </item>
    <item>
      <title>[논문리뷰] SAM 3D: 3Dfy Anything in Images</title>
      <description>이 [arXiv]에 게시한 &#39;SAM 3D: 3Dfy Anything in Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-SAM_3D_3Dfy_Anything_in_Images/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-SAM_3D_3Dfy_Anything_in_Images/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Reconstruction</category><category>Generative Models</category><category>Single Image 3D</category><category>Object Reconstruction</category><category>Scene Understanding</category><category>Data Engine</category><category>Model-in-the-Loop</category><category>Human Preference</category>
    </item>
    <item>
      <title>[논문리뷰] SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</title>
      <description>이 [arXiv]에 게시한 &#39;SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-SAM2S_Segment_Anything_in_Surgical_Videos_via_Semantic_Long-term_Tracking/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-SAM2S_Segment_Anything_in_Surgical_Videos_via_Semantic_Long-term_Tracking/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Surgical Video Segmentation</category><category>Interactive Video Object Segmentation</category><category>Long-term Tracking</category><category>Foundation Models</category><category>Domain Adaptation</category><category>Semantic Learning</category><category>Prompt-based Segmentation</category>
    </item>
    <item>
      <title>[논문리뷰] PartUV: Part-Based UV Unwrapping of 3D Meshes</title>
      <description>Hao Su이 [arXiv]에 게시한 &#39;PartUV: Part-Based UV Unwrapping of 3D Meshes&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-PartUV_Part-Based_UV_Unwrapping_of_3D_Meshes/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-PartUV_Part-Based_UV_Unwrapping_of_3D_Meshes/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>UV Unwrapping</category><category>3D Meshes</category><category>Part-Based Decomposition</category><category>Neural Fields</category><category>Geometric Heuristics</category><category>Parameterization</category><category>Texture Mapping</category>
    </item>
    <item>
      <title>[논문리뷰] Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs</title>
      <description>이 [arXiv]에 게시한 &#39;Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-Nemotron_Elastic_Towards_Efficient_Many-in-One_Reasoning_LLMs/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-Nemotron_Elastic_Towards_Efficient_Many-in-One_Reasoning_LLMs/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>LLM Compression</category><category>Elastic Networks</category><category>Knowledge Distillation</category><category>Hybrid Mamba-Attention</category><category>Reasoning LLMs</category><category>Multi-Budget Training</category><category>Zero-Shot Deployment</category>
    </item>
    <item>
      <title>[논문리뷰] NaTex: Seamless Texture Generation as Latent Color Diffusion</title>
      <description>이 [arXiv]에 게시한 &#39;NaTex: Seamless Texture Generation as Latent Color Diffusion&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-NaTex_Seamless_Texture_Generation_as_Latent_Color_Diffusion/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-NaTex_Seamless_Texture_Generation_as_Latent_Color_Diffusion/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>3D Texture Generation</category><category>Latent Diffusion Model</category><category>Geometry-Aware VAE</category><category>Multi-Control DiT</category><category>Color Point Cloud</category><category>Texture Synthesis</category><category>3D Asset Creation</category>
    </item>
    <item>
      <title>[논문리뷰] MiMo-Embodied: X-Embodied Foundation Model Technical Report</title>
      <description>이 [arXiv]에 게시한 &#39;MiMo-Embodied: X-Embodied Foundation Model Technical Report&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-MiMo-Embodied_X-Embodied_Foundation_Model_Technical_Report/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-MiMo-Embodied_X-Embodied_Foundation_Model_Technical_Report/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Vision-Language Model (VLM)</category><category>Embodied AI</category><category>Autonomous Driving</category><category>Foundation Model</category><category>Multimodal Learning</category><category>Task Planning</category><category>Affordance Prediction</category><category>Spatial Understanding</category><category>Reinforcement Learning</category>
    </item>
    <item>
      <title>[논문리뷰] First Frame Is the Place to Go for Video Content Customization</title>
      <description>이 [arXiv]에 게시한 &#39;First Frame Is the Place to Go for Video Content Customization&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-First_Frame_Is_the_Place_to_Go_for_Video_Content_Customization/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-First_Frame_Is_the_Place_to_Go_for_Video_Content_Customization/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Generation</category><category>Content Customization</category><category>Few-shot Learning</category><category>LoRA</category><category>Vision-Language Models (VLMs)</category><category>First Frame Conditioning</category><category>Reference-based Generation</category>
    </item>
    <item>
      <title>[논문리뷰] Draft and Refine with Visual Experts</title>
      <description>이 [arXiv]에 게시한 &#39;Draft and Refine with Visual Experts&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-21-Draft_and_Refine_with_Visual_Experts/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-21-Draft_and_Refine_with_Visual_Experts/</guid>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Large Vision-Language Models (LVLMs)</category><category>Visual Grounding</category><category>Hallucination Mitigation</category><category>Agent Framework</category><category>Visual Question Answering (VQA)</category><category>Expert Coordination</category><category>Relevance Map</category><category>Multi-modal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity</title>
      <description>이 [arXiv]에 게시한 &#39;What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-What_Does_It_Take_to_Be_a_Good_AI_Research_Agent_Studying_the_Role_of_Ideation_Diversity/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>AI Research Agents</category><category>Ideation Diversity</category><category>MLE-bench</category><category>LLM Backbones</category><category>Agentic Scaffolds</category><category>Shannon Entropy</category><category>Machine Learning Engineering</category><category>Performance Metrics</category>
    </item>
    <item>
      <title>[논문리뷰] VisPlay: Self-Evolving Vision-Language Models from Images</title>
      <description>이 [arXiv]에 게시한 &#39;VisPlay: Self-Evolving Vision-Language Models from Images&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-VisPlay_Self-Evolving_Vision-Language_Models_from_Images/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Self-Evolving</category><category>Vision-Language Models</category><category>Reinforcement Learning</category><category>Self-Play</category><category>Unlabeled Data</category><category>Multimodal Reasoning</category><category>Group Relative Policy Optimization</category><category>Hallucination Mitigation</category>
    </item>
    <item>
      <title>[논문리뷰] Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks</title>
      <description>Yiran Peng이 [arXiv]에 게시한 &#39;Reasoning via Video: The First Evaluation of Video Models&#39; Reasoning Abilities through Maze-Solving Tasks&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Reasoning_via_Video_The_First_Evaluation_of_Video_Models_Reasoning_Abilities_through_Maze-Solving_Tasks/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Video Models</category><category>Spatial Reasoning</category><category>Maze Solving</category><category>Video Generation</category><category>Benchmark</category><category>Supervised Fine-tuning</category><category>Test-Time Scaling</category><category>Multimodal Reasoning</category>
    </item>
    <item>
      <title>[논문리뷰] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</title>
      <description>이 [arXiv]에 게시한 &#39;Mixture of States: Routing Token-Level Dynamics for Multimodal Generation&#39; 논문에 대한 자세한 리뷰입니다.</description>
      <link>https://secrett2633.github.io/ai/review/2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation/</link>
      <guid isPermaLink="true">https://secrett2633.github.io/ai/review/2025-11-20-Mixture_of_States_Routing_Token-Level_Dynamics_for_Multimodal_Generation/</guid>
      <pubDate>Wed, 19 Nov 2025 15:00:00 GMT</pubDate>
      <category>Review</category>
      <category>Review</category><category>Multimodal Diffusion</category><category>Mixture of States (MoS)</category><category>Token-Level Routing</category><category>Dynamic Conditional Fusion</category><category>Text-to-Image Generation</category><category>Image Editing</category><category>Transformer Architecture</category>
    </item>
    
  </channel>
</rss>